{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "INPUT_PATH = \"/var/www/nextcloud/data/dbc2017/files/nodule_npy/\"\n",
    "OUTPUT_PATH = \"/var/www/nextcloud/data/dbc2017/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "randnum = random.randint(0,100)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "nz = 100\n",
    "n_epochs = 50\n",
    "ngf = 64\n",
    "ngpu = 1\n",
    "LEVELS_NUM = 2\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def chunks(arr, m):\n",
    "    nchunk = int(math.ceil(len(arr) / float(m)))\n",
    "    return [arr[i:i + nchunk] for i in range(0, len(arr), nchunk)]\n",
    "\n",
    "def ten_folder(arr, number):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    for j in range(len(arr)):\n",
    "        if number == j:\n",
    "            test_set.extend(arr[j])\n",
    "        else:\n",
    "            training_set.extend(arr[j])\n",
    "    return training_set, test_set\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images,labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        img = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.label_emb = nn.Embedding(LEVELS_NUM, 100)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "          nn.Linear(nz, 1 * 5 * 5 * ngf * 4, bias=False)\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "          # input is Z, going into a deconvolution\n",
    "          # state size. BATCH_SIZE x (ngf*4) x 1 x 5 x 5\n",
    "          nn.ConvTranspose3d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.BatchNorm3d(ngf * 2),\n",
    "          nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf*2) x 2 x 10 x 10\n",
    "          nn.ConvTranspose3d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.BatchNorm3d(ngf),\n",
    "          nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf) x 4 x 20 x 20\n",
    "          nn.ConvTranspose3d(ngf, 1, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.Tanh()\n",
    "          # state size. BATCH_SIZE x 1 x 8 x 40 x 40\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        \n",
    "        x = self.project(gen_input)\n",
    "        # Conv3d的规定输入数据格式为(batch, channel, Depth, Height, Width)\n",
    "        x = x.view(-1, ngf * 4, 1, 5, 5)\n",
    "        \n",
    "        if noise.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.deconv, x, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.deconv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "def softmax(input, dim=1):\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=256, kernel_size=(2,9,9)):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels,\n",
    "                                out_channels=out_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                stride=1\n",
    "                              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=(2,9,9), num_routes=32 * 12 * 12 * 6):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "          nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=(1,2,2), padding=0) \n",
    "                      for _ in range(num_capsules)])\n",
    "  \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "        u = torch.cat(u, dim=-1)\n",
    "        return self.squash(u)\n",
    "  \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=1, num_routes=32 * 12 * 12 * 6, in_channels=8, out_channels=16, num_iterations=3):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "        self.num_iterations = num_iterations\n",
    "        self.route_weights = nn.Parameter(torch.randn(num_capsules, num_routes, in_channels, out_channels)).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 矩阵相乘\n",
    "        # x.size(): [1, batch_size, in_capsules, 1, dim_in_capsule]\n",
    "        # weight.size(): [num_capsules, 1, num_route, in_channels, out_channels]\n",
    "        priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :]\n",
    "        logits = Variable(torch.zeros(*priors.size())).to(device)\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            probs = softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "\n",
    "            if i != self.num_routes - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        \n",
    "        outputs = outputs.squeeze()\n",
    "#         print(\"OUTPUT:\", outputs.shape)\n",
    "        return outputs\n",
    "  \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.D_digit_capsules = DigitCaps()\n",
    "        self.C_digit_capsules = DigitCaps(num_capsules=LEVELS_NUM)\n",
    "\n",
    "    def forward(self, data):\n",
    "        if data.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.conv_layer, data, range(self.ngpu))\n",
    "            output = nn.parallel.data_parallel(self.primary_capsules, output, range(self.ngpu))\n",
    "            D_output = nn.parallel.data_parallel(self.D_digit_capsules, output, range(self.ngpu))\n",
    "            C_output = nn.parallel.data_parallel(self.C_digit_capsules, output, range(self.ngpu))\n",
    "            C_output = C_output.transpose(0,1)\n",
    "        else:\n",
    "            output = self.primary_capsules(self.conv_layer(data))\n",
    "            D_output = self.D_digit_capsules(output)\n",
    "            C_output = self.C_digit_capsules(output).transpose(0,1)\n",
    "            \n",
    "\n",
    "        D_classes = (D_output ** 2).sum(dim=-1) ** 0.5\n",
    "        C_classes = (C_output ** 2).sum(dim=-1) ** 0.5\n",
    "#         print(D_classes, C_classes)\n",
    "        return D_classes, C_classes\n",
    "\n",
    "\n",
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "\n",
    "    def forward(self, classes, labels):\n",
    "        left = F.relu(0.9 - classes, inplace=True) ** 2\n",
    "        right = F.relu(classes - 0.1, inplace=True) ** 2\n",
    "\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "\n",
    "        return margin_loss / classes.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "discriminator = CapsNet(ngpu).to(device)\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "generator = Generator(ngpu).to(device)\n",
    "generator.apply(weights_init)\n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "def train(train_loader, test_loader):\n",
    "    epoch_num = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"-----THE [{}/{}] epoch start-----\".format(epoch + 1, n_epochs))\n",
    "        for j, (data,labels) in enumerate(train_loader, 0):\n",
    "#                     print(j, data.shape) #torch.Size([32, 1, 8, 40, 40])\n",
    "\n",
    "#                     plt.figure()\n",
    "#                     for lenc in range(data.shape[0]):\n",
    "#                         for len_img in range(8):\n",
    "#                             plt.subplot(2, 4, len_img + 1)\n",
    "#                             pixel_array = data[lenc][0][len_img]\n",
    "#                             print(pixel_array)\n",
    "#                             plt.imshow(pixel_array, cmap=\"gray\")\n",
    "#                         plt.show()\n",
    "\n",
    "\n",
    "#                     pixel_array = data[0][0][0][20]\n",
    "#                     print(pixel_array)\n",
    "\n",
    "#                     data = data / 255.0\n",
    "#                     pixel_array = data[0][0][0][20]\n",
    "#                     print(pixel_array)\n",
    "\n",
    "            \n",
    "            real_imgs = data.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(FloatTensor(BATCH_SIZE, 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(FloatTensor(BATCH_SIZE, 1).fill_(0.0), requires_grad=False)\n",
    "            \n",
    "            \n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (BATCH_SIZE, 100))))\n",
    "            gen_labels = Variable(LongTensor(np.random.randint(0, LEVELS_NUM, BATCH_SIZE)))\n",
    "    \n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            discriminator.train()\n",
    "            discriminator.zero_grad()\n",
    "            \n",
    "            # Loss for real images\n",
    "            real_pred, real_aux = discriminator(real_imgs)\n",
    "            d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "            # Loss for fake images\n",
    "            fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "            d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            # Calculate discriminator accuracy\n",
    "            pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "            gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "            \n",
    "            pred = np.argmax(pred, axis=1)\n",
    "            \n",
    "            d_acc = np.mean(pred == gt)\n",
    "            \n",
    "#             print(pred)\n",
    "#             print(gt)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "            \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            \n",
    "            generator.zero_grad()\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            validity, pred_label = discriminator(gen_imgs)\n",
    "            \n",
    "            g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "\n",
    "            print('[BATCH %d/%d] Loss_D: %.4f Loss_G: %.4f acc: %.1f%%'\n",
    "                  % (j + 1, len(train_loader), d_loss.item(), g_loss.item(), 100 * d_acc))\n",
    "            \n",
    "            d_losses.append(d_loss)\n",
    "            g_losses.append(g_loss)\n",
    "            train_acc.append(d_acc)\n",
    "            \n",
    "            epoch_num += 1\n",
    "            d_acc_total = 0\n",
    "            if epoch_num % 50 == 0:\n",
    "                discriminator.eval()\n",
    "                for j, (data,labels) in enumerate(test_loader, 0):\n",
    "                    real_imgs = data.to(device, dtype=torch.float)\n",
    "                    labels = labels.to(device)\n",
    "                    real_pred, real_aux = discriminator(real_imgs)\n",
    "                    pred = real_aux.data.cpu().numpy()\n",
    "                    pred = np.argmax(pred, axis=1)\n",
    "                    gt = labels.data.cpu().numpy()\n",
    "                    d_acc = np.mean(pred == gt)\n",
    "                    d_acc_total += d_acc\n",
    "                single_test_acc = d_acc_total / len(test_loader)\n",
    "                print(\"[EPOCH %d] TEST ACC is : %.1f%%\" % (epoch_num, 100 * single_test_acc))\n",
    "                test_acc.append(single_test_acc)\n",
    "                \n",
    "\n",
    "        print(\"-----THE [{}/{}] epoch end-----\".format(epoch + 1, n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW the training STARTS:\n",
      "The 1 * 50 epochs train starts:\n",
      "-----THE [1/50] epoch start-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 1/149] Loss_D: 2.8948 Loss_G: 3.1995 acc: 48.4%\n",
      "[BATCH 2/149] Loss_D: 1.5061 Loss_G: 2.7955 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 1.4394 Loss_G: 2.6645 acc: 50.0%\n",
      "[BATCH 4/149] Loss_D: 1.4028 Loss_G: 2.5968 acc: 42.2%\n",
      "[BATCH 5/149] Loss_D: 1.3984 Loss_G: 2.5500 acc: 39.1%\n",
      "[BATCH 6/149] Loss_D: 1.3744 Loss_G: 2.5259 acc: 54.7%\n",
      "[BATCH 7/149] Loss_D: 1.3696 Loss_G: 2.5084 acc: 48.4%\n",
      "[BATCH 8/149] Loss_D: 1.3783 Loss_G: 2.4910 acc: 50.0%\n",
      "[BATCH 9/149] Loss_D: 1.3498 Loss_G: 2.4874 acc: 53.1%\n",
      "[BATCH 10/149] Loss_D: 1.3664 Loss_G: 2.4827 acc: 48.4%\n",
      "[BATCH 11/149] Loss_D: 1.3078 Loss_G: 2.4832 acc: 51.6%\n",
      "[BATCH 12/149] Loss_D: 1.3130 Loss_G: 2.4771 acc: 56.2%\n",
      "[BATCH 13/149] Loss_D: 1.3052 Loss_G: 2.4694 acc: 48.4%\n",
      "[BATCH 14/149] Loss_D: 1.3129 Loss_G: 2.4653 acc: 50.0%\n",
      "[BATCH 15/149] Loss_D: 1.2765 Loss_G: 2.4643 acc: 59.4%\n",
      "[BATCH 16/149] Loss_D: 1.2770 Loss_G: 2.4666 acc: 57.8%\n",
      "[BATCH 17/149] Loss_D: 1.2575 Loss_G: 2.4631 acc: 42.2%\n",
      "[BATCH 18/149] Loss_D: 1.2278 Loss_G: 2.4850 acc: 34.4%\n",
      "[BATCH 19/149] Loss_D: 1.2581 Loss_G: 2.4901 acc: 40.6%\n",
      "[BATCH 20/149] Loss_D: 1.2291 Loss_G: 2.5044 acc: 48.4%\n",
      "[BATCH 21/149] Loss_D: 1.2216 Loss_G: 2.5055 acc: 50.0%\n",
      "[BATCH 22/149] Loss_D: 1.1760 Loss_G: 2.5024 acc: 48.4%\n",
      "[BATCH 23/149] Loss_D: 1.1905 Loss_G: 2.4650 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 1.1873 Loss_G: 2.4016 acc: 56.2%\n",
      "[BATCH 25/149] Loss_D: 1.1945 Loss_G: 2.3439 acc: 53.1%\n",
      "[BATCH 26/149] Loss_D: 1.1927 Loss_G: 2.2754 acc: 43.8%\n",
      "[BATCH 27/149] Loss_D: 1.1513 Loss_G: 2.1900 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 1.1681 Loss_G: 2.1384 acc: 45.3%\n",
      "[BATCH 29/149] Loss_D: 1.1248 Loss_G: 1.9842 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 1.1188 Loss_G: 1.9545 acc: 51.6%\n",
      "[BATCH 31/149] Loss_D: 1.1412 Loss_G: 1.8882 acc: 46.9%\n",
      "[BATCH 32/149] Loss_D: 1.1466 Loss_G: 1.7901 acc: 40.6%\n",
      "[BATCH 33/149] Loss_D: 1.1435 Loss_G: 1.8001 acc: 51.6%\n",
      "[BATCH 34/149] Loss_D: 1.1232 Loss_G: 1.7021 acc: 54.7%\n",
      "[BATCH 35/149] Loss_D: 1.1522 Loss_G: 1.6919 acc: 46.9%\n",
      "[BATCH 36/149] Loss_D: 1.1097 Loss_G: 1.6771 acc: 42.2%\n",
      "[BATCH 37/149] Loss_D: 1.1047 Loss_G: 1.5970 acc: 45.3%\n",
      "[BATCH 38/149] Loss_D: 1.1158 Loss_G: 1.5861 acc: 42.2%\n",
      "[BATCH 39/149] Loss_D: 1.1055 Loss_G: 1.5773 acc: 50.0%\n",
      "[BATCH 40/149] Loss_D: 1.0873 Loss_G: 1.5628 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 1.1098 Loss_G: 1.5364 acc: 50.0%\n",
      "[BATCH 42/149] Loss_D: 1.0390 Loss_G: 1.5206 acc: 37.5%\n",
      "[BATCH 43/149] Loss_D: 1.0799 Loss_G: 1.4946 acc: 53.1%\n",
      "[BATCH 44/149] Loss_D: 1.0924 Loss_G: 1.4702 acc: 43.8%\n",
      "[BATCH 45/149] Loss_D: 1.1019 Loss_G: 1.5021 acc: 50.0%\n",
      "[BATCH 46/149] Loss_D: 1.0975 Loss_G: 1.4404 acc: 50.0%\n",
      "[BATCH 47/149] Loss_D: 1.0943 Loss_G: 1.4443 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 1.1144 Loss_G: 1.4245 acc: 56.2%\n",
      "[BATCH 49/149] Loss_D: 1.0919 Loss_G: 1.4145 acc: 46.9%\n",
      "[BATCH 50/149] Loss_D: 1.0861 Loss_G: 1.4032 acc: 54.7%\n",
      "[EPOCH 50] TEST ACC is : 55.7%\n",
      "[BATCH 51/149] Loss_D: 1.1130 Loss_G: 1.3995 acc: 51.6%\n",
      "[BATCH 52/149] Loss_D: 1.0970 Loss_G: 1.4009 acc: 48.4%\n",
      "[BATCH 53/149] Loss_D: 1.0504 Loss_G: 1.3836 acc: 53.1%\n",
      "[BATCH 54/149] Loss_D: 1.0843 Loss_G: 1.3856 acc: 56.2%\n",
      "[BATCH 55/149] Loss_D: 1.0632 Loss_G: 1.3671 acc: 56.2%\n",
      "[BATCH 56/149] Loss_D: 1.0358 Loss_G: 1.3769 acc: 46.9%\n",
      "[BATCH 57/149] Loss_D: 1.1095 Loss_G: 1.3528 acc: 51.6%\n",
      "[BATCH 58/149] Loss_D: 1.0477 Loss_G: 1.3436 acc: 40.6%\n",
      "[BATCH 59/149] Loss_D: 1.1417 Loss_G: 1.3506 acc: 45.3%\n",
      "[BATCH 60/149] Loss_D: 1.0506 Loss_G: 1.3327 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 1.0434 Loss_G: 1.3414 acc: 53.1%\n",
      "[BATCH 62/149] Loss_D: 1.0181 Loss_G: 1.3213 acc: 53.1%\n",
      "[BATCH 63/149] Loss_D: 1.0715 Loss_G: 1.3335 acc: 51.6%\n",
      "[BATCH 64/149] Loss_D: 1.0667 Loss_G: 1.3045 acc: 54.7%\n",
      "[BATCH 65/149] Loss_D: 1.0239 Loss_G: 1.2978 acc: 57.8%\n",
      "[BATCH 66/149] Loss_D: 1.0291 Loss_G: 1.3025 acc: 54.7%\n",
      "[BATCH 67/149] Loss_D: 1.0280 Loss_G: 1.2904 acc: 45.3%\n",
      "[BATCH 68/149] Loss_D: 1.1359 Loss_G: 1.3150 acc: 45.3%\n",
      "[BATCH 69/149] Loss_D: 1.0537 Loss_G: 1.2757 acc: 53.1%\n",
      "[BATCH 70/149] Loss_D: 0.9947 Loss_G: 1.2768 acc: 50.0%\n",
      "[BATCH 71/149] Loss_D: 1.1001 Loss_G: 1.2703 acc: 46.9%\n",
      "[BATCH 72/149] Loss_D: 1.0512 Loss_G: 1.2639 acc: 54.7%\n",
      "[BATCH 73/149] Loss_D: 1.0870 Loss_G: 1.2605 acc: 48.4%\n",
      "[BATCH 74/149] Loss_D: 0.9961 Loss_G: 1.2583 acc: 50.0%\n",
      "[BATCH 75/149] Loss_D: 1.0611 Loss_G: 1.2477 acc: 53.1%\n",
      "[BATCH 76/149] Loss_D: 1.0221 Loss_G: 1.2495 acc: 53.1%\n",
      "[BATCH 77/149] Loss_D: 0.9936 Loss_G: 1.2349 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 1.0474 Loss_G: 1.2293 acc: 50.0%\n",
      "[BATCH 79/149] Loss_D: 1.0166 Loss_G: 1.2329 acc: 51.6%\n",
      "[BATCH 80/149] Loss_D: 0.9605 Loss_G: 1.2136 acc: 50.0%\n",
      "[BATCH 81/149] Loss_D: 1.0145 Loss_G: 1.2116 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.9402 Loss_G: 1.2113 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 1.0675 Loss_G: 1.2096 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 1.0483 Loss_G: 1.2095 acc: 57.8%\n",
      "[BATCH 85/149] Loss_D: 1.0204 Loss_G: 1.2054 acc: 50.0%\n",
      "[BATCH 86/149] Loss_D: 1.0277 Loss_G: 1.2058 acc: 53.1%\n",
      "[BATCH 87/149] Loss_D: 0.9962 Loss_G: 1.2002 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 1.0042 Loss_G: 1.1964 acc: 45.3%\n",
      "[BATCH 89/149] Loss_D: 0.9502 Loss_G: 1.1889 acc: 37.5%\n",
      "[BATCH 90/149] Loss_D: 1.0198 Loss_G: 1.2001 acc: 46.9%\n",
      "[BATCH 91/149] Loss_D: 0.9891 Loss_G: 1.1791 acc: 50.0%\n",
      "[BATCH 92/149] Loss_D: 0.9713 Loss_G: 1.1779 acc: 51.6%\n",
      "[BATCH 93/149] Loss_D: 1.0589 Loss_G: 1.1993 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 1.0731 Loss_G: 1.1971 acc: 51.6%\n",
      "[BATCH 95/149] Loss_D: 1.1117 Loss_G: 1.2072 acc: 57.8%\n",
      "[BATCH 96/149] Loss_D: 0.9822 Loss_G: 1.1912 acc: 48.4%\n",
      "[BATCH 97/149] Loss_D: 0.9959 Loss_G: 1.1849 acc: 57.8%\n",
      "[BATCH 98/149] Loss_D: 1.0019 Loss_G: 1.1916 acc: 56.2%\n",
      "[BATCH 99/149] Loss_D: 1.0780 Loss_G: 1.1905 acc: 46.9%\n",
      "[BATCH 100/149] Loss_D: 1.0066 Loss_G: 1.1903 acc: 54.7%\n",
      "[EPOCH 100] TEST ACC is : 54.1%\n",
      "[BATCH 101/149] Loss_D: 0.9981 Loss_G: 1.1888 acc: 46.9%\n",
      "[BATCH 102/149] Loss_D: 1.0287 Loss_G: 1.1911 acc: 46.9%\n",
      "[BATCH 103/149] Loss_D: 1.0041 Loss_G: 1.1795 acc: 56.2%\n",
      "[BATCH 104/149] Loss_D: 1.0074 Loss_G: 1.1803 acc: 54.7%\n",
      "[BATCH 105/149] Loss_D: 0.9905 Loss_G: 1.1751 acc: 50.0%\n",
      "[BATCH 106/149] Loss_D: 1.0369 Loss_G: 1.1667 acc: 46.9%\n",
      "[BATCH 107/149] Loss_D: 1.0080 Loss_G: 1.1792 acc: 56.2%\n",
      "[BATCH 108/149] Loss_D: 1.0713 Loss_G: 1.1888 acc: 54.7%\n",
      "[BATCH 109/149] Loss_D: 0.9513 Loss_G: 1.1696 acc: 51.6%\n",
      "[BATCH 110/149] Loss_D: 0.9819 Loss_G: 1.1659 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 1.0341 Loss_G: 1.1794 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 1.0345 Loss_G: 1.1802 acc: 48.4%\n",
      "[BATCH 113/149] Loss_D: 0.9966 Loss_G: 1.1701 acc: 59.4%\n",
      "[BATCH 114/149] Loss_D: 1.0136 Loss_G: 1.1779 acc: 53.1%\n",
      "[BATCH 115/149] Loss_D: 0.9683 Loss_G: 1.1682 acc: 48.4%\n",
      "[BATCH 116/149] Loss_D: 0.9794 Loss_G: 1.1611 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 1.0122 Loss_G: 1.1777 acc: 53.1%\n",
      "[BATCH 118/149] Loss_D: 0.9592 Loss_G: 1.1622 acc: 51.6%\n",
      "[BATCH 119/149] Loss_D: 0.9534 Loss_G: 1.1529 acc: 43.8%\n",
      "[BATCH 120/149] Loss_D: 1.0200 Loss_G: 1.1559 acc: 46.9%\n",
      "[BATCH 121/149] Loss_D: 1.0439 Loss_G: 1.1707 acc: 56.2%\n",
      "[BATCH 122/149] Loss_D: 0.9564 Loss_G: 1.1505 acc: 50.0%\n",
      "[BATCH 123/149] Loss_D: 0.9291 Loss_G: 1.1363 acc: 50.0%\n",
      "[BATCH 124/149] Loss_D: 0.9966 Loss_G: 1.1587 acc: 53.1%\n",
      "[BATCH 125/149] Loss_D: 1.0398 Loss_G: 1.1534 acc: 53.1%\n",
      "[BATCH 126/149] Loss_D: 0.9865 Loss_G: 1.1469 acc: 51.6%\n",
      "[BATCH 127/149] Loss_D: 0.9517 Loss_G: 1.1453 acc: 43.8%\n",
      "[BATCH 128/149] Loss_D: 0.9440 Loss_G: 1.1354 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 1.0361 Loss_G: 1.1546 acc: 53.1%\n",
      "[BATCH 130/149] Loss_D: 1.0082 Loss_G: 1.1556 acc: 45.3%\n",
      "[BATCH 131/149] Loss_D: 1.0154 Loss_G: 1.1497 acc: 48.4%\n",
      "[BATCH 132/149] Loss_D: 0.9893 Loss_G: 1.1448 acc: 57.8%\n",
      "[BATCH 133/149] Loss_D: 0.9441 Loss_G: 1.1448 acc: 51.6%\n",
      "[BATCH 134/149] Loss_D: 0.9342 Loss_G: 1.1406 acc: 53.1%\n",
      "[BATCH 135/149] Loss_D: 0.9539 Loss_G: 1.1383 acc: 50.0%\n",
      "[BATCH 136/149] Loss_D: 1.0314 Loss_G: 1.1655 acc: 45.3%\n",
      "[BATCH 137/149] Loss_D: 0.9843 Loss_G: 1.1629 acc: 57.8%\n",
      "[BATCH 138/149] Loss_D: 0.9818 Loss_G: 1.1436 acc: 57.8%\n",
      "[BATCH 139/149] Loss_D: 1.0482 Loss_G: 1.1510 acc: 48.4%\n",
      "[BATCH 140/149] Loss_D: 0.9729 Loss_G: 1.1436 acc: 57.8%\n",
      "[BATCH 141/149] Loss_D: 0.9937 Loss_G: 1.1430 acc: 50.0%\n",
      "[BATCH 142/149] Loss_D: 0.9552 Loss_G: 1.1416 acc: 51.6%\n",
      "[BATCH 143/149] Loss_D: 1.0090 Loss_G: 1.1428 acc: 54.7%\n",
      "[BATCH 144/149] Loss_D: 0.9158 Loss_G: 1.1312 acc: 57.8%\n",
      "[BATCH 145/149] Loss_D: 0.9542 Loss_G: 1.1410 acc: 46.9%\n",
      "[BATCH 146/149] Loss_D: 0.9636 Loss_G: 1.1265 acc: 40.6%\n",
      "[BATCH 147/149] Loss_D: 1.0144 Loss_G: 1.1392 acc: 60.9%\n",
      "[BATCH 148/149] Loss_D: 0.9141 Loss_G: 1.1325 acc: 56.2%\n",
      "[BATCH 149/149] Loss_D: 0.9696 Loss_G: 1.1260 acc: 48.4%\n",
      "-----THE [1/50] epoch end-----\n",
      "-----THE [2/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.9997 Loss_G: 1.1306 acc: 46.9%\n",
      "[EPOCH 150] TEST ACC is : 54.1%\n",
      "[BATCH 2/149] Loss_D: 1.0452 Loss_G: 1.1367 acc: 48.4%\n",
      "[BATCH 3/149] Loss_D: 0.9845 Loss_G: 1.1361 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.9938 Loss_G: 1.1302 acc: 50.0%\n",
      "[BATCH 5/149] Loss_D: 0.9595 Loss_G: 1.1243 acc: 45.3%\n",
      "[BATCH 6/149] Loss_D: 0.9199 Loss_G: 1.1155 acc: 51.6%\n",
      "[BATCH 7/149] Loss_D: 0.9870 Loss_G: 1.1263 acc: 45.3%\n",
      "[BATCH 8/149] Loss_D: 0.9537 Loss_G: 1.1300 acc: 46.9%\n",
      "[BATCH 9/149] Loss_D: 1.0194 Loss_G: 1.1350 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.9350 Loss_G: 1.1229 acc: 51.6%\n",
      "[BATCH 11/149] Loss_D: 0.9742 Loss_G: 1.1201 acc: 62.5%\n",
      "[BATCH 12/149] Loss_D: 0.9550 Loss_G: 1.1277 acc: 42.2%\n",
      "[BATCH 13/149] Loss_D: 1.0304 Loss_G: 1.1444 acc: 50.0%\n",
      "[BATCH 14/149] Loss_D: 0.9390 Loss_G: 1.1326 acc: 40.6%\n",
      "[BATCH 15/149] Loss_D: 0.9543 Loss_G: 1.1216 acc: 54.7%\n",
      "[BATCH 16/149] Loss_D: 0.9199 Loss_G: 1.1088 acc: 45.3%\n",
      "[BATCH 17/149] Loss_D: 0.9633 Loss_G: 1.1153 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.9160 Loss_G: 1.1268 acc: 43.8%\n",
      "[BATCH 19/149] Loss_D: 0.8869 Loss_G: 1.1016 acc: 54.7%\n",
      "[BATCH 20/149] Loss_D: 0.9095 Loss_G: 1.0993 acc: 51.6%\n",
      "[BATCH 21/149] Loss_D: 0.9867 Loss_G: 1.1203 acc: 50.0%\n",
      "[BATCH 22/149] Loss_D: 0.9129 Loss_G: 1.1016 acc: 53.1%\n",
      "[BATCH 23/149] Loss_D: 0.9557 Loss_G: 1.0973 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.9736 Loss_G: 1.1012 acc: 51.6%\n",
      "[BATCH 25/149] Loss_D: 0.9726 Loss_G: 1.1094 acc: 54.7%\n",
      "[BATCH 26/149] Loss_D: 0.9358 Loss_G: 1.1083 acc: 46.9%\n",
      "[BATCH 27/149] Loss_D: 0.9404 Loss_G: 1.1027 acc: 46.9%\n",
      "[BATCH 28/149] Loss_D: 1.0288 Loss_G: 1.1143 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.9349 Loss_G: 1.1104 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 0.9244 Loss_G: 1.1005 acc: 51.6%\n",
      "[BATCH 31/149] Loss_D: 0.9312 Loss_G: 1.1000 acc: 48.4%\n",
      "[BATCH 32/149] Loss_D: 0.9476 Loss_G: 1.1037 acc: 37.5%\n",
      "[BATCH 33/149] Loss_D: 0.9322 Loss_G: 1.1018 acc: 46.9%\n",
      "[BATCH 34/149] Loss_D: 0.9688 Loss_G: 1.0996 acc: 56.2%\n",
      "[BATCH 35/149] Loss_D: 0.9803 Loss_G: 1.1049 acc: 50.0%\n",
      "[BATCH 36/149] Loss_D: 0.9712 Loss_G: 1.1161 acc: 50.0%\n",
      "[BATCH 37/149] Loss_D: 0.9376 Loss_G: 1.1122 acc: 56.2%\n",
      "[BATCH 38/149] Loss_D: 0.9226 Loss_G: 1.1079 acc: 45.3%\n",
      "[BATCH 39/149] Loss_D: 0.9765 Loss_G: 1.1138 acc: 56.2%\n",
      "[BATCH 40/149] Loss_D: 0.8912 Loss_G: 1.1105 acc: 54.7%\n",
      "[BATCH 41/149] Loss_D: 1.0146 Loss_G: 1.1282 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.9367 Loss_G: 1.1346 acc: 43.8%\n",
      "[BATCH 43/149] Loss_D: 0.9698 Loss_G: 1.1322 acc: 50.0%\n",
      "[BATCH 44/149] Loss_D: 0.9299 Loss_G: 1.1287 acc: 43.8%\n",
      "[BATCH 45/149] Loss_D: 0.9591 Loss_G: 1.1394 acc: 51.6%\n",
      "[BATCH 46/149] Loss_D: 0.9376 Loss_G: 1.1357 acc: 53.1%\n",
      "[BATCH 47/149] Loss_D: 0.9829 Loss_G: 1.1337 acc: 46.9%\n",
      "[BATCH 48/149] Loss_D: 0.9017 Loss_G: 1.1212 acc: 48.4%\n",
      "[BATCH 49/149] Loss_D: 0.9614 Loss_G: 1.1084 acc: 56.2%\n",
      "[BATCH 50/149] Loss_D: 0.9021 Loss_G: 1.0945 acc: 56.2%\n",
      "[BATCH 51/149] Loss_D: 0.9636 Loss_G: 1.0985 acc: 56.2%\n",
      "[EPOCH 200] TEST ACC is : 54.1%\n",
      "[BATCH 52/149] Loss_D: 0.9456 Loss_G: 1.0941 acc: 60.9%\n",
      "[BATCH 53/149] Loss_D: 0.9842 Loss_G: 1.0950 acc: 53.1%\n",
      "[BATCH 54/149] Loss_D: 0.9527 Loss_G: 1.0991 acc: 48.4%\n",
      "[BATCH 55/149] Loss_D: 0.9098 Loss_G: 1.0976 acc: 59.4%\n",
      "[BATCH 56/149] Loss_D: 1.0490 Loss_G: 1.1203 acc: 59.4%\n",
      "[BATCH 57/149] Loss_D: 1.0562 Loss_G: 1.1706 acc: 51.6%\n",
      "[BATCH 58/149] Loss_D: 0.9222 Loss_G: 1.1354 acc: 48.4%\n",
      "[BATCH 59/149] Loss_D: 0.9610 Loss_G: 1.1151 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.9477 Loss_G: 1.1077 acc: 54.7%\n",
      "[BATCH 61/149] Loss_D: 0.9049 Loss_G: 1.1067 acc: 34.4%\n",
      "[BATCH 62/149] Loss_D: 0.9650 Loss_G: 1.1156 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.9480 Loss_G: 1.1089 acc: 53.1%\n",
      "[BATCH 64/149] Loss_D: 0.9165 Loss_G: 1.0953 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.9178 Loss_G: 1.0977 acc: 40.6%\n",
      "[BATCH 66/149] Loss_D: 0.9466 Loss_G: 1.0892 acc: 53.1%\n",
      "[BATCH 67/149] Loss_D: 0.9149 Loss_G: 1.0890 acc: 54.7%\n",
      "[BATCH 68/149] Loss_D: 0.9517 Loss_G: 1.0870 acc: 46.9%\n",
      "[BATCH 69/149] Loss_D: 1.0106 Loss_G: 1.0964 acc: 40.6%\n",
      "[BATCH 70/149] Loss_D: 0.9215 Loss_G: 1.0957 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.9814 Loss_G: 1.1157 acc: 54.7%\n",
      "[BATCH 72/149] Loss_D: 0.9258 Loss_G: 1.1019 acc: 51.6%\n",
      "[BATCH 73/149] Loss_D: 0.9463 Loss_G: 1.0924 acc: 53.1%\n",
      "[BATCH 74/149] Loss_D: 0.9553 Loss_G: 1.0885 acc: 56.2%\n",
      "[BATCH 75/149] Loss_D: 0.9752 Loss_G: 1.1004 acc: 62.5%\n",
      "[BATCH 76/149] Loss_D: 0.9621 Loss_G: 1.1103 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.9331 Loss_G: 1.1017 acc: 56.2%\n",
      "[BATCH 78/149] Loss_D: 0.9333 Loss_G: 1.0989 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.9267 Loss_G: 1.0920 acc: 50.0%\n",
      "[BATCH 80/149] Loss_D: 1.0290 Loss_G: 1.1181 acc: 50.0%\n",
      "[BATCH 81/149] Loss_D: 0.9780 Loss_G: 1.1262 acc: 51.6%\n",
      "[BATCH 82/149] Loss_D: 0.9278 Loss_G: 1.1176 acc: 59.4%\n",
      "[BATCH 83/149] Loss_D: 0.9605 Loss_G: 1.1036 acc: 57.8%\n",
      "[BATCH 84/149] Loss_D: 0.8689 Loss_G: 1.0994 acc: 51.6%\n",
      "[BATCH 85/149] Loss_D: 0.9056 Loss_G: 1.0906 acc: 50.0%\n",
      "[BATCH 86/149] Loss_D: 0.9670 Loss_G: 1.1017 acc: 54.7%\n",
      "[BATCH 87/149] Loss_D: 0.9571 Loss_G: 1.1035 acc: 48.4%\n",
      "[BATCH 88/149] Loss_D: 0.9742 Loss_G: 1.0956 acc: 51.6%\n",
      "[BATCH 89/149] Loss_D: 0.9455 Loss_G: 1.0838 acc: 48.4%\n",
      "[BATCH 90/149] Loss_D: 0.9166 Loss_G: 1.0759 acc: 53.1%\n",
      "[BATCH 91/149] Loss_D: 0.9374 Loss_G: 1.0737 acc: 37.5%\n",
      "[BATCH 92/149] Loss_D: 0.9609 Loss_G: 1.0870 acc: 43.8%\n",
      "[BATCH 93/149] Loss_D: 0.9343 Loss_G: 1.0802 acc: 54.7%\n",
      "[BATCH 94/149] Loss_D: 0.9323 Loss_G: 1.0941 acc: 53.1%\n",
      "[BATCH 95/149] Loss_D: 0.9959 Loss_G: 1.1014 acc: 56.2%\n",
      "[BATCH 96/149] Loss_D: 0.9092 Loss_G: 1.1101 acc: 51.6%\n",
      "[BATCH 97/149] Loss_D: 0.9028 Loss_G: 1.1186 acc: 45.3%\n",
      "[BATCH 98/149] Loss_D: 0.9086 Loss_G: 1.1219 acc: 54.7%\n",
      "[BATCH 99/149] Loss_D: 0.9400 Loss_G: 1.1257 acc: 51.6%\n",
      "[BATCH 100/149] Loss_D: 0.9146 Loss_G: 1.1341 acc: 43.8%\n",
      "[BATCH 101/149] Loss_D: 0.8700 Loss_G: 1.1319 acc: 54.7%\n",
      "[EPOCH 250] TEST ACC is : 54.5%\n",
      "[BATCH 102/149] Loss_D: 0.9499 Loss_G: 1.1389 acc: 53.1%\n",
      "[BATCH 103/149] Loss_D: 0.9389 Loss_G: 1.1462 acc: 53.1%\n",
      "[BATCH 104/149] Loss_D: 0.9546 Loss_G: 1.1495 acc: 53.1%\n",
      "[BATCH 105/149] Loss_D: 0.9214 Loss_G: 1.1545 acc: 40.6%\n",
      "[BATCH 106/149] Loss_D: 0.9763 Loss_G: 1.1618 acc: 48.4%\n",
      "[BATCH 107/149] Loss_D: 0.9000 Loss_G: 1.1564 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.8693 Loss_G: 1.1552 acc: 54.7%\n",
      "[BATCH 109/149] Loss_D: 1.0199 Loss_G: 1.1629 acc: 50.0%\n",
      "[BATCH 110/149] Loss_D: 0.9283 Loss_G: 1.1627 acc: 56.2%\n",
      "[BATCH 111/149] Loss_D: 0.9013 Loss_G: 1.1554 acc: 48.4%\n",
      "[BATCH 112/149] Loss_D: 0.8936 Loss_G: 1.1565 acc: 53.1%\n",
      "[BATCH 113/149] Loss_D: 1.0026 Loss_G: 1.1649 acc: 50.0%\n",
      "[BATCH 114/149] Loss_D: 0.9147 Loss_G: 1.1433 acc: 51.6%\n",
      "[BATCH 115/149] Loss_D: 0.9933 Loss_G: 1.1186 acc: 43.8%\n",
      "[BATCH 116/149] Loss_D: 1.0591 Loss_G: 1.0964 acc: 54.7%\n",
      "[BATCH 117/149] Loss_D: 0.9057 Loss_G: 1.0642 acc: 48.4%\n",
      "[BATCH 118/149] Loss_D: 0.9077 Loss_G: 1.0267 acc: 48.4%\n",
      "[BATCH 119/149] Loss_D: 0.9238 Loss_G: 1.0118 acc: 54.7%\n",
      "[BATCH 120/149] Loss_D: 0.9039 Loss_G: 1.0084 acc: 53.1%\n",
      "[BATCH 121/149] Loss_D: 0.8936 Loss_G: 1.0053 acc: 54.7%\n",
      "[BATCH 122/149] Loss_D: 0.8946 Loss_G: 1.0082 acc: 40.6%\n",
      "[BATCH 123/149] Loss_D: 0.8767 Loss_G: 1.0088 acc: 56.2%\n",
      "[BATCH 124/149] Loss_D: 0.9269 Loss_G: 1.0186 acc: 54.7%\n",
      "[BATCH 125/149] Loss_D: 0.8969 Loss_G: 1.0142 acc: 50.0%\n",
      "[BATCH 126/149] Loss_D: 0.9276 Loss_G: 1.0269 acc: 54.7%\n",
      "[BATCH 127/149] Loss_D: 0.9539 Loss_G: 1.0342 acc: 51.6%\n",
      "[BATCH 128/149] Loss_D: 0.9543 Loss_G: 1.0413 acc: 51.6%\n",
      "[BATCH 129/149] Loss_D: 0.8325 Loss_G: 1.0158 acc: 46.9%\n",
      "[BATCH 130/149] Loss_D: 0.9672 Loss_G: 1.0179 acc: 54.7%\n",
      "[BATCH 131/149] Loss_D: 0.9878 Loss_G: 1.0365 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.8602 Loss_G: 1.0329 acc: 59.4%\n",
      "[BATCH 133/149] Loss_D: 0.9802 Loss_G: 1.0408 acc: 51.6%\n",
      "[BATCH 134/149] Loss_D: 0.9175 Loss_G: 1.0394 acc: 48.4%\n",
      "[BATCH 135/149] Loss_D: 0.9109 Loss_G: 1.0373 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.8977 Loss_G: 1.0265 acc: 54.7%\n",
      "[BATCH 137/149] Loss_D: 0.8961 Loss_G: 1.0229 acc: 50.0%\n",
      "[BATCH 138/149] Loss_D: 0.9605 Loss_G: 1.0302 acc: 62.5%\n",
      "[BATCH 139/149] Loss_D: 0.9755 Loss_G: 1.0520 acc: 53.1%\n",
      "[BATCH 140/149] Loss_D: 0.8890 Loss_G: 1.0406 acc: 50.0%\n",
      "[BATCH 141/149] Loss_D: 0.8940 Loss_G: 1.0310 acc: 53.1%\n",
      "[BATCH 142/149] Loss_D: 0.8674 Loss_G: 1.0182 acc: 42.2%\n",
      "[BATCH 143/149] Loss_D: 0.9495 Loss_G: 1.0220 acc: 43.8%\n",
      "[BATCH 144/149] Loss_D: 0.8871 Loss_G: 1.0246 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.9055 Loss_G: 1.0251 acc: 50.0%\n",
      "[BATCH 146/149] Loss_D: 0.8864 Loss_G: 1.0261 acc: 54.7%\n",
      "[BATCH 147/149] Loss_D: 0.9535 Loss_G: 1.0380 acc: 59.4%\n",
      "[BATCH 148/149] Loss_D: 0.9317 Loss_G: 1.0391 acc: 54.7%\n",
      "[BATCH 149/149] Loss_D: 0.9507 Loss_G: 1.0413 acc: 42.2%\n",
      "-----THE [2/50] epoch end-----\n",
      "-----THE [3/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8824 Loss_G: 1.0343 acc: 46.9%\n",
      "[BATCH 2/149] Loss_D: 0.8941 Loss_G: 1.0322 acc: 65.6%\n",
      "[EPOCH 300] TEST ACC is : 55.1%\n",
      "[BATCH 3/149] Loss_D: 0.8999 Loss_G: 1.0299 acc: 50.0%\n",
      "[BATCH 4/149] Loss_D: 0.9116 Loss_G: 1.0354 acc: 50.0%\n",
      "[BATCH 5/149] Loss_D: 0.9558 Loss_G: 1.0385 acc: 57.8%\n",
      "[BATCH 6/149] Loss_D: 0.9238 Loss_G: 1.0389 acc: 50.0%\n",
      "[BATCH 7/149] Loss_D: 0.9747 Loss_G: 1.0498 acc: 57.8%\n",
      "[BATCH 8/149] Loss_D: 0.9362 Loss_G: 1.0511 acc: 53.1%\n",
      "[BATCH 9/149] Loss_D: 0.9286 Loss_G: 1.0531 acc: 51.6%\n",
      "[BATCH 10/149] Loss_D: 0.8791 Loss_G: 1.0411 acc: 39.1%\n",
      "[BATCH 11/149] Loss_D: 0.8878 Loss_G: 1.0318 acc: 45.3%\n",
      "[BATCH 12/149] Loss_D: 0.8894 Loss_G: 1.0373 acc: 51.6%\n",
      "[BATCH 13/149] Loss_D: 0.9020 Loss_G: 1.0386 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.8959 Loss_G: 1.0392 acc: 35.9%\n",
      "[BATCH 15/149] Loss_D: 0.9036 Loss_G: 1.0415 acc: 60.9%\n",
      "[BATCH 16/149] Loss_D: 0.9309 Loss_G: 1.0393 acc: 53.1%\n",
      "[BATCH 17/149] Loss_D: 0.8979 Loss_G: 1.0357 acc: 50.0%\n",
      "[BATCH 18/149] Loss_D: 0.9312 Loss_G: 1.0351 acc: 50.0%\n",
      "[BATCH 19/149] Loss_D: 0.9099 Loss_G: 1.0360 acc: 54.7%\n",
      "[BATCH 20/149] Loss_D: 0.9078 Loss_G: 1.0432 acc: 48.4%\n",
      "[BATCH 21/149] Loss_D: 0.9035 Loss_G: 1.0359 acc: 40.6%\n",
      "[BATCH 22/149] Loss_D: 0.8696 Loss_G: 1.0325 acc: 48.4%\n",
      "[BATCH 23/149] Loss_D: 0.9242 Loss_G: 1.0396 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.9815 Loss_G: 1.0498 acc: 57.8%\n",
      "[BATCH 25/149] Loss_D: 0.9922 Loss_G: 1.0728 acc: 50.0%\n",
      "[BATCH 26/149] Loss_D: 0.9485 Loss_G: 1.0827 acc: 57.8%\n",
      "[BATCH 27/149] Loss_D: 0.9563 Loss_G: 1.0860 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.9182 Loss_G: 1.0739 acc: 43.8%\n",
      "[BATCH 29/149] Loss_D: 0.9290 Loss_G: 1.0661 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.9233 Loss_G: 1.0549 acc: 53.1%\n",
      "[BATCH 31/149] Loss_D: 0.9677 Loss_G: 1.0562 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.9743 Loss_G: 1.0577 acc: 56.2%\n",
      "[BATCH 33/149] Loss_D: 0.8423 Loss_G: 1.0439 acc: 51.6%\n",
      "[BATCH 34/149] Loss_D: 0.9352 Loss_G: 1.0404 acc: 42.2%\n",
      "[BATCH 35/149] Loss_D: 0.9094 Loss_G: 1.0466 acc: 43.8%\n",
      "[BATCH 36/149] Loss_D: 1.0094 Loss_G: 1.0719 acc: 45.3%\n",
      "[BATCH 37/149] Loss_D: 0.9283 Loss_G: 1.0702 acc: 48.4%\n",
      "[BATCH 38/149] Loss_D: 0.8922 Loss_G: 1.0643 acc: 51.6%\n",
      "[BATCH 39/149] Loss_D: 0.8842 Loss_G: 1.0619 acc: 56.2%\n",
      "[BATCH 40/149] Loss_D: 0.9140 Loss_G: 1.0653 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.9139 Loss_G: 1.0743 acc: 51.6%\n",
      "[BATCH 42/149] Loss_D: 0.8668 Loss_G: 1.0749 acc: 54.7%\n",
      "[BATCH 43/149] Loss_D: 0.9141 Loss_G: 1.0836 acc: 50.0%\n",
      "[BATCH 44/149] Loss_D: 0.8582 Loss_G: 1.0903 acc: 51.6%\n",
      "[BATCH 45/149] Loss_D: 0.9351 Loss_G: 1.0967 acc: 53.1%\n",
      "[BATCH 46/149] Loss_D: 0.9676 Loss_G: 1.1052 acc: 35.9%\n",
      "[BATCH 47/149] Loss_D: 0.9238 Loss_G: 1.1193 acc: 48.4%\n",
      "[BATCH 48/149] Loss_D: 0.8369 Loss_G: 1.1172 acc: 57.8%\n",
      "[BATCH 49/149] Loss_D: 0.8947 Loss_G: 1.1116 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.8649 Loss_G: 1.1162 acc: 46.9%\n",
      "[BATCH 51/149] Loss_D: 0.9349 Loss_G: 1.1246 acc: 53.1%\n",
      "[BATCH 52/149] Loss_D: 0.9660 Loss_G: 1.1367 acc: 51.6%\n",
      "[EPOCH 350] TEST ACC is : 54.7%\n",
      "[BATCH 53/149] Loss_D: 0.8227 Loss_G: 1.1333 acc: 56.2%\n",
      "[BATCH 54/149] Loss_D: 0.8791 Loss_G: 1.1254 acc: 45.3%\n",
      "[BATCH 55/149] Loss_D: 0.9029 Loss_G: 1.1056 acc: 48.4%\n",
      "[BATCH 56/149] Loss_D: 0.8708 Loss_G: 1.0625 acc: 57.8%\n",
      "[BATCH 57/149] Loss_D: 0.9013 Loss_G: 1.0441 acc: 54.7%\n",
      "[BATCH 58/149] Loss_D: 0.9761 Loss_G: 1.0024 acc: 51.6%\n",
      "[BATCH 59/149] Loss_D: 0.9481 Loss_G: 0.9939 acc: 42.2%\n",
      "[BATCH 60/149] Loss_D: 0.9529 Loss_G: 0.9920 acc: 53.1%\n",
      "[BATCH 61/149] Loss_D: 0.9111 Loss_G: 0.9869 acc: 51.6%\n",
      "[BATCH 62/149] Loss_D: 0.8585 Loss_G: 0.9713 acc: 57.8%\n",
      "[BATCH 63/149] Loss_D: 0.8912 Loss_G: 0.9651 acc: 51.6%\n",
      "[BATCH 64/149] Loss_D: 0.8581 Loss_G: 0.9668 acc: 46.9%\n",
      "[BATCH 65/149] Loss_D: 0.9511 Loss_G: 0.9879 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.9155 Loss_G: 1.0094 acc: 53.1%\n",
      "[BATCH 67/149] Loss_D: 0.9084 Loss_G: 1.0192 acc: 51.6%\n",
      "[BATCH 68/149] Loss_D: 0.8676 Loss_G: 1.0226 acc: 50.0%\n",
      "[BATCH 69/149] Loss_D: 0.9238 Loss_G: 1.0096 acc: 40.6%\n",
      "[BATCH 70/149] Loss_D: 0.8858 Loss_G: 1.0050 acc: 50.0%\n",
      "[BATCH 71/149] Loss_D: 0.8873 Loss_G: 0.9961 acc: 43.8%\n",
      "[BATCH 72/149] Loss_D: 0.8877 Loss_G: 1.0075 acc: 53.1%\n",
      "[BATCH 73/149] Loss_D: 0.8694 Loss_G: 1.0061 acc: 53.1%\n",
      "[BATCH 74/149] Loss_D: 0.9418 Loss_G: 1.0102 acc: 50.0%\n",
      "[BATCH 75/149] Loss_D: 0.9091 Loss_G: 1.0137 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.8852 Loss_G: 1.0127 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.8731 Loss_G: 1.0062 acc: 40.6%\n",
      "[BATCH 78/149] Loss_D: 0.9191 Loss_G: 1.0069 acc: 50.0%\n",
      "[BATCH 79/149] Loss_D: 0.8318 Loss_G: 1.0020 acc: 50.0%\n",
      "[BATCH 80/149] Loss_D: 0.9523 Loss_G: 1.0160 acc: 53.1%\n",
      "[BATCH 81/149] Loss_D: 0.8377 Loss_G: 1.0105 acc: 54.7%\n",
      "[BATCH 82/149] Loss_D: 0.9078 Loss_G: 1.0143 acc: 40.6%\n",
      "[BATCH 83/149] Loss_D: 0.8865 Loss_G: 1.0220 acc: 50.0%\n",
      "[BATCH 84/149] Loss_D: 0.9218 Loss_G: 1.0273 acc: 64.1%\n",
      "[BATCH 85/149] Loss_D: 0.9183 Loss_G: 1.0329 acc: 53.1%\n",
      "[BATCH 86/149] Loss_D: 0.9282 Loss_G: 1.0408 acc: 46.9%\n",
      "[BATCH 87/149] Loss_D: 0.9069 Loss_G: 1.0436 acc: 54.7%\n",
      "[BATCH 88/149] Loss_D: 0.8896 Loss_G: 1.0419 acc: 46.9%\n",
      "[BATCH 89/149] Loss_D: 0.8858 Loss_G: 1.0398 acc: 51.6%\n",
      "[BATCH 90/149] Loss_D: 0.9451 Loss_G: 1.0549 acc: 57.8%\n",
      "[BATCH 91/149] Loss_D: 0.8983 Loss_G: 1.0549 acc: 62.5%\n",
      "[BATCH 92/149] Loss_D: 0.9108 Loss_G: 1.0518 acc: 37.5%\n",
      "[BATCH 93/149] Loss_D: 0.8373 Loss_G: 1.0309 acc: 40.6%\n",
      "[BATCH 94/149] Loss_D: 0.9277 Loss_G: 1.0234 acc: 43.8%\n",
      "[BATCH 95/149] Loss_D: 0.8440 Loss_G: 1.0138 acc: 53.1%\n",
      "[BATCH 96/149] Loss_D: 0.8698 Loss_G: 1.0118 acc: 53.1%\n",
      "[BATCH 97/149] Loss_D: 0.8484 Loss_G: 1.0197 acc: 54.7%\n",
      "[BATCH 98/149] Loss_D: 0.8968 Loss_G: 1.0282 acc: 53.1%\n",
      "[BATCH 99/149] Loss_D: 0.9482 Loss_G: 1.0528 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.8514 Loss_G: 1.0596 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.9267 Loss_G: 1.0676 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.9365 Loss_G: 1.0779 acc: 56.2%\n",
      "[EPOCH 400] TEST ACC is : 53.1%\n",
      "[BATCH 103/149] Loss_D: 0.8792 Loss_G: 1.0714 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.9190 Loss_G: 1.0658 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.9122 Loss_G: 1.0520 acc: 45.3%\n",
      "[BATCH 106/149] Loss_D: 0.9318 Loss_G: 1.0394 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 1.0032 Loss_G: 1.0415 acc: 50.0%\n",
      "[BATCH 108/149] Loss_D: 0.9133 Loss_G: 1.0323 acc: 56.2%\n",
      "[BATCH 109/149] Loss_D: 0.8640 Loss_G: 1.0241 acc: 53.1%\n",
      "[BATCH 110/149] Loss_D: 0.8886 Loss_G: 1.0191 acc: 50.0%\n",
      "[BATCH 111/149] Loss_D: 0.8532 Loss_G: 1.0277 acc: 48.4%\n",
      "[BATCH 112/149] Loss_D: 0.9192 Loss_G: 1.0343 acc: 50.0%\n",
      "[BATCH 113/149] Loss_D: 0.9310 Loss_G: 1.0437 acc: 37.5%\n",
      "[BATCH 114/149] Loss_D: 0.8942 Loss_G: 1.0450 acc: 42.2%\n",
      "[BATCH 115/149] Loss_D: 0.9004 Loss_G: 1.0585 acc: 54.7%\n",
      "[BATCH 116/149] Loss_D: 0.8923 Loss_G: 1.0725 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.9119 Loss_G: 1.0695 acc: 54.7%\n",
      "[BATCH 118/149] Loss_D: 0.9422 Loss_G: 1.0665 acc: 54.7%\n",
      "[BATCH 119/149] Loss_D: 0.9146 Loss_G: 1.0785 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.9182 Loss_G: 1.0770 acc: 45.3%\n",
      "[BATCH 121/149] Loss_D: 0.8723 Loss_G: 1.0823 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.8684 Loss_G: 1.0933 acc: 45.3%\n",
      "[BATCH 123/149] Loss_D: 0.9303 Loss_G: 1.1038 acc: 45.3%\n",
      "[BATCH 124/149] Loss_D: 0.8563 Loss_G: 1.1111 acc: 43.8%\n",
      "[BATCH 125/149] Loss_D: 0.8872 Loss_G: 1.1093 acc: 53.1%\n",
      "[BATCH 126/149] Loss_D: 0.8622 Loss_G: 1.1175 acc: 45.3%\n",
      "[BATCH 127/149] Loss_D: 0.8127 Loss_G: 1.1209 acc: 48.4%\n",
      "[BATCH 128/149] Loss_D: 0.8279 Loss_G: 1.1328 acc: 65.6%\n",
      "[BATCH 129/149] Loss_D: 0.9591 Loss_G: 1.1530 acc: 60.9%\n",
      "[BATCH 130/149] Loss_D: 1.0161 Loss_G: 1.1657 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.8979 Loss_G: 1.1594 acc: 57.8%\n",
      "[BATCH 132/149] Loss_D: 0.8649 Loss_G: 1.1547 acc: 48.4%\n",
      "[BATCH 133/149] Loss_D: 0.9390 Loss_G: 1.1638 acc: 53.1%\n",
      "[BATCH 134/149] Loss_D: 0.8676 Loss_G: 1.1602 acc: 50.0%\n",
      "[BATCH 135/149] Loss_D: 0.8132 Loss_G: 1.1537 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.8467 Loss_G: 1.1303 acc: 54.7%\n",
      "[BATCH 137/149] Loss_D: 0.8179 Loss_G: 1.0684 acc: 51.6%\n",
      "[BATCH 138/149] Loss_D: 0.8756 Loss_G: 0.9909 acc: 48.4%\n",
      "[BATCH 139/149] Loss_D: 0.8666 Loss_G: 0.9548 acc: 43.8%\n",
      "[BATCH 140/149] Loss_D: 0.8555 Loss_G: 0.9028 acc: 43.8%\n",
      "[BATCH 141/149] Loss_D: 0.8848 Loss_G: 0.9003 acc: 48.4%\n",
      "[BATCH 142/149] Loss_D: 0.9537 Loss_G: 0.9185 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.9367 Loss_G: 0.9304 acc: 48.4%\n",
      "[BATCH 144/149] Loss_D: 0.8559 Loss_G: 0.9179 acc: 51.6%\n",
      "[BATCH 145/149] Loss_D: 0.9121 Loss_G: 0.9257 acc: 46.9%\n",
      "[BATCH 146/149] Loss_D: 0.9244 Loss_G: 0.9305 acc: 46.9%\n",
      "[BATCH 147/149] Loss_D: 0.9439 Loss_G: 0.9548 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.8846 Loss_G: 0.9597 acc: 53.1%\n",
      "[BATCH 149/149] Loss_D: 0.9015 Loss_G: 0.9613 acc: 57.8%\n",
      "-----THE [3/50] epoch end-----\n",
      "-----THE [4/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.9298 Loss_G: 0.9622 acc: 50.0%\n",
      "[BATCH 2/149] Loss_D: 0.9207 Loss_G: 0.9710 acc: 56.2%\n",
      "[BATCH 3/149] Loss_D: 0.8719 Loss_G: 0.9660 acc: 53.1%\n",
      "[EPOCH 450] TEST ACC is : 53.9%\n",
      "[BATCH 4/149] Loss_D: 0.9632 Loss_G: 0.9747 acc: 67.2%\n",
      "[BATCH 5/149] Loss_D: 0.9414 Loss_G: 0.9883 acc: 48.4%\n",
      "[BATCH 6/149] Loss_D: 0.9380 Loss_G: 1.0033 acc: 46.9%\n",
      "[BATCH 7/149] Loss_D: 0.8503 Loss_G: 0.9898 acc: 57.8%\n",
      "[BATCH 8/149] Loss_D: 0.9154 Loss_G: 0.9821 acc: 53.1%\n",
      "[BATCH 9/149] Loss_D: 0.9271 Loss_G: 0.9820 acc: 53.1%\n",
      "[BATCH 10/149] Loss_D: 0.9077 Loss_G: 0.9805 acc: 42.2%\n",
      "[BATCH 11/149] Loss_D: 0.8958 Loss_G: 0.9822 acc: 53.1%\n",
      "[BATCH 12/149] Loss_D: 0.9488 Loss_G: 0.9944 acc: 50.0%\n",
      "[BATCH 13/149] Loss_D: 0.8994 Loss_G: 0.9995 acc: 45.3%\n",
      "[BATCH 14/149] Loss_D: 0.8450 Loss_G: 0.9908 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.8934 Loss_G: 0.9876 acc: 48.4%\n",
      "[BATCH 16/149] Loss_D: 0.8343 Loss_G: 0.9804 acc: 51.6%\n",
      "[BATCH 17/149] Loss_D: 0.8938 Loss_G: 0.9763 acc: 43.8%\n",
      "[BATCH 18/149] Loss_D: 0.8834 Loss_G: 0.9751 acc: 43.8%\n",
      "[BATCH 19/149] Loss_D: 0.8237 Loss_G: 0.9717 acc: 40.6%\n",
      "[BATCH 20/149] Loss_D: 0.9219 Loss_G: 0.9771 acc: 43.8%\n",
      "[BATCH 21/149] Loss_D: 0.8546 Loss_G: 0.9790 acc: 43.8%\n",
      "[BATCH 22/149] Loss_D: 0.9142 Loss_G: 0.9925 acc: 45.3%\n",
      "[BATCH 23/149] Loss_D: 0.9172 Loss_G: 1.0088 acc: 48.4%\n",
      "[BATCH 24/149] Loss_D: 0.8668 Loss_G: 1.0126 acc: 53.1%\n",
      "[BATCH 25/149] Loss_D: 0.8757 Loss_G: 1.0041 acc: 59.4%\n",
      "[BATCH 26/149] Loss_D: 0.8900 Loss_G: 1.0002 acc: 46.9%\n",
      "[BATCH 27/149] Loss_D: 0.9875 Loss_G: 1.0212 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.8596 Loss_G: 1.0007 acc: 40.6%\n",
      "[BATCH 29/149] Loss_D: 0.8963 Loss_G: 0.9953 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.8607 Loss_G: 0.9829 acc: 51.6%\n",
      "[BATCH 31/149] Loss_D: 0.8721 Loss_G: 0.9858 acc: 46.9%\n",
      "[BATCH 32/149] Loss_D: 0.8932 Loss_G: 0.9948 acc: 56.2%\n",
      "[BATCH 33/149] Loss_D: 0.8806 Loss_G: 1.0001 acc: 53.1%\n",
      "[BATCH 34/149] Loss_D: 0.8377 Loss_G: 1.0039 acc: 45.3%\n",
      "[BATCH 35/149] Loss_D: 0.9112 Loss_G: 1.0072 acc: 54.7%\n",
      "[BATCH 36/149] Loss_D: 0.9230 Loss_G: 1.0228 acc: 42.2%\n",
      "[BATCH 37/149] Loss_D: 0.8814 Loss_G: 1.0374 acc: 48.4%\n",
      "[BATCH 38/149] Loss_D: 0.9456 Loss_G: 1.0509 acc: 54.7%\n",
      "[BATCH 39/149] Loss_D: 0.9171 Loss_G: 1.0515 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.9138 Loss_G: 1.0431 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.8591 Loss_G: 1.0192 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.8755 Loss_G: 1.0053 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.8797 Loss_G: 0.9962 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.8835 Loss_G: 0.9957 acc: 54.7%\n",
      "[BATCH 45/149] Loss_D: 0.9523 Loss_G: 1.0010 acc: 53.1%\n",
      "[BATCH 46/149] Loss_D: 0.9071 Loss_G: 1.0087 acc: 43.8%\n",
      "[BATCH 47/149] Loss_D: 0.8972 Loss_G: 1.0095 acc: 54.7%\n",
      "[BATCH 48/149] Loss_D: 0.8773 Loss_G: 0.9987 acc: 43.8%\n",
      "[BATCH 49/149] Loss_D: 0.8819 Loss_G: 1.0010 acc: 51.6%\n",
      "[BATCH 50/149] Loss_D: 0.9392 Loss_G: 1.0130 acc: 51.6%\n",
      "[BATCH 51/149] Loss_D: 0.9330 Loss_G: 1.0301 acc: 53.1%\n",
      "[BATCH 52/149] Loss_D: 0.8744 Loss_G: 1.0257 acc: 39.1%\n",
      "[BATCH 53/149] Loss_D: 0.9204 Loss_G: 1.0257 acc: 64.1%\n",
      "[EPOCH 500] TEST ACC is : 53.9%\n",
      "[BATCH 54/149] Loss_D: 0.8899 Loss_G: 1.0166 acc: 54.7%\n",
      "[BATCH 55/149] Loss_D: 0.8856 Loss_G: 1.0135 acc: 57.8%\n",
      "[BATCH 56/149] Loss_D: 0.8605 Loss_G: 1.0001 acc: 42.2%\n",
      "[BATCH 57/149] Loss_D: 0.8375 Loss_G: 0.9901 acc: 45.3%\n",
      "[BATCH 58/149] Loss_D: 0.9093 Loss_G: 0.9998 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.8523 Loss_G: 0.9940 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.8502 Loss_G: 0.9826 acc: 56.2%\n",
      "[BATCH 61/149] Loss_D: 0.8760 Loss_G: 0.9865 acc: 56.2%\n",
      "[BATCH 62/149] Loss_D: 0.8987 Loss_G: 0.9888 acc: 48.4%\n",
      "[BATCH 63/149] Loss_D: 0.9190 Loss_G: 0.9981 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.8635 Loss_G: 0.9985 acc: 51.6%\n",
      "[BATCH 65/149] Loss_D: 0.9565 Loss_G: 1.0077 acc: 56.2%\n",
      "[BATCH 66/149] Loss_D: 0.8901 Loss_G: 1.0030 acc: 54.7%\n",
      "[BATCH 67/149] Loss_D: 0.8710 Loss_G: 1.0049 acc: 54.7%\n",
      "[BATCH 68/149] Loss_D: 0.8826 Loss_G: 1.0009 acc: 53.1%\n",
      "[BATCH 69/149] Loss_D: 0.8351 Loss_G: 1.0039 acc: 45.3%\n",
      "[BATCH 70/149] Loss_D: 0.8648 Loss_G: 0.9982 acc: 50.0%\n",
      "[BATCH 71/149] Loss_D: 0.8817 Loss_G: 1.0032 acc: 42.2%\n",
      "[BATCH 72/149] Loss_D: 0.8845 Loss_G: 1.0018 acc: 54.7%\n",
      "[BATCH 73/149] Loss_D: 0.8564 Loss_G: 1.0003 acc: 56.2%\n",
      "[BATCH 74/149] Loss_D: 0.9105 Loss_G: 1.0016 acc: 45.3%\n",
      "[BATCH 75/149] Loss_D: 0.9060 Loss_G: 1.0093 acc: 48.4%\n",
      "[BATCH 76/149] Loss_D: 0.8793 Loss_G: 1.0051 acc: 32.8%\n",
      "[BATCH 77/149] Loss_D: 0.8427 Loss_G: 0.9900 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.8631 Loss_G: 0.9819 acc: 54.7%\n",
      "[BATCH 79/149] Loss_D: 0.8371 Loss_G: 0.9696 acc: 42.2%\n",
      "[BATCH 80/149] Loss_D: 0.8526 Loss_G: 0.9786 acc: 50.0%\n",
      "[BATCH 81/149] Loss_D: 0.8928 Loss_G: 0.9545 acc: 53.1%\n",
      "[BATCH 82/149] Loss_D: 0.8652 Loss_G: 0.9683 acc: 50.0%\n",
      "[BATCH 83/149] Loss_D: 0.8588 Loss_G: 0.9518 acc: 42.2%\n",
      "[BATCH 84/149] Loss_D: 0.9144 Loss_G: 0.9605 acc: 51.6%\n",
      "[BATCH 85/149] Loss_D: 0.8993 Loss_G: 0.9733 acc: 46.9%\n",
      "[BATCH 86/149] Loss_D: 0.9049 Loss_G: 0.9766 acc: 50.0%\n",
      "[BATCH 87/149] Loss_D: 0.9106 Loss_G: 0.9797 acc: 50.0%\n",
      "[BATCH 88/149] Loss_D: 0.8574 Loss_G: 0.9750 acc: 51.6%\n",
      "[BATCH 89/149] Loss_D: 0.8730 Loss_G: 0.9694 acc: 46.9%\n",
      "[BATCH 90/149] Loss_D: 0.9226 Loss_G: 0.9707 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.8529 Loss_G: 0.9651 acc: 54.7%\n",
      "[BATCH 92/149] Loss_D: 0.8438 Loss_G: 0.9636 acc: 43.8%\n",
      "[BATCH 93/149] Loss_D: 0.8591 Loss_G: 0.9643 acc: 53.1%\n",
      "[BATCH 94/149] Loss_D: 0.8334 Loss_G: 0.9580 acc: 56.2%\n",
      "[BATCH 95/149] Loss_D: 0.8957 Loss_G: 0.9649 acc: 45.3%\n",
      "[BATCH 96/149] Loss_D: 0.8622 Loss_G: 0.9610 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.8611 Loss_G: 0.9596 acc: 46.9%\n",
      "[BATCH 98/149] Loss_D: 0.8671 Loss_G: 0.9418 acc: 48.4%\n",
      "[BATCH 99/149] Loss_D: 0.8991 Loss_G: 0.9489 acc: 45.3%\n",
      "[BATCH 100/149] Loss_D: 0.9421 Loss_G: 0.9591 acc: 50.0%\n",
      "[BATCH 101/149] Loss_D: 0.8911 Loss_G: 0.9624 acc: 48.4%\n",
      "[BATCH 102/149] Loss_D: 0.9039 Loss_G: 0.9663 acc: 46.9%\n",
      "[BATCH 103/149] Loss_D: 0.9477 Loss_G: 0.9807 acc: 45.3%\n",
      "[EPOCH 550] TEST ACC is : 53.9%\n",
      "[BATCH 104/149] Loss_D: 0.8799 Loss_G: 0.9743 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.8647 Loss_G: 0.9641 acc: 48.4%\n",
      "[BATCH 106/149] Loss_D: 0.9247 Loss_G: 0.9726 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.8496 Loss_G: 0.9739 acc: 39.1%\n",
      "[BATCH 108/149] Loss_D: 0.8816 Loss_G: 0.9625 acc: 51.6%\n",
      "[BATCH 109/149] Loss_D: 0.8546 Loss_G: 0.9624 acc: 50.0%\n",
      "[BATCH 110/149] Loss_D: 0.9160 Loss_G: 0.9579 acc: 46.9%\n",
      "[BATCH 111/149] Loss_D: 0.9076 Loss_G: 0.9727 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8711 Loss_G: 0.9628 acc: 46.9%\n",
      "[BATCH 113/149] Loss_D: 0.8627 Loss_G: 0.9613 acc: 54.7%\n",
      "[BATCH 114/149] Loss_D: 0.8723 Loss_G: 0.9659 acc: 57.8%\n",
      "[BATCH 115/149] Loss_D: 0.9230 Loss_G: 0.9648 acc: 51.6%\n",
      "[BATCH 116/149] Loss_D: 0.8696 Loss_G: 0.9526 acc: 48.4%\n",
      "[BATCH 117/149] Loss_D: 0.8761 Loss_G: 0.9667 acc: 50.0%\n",
      "[BATCH 118/149] Loss_D: 0.9308 Loss_G: 0.9699 acc: 54.7%\n",
      "[BATCH 119/149] Loss_D: 0.8950 Loss_G: 0.9718 acc: 48.4%\n",
      "[BATCH 120/149] Loss_D: 0.8745 Loss_G: 0.9686 acc: 53.1%\n",
      "[BATCH 121/149] Loss_D: 0.9364 Loss_G: 0.9724 acc: 53.1%\n",
      "[BATCH 122/149] Loss_D: 0.8841 Loss_G: 0.9681 acc: 53.1%\n",
      "[BATCH 123/149] Loss_D: 0.8303 Loss_G: 0.9676 acc: 56.2%\n",
      "[BATCH 124/149] Loss_D: 0.8029 Loss_G: 0.9442 acc: 53.1%\n",
      "[BATCH 125/149] Loss_D: 0.8690 Loss_G: 0.9472 acc: 56.2%\n",
      "[BATCH 126/149] Loss_D: 0.8624 Loss_G: 0.9428 acc: 54.7%\n",
      "[BATCH 127/149] Loss_D: 0.8833 Loss_G: 0.9453 acc: 48.4%\n",
      "[BATCH 128/149] Loss_D: 0.8592 Loss_G: 0.9463 acc: 54.7%\n",
      "[BATCH 129/149] Loss_D: 0.8471 Loss_G: 0.9382 acc: 45.3%\n",
      "[BATCH 130/149] Loss_D: 0.8932 Loss_G: 0.9421 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.8498 Loss_G: 0.9402 acc: 45.3%\n",
      "[BATCH 132/149] Loss_D: 0.8530 Loss_G: 0.9347 acc: 46.9%\n",
      "[BATCH 133/149] Loss_D: 0.9098 Loss_G: 0.9431 acc: 51.6%\n",
      "[BATCH 134/149] Loss_D: 0.9581 Loss_G: 0.9520 acc: 54.7%\n",
      "[BATCH 135/149] Loss_D: 0.9223 Loss_G: 0.9589 acc: 45.3%\n",
      "[BATCH 136/149] Loss_D: 0.9156 Loss_G: 0.9629 acc: 51.6%\n",
      "[BATCH 137/149] Loss_D: 0.8636 Loss_G: 0.9461 acc: 53.1%\n",
      "[BATCH 138/149] Loss_D: 0.8630 Loss_G: 0.9411 acc: 51.6%\n",
      "[BATCH 139/149] Loss_D: 0.9092 Loss_G: 0.9428 acc: 57.8%\n",
      "[BATCH 140/149] Loss_D: 0.8391 Loss_G: 0.9432 acc: 50.0%\n",
      "[BATCH 141/149] Loss_D: 0.9211 Loss_G: 0.9582 acc: 46.9%\n",
      "[BATCH 142/149] Loss_D: 0.8693 Loss_G: 0.9479 acc: 54.7%\n",
      "[BATCH 143/149] Loss_D: 0.8604 Loss_G: 0.9379 acc: 60.9%\n",
      "[BATCH 144/149] Loss_D: 0.9034 Loss_G: 0.9457 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.9066 Loss_G: 0.9472 acc: 59.4%\n",
      "[BATCH 146/149] Loss_D: 0.8832 Loss_G: 0.9532 acc: 59.4%\n",
      "[BATCH 147/149] Loss_D: 0.9022 Loss_G: 0.9586 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.9119 Loss_G: 0.9652 acc: 54.7%\n",
      "[BATCH 149/149] Loss_D: 0.8636 Loss_G: 0.9582 acc: 56.2%\n",
      "-----THE [4/50] epoch end-----\n",
      "-----THE [5/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.9533 Loss_G: 0.9603 acc: 50.0%\n",
      "[BATCH 2/149] Loss_D: 0.8661 Loss_G: 0.9466 acc: 39.1%\n",
      "[BATCH 3/149] Loss_D: 0.8884 Loss_G: 0.9495 acc: 42.2%\n",
      "[BATCH 4/149] Loss_D: 0.8446 Loss_G: 0.9399 acc: 51.6%\n",
      "[EPOCH 600] TEST ACC is : 54.1%\n",
      "[BATCH 5/149] Loss_D: 0.8821 Loss_G: 0.9356 acc: 43.8%\n",
      "[BATCH 6/149] Loss_D: 0.8588 Loss_G: 0.9362 acc: 53.1%\n",
      "[BATCH 7/149] Loss_D: 0.8704 Loss_G: 0.9369 acc: 59.4%\n",
      "[BATCH 8/149] Loss_D: 0.8690 Loss_G: 0.9398 acc: 57.8%\n",
      "[BATCH 9/149] Loss_D: 0.9353 Loss_G: 0.9527 acc: 70.3%\n",
      "[BATCH 10/149] Loss_D: 0.8533 Loss_G: 0.9478 acc: 46.9%\n",
      "[BATCH 11/149] Loss_D: 0.8606 Loss_G: 0.9386 acc: 53.1%\n",
      "[BATCH 12/149] Loss_D: 0.8873 Loss_G: 0.9354 acc: 56.2%\n",
      "[BATCH 13/149] Loss_D: 0.8669 Loss_G: 0.9391 acc: 46.9%\n",
      "[BATCH 14/149] Loss_D: 0.8606 Loss_G: 0.9369 acc: 51.6%\n",
      "[BATCH 15/149] Loss_D: 0.8751 Loss_G: 0.9372 acc: 46.9%\n",
      "[BATCH 16/149] Loss_D: 0.8239 Loss_G: 0.9278 acc: 51.6%\n",
      "[BATCH 17/149] Loss_D: 0.8417 Loss_G: 0.9285 acc: 37.5%\n",
      "[BATCH 18/149] Loss_D: 0.7986 Loss_G: 0.9226 acc: 51.6%\n",
      "[BATCH 19/149] Loss_D: 0.9675 Loss_G: 0.9493 acc: 46.9%\n",
      "[BATCH 20/149] Loss_D: 0.8773 Loss_G: 0.9622 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8096 Loss_G: 0.9511 acc: 43.8%\n",
      "[BATCH 22/149] Loss_D: 0.8021 Loss_G: 0.9283 acc: 51.6%\n",
      "[BATCH 23/149] Loss_D: 0.9024 Loss_G: 0.9244 acc: 60.9%\n",
      "[BATCH 24/149] Loss_D: 0.9111 Loss_G: 0.9312 acc: 54.7%\n",
      "[BATCH 25/149] Loss_D: 0.9597 Loss_G: 0.9535 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.8496 Loss_G: 0.9605 acc: 54.7%\n",
      "[BATCH 27/149] Loss_D: 0.8663 Loss_G: 0.9455 acc: 37.5%\n",
      "[BATCH 28/149] Loss_D: 0.8373 Loss_G: 0.9404 acc: 53.1%\n",
      "[BATCH 29/149] Loss_D: 0.9570 Loss_G: 0.9450 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.8614 Loss_G: 0.9442 acc: 60.9%\n",
      "[BATCH 31/149] Loss_D: 0.8672 Loss_G: 0.9476 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.9264 Loss_G: 0.9528 acc: 51.6%\n",
      "[BATCH 33/149] Loss_D: 0.8829 Loss_G: 0.9583 acc: 67.2%\n",
      "[BATCH 34/149] Loss_D: 0.8775 Loss_G: 0.9506 acc: 48.4%\n",
      "[BATCH 35/149] Loss_D: 0.8667 Loss_G: 0.9462 acc: 54.7%\n",
      "[BATCH 36/149] Loss_D: 0.8420 Loss_G: 0.9395 acc: 51.6%\n",
      "[BATCH 37/149] Loss_D: 0.8598 Loss_G: 0.9341 acc: 42.2%\n",
      "[BATCH 38/149] Loss_D: 0.8728 Loss_G: 0.9315 acc: 53.1%\n",
      "[BATCH 39/149] Loss_D: 0.8779 Loss_G: 0.9386 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.8568 Loss_G: 0.9368 acc: 46.9%\n",
      "[BATCH 41/149] Loss_D: 0.9355 Loss_G: 0.9604 acc: 54.7%\n",
      "[BATCH 42/149] Loss_D: 0.9369 Loss_G: 0.9643 acc: 51.6%\n",
      "[BATCH 43/149] Loss_D: 0.8280 Loss_G: 0.9514 acc: 50.0%\n",
      "[BATCH 44/149] Loss_D: 0.8893 Loss_G: 0.9449 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.9409 Loss_G: 0.9477 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.9314 Loss_G: 0.9540 acc: 59.4%\n",
      "[BATCH 47/149] Loss_D: 0.8868 Loss_G: 0.9544 acc: 53.1%\n",
      "[BATCH 48/149] Loss_D: 0.8857 Loss_G: 0.9494 acc: 46.9%\n",
      "[BATCH 49/149] Loss_D: 0.8057 Loss_G: 0.9323 acc: 46.9%\n",
      "[BATCH 50/149] Loss_D: 0.9054 Loss_G: 0.9412 acc: 46.9%\n",
      "[BATCH 51/149] Loss_D: 0.8885 Loss_G: 0.9454 acc: 56.2%\n",
      "[BATCH 52/149] Loss_D: 0.8620 Loss_G: 0.9417 acc: 59.4%\n",
      "[BATCH 53/149] Loss_D: 0.8641 Loss_G: 0.9416 acc: 53.1%\n",
      "[BATCH 54/149] Loss_D: 0.8618 Loss_G: 0.9356 acc: 50.0%\n",
      "[EPOCH 650] TEST ACC is : 53.3%\n",
      "[BATCH 55/149] Loss_D: 0.8538 Loss_G: 0.9322 acc: 54.7%\n",
      "[BATCH 56/149] Loss_D: 0.8759 Loss_G: 0.9315 acc: 51.6%\n",
      "[BATCH 57/149] Loss_D: 0.8706 Loss_G: 0.9334 acc: 46.9%\n",
      "[BATCH 58/149] Loss_D: 0.8682 Loss_G: 0.9263 acc: 56.2%\n",
      "[BATCH 59/149] Loss_D: 0.8661 Loss_G: 0.9317 acc: 54.7%\n",
      "[BATCH 60/149] Loss_D: 0.9148 Loss_G: 0.9386 acc: 48.4%\n",
      "[BATCH 61/149] Loss_D: 0.9169 Loss_G: 0.9489 acc: 45.3%\n",
      "[BATCH 62/149] Loss_D: 0.9148 Loss_G: 0.9499 acc: 50.0%\n",
      "[BATCH 63/149] Loss_D: 0.8161 Loss_G: 0.9362 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.9199 Loss_G: 0.9456 acc: 43.8%\n",
      "[BATCH 65/149] Loss_D: 0.8276 Loss_G: 0.9356 acc: 53.1%\n",
      "[BATCH 66/149] Loss_D: 0.8935 Loss_G: 0.9378 acc: 50.0%\n",
      "[BATCH 67/149] Loss_D: 0.9482 Loss_G: 0.9581 acc: 45.3%\n",
      "[BATCH 68/149] Loss_D: 0.8376 Loss_G: 0.9440 acc: 51.6%\n",
      "[BATCH 69/149] Loss_D: 0.8058 Loss_G: 0.9360 acc: 48.4%\n",
      "[BATCH 70/149] Loss_D: 0.8991 Loss_G: 0.9286 acc: 42.2%\n",
      "[BATCH 71/149] Loss_D: 0.8348 Loss_G: 0.9288 acc: 46.9%\n",
      "[BATCH 72/149] Loss_D: 0.9547 Loss_G: 0.9444 acc: 48.4%\n",
      "[BATCH 73/149] Loss_D: 0.8736 Loss_G: 0.9475 acc: 54.7%\n",
      "[BATCH 74/149] Loss_D: 0.8556 Loss_G: 0.9481 acc: 56.2%\n",
      "[BATCH 75/149] Loss_D: 0.8615 Loss_G: 0.9433 acc: 37.5%\n",
      "[BATCH 76/149] Loss_D: 0.8541 Loss_G: 0.9378 acc: 51.6%\n",
      "[BATCH 77/149] Loss_D: 0.8533 Loss_G: 0.9346 acc: 45.3%\n",
      "[BATCH 78/149] Loss_D: 0.8098 Loss_G: 0.9307 acc: 46.9%\n",
      "[BATCH 79/149] Loss_D: 0.9302 Loss_G: 0.9357 acc: 48.4%\n",
      "[BATCH 80/149] Loss_D: 0.8589 Loss_G: 0.9348 acc: 53.1%\n",
      "[BATCH 81/149] Loss_D: 0.8738 Loss_G: 0.9352 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.8538 Loss_G: 0.9367 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.9010 Loss_G: 0.9445 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.8790 Loss_G: 0.9390 acc: 53.1%\n",
      "[BATCH 85/149] Loss_D: 0.9064 Loss_G: 0.9412 acc: 45.3%\n",
      "[BATCH 86/149] Loss_D: 0.8694 Loss_G: 0.9394 acc: 35.9%\n",
      "[BATCH 87/149] Loss_D: 0.8894 Loss_G: 0.9374 acc: 46.9%\n",
      "[BATCH 88/149] Loss_D: 0.8681 Loss_G: 0.9424 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.8228 Loss_G: 0.9331 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.8668 Loss_G: 0.9327 acc: 45.3%\n",
      "[BATCH 91/149] Loss_D: 0.9034 Loss_G: 0.9394 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.8979 Loss_G: 0.9491 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.9035 Loss_G: 0.9460 acc: 56.2%\n",
      "[BATCH 94/149] Loss_D: 0.8322 Loss_G: 0.9317 acc: 46.9%\n",
      "[BATCH 95/149] Loss_D: 0.8578 Loss_G: 0.9222 acc: 43.8%\n",
      "[BATCH 96/149] Loss_D: 0.9040 Loss_G: 0.9245 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.8588 Loss_G: 0.9261 acc: 51.6%\n",
      "[BATCH 98/149] Loss_D: 0.8569 Loss_G: 0.9301 acc: 51.6%\n",
      "[BATCH 99/149] Loss_D: 0.8871 Loss_G: 0.9339 acc: 45.3%\n",
      "[BATCH 100/149] Loss_D: 0.8457 Loss_G: 0.9339 acc: 50.0%\n",
      "[BATCH 101/149] Loss_D: 0.8480 Loss_G: 0.9284 acc: 57.8%\n",
      "[BATCH 102/149] Loss_D: 0.8812 Loss_G: 0.9291 acc: 53.1%\n",
      "[BATCH 103/149] Loss_D: 0.9129 Loss_G: 0.9341 acc: 59.4%\n",
      "[BATCH 104/149] Loss_D: 0.8946 Loss_G: 0.9359 acc: 42.2%\n",
      "[EPOCH 700] TEST ACC is : 54.5%\n",
      "[BATCH 105/149] Loss_D: 0.8623 Loss_G: 0.9336 acc: 53.1%\n",
      "[BATCH 106/149] Loss_D: 0.8859 Loss_G: 0.9340 acc: 51.6%\n",
      "[BATCH 107/149] Loss_D: 0.9719 Loss_G: 0.9560 acc: 53.1%\n",
      "[BATCH 108/149] Loss_D: 0.8740 Loss_G: 0.9545 acc: 50.0%\n",
      "[BATCH 109/149] Loss_D: 0.8579 Loss_G: 0.9433 acc: 46.9%\n",
      "[BATCH 110/149] Loss_D: 0.8904 Loss_G: 0.9384 acc: 50.0%\n",
      "[BATCH 111/149] Loss_D: 0.8837 Loss_G: 0.9390 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.9115 Loss_G: 0.9374 acc: 51.6%\n",
      "[BATCH 113/149] Loss_D: 0.8682 Loss_G: 0.9382 acc: 54.7%\n",
      "[BATCH 114/149] Loss_D: 0.8480 Loss_G: 0.9354 acc: 57.8%\n",
      "[BATCH 115/149] Loss_D: 0.8725 Loss_G: 0.9333 acc: 57.8%\n",
      "[BATCH 116/149] Loss_D: 0.8747 Loss_G: 0.9385 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.8947 Loss_G: 0.9570 acc: 51.6%\n",
      "[BATCH 118/149] Loss_D: 0.8227 Loss_G: 0.9408 acc: 45.3%\n",
      "[BATCH 119/149] Loss_D: 0.8820 Loss_G: 0.9347 acc: 54.7%\n",
      "[BATCH 120/149] Loss_D: 0.8288 Loss_G: 0.9320 acc: 46.9%\n",
      "[BATCH 121/149] Loss_D: 0.9245 Loss_G: 0.9327 acc: 53.1%\n",
      "[BATCH 122/149] Loss_D: 0.8641 Loss_G: 0.9355 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.9096 Loss_G: 0.9426 acc: 43.8%\n",
      "[BATCH 124/149] Loss_D: 0.8903 Loss_G: 0.9433 acc: 51.6%\n",
      "[BATCH 125/149] Loss_D: 0.8948 Loss_G: 0.9456 acc: 50.0%\n",
      "[BATCH 126/149] Loss_D: 0.8371 Loss_G: 0.9397 acc: 34.4%\n",
      "[BATCH 127/149] Loss_D: 0.8623 Loss_G: 0.9245 acc: 46.9%\n",
      "[BATCH 128/149] Loss_D: 0.9003 Loss_G: 0.9359 acc: 48.4%\n",
      "[BATCH 129/149] Loss_D: 0.9368 Loss_G: 0.9512 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.8240 Loss_G: 0.9481 acc: 59.4%\n",
      "[BATCH 131/149] Loss_D: 0.8467 Loss_G: 0.9419 acc: 59.4%\n",
      "[BATCH 132/149] Loss_D: 0.9007 Loss_G: 0.9391 acc: 45.3%\n",
      "[BATCH 133/149] Loss_D: 0.8505 Loss_G: 0.9329 acc: 50.0%\n",
      "[BATCH 134/149] Loss_D: 0.8995 Loss_G: 0.9381 acc: 56.2%\n",
      "[BATCH 135/149] Loss_D: 0.8714 Loss_G: 0.9476 acc: 45.3%\n",
      "[BATCH 136/149] Loss_D: 0.8628 Loss_G: 0.9450 acc: 56.2%\n",
      "[BATCH 137/149] Loss_D: 0.8918 Loss_G: 0.9449 acc: 53.1%\n",
      "[BATCH 138/149] Loss_D: 0.9336 Loss_G: 0.9453 acc: 48.4%\n",
      "[BATCH 139/149] Loss_D: 0.8712 Loss_G: 0.9423 acc: 51.6%\n",
      "[BATCH 140/149] Loss_D: 0.8752 Loss_G: 0.9456 acc: 51.6%\n",
      "[BATCH 141/149] Loss_D: 0.8408 Loss_G: 0.9424 acc: 57.8%\n",
      "[BATCH 142/149] Loss_D: 0.8424 Loss_G: 0.9413 acc: 43.8%\n",
      "[BATCH 143/149] Loss_D: 0.8227 Loss_G: 0.9284 acc: 51.6%\n",
      "[BATCH 144/149] Loss_D: 0.8695 Loss_G: 0.9314 acc: 50.0%\n",
      "[BATCH 145/149] Loss_D: 0.8777 Loss_G: 0.9362 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.8121 Loss_G: 0.9322 acc: 50.0%\n",
      "[BATCH 147/149] Loss_D: 0.8912 Loss_G: 0.9292 acc: 42.2%\n",
      "[BATCH 148/149] Loss_D: 0.9527 Loss_G: 0.9447 acc: 53.1%\n",
      "[BATCH 149/149] Loss_D: 0.9308 Loss_G: 0.9528 acc: 43.8%\n",
      "-----THE [5/50] epoch end-----\n",
      "-----THE [6/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8447 Loss_G: 0.9407 acc: 51.6%\n",
      "[BATCH 2/149] Loss_D: 0.8261 Loss_G: 0.9273 acc: 51.6%\n",
      "[BATCH 3/149] Loss_D: 0.7971 Loss_G: 0.9214 acc: 46.9%\n",
      "[BATCH 4/149] Loss_D: 0.9642 Loss_G: 0.9357 acc: 53.1%\n",
      "[BATCH 5/149] Loss_D: 0.8561 Loss_G: 0.9382 acc: 50.0%\n",
      "[EPOCH 750] TEST ACC is : 53.1%\n",
      "[BATCH 6/149] Loss_D: 0.8694 Loss_G: 0.9366 acc: 46.9%\n",
      "[BATCH 7/149] Loss_D: 0.8528 Loss_G: 0.9298 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.8561 Loss_G: 0.9267 acc: 40.6%\n",
      "[BATCH 9/149] Loss_D: 0.8639 Loss_G: 0.9264 acc: 57.8%\n",
      "[BATCH 10/149] Loss_D: 0.8642 Loss_G: 0.9288 acc: 54.7%\n",
      "[BATCH 11/149] Loss_D: 0.8440 Loss_G: 0.9259 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.8569 Loss_G: 0.9274 acc: 46.9%\n",
      "[BATCH 13/149] Loss_D: 0.8255 Loss_G: 0.9224 acc: 53.1%\n",
      "[BATCH 14/149] Loss_D: 0.8800 Loss_G: 0.9277 acc: 46.9%\n",
      "[BATCH 15/149] Loss_D: 0.8656 Loss_G: 0.9308 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.8563 Loss_G: 0.9290 acc: 48.4%\n",
      "[BATCH 17/149] Loss_D: 0.8942 Loss_G: 0.9302 acc: 56.2%\n",
      "[BATCH 18/149] Loss_D: 0.9437 Loss_G: 0.9527 acc: 51.6%\n",
      "[BATCH 19/149] Loss_D: 0.8384 Loss_G: 0.9420 acc: 46.9%\n",
      "[BATCH 20/149] Loss_D: 0.9228 Loss_G: 0.9447 acc: 54.7%\n",
      "[BATCH 21/149] Loss_D: 0.8649 Loss_G: 0.9409 acc: 45.3%\n",
      "[BATCH 22/149] Loss_D: 0.8636 Loss_G: 0.9332 acc: 43.8%\n",
      "[BATCH 23/149] Loss_D: 0.8559 Loss_G: 0.9335 acc: 51.6%\n",
      "[BATCH 24/149] Loss_D: 0.8544 Loss_G: 0.9316 acc: 57.8%\n",
      "[BATCH 25/149] Loss_D: 0.8311 Loss_G: 0.9238 acc: 45.3%\n",
      "[BATCH 26/149] Loss_D: 0.9046 Loss_G: 0.9268 acc: 59.4%\n",
      "[BATCH 27/149] Loss_D: 0.9071 Loss_G: 0.9383 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.8836 Loss_G: 0.9340 acc: 42.2%\n",
      "[BATCH 29/149] Loss_D: 0.8661 Loss_G: 0.9338 acc: 48.4%\n",
      "[BATCH 30/149] Loss_D: 0.8318 Loss_G: 0.9253 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.8888 Loss_G: 0.9254 acc: 53.1%\n",
      "[BATCH 32/149] Loss_D: 0.8294 Loss_G: 0.9227 acc: 57.8%\n",
      "[BATCH 33/149] Loss_D: 0.8361 Loss_G: 0.9200 acc: 56.2%\n",
      "[BATCH 34/149] Loss_D: 0.8242 Loss_G: 0.9177 acc: 48.4%\n",
      "[BATCH 35/149] Loss_D: 0.8230 Loss_G: 0.9117 acc: 53.1%\n",
      "[BATCH 36/149] Loss_D: 0.8529 Loss_G: 0.9118 acc: 48.4%\n",
      "[BATCH 37/149] Loss_D: 0.8528 Loss_G: 0.9164 acc: 46.9%\n",
      "[BATCH 38/149] Loss_D: 0.8482 Loss_G: 0.9157 acc: 40.6%\n",
      "[BATCH 39/149] Loss_D: 0.7946 Loss_G: 0.9109 acc: 50.0%\n",
      "[BATCH 40/149] Loss_D: 0.9260 Loss_G: 0.9172 acc: 46.9%\n",
      "[BATCH 41/149] Loss_D: 0.8638 Loss_G: 0.9226 acc: 46.9%\n",
      "[BATCH 42/149] Loss_D: 0.8589 Loss_G: 0.9236 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.8923 Loss_G: 0.9280 acc: 40.6%\n",
      "[BATCH 44/149] Loss_D: 0.8203 Loss_G: 0.9212 acc: 57.8%\n",
      "[BATCH 45/149] Loss_D: 0.8660 Loss_G: 0.9219 acc: 51.6%\n",
      "[BATCH 46/149] Loss_D: 0.8335 Loss_G: 0.9192 acc: 46.9%\n",
      "[BATCH 47/149] Loss_D: 0.8700 Loss_G: 0.9218 acc: 54.7%\n",
      "[BATCH 48/149] Loss_D: 0.8698 Loss_G: 0.9235 acc: 51.6%\n",
      "[BATCH 49/149] Loss_D: 0.8921 Loss_G: 0.9355 acc: 54.7%\n",
      "[BATCH 50/149] Loss_D: 0.8591 Loss_G: 0.9295 acc: 51.6%\n",
      "[BATCH 51/149] Loss_D: 0.8177 Loss_G: 0.9216 acc: 53.1%\n",
      "[BATCH 52/149] Loss_D: 0.8423 Loss_G: 0.9124 acc: 45.3%\n",
      "[BATCH 53/149] Loss_D: 0.8210 Loss_G: 0.9139 acc: 35.9%\n",
      "[BATCH 54/149] Loss_D: 0.8579 Loss_G: 0.9176 acc: 43.8%\n",
      "[BATCH 55/149] Loss_D: 0.8141 Loss_G: 0.9144 acc: 51.6%\n",
      "[EPOCH 800] TEST ACC is : 54.3%\n",
      "[BATCH 56/149] Loss_D: 0.8191 Loss_G: 0.9102 acc: 56.2%\n",
      "[BATCH 57/149] Loss_D: 0.8133 Loss_G: 0.9070 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.9168 Loss_G: 0.9195 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.8482 Loss_G: 0.9221 acc: 45.3%\n",
      "[BATCH 60/149] Loss_D: 0.8737 Loss_G: 0.9287 acc: 56.2%\n",
      "[BATCH 61/149] Loss_D: 0.8882 Loss_G: 0.9219 acc: 48.4%\n",
      "[BATCH 62/149] Loss_D: 0.8833 Loss_G: 0.9285 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.8410 Loss_G: 0.9248 acc: 54.7%\n",
      "[BATCH 64/149] Loss_D: 0.8755 Loss_G: 0.9232 acc: 56.2%\n",
      "[BATCH 65/149] Loss_D: 0.8560 Loss_G: 0.9282 acc: 56.2%\n",
      "[BATCH 66/149] Loss_D: 1.0071 Loss_G: 0.9625 acc: 48.4%\n",
      "[BATCH 67/149] Loss_D: 0.8770 Loss_G: 0.9568 acc: 59.4%\n",
      "[BATCH 68/149] Loss_D: 0.8774 Loss_G: 0.9473 acc: 53.1%\n",
      "[BATCH 69/149] Loss_D: 0.8264 Loss_G: 0.9319 acc: 46.9%\n",
      "[BATCH 70/149] Loss_D: 0.9342 Loss_G: 0.9314 acc: 54.7%\n",
      "[BATCH 71/149] Loss_D: 0.8625 Loss_G: 0.9335 acc: 56.2%\n",
      "[BATCH 72/149] Loss_D: 0.8593 Loss_G: 0.9291 acc: 42.2%\n",
      "[BATCH 73/149] Loss_D: 0.9158 Loss_G: 0.9341 acc: 51.6%\n",
      "[BATCH 74/149] Loss_D: 0.9014 Loss_G: 0.9454 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.8843 Loss_G: 0.9476 acc: 57.8%\n",
      "[BATCH 76/149] Loss_D: 0.8411 Loss_G: 0.9349 acc: 50.0%\n",
      "[BATCH 77/149] Loss_D: 0.8478 Loss_G: 0.9293 acc: 48.4%\n",
      "[BATCH 78/149] Loss_D: 0.8914 Loss_G: 0.9294 acc: 48.4%\n",
      "[BATCH 79/149] Loss_D: 0.8326 Loss_G: 0.9263 acc: 46.9%\n",
      "[BATCH 80/149] Loss_D: 0.8779 Loss_G: 0.9374 acc: 48.4%\n",
      "[BATCH 81/149] Loss_D: 0.8796 Loss_G: 0.9410 acc: 54.7%\n",
      "[BATCH 82/149] Loss_D: 0.8808 Loss_G: 0.9411 acc: 50.0%\n",
      "[BATCH 83/149] Loss_D: 0.8117 Loss_G: 0.9229 acc: 53.1%\n",
      "[BATCH 84/149] Loss_D: 0.8672 Loss_G: 0.9229 acc: 56.2%\n",
      "[BATCH 85/149] Loss_D: 0.8551 Loss_G: 0.9193 acc: 53.1%\n",
      "[BATCH 86/149] Loss_D: 0.8772 Loss_G: 0.9229 acc: 57.8%\n",
      "[BATCH 87/149] Loss_D: 0.8654 Loss_G: 0.9310 acc: 48.4%\n",
      "[BATCH 88/149] Loss_D: 0.9221 Loss_G: 0.9369 acc: 43.8%\n",
      "[BATCH 89/149] Loss_D: 0.8433 Loss_G: 0.9324 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.8272 Loss_G: 0.9200 acc: 40.6%\n",
      "[BATCH 91/149] Loss_D: 0.9434 Loss_G: 0.9370 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.9082 Loss_G: 0.9403 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.8728 Loss_G: 0.9449 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 0.8752 Loss_G: 0.9393 acc: 46.9%\n",
      "[BATCH 95/149] Loss_D: 0.8660 Loss_G: 0.9344 acc: 51.6%\n",
      "[BATCH 96/149] Loss_D: 0.9107 Loss_G: 0.9408 acc: 51.6%\n",
      "[BATCH 97/149] Loss_D: 0.9310 Loss_G: 0.9515 acc: 62.5%\n",
      "[BATCH 98/149] Loss_D: 0.9072 Loss_G: 0.9615 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.8786 Loss_G: 0.9510 acc: 54.7%\n",
      "[BATCH 100/149] Loss_D: 0.8386 Loss_G: 0.9334 acc: 40.6%\n",
      "[BATCH 101/149] Loss_D: 0.9566 Loss_G: 0.9395 acc: 56.2%\n",
      "[BATCH 102/149] Loss_D: 0.8582 Loss_G: 0.9359 acc: 50.0%\n",
      "[BATCH 103/149] Loss_D: 0.8386 Loss_G: 0.9287 acc: 48.4%\n",
      "[BATCH 104/149] Loss_D: 0.9049 Loss_G: 0.9363 acc: 56.2%\n",
      "[BATCH 105/149] Loss_D: 0.8517 Loss_G: 0.9340 acc: 45.3%\n",
      "[EPOCH 850] TEST ACC is : 53.9%\n",
      "[BATCH 106/149] Loss_D: 0.8856 Loss_G: 0.9386 acc: 40.6%\n",
      "[BATCH 107/149] Loss_D: 0.8338 Loss_G: 0.9224 acc: 62.5%\n",
      "[BATCH 108/149] Loss_D: 0.8629 Loss_G: 0.9229 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.8251 Loss_G: 0.9173 acc: 50.0%\n",
      "[BATCH 110/149] Loss_D: 0.8431 Loss_G: 0.9148 acc: 37.5%\n",
      "[BATCH 111/149] Loss_D: 0.8820 Loss_G: 0.9199 acc: 51.6%\n",
      "[BATCH 112/149] Loss_D: 0.8500 Loss_G: 0.9205 acc: 56.2%\n",
      "[BATCH 113/149] Loss_D: 0.8845 Loss_G: 0.9298 acc: 50.0%\n",
      "[BATCH 114/149] Loss_D: 0.8420 Loss_G: 0.9238 acc: 51.6%\n",
      "[BATCH 115/149] Loss_D: 0.8559 Loss_G: 0.9198 acc: 53.1%\n",
      "[BATCH 116/149] Loss_D: 0.9036 Loss_G: 0.9230 acc: 39.1%\n",
      "[BATCH 117/149] Loss_D: 0.8542 Loss_G: 0.9170 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.9263 Loss_G: 0.9292 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.8547 Loss_G: 0.9263 acc: 59.4%\n",
      "[BATCH 120/149] Loss_D: 0.8216 Loss_G: 0.9217 acc: 45.3%\n",
      "[BATCH 121/149] Loss_D: 0.8868 Loss_G: 0.9199 acc: 45.3%\n",
      "[BATCH 122/149] Loss_D: 0.9270 Loss_G: 0.9303 acc: 46.9%\n",
      "[BATCH 123/149] Loss_D: 0.8397 Loss_G: 0.9283 acc: 50.0%\n",
      "[BATCH 124/149] Loss_D: 0.8892 Loss_G: 0.9277 acc: 34.4%\n",
      "[BATCH 125/149] Loss_D: 0.9098 Loss_G: 0.9332 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.8754 Loss_G: 0.9301 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.8942 Loss_G: 0.9347 acc: 43.8%\n",
      "[BATCH 128/149] Loss_D: 0.8724 Loss_G: 0.9329 acc: 53.1%\n",
      "[BATCH 129/149] Loss_D: 0.8646 Loss_G: 0.9232 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.9232 Loss_G: 0.9347 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.8360 Loss_G: 0.9309 acc: 48.4%\n",
      "[BATCH 132/149] Loss_D: 0.8719 Loss_G: 0.9262 acc: 46.9%\n",
      "[BATCH 133/149] Loss_D: 0.8727 Loss_G: 0.9278 acc: 51.6%\n",
      "[BATCH 134/149] Loss_D: 0.8433 Loss_G: 0.9241 acc: 45.3%\n",
      "[BATCH 135/149] Loss_D: 0.8707 Loss_G: 0.9235 acc: 46.9%\n",
      "[BATCH 136/149] Loss_D: 0.8363 Loss_G: 0.9210 acc: 53.1%\n",
      "[BATCH 137/149] Loss_D: 0.8608 Loss_G: 0.9232 acc: 39.1%\n",
      "[BATCH 138/149] Loss_D: 0.9004 Loss_G: 0.9256 acc: 46.9%\n",
      "[BATCH 139/149] Loss_D: 0.8945 Loss_G: 0.9397 acc: 53.1%\n",
      "[BATCH 140/149] Loss_D: 0.8130 Loss_G: 0.9225 acc: 54.7%\n",
      "[BATCH 141/149] Loss_D: 0.8486 Loss_G: 0.9135 acc: 50.0%\n",
      "[BATCH 142/149] Loss_D: 0.8226 Loss_G: 0.9087 acc: 50.0%\n",
      "[BATCH 143/149] Loss_D: 0.9188 Loss_G: 0.9140 acc: 43.8%\n",
      "[BATCH 144/149] Loss_D: 0.8570 Loss_G: 0.9160 acc: 43.8%\n",
      "[BATCH 145/149] Loss_D: 0.8896 Loss_G: 0.9246 acc: 57.8%\n",
      "[BATCH 146/149] Loss_D: 0.8877 Loss_G: 0.9320 acc: 57.8%\n",
      "[BATCH 147/149] Loss_D: 0.8780 Loss_G: 0.9370 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.8625 Loss_G: 0.9273 acc: 43.8%\n",
      "[BATCH 149/149] Loss_D: 0.8516 Loss_G: 0.9253 acc: 56.2%\n",
      "-----THE [6/50] epoch end-----\n",
      "-----THE [7/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8531 Loss_G: 0.9198 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.9205 Loss_G: 0.9310 acc: 57.8%\n",
      "[BATCH 3/149] Loss_D: 0.9062 Loss_G: 0.9456 acc: 56.2%\n",
      "[BATCH 4/149] Loss_D: 0.8935 Loss_G: 0.9489 acc: 40.6%\n",
      "[BATCH 5/149] Loss_D: 0.8521 Loss_G: 0.9367 acc: 51.6%\n",
      "[BATCH 6/149] Loss_D: 0.8727 Loss_G: 0.9281 acc: 42.2%\n",
      "[EPOCH 900] TEST ACC is : 53.3%\n",
      "[BATCH 7/149] Loss_D: 0.9070 Loss_G: 0.9346 acc: 53.1%\n",
      "[BATCH 8/149] Loss_D: 0.8607 Loss_G: 0.9318 acc: 50.0%\n",
      "[BATCH 9/149] Loss_D: 0.8820 Loss_G: 0.9326 acc: 42.2%\n",
      "[BATCH 10/149] Loss_D: 0.9020 Loss_G: 0.9356 acc: 54.7%\n",
      "[BATCH 11/149] Loss_D: 0.8892 Loss_G: 0.9377 acc: 51.6%\n",
      "[BATCH 12/149] Loss_D: 0.8401 Loss_G: 0.9352 acc: 51.6%\n",
      "[BATCH 13/149] Loss_D: 0.8204 Loss_G: 0.9236 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.8642 Loss_G: 0.9208 acc: 54.7%\n",
      "[BATCH 15/149] Loss_D: 0.8445 Loss_G: 0.9169 acc: 54.7%\n",
      "[BATCH 16/149] Loss_D: 0.9516 Loss_G: 0.9314 acc: 54.7%\n",
      "[BATCH 17/149] Loss_D: 0.8454 Loss_G: 0.9375 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.8348 Loss_G: 0.9272 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.8997 Loss_G: 0.9286 acc: 53.1%\n",
      "[BATCH 20/149] Loss_D: 0.8432 Loss_G: 0.9226 acc: 43.8%\n",
      "[BATCH 21/149] Loss_D: 0.8821 Loss_G: 0.9242 acc: 43.8%\n",
      "[BATCH 22/149] Loss_D: 0.8263 Loss_G: 0.9200 acc: 54.7%\n",
      "[BATCH 23/149] Loss_D: 0.8200 Loss_G: 0.9217 acc: 51.6%\n",
      "[BATCH 24/149] Loss_D: 0.8346 Loss_G: 0.9202 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.8706 Loss_G: 0.9182 acc: 56.2%\n",
      "[BATCH 26/149] Loss_D: 0.7930 Loss_G: 0.9133 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.8632 Loss_G: 0.9091 acc: 51.6%\n",
      "[BATCH 28/149] Loss_D: 0.8660 Loss_G: 0.9158 acc: 50.0%\n",
      "[BATCH 29/149] Loss_D: 0.8176 Loss_G: 0.9097 acc: 46.9%\n",
      "[BATCH 30/149] Loss_D: 0.8534 Loss_G: 0.9035 acc: 40.6%\n",
      "[BATCH 31/149] Loss_D: 0.8373 Loss_G: 0.9105 acc: 57.8%\n",
      "[BATCH 32/149] Loss_D: 0.8577 Loss_G: 0.9033 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.8422 Loss_G: 0.9024 acc: 48.4%\n",
      "[BATCH 34/149] Loss_D: 0.8154 Loss_G: 0.9063 acc: 40.6%\n",
      "[BATCH 35/149] Loss_D: 0.8507 Loss_G: 0.9165 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.9268 Loss_G: 0.9356 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.8611 Loss_G: 0.9283 acc: 43.8%\n",
      "[BATCH 38/149] Loss_D: 0.8796 Loss_G: 0.9278 acc: 45.3%\n",
      "[BATCH 39/149] Loss_D: 0.8515 Loss_G: 0.9130 acc: 56.2%\n",
      "[BATCH 40/149] Loss_D: 0.8919 Loss_G: 0.9161 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.8252 Loss_G: 0.9088 acc: 48.4%\n",
      "[BATCH 42/149] Loss_D: 0.8346 Loss_G: 0.9022 acc: 60.9%\n",
      "[BATCH 43/149] Loss_D: 0.8492 Loss_G: 0.9057 acc: 35.9%\n",
      "[BATCH 44/149] Loss_D: 0.8009 Loss_G: 0.8987 acc: 48.4%\n",
      "[BATCH 45/149] Loss_D: 0.8413 Loss_G: 0.8993 acc: 54.7%\n",
      "[BATCH 46/149] Loss_D: 0.8258 Loss_G: 0.8962 acc: 51.6%\n",
      "[BATCH 47/149] Loss_D: 0.8266 Loss_G: 0.8923 acc: 39.1%\n",
      "[BATCH 48/149] Loss_D: 0.8460 Loss_G: 0.8958 acc: 39.1%\n",
      "[BATCH 49/149] Loss_D: 0.8695 Loss_G: 0.8999 acc: 57.8%\n",
      "[BATCH 50/149] Loss_D: 0.8857 Loss_G: 0.9112 acc: 56.2%\n",
      "[BATCH 51/149] Loss_D: 0.8862 Loss_G: 0.9201 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.8508 Loss_G: 0.9210 acc: 59.4%\n",
      "[BATCH 53/149] Loss_D: 0.8334 Loss_G: 0.9152 acc: 50.0%\n",
      "[BATCH 54/149] Loss_D: 0.8521 Loss_G: 0.9085 acc: 48.4%\n",
      "[BATCH 55/149] Loss_D: 0.8764 Loss_G: 0.9106 acc: 53.1%\n",
      "[BATCH 56/149] Loss_D: 0.8960 Loss_G: 0.9209 acc: 57.8%\n",
      "[EPOCH 950] TEST ACC is : 53.3%\n",
      "[BATCH 57/149] Loss_D: 0.8625 Loss_G: 0.9254 acc: 46.9%\n",
      "[BATCH 58/149] Loss_D: 0.8728 Loss_G: 0.9226 acc: 45.3%\n",
      "[BATCH 59/149] Loss_D: 0.8469 Loss_G: 0.9104 acc: 51.6%\n",
      "[BATCH 60/149] Loss_D: 0.8287 Loss_G: 0.9068 acc: 46.9%\n",
      "[BATCH 61/149] Loss_D: 0.9107 Loss_G: 0.9126 acc: 60.9%\n",
      "[BATCH 62/149] Loss_D: 0.9580 Loss_G: 0.9321 acc: 46.9%\n",
      "[BATCH 63/149] Loss_D: 0.8454 Loss_G: 0.9313 acc: 48.4%\n",
      "[BATCH 64/149] Loss_D: 0.8565 Loss_G: 0.9245 acc: 46.9%\n",
      "[BATCH 65/149] Loss_D: 0.8750 Loss_G: 0.9175 acc: 50.0%\n",
      "[BATCH 66/149] Loss_D: 0.9128 Loss_G: 0.9262 acc: 50.0%\n",
      "[BATCH 67/149] Loss_D: 0.8546 Loss_G: 0.9308 acc: 50.0%\n",
      "[BATCH 68/149] Loss_D: 0.8305 Loss_G: 0.9194 acc: 57.8%\n",
      "[BATCH 69/149] Loss_D: 0.8377 Loss_G: 0.9151 acc: 51.6%\n",
      "[BATCH 70/149] Loss_D: 0.8435 Loss_G: 0.9060 acc: 37.5%\n",
      "[BATCH 71/149] Loss_D: 0.9304 Loss_G: 0.9174 acc: 50.0%\n",
      "[BATCH 72/149] Loss_D: 0.8295 Loss_G: 0.9202 acc: 54.7%\n",
      "[BATCH 73/149] Loss_D: 0.8045 Loss_G: 0.9103 acc: 39.1%\n",
      "[BATCH 74/149] Loss_D: 0.8009 Loss_G: 0.8989 acc: 40.6%\n",
      "[BATCH 75/149] Loss_D: 0.9231 Loss_G: 0.9117 acc: 54.7%\n",
      "[BATCH 76/149] Loss_D: 0.9266 Loss_G: 0.9326 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.8800 Loss_G: 0.9316 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.8716 Loss_G: 0.9270 acc: 54.7%\n",
      "[BATCH 79/149] Loss_D: 0.8517 Loss_G: 0.9199 acc: 43.8%\n",
      "[BATCH 80/149] Loss_D: 0.8489 Loss_G: 0.9139 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.8369 Loss_G: 0.9126 acc: 46.9%\n",
      "[BATCH 82/149] Loss_D: 0.8413 Loss_G: 0.9089 acc: 46.9%\n",
      "[BATCH 83/149] Loss_D: 0.8931 Loss_G: 0.9169 acc: 43.8%\n",
      "[BATCH 84/149] Loss_D: 0.8594 Loss_G: 0.9142 acc: 46.9%\n",
      "[BATCH 85/149] Loss_D: 0.8273 Loss_G: 0.9088 acc: 42.2%\n",
      "[BATCH 86/149] Loss_D: 0.8686 Loss_G: 0.9024 acc: 46.9%\n",
      "[BATCH 87/149] Loss_D: 0.8906 Loss_G: 0.9079 acc: 51.6%\n",
      "[BATCH 88/149] Loss_D: 0.8637 Loss_G: 0.9063 acc: 45.3%\n",
      "[BATCH 89/149] Loss_D: 0.8074 Loss_G: 0.9019 acc: 50.0%\n",
      "[BATCH 90/149] Loss_D: 0.8524 Loss_G: 0.9005 acc: 45.3%\n",
      "[BATCH 91/149] Loss_D: 0.9382 Loss_G: 0.9134 acc: 53.1%\n",
      "[BATCH 92/149] Loss_D: 0.8234 Loss_G: 0.9083 acc: 43.8%\n",
      "[BATCH 93/149] Loss_D: 0.9206 Loss_G: 0.9176 acc: 54.7%\n",
      "[BATCH 94/149] Loss_D: 0.8339 Loss_G: 0.9144 acc: 48.4%\n",
      "[BATCH 95/149] Loss_D: 0.8234 Loss_G: 0.9082 acc: 45.3%\n",
      "[BATCH 96/149] Loss_D: 0.8666 Loss_G: 0.9039 acc: 50.0%\n",
      "[BATCH 97/149] Loss_D: 0.9439 Loss_G: 0.9297 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.8066 Loss_G: 0.9136 acc: 48.4%\n",
      "[BATCH 99/149] Loss_D: 0.8228 Loss_G: 0.9088 acc: 45.3%\n",
      "[BATCH 100/149] Loss_D: 0.8727 Loss_G: 0.9072 acc: 53.1%\n",
      "[BATCH 101/149] Loss_D: 0.8725 Loss_G: 0.9157 acc: 43.8%\n",
      "[BATCH 102/149] Loss_D: 0.7846 Loss_G: 0.9082 acc: 48.4%\n",
      "[BATCH 103/149] Loss_D: 0.9255 Loss_G: 0.9269 acc: 51.6%\n",
      "[BATCH 104/149] Loss_D: 0.8869 Loss_G: 0.9258 acc: 51.6%\n",
      "[BATCH 105/149] Loss_D: 0.8926 Loss_G: 0.9296 acc: 56.2%\n",
      "[BATCH 106/149] Loss_D: 0.8667 Loss_G: 0.9215 acc: 53.1%\n",
      "[EPOCH 1000] TEST ACC is : 53.5%\n",
      "[BATCH 107/149] Loss_D: 0.8040 Loss_G: 0.9102 acc: 54.7%\n",
      "[BATCH 108/149] Loss_D: 0.8441 Loss_G: 0.9058 acc: 51.6%\n",
      "[BATCH 109/149] Loss_D: 0.8874 Loss_G: 0.9099 acc: 42.2%\n",
      "[BATCH 110/149] Loss_D: 0.9112 Loss_G: 0.9201 acc: 39.1%\n",
      "[BATCH 111/149] Loss_D: 0.8209 Loss_G: 0.9168 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8728 Loss_G: 0.9157 acc: 46.9%\n",
      "[BATCH 113/149] Loss_D: 0.8551 Loss_G: 0.9161 acc: 50.0%\n",
      "[BATCH 114/149] Loss_D: 0.8902 Loss_G: 0.9233 acc: 43.8%\n",
      "[BATCH 115/149] Loss_D: 0.8837 Loss_G: 0.9262 acc: 51.6%\n",
      "[BATCH 116/149] Loss_D: 0.8476 Loss_G: 0.9171 acc: 50.0%\n",
      "[BATCH 117/149] Loss_D: 0.8275 Loss_G: 0.9180 acc: 54.7%\n",
      "[BATCH 118/149] Loss_D: 0.8602 Loss_G: 0.9146 acc: 53.1%\n",
      "[BATCH 119/149] Loss_D: 0.8139 Loss_G: 0.9176 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.8391 Loss_G: 0.9139 acc: 43.8%\n",
      "[BATCH 121/149] Loss_D: 0.8409 Loss_G: 0.9136 acc: 50.0%\n",
      "[BATCH 122/149] Loss_D: 0.8348 Loss_G: 0.9062 acc: 48.4%\n",
      "[BATCH 123/149] Loss_D: 0.8737 Loss_G: 0.9096 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.9255 Loss_G: 0.9319 acc: 56.2%\n",
      "[BATCH 125/149] Loss_D: 0.8643 Loss_G: 0.9376 acc: 56.2%\n",
      "[BATCH 126/149] Loss_D: 0.9010 Loss_G: 0.9313 acc: 53.1%\n",
      "[BATCH 127/149] Loss_D: 0.8447 Loss_G: 0.9241 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.8871 Loss_G: 0.9156 acc: 54.7%\n",
      "[BATCH 129/149] Loss_D: 0.8101 Loss_G: 0.9085 acc: 54.7%\n",
      "[BATCH 130/149] Loss_D: 0.8477 Loss_G: 0.9039 acc: 48.4%\n",
      "[BATCH 131/149] Loss_D: 0.8926 Loss_G: 0.9094 acc: 51.6%\n",
      "[BATCH 132/149] Loss_D: 0.7986 Loss_G: 0.9052 acc: 59.4%\n",
      "[BATCH 133/149] Loss_D: 0.8016 Loss_G: 0.9021 acc: 54.7%\n",
      "[BATCH 134/149] Loss_D: 0.8591 Loss_G: 0.9038 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.8282 Loss_G: 0.9054 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.9059 Loss_G: 0.9157 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.8442 Loss_G: 0.9161 acc: 40.6%\n",
      "[BATCH 138/149] Loss_D: 0.8458 Loss_G: 0.9161 acc: 56.2%\n",
      "[BATCH 139/149] Loss_D: 0.8613 Loss_G: 0.9094 acc: 53.1%\n",
      "[BATCH 140/149] Loss_D: 0.8780 Loss_G: 0.9106 acc: 53.1%\n",
      "[BATCH 141/149] Loss_D: 0.8883 Loss_G: 0.9219 acc: 54.7%\n",
      "[BATCH 142/149] Loss_D: 0.8238 Loss_G: 0.9188 acc: 50.0%\n",
      "[BATCH 143/149] Loss_D: 0.8890 Loss_G: 0.9178 acc: 50.0%\n",
      "[BATCH 144/149] Loss_D: 0.8708 Loss_G: 0.9178 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.8543 Loss_G: 0.9164 acc: 59.4%\n",
      "[BATCH 146/149] Loss_D: 0.8891 Loss_G: 0.9197 acc: 45.3%\n",
      "[BATCH 147/149] Loss_D: 0.8364 Loss_G: 0.9118 acc: 43.8%\n",
      "[BATCH 148/149] Loss_D: 0.8324 Loss_G: 0.9046 acc: 40.6%\n",
      "[BATCH 149/149] Loss_D: 0.8312 Loss_G: 0.8983 acc: 42.2%\n",
      "-----THE [7/50] epoch end-----\n",
      "-----THE [8/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8517 Loss_G: 0.8991 acc: 57.8%\n",
      "[BATCH 2/149] Loss_D: 0.8429 Loss_G: 0.9035 acc: 46.9%\n",
      "[BATCH 3/149] Loss_D: 0.8276 Loss_G: 0.9030 acc: 54.7%\n",
      "[BATCH 4/149] Loss_D: 0.8442 Loss_G: 0.8989 acc: 45.3%\n",
      "[BATCH 5/149] Loss_D: 0.8735 Loss_G: 0.8984 acc: 56.2%\n",
      "[BATCH 6/149] Loss_D: 0.8554 Loss_G: 0.9014 acc: 43.8%\n",
      "[BATCH 7/149] Loss_D: 0.8625 Loss_G: 0.9026 acc: 54.7%\n",
      "[EPOCH 1050] TEST ACC is : 53.9%\n",
      "[BATCH 8/149] Loss_D: 0.7995 Loss_G: 0.8968 acc: 45.3%\n",
      "[BATCH 9/149] Loss_D: 0.8031 Loss_G: 0.8875 acc: 46.9%\n",
      "[BATCH 10/149] Loss_D: 0.8039 Loss_G: 0.8812 acc: 50.0%\n",
      "[BATCH 11/149] Loss_D: 0.8966 Loss_G: 0.8989 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.8447 Loss_G: 0.9025 acc: 46.9%\n",
      "[BATCH 13/149] Loss_D: 0.8126 Loss_G: 0.8911 acc: 43.8%\n",
      "[BATCH 14/149] Loss_D: 0.8459 Loss_G: 0.8911 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.8774 Loss_G: 0.9012 acc: 43.8%\n",
      "[BATCH 16/149] Loss_D: 0.8699 Loss_G: 0.9138 acc: 53.1%\n",
      "[BATCH 17/149] Loss_D: 0.8358 Loss_G: 0.9122 acc: 56.2%\n",
      "[BATCH 18/149] Loss_D: 0.8607 Loss_G: 0.9047 acc: 50.0%\n",
      "[BATCH 19/149] Loss_D: 0.8288 Loss_G: 0.8981 acc: 39.1%\n",
      "[BATCH 20/149] Loss_D: 0.8784 Loss_G: 0.9040 acc: 51.6%\n",
      "[BATCH 21/149] Loss_D: 0.9035 Loss_G: 0.9091 acc: 45.3%\n",
      "[BATCH 22/149] Loss_D: 0.7846 Loss_G: 0.9055 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.8220 Loss_G: 0.9003 acc: 51.6%\n",
      "[BATCH 24/149] Loss_D: 0.8740 Loss_G: 0.9019 acc: 53.1%\n",
      "[BATCH 25/149] Loss_D: 0.8345 Loss_G: 0.9013 acc: 42.2%\n",
      "[BATCH 26/149] Loss_D: 0.8855 Loss_G: 0.9110 acc: 51.6%\n",
      "[BATCH 27/149] Loss_D: 0.8962 Loss_G: 0.9261 acc: 54.7%\n",
      "[BATCH 28/149] Loss_D: 0.8911 Loss_G: 0.9277 acc: 45.3%\n",
      "[BATCH 29/149] Loss_D: 0.8192 Loss_G: 0.9120 acc: 46.9%\n",
      "[BATCH 30/149] Loss_D: 0.8486 Loss_G: 0.9049 acc: 51.6%\n",
      "[BATCH 31/149] Loss_D: 0.8705 Loss_G: 0.9010 acc: 42.2%\n",
      "[BATCH 32/149] Loss_D: 0.8525 Loss_G: 0.8999 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.8555 Loss_G: 0.8979 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.8193 Loss_G: 0.8964 acc: 50.0%\n",
      "[BATCH 35/149] Loss_D: 0.8723 Loss_G: 0.9029 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.8958 Loss_G: 0.9116 acc: 56.2%\n",
      "[BATCH 37/149] Loss_D: 0.8274 Loss_G: 0.9131 acc: 62.5%\n",
      "[BATCH 38/149] Loss_D: 0.8504 Loss_G: 0.9067 acc: 50.0%\n",
      "[BATCH 39/149] Loss_D: 0.8204 Loss_G: 0.9010 acc: 39.1%\n",
      "[BATCH 40/149] Loss_D: 0.8473 Loss_G: 0.8969 acc: 48.4%\n",
      "[BATCH 41/149] Loss_D: 0.8539 Loss_G: 0.8970 acc: 54.7%\n",
      "[BATCH 42/149] Loss_D: 0.8936 Loss_G: 0.9048 acc: 46.9%\n",
      "[BATCH 43/149] Loss_D: 0.8915 Loss_G: 0.9258 acc: 56.2%\n",
      "[BATCH 44/149] Loss_D: 0.8602 Loss_G: 0.9196 acc: 59.4%\n",
      "[BATCH 45/149] Loss_D: 0.8470 Loss_G: 0.9113 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.8518 Loss_G: 0.9058 acc: 42.2%\n",
      "[BATCH 47/149] Loss_D: 0.8315 Loss_G: 0.9043 acc: 39.1%\n",
      "[BATCH 48/149] Loss_D: 0.9362 Loss_G: 0.9190 acc: 56.2%\n",
      "[BATCH 49/149] Loss_D: 0.8815 Loss_G: 0.9263 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.8833 Loss_G: 0.9255 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.8624 Loss_G: 0.9137 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.8353 Loss_G: 0.9042 acc: 57.8%\n",
      "[BATCH 53/149] Loss_D: 0.8996 Loss_G: 0.9050 acc: 50.0%\n",
      "[BATCH 54/149] Loss_D: 0.8495 Loss_G: 0.9031 acc: 43.8%\n",
      "[BATCH 55/149] Loss_D: 0.8741 Loss_G: 0.9067 acc: 45.3%\n",
      "[BATCH 56/149] Loss_D: 0.9250 Loss_G: 0.9195 acc: 53.1%\n",
      "[BATCH 57/149] Loss_D: 0.8080 Loss_G: 0.9078 acc: 51.6%\n",
      "[EPOCH 1100] TEST ACC is : 52.9%\n",
      "[BATCH 58/149] Loss_D: 0.8757 Loss_G: 0.9044 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.8439 Loss_G: 0.9008 acc: 57.8%\n",
      "[BATCH 60/149] Loss_D: 0.8925 Loss_G: 0.9094 acc: 46.9%\n",
      "[BATCH 61/149] Loss_D: 0.8452 Loss_G: 0.9076 acc: 56.2%\n",
      "[BATCH 62/149] Loss_D: 0.8277 Loss_G: 0.9013 acc: 48.4%\n",
      "[BATCH 63/149] Loss_D: 0.8845 Loss_G: 0.9067 acc: 56.2%\n",
      "[BATCH 64/149] Loss_D: 0.8668 Loss_G: 0.9094 acc: 51.6%\n",
      "[BATCH 65/149] Loss_D: 0.9378 Loss_G: 0.9236 acc: 50.0%\n",
      "[BATCH 66/149] Loss_D: 0.9196 Loss_G: 0.9337 acc: 48.4%\n",
      "[BATCH 67/149] Loss_D: 0.9048 Loss_G: 0.9236 acc: 46.9%\n",
      "[BATCH 68/149] Loss_D: 0.7800 Loss_G: 0.9063 acc: 53.1%\n",
      "[BATCH 69/149] Loss_D: 0.8628 Loss_G: 0.9033 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.7912 Loss_G: 0.8965 acc: 50.0%\n",
      "[BATCH 71/149] Loss_D: 0.8635 Loss_G: 0.8987 acc: 50.0%\n",
      "[BATCH 72/149] Loss_D: 0.8584 Loss_G: 0.8990 acc: 51.6%\n",
      "[BATCH 73/149] Loss_D: 0.9182 Loss_G: 0.9115 acc: 54.7%\n",
      "[BATCH 74/149] Loss_D: 0.9064 Loss_G: 0.9316 acc: 43.8%\n",
      "[BATCH 75/149] Loss_D: 0.8235 Loss_G: 0.9161 acc: 51.6%\n",
      "[BATCH 76/149] Loss_D: 0.8665 Loss_G: 0.9039 acc: 51.6%\n",
      "[BATCH 77/149] Loss_D: 0.8806 Loss_G: 0.9041 acc: 40.6%\n",
      "[BATCH 78/149] Loss_D: 0.8065 Loss_G: 0.8946 acc: 46.9%\n",
      "[BATCH 79/149] Loss_D: 0.8398 Loss_G: 0.8915 acc: 57.8%\n",
      "[BATCH 80/149] Loss_D: 0.8375 Loss_G: 0.8934 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.8700 Loss_G: 0.8960 acc: 51.6%\n",
      "[BATCH 82/149] Loss_D: 0.8390 Loss_G: 0.8975 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8036 Loss_G: 0.8931 acc: 42.2%\n",
      "[BATCH 84/149] Loss_D: 0.8304 Loss_G: 0.8902 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.8237 Loss_G: 0.8879 acc: 46.9%\n",
      "[BATCH 86/149] Loss_D: 0.8009 Loss_G: 0.8878 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.8631 Loss_G: 0.8949 acc: 48.4%\n",
      "[BATCH 88/149] Loss_D: 0.8584 Loss_G: 0.9022 acc: 53.1%\n",
      "[BATCH 89/149] Loss_D: 0.9055 Loss_G: 0.9099 acc: 59.4%\n",
      "[BATCH 90/149] Loss_D: 0.9029 Loss_G: 0.9212 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.8620 Loss_G: 0.9184 acc: 48.4%\n",
      "[BATCH 92/149] Loss_D: 0.8746 Loss_G: 0.9085 acc: 54.7%\n",
      "[BATCH 93/149] Loss_D: 0.7969 Loss_G: 0.9008 acc: 46.9%\n",
      "[BATCH 94/149] Loss_D: 0.8069 Loss_G: 0.8942 acc: 48.4%\n",
      "[BATCH 95/149] Loss_D: 0.8883 Loss_G: 0.8996 acc: 46.9%\n",
      "[BATCH 96/149] Loss_D: 0.8383 Loss_G: 0.9002 acc: 56.2%\n",
      "[BATCH 97/149] Loss_D: 0.8457 Loss_G: 0.9022 acc: 50.0%\n",
      "[BATCH 98/149] Loss_D: 0.8224 Loss_G: 0.8976 acc: 54.7%\n",
      "[BATCH 99/149] Loss_D: 0.8887 Loss_G: 0.9025 acc: 48.4%\n",
      "[BATCH 100/149] Loss_D: 0.8051 Loss_G: 0.8932 acc: 39.1%\n",
      "[BATCH 101/149] Loss_D: 0.8590 Loss_G: 0.8965 acc: 57.8%\n",
      "[BATCH 102/149] Loss_D: 0.8343 Loss_G: 0.8867 acc: 46.9%\n",
      "[BATCH 103/149] Loss_D: 0.8533 Loss_G: 0.8906 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8363 Loss_G: 0.8908 acc: 50.0%\n",
      "[BATCH 105/149] Loss_D: 0.9082 Loss_G: 0.9020 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.8696 Loss_G: 0.9113 acc: 48.4%\n",
      "[BATCH 107/149] Loss_D: 0.8919 Loss_G: 0.9147 acc: 60.9%\n",
      "[EPOCH 1150] TEST ACC is : 53.5%\n",
      "[BATCH 108/149] Loss_D: 0.8514 Loss_G: 0.9118 acc: 45.3%\n",
      "[BATCH 109/149] Loss_D: 0.9241 Loss_G: 0.9236 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.8424 Loss_G: 0.9275 acc: 50.0%\n",
      "[BATCH 111/149] Loss_D: 0.8238 Loss_G: 0.9177 acc: 60.9%\n",
      "[BATCH 112/149] Loss_D: 0.8302 Loss_G: 0.9103 acc: 56.2%\n",
      "[BATCH 113/149] Loss_D: 0.8612 Loss_G: 0.9023 acc: 56.2%\n",
      "[BATCH 114/149] Loss_D: 0.8288 Loss_G: 0.9046 acc: 53.1%\n",
      "[BATCH 115/149] Loss_D: 0.8270 Loss_G: 0.9007 acc: 54.7%\n",
      "[BATCH 116/149] Loss_D: 0.8251 Loss_G: 0.8978 acc: 54.7%\n",
      "[BATCH 117/149] Loss_D: 0.9041 Loss_G: 0.9091 acc: 57.8%\n",
      "[BATCH 118/149] Loss_D: 0.9002 Loss_G: 0.9191 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.8011 Loss_G: 0.9096 acc: 57.8%\n",
      "[BATCH 120/149] Loss_D: 0.8287 Loss_G: 0.8976 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.8397 Loss_G: 0.8980 acc: 64.1%\n",
      "[BATCH 122/149] Loss_D: 0.8055 Loss_G: 0.8883 acc: 62.5%\n",
      "[BATCH 123/149] Loss_D: 0.8658 Loss_G: 0.8949 acc: 54.7%\n",
      "[BATCH 124/149] Loss_D: 0.8425 Loss_G: 0.8935 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.8267 Loss_G: 0.8883 acc: 48.4%\n",
      "[BATCH 126/149] Loss_D: 0.8059 Loss_G: 0.8880 acc: 48.4%\n",
      "[BATCH 127/149] Loss_D: 0.8588 Loss_G: 0.8936 acc: 51.6%\n",
      "[BATCH 128/149] Loss_D: 0.8695 Loss_G: 0.8988 acc: 46.9%\n",
      "[BATCH 129/149] Loss_D: 0.9091 Loss_G: 0.9134 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.8827 Loss_G: 0.9153 acc: 45.3%\n",
      "[BATCH 131/149] Loss_D: 0.9201 Loss_G: 0.9212 acc: 59.4%\n",
      "[BATCH 132/149] Loss_D: 0.8172 Loss_G: 0.9163 acc: 51.6%\n",
      "[BATCH 133/149] Loss_D: 0.8416 Loss_G: 0.9037 acc: 57.8%\n",
      "[BATCH 134/149] Loss_D: 0.8316 Loss_G: 0.9041 acc: 56.2%\n",
      "[BATCH 135/149] Loss_D: 0.8601 Loss_G: 0.9087 acc: 50.0%\n",
      "[BATCH 136/149] Loss_D: 0.8187 Loss_G: 0.9089 acc: 43.8%\n",
      "[BATCH 137/149] Loss_D: 0.8872 Loss_G: 0.9114 acc: 51.6%\n",
      "[BATCH 138/149] Loss_D: 0.8580 Loss_G: 0.9206 acc: 42.2%\n",
      "[BATCH 139/149] Loss_D: 0.8856 Loss_G: 0.9282 acc: 35.9%\n",
      "[BATCH 140/149] Loss_D: 0.8245 Loss_G: 0.9213 acc: 48.4%\n",
      "[BATCH 141/149] Loss_D: 0.8325 Loss_G: 0.9116 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8439 Loss_G: 0.9005 acc: 40.6%\n",
      "[BATCH 143/149] Loss_D: 0.8245 Loss_G: 0.9010 acc: 54.7%\n",
      "[BATCH 144/149] Loss_D: 0.9485 Loss_G: 0.9076 acc: 46.9%\n",
      "[BATCH 145/149] Loss_D: 0.8605 Loss_G: 0.9153 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.8687 Loss_G: 0.9179 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.8409 Loss_G: 0.9087 acc: 51.6%\n",
      "[BATCH 148/149] Loss_D: 0.8791 Loss_G: 0.9108 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.8342 Loss_G: 0.9022 acc: 43.8%\n",
      "-----THE [8/50] epoch end-----\n",
      "-----THE [9/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8482 Loss_G: 0.9043 acc: 54.7%\n",
      "[BATCH 2/149] Loss_D: 0.8620 Loss_G: 0.9020 acc: 48.4%\n",
      "[BATCH 3/149] Loss_D: 0.8618 Loss_G: 0.9131 acc: 51.6%\n",
      "[BATCH 4/149] Loss_D: 0.8319 Loss_G: 0.9081 acc: 62.5%\n",
      "[BATCH 5/149] Loss_D: 0.8241 Loss_G: 0.8958 acc: 43.8%\n",
      "[BATCH 6/149] Loss_D: 0.8105 Loss_G: 0.8860 acc: 45.3%\n",
      "[BATCH 7/149] Loss_D: 0.8380 Loss_G: 0.8838 acc: 51.6%\n",
      "[BATCH 8/149] Loss_D: 0.8730 Loss_G: 0.8897 acc: 50.0%\n",
      "[EPOCH 1200] TEST ACC is : 55.3%\n",
      "[BATCH 9/149] Loss_D: 0.8443 Loss_G: 0.8929 acc: 54.7%\n",
      "[BATCH 10/149] Loss_D: 0.8440 Loss_G: 0.8977 acc: 54.7%\n",
      "[BATCH 11/149] Loss_D: 0.8299 Loss_G: 0.8950 acc: 46.9%\n",
      "[BATCH 12/149] Loss_D: 0.8274 Loss_G: 0.8978 acc: 46.9%\n",
      "[BATCH 13/149] Loss_D: 0.8974 Loss_G: 0.9061 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.8508 Loss_G: 0.9111 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.8384 Loss_G: 0.9089 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.8714 Loss_G: 0.9077 acc: 51.6%\n",
      "[BATCH 17/149] Loss_D: 0.9162 Loss_G: 0.9159 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.8094 Loss_G: 0.9036 acc: 56.2%\n",
      "[BATCH 19/149] Loss_D: 0.9190 Loss_G: 0.9130 acc: 51.6%\n",
      "[BATCH 20/149] Loss_D: 0.8660 Loss_G: 0.9062 acc: 51.6%\n",
      "[BATCH 21/149] Loss_D: 0.7933 Loss_G: 0.8903 acc: 57.8%\n",
      "[BATCH 22/149] Loss_D: 0.8537 Loss_G: 0.8922 acc: 54.7%\n",
      "[BATCH 23/149] Loss_D: 0.8550 Loss_G: 0.8897 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.8690 Loss_G: 0.8924 acc: 48.4%\n",
      "[BATCH 25/149] Loss_D: 0.8038 Loss_G: 0.8911 acc: 50.0%\n",
      "[BATCH 26/149] Loss_D: 0.8480 Loss_G: 0.8868 acc: 50.0%\n",
      "[BATCH 27/149] Loss_D: 0.8035 Loss_G: 0.8830 acc: 53.1%\n",
      "[BATCH 28/149] Loss_D: 0.8865 Loss_G: 0.8895 acc: 37.5%\n",
      "[BATCH 29/149] Loss_D: 0.9007 Loss_G: 0.9024 acc: 48.4%\n",
      "[BATCH 30/149] Loss_D: 0.8332 Loss_G: 0.8976 acc: 45.3%\n",
      "[BATCH 31/149] Loss_D: 0.8999 Loss_G: 0.8990 acc: 42.2%\n",
      "[BATCH 32/149] Loss_D: 0.8151 Loss_G: 0.8886 acc: 45.3%\n",
      "[BATCH 33/149] Loss_D: 0.8371 Loss_G: 0.8868 acc: 60.9%\n",
      "[BATCH 34/149] Loss_D: 0.8609 Loss_G: 0.8874 acc: 57.8%\n",
      "[BATCH 35/149] Loss_D: 0.8357 Loss_G: 0.8865 acc: 57.8%\n",
      "[BATCH 36/149] Loss_D: 0.8980 Loss_G: 0.8944 acc: 57.8%\n",
      "[BATCH 37/149] Loss_D: 0.8323 Loss_G: 0.8969 acc: 42.2%\n",
      "[BATCH 38/149] Loss_D: 0.8905 Loss_G: 0.9023 acc: 45.3%\n",
      "[BATCH 39/149] Loss_D: 0.8441 Loss_G: 0.9072 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.8787 Loss_G: 0.9024 acc: 45.3%\n",
      "[BATCH 41/149] Loss_D: 0.8110 Loss_G: 0.8959 acc: 54.7%\n",
      "[BATCH 42/149] Loss_D: 0.8233 Loss_G: 0.8917 acc: 53.1%\n",
      "[BATCH 43/149] Loss_D: 0.8207 Loss_G: 0.8879 acc: 48.4%\n",
      "[BATCH 44/149] Loss_D: 0.8552 Loss_G: 0.8922 acc: 53.1%\n",
      "[BATCH 45/149] Loss_D: 0.8270 Loss_G: 0.8956 acc: 48.4%\n",
      "[BATCH 46/149] Loss_D: 0.8697 Loss_G: 0.9012 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.9093 Loss_G: 0.9068 acc: 46.9%\n",
      "[BATCH 48/149] Loss_D: 0.8453 Loss_G: 0.8995 acc: 50.0%\n",
      "[BATCH 49/149] Loss_D: 0.7911 Loss_G: 0.8930 acc: 42.2%\n",
      "[BATCH 50/149] Loss_D: 0.8821 Loss_G: 0.8928 acc: 54.7%\n",
      "[BATCH 51/149] Loss_D: 0.8320 Loss_G: 0.8858 acc: 50.0%\n",
      "[BATCH 52/149] Loss_D: 0.7771 Loss_G: 0.8792 acc: 42.2%\n",
      "[BATCH 53/149] Loss_D: 0.8255 Loss_G: 0.8810 acc: 48.4%\n",
      "[BATCH 54/149] Loss_D: 0.8129 Loss_G: 0.8812 acc: 53.1%\n",
      "[BATCH 55/149] Loss_D: 0.8623 Loss_G: 0.8852 acc: 46.9%\n",
      "[BATCH 56/149] Loss_D: 0.8704 Loss_G: 0.8922 acc: 50.0%\n",
      "[BATCH 57/149] Loss_D: 0.8052 Loss_G: 0.8928 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.8082 Loss_G: 0.8876 acc: 56.2%\n",
      "[EPOCH 1250] TEST ACC is : 54.5%\n",
      "[BATCH 59/149] Loss_D: 0.8039 Loss_G: 0.8821 acc: 54.7%\n",
      "[BATCH 60/149] Loss_D: 0.8753 Loss_G: 0.8889 acc: 51.6%\n",
      "[BATCH 61/149] Loss_D: 0.8979 Loss_G: 0.9011 acc: 43.8%\n",
      "[BATCH 62/149] Loss_D: 0.8267 Loss_G: 0.9003 acc: 45.3%\n",
      "[BATCH 63/149] Loss_D: 0.8758 Loss_G: 0.8969 acc: 43.8%\n",
      "[BATCH 64/149] Loss_D: 0.8437 Loss_G: 0.8972 acc: 51.6%\n",
      "[BATCH 65/149] Loss_D: 0.8683 Loss_G: 0.8998 acc: 45.3%\n",
      "[BATCH 66/149] Loss_D: 0.8230 Loss_G: 0.8991 acc: 56.2%\n",
      "[BATCH 67/149] Loss_D: 0.8960 Loss_G: 0.9100 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.9110 Loss_G: 0.9287 acc: 54.7%\n",
      "[BATCH 69/149] Loss_D: 0.8758 Loss_G: 0.9277 acc: 51.6%\n",
      "[BATCH 70/149] Loss_D: 0.8255 Loss_G: 0.9221 acc: 59.4%\n",
      "[BATCH 71/149] Loss_D: 0.8287 Loss_G: 0.9097 acc: 54.7%\n",
      "[BATCH 72/149] Loss_D: 0.9046 Loss_G: 0.9076 acc: 54.7%\n",
      "[BATCH 73/149] Loss_D: 0.8292 Loss_G: 0.9026 acc: 51.6%\n",
      "[BATCH 74/149] Loss_D: 0.8434 Loss_G: 0.8946 acc: 48.4%\n",
      "[BATCH 75/149] Loss_D: 0.8393 Loss_G: 0.8915 acc: 48.4%\n",
      "[BATCH 76/149] Loss_D: 0.8198 Loss_G: 0.8899 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.7789 Loss_G: 0.8873 acc: 59.4%\n",
      "[BATCH 78/149] Loss_D: 0.8410 Loss_G: 0.8898 acc: 57.8%\n",
      "[BATCH 79/149] Loss_D: 0.8152 Loss_G: 0.8995 acc: 53.1%\n",
      "[BATCH 80/149] Loss_D: 0.8894 Loss_G: 0.9029 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.8777 Loss_G: 0.9279 acc: 53.1%\n",
      "[BATCH 82/149] Loss_D: 0.8909 Loss_G: 0.9324 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8390 Loss_G: 0.9187 acc: 57.8%\n",
      "[BATCH 84/149] Loss_D: 0.8432 Loss_G: 0.9071 acc: 46.9%\n",
      "[BATCH 85/149] Loss_D: 0.8277 Loss_G: 0.9048 acc: 54.7%\n",
      "[BATCH 86/149] Loss_D: 0.8636 Loss_G: 0.9058 acc: 56.2%\n",
      "[BATCH 87/149] Loss_D: 0.8756 Loss_G: 0.9046 acc: 48.4%\n",
      "[BATCH 88/149] Loss_D: 0.9077 Loss_G: 0.9163 acc: 51.6%\n",
      "[BATCH 89/149] Loss_D: 0.8691 Loss_G: 0.9149 acc: 45.3%\n",
      "[BATCH 90/149] Loss_D: 0.8246 Loss_G: 0.9057 acc: 46.9%\n",
      "[BATCH 91/149] Loss_D: 0.8526 Loss_G: 0.9002 acc: 51.6%\n",
      "[BATCH 92/149] Loss_D: 0.8644 Loss_G: 0.8988 acc: 48.4%\n",
      "[BATCH 93/149] Loss_D: 0.8830 Loss_G: 0.9058 acc: 51.6%\n",
      "[BATCH 94/149] Loss_D: 0.8852 Loss_G: 0.9219 acc: 51.6%\n",
      "[BATCH 95/149] Loss_D: 0.8609 Loss_G: 0.9194 acc: 60.9%\n",
      "[BATCH 96/149] Loss_D: 0.9182 Loss_G: 0.9243 acc: 53.1%\n",
      "[BATCH 97/149] Loss_D: 0.8358 Loss_G: 0.9146 acc: 51.6%\n",
      "[BATCH 98/149] Loss_D: 0.8684 Loss_G: 0.9095 acc: 54.7%\n",
      "[BATCH 99/149] Loss_D: 0.8357 Loss_G: 0.8984 acc: 56.2%\n",
      "[BATCH 100/149] Loss_D: 0.7891 Loss_G: 0.8904 acc: 51.6%\n",
      "[BATCH 101/149] Loss_D: 0.9140 Loss_G: 0.8983 acc: 43.8%\n",
      "[BATCH 102/149] Loss_D: 0.8040 Loss_G: 0.8912 acc: 48.4%\n",
      "[BATCH 103/149] Loss_D: 0.8568 Loss_G: 0.8937 acc: 51.6%\n",
      "[BATCH 104/149] Loss_D: 0.8374 Loss_G: 0.8921 acc: 50.0%\n",
      "[BATCH 105/149] Loss_D: 0.8290 Loss_G: 0.8932 acc: 40.6%\n",
      "[BATCH 106/149] Loss_D: 0.8177 Loss_G: 0.8809 acc: 57.8%\n",
      "[BATCH 107/149] Loss_D: 0.8016 Loss_G: 0.8789 acc: 43.8%\n",
      "[BATCH 108/149] Loss_D: 0.8693 Loss_G: 0.8813 acc: 53.1%\n",
      "[EPOCH 1300] TEST ACC is : 54.7%\n",
      "[BATCH 109/149] Loss_D: 0.7853 Loss_G: 0.8751 acc: 46.9%\n",
      "[BATCH 110/149] Loss_D: 0.8437 Loss_G: 0.8762 acc: 48.4%\n",
      "[BATCH 111/149] Loss_D: 0.8576 Loss_G: 0.8727 acc: 53.1%\n",
      "[BATCH 112/149] Loss_D: 0.8533 Loss_G: 0.8809 acc: 43.8%\n",
      "[BATCH 113/149] Loss_D: 0.8695 Loss_G: 0.8901 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.8473 Loss_G: 0.8962 acc: 57.8%\n",
      "[BATCH 115/149] Loss_D: 0.7986 Loss_G: 0.8855 acc: 46.9%\n",
      "[BATCH 116/149] Loss_D: 0.8430 Loss_G: 0.8819 acc: 54.7%\n",
      "[BATCH 117/149] Loss_D: 0.7799 Loss_G: 0.8741 acc: 37.5%\n",
      "[BATCH 118/149] Loss_D: 0.8798 Loss_G: 0.8853 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.8521 Loss_G: 0.8869 acc: 42.2%\n",
      "[BATCH 120/149] Loss_D: 0.8270 Loss_G: 0.8861 acc: 48.4%\n",
      "[BATCH 121/149] Loss_D: 0.8628 Loss_G: 0.8923 acc: 54.7%\n",
      "[BATCH 122/149] Loss_D: 0.8822 Loss_G: 0.9129 acc: 53.1%\n",
      "[BATCH 123/149] Loss_D: 0.8774 Loss_G: 0.9094 acc: 48.4%\n",
      "[BATCH 124/149] Loss_D: 0.8224 Loss_G: 0.9022 acc: 40.6%\n",
      "[BATCH 125/149] Loss_D: 0.9057 Loss_G: 0.9040 acc: 50.0%\n",
      "[BATCH 126/149] Loss_D: 0.8953 Loss_G: 0.9065 acc: 53.1%\n",
      "[BATCH 127/149] Loss_D: 0.8417 Loss_G: 0.9072 acc: 46.9%\n",
      "[BATCH 128/149] Loss_D: 0.8980 Loss_G: 0.9187 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.8525 Loss_G: 0.9156 acc: 51.6%\n",
      "[BATCH 130/149] Loss_D: 0.9328 Loss_G: 0.9291 acc: 54.7%\n",
      "[BATCH 131/149] Loss_D: 0.8530 Loss_G: 0.9251 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.8503 Loss_G: 0.9274 acc: 48.4%\n",
      "[BATCH 133/149] Loss_D: 0.8643 Loss_G: 0.9269 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.8295 Loss_G: 0.9146 acc: 45.3%\n",
      "[BATCH 135/149] Loss_D: 0.8111 Loss_G: 0.9052 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.8482 Loss_G: 0.9026 acc: 46.9%\n",
      "[BATCH 137/149] Loss_D: 0.7928 Loss_G: 0.8983 acc: 45.3%\n",
      "[BATCH 138/149] Loss_D: 0.8348 Loss_G: 0.8964 acc: 53.1%\n",
      "[BATCH 139/149] Loss_D: 0.8818 Loss_G: 0.9012 acc: 50.0%\n",
      "[BATCH 140/149] Loss_D: 0.8363 Loss_G: 0.9002 acc: 48.4%\n",
      "[BATCH 141/149] Loss_D: 0.8343 Loss_G: 0.9039 acc: 45.3%\n",
      "[BATCH 142/149] Loss_D: 0.8240 Loss_G: 0.8908 acc: 45.3%\n",
      "[BATCH 143/149] Loss_D: 0.8652 Loss_G: 0.8864 acc: 45.3%\n",
      "[BATCH 144/149] Loss_D: 0.8354 Loss_G: 0.8892 acc: 56.2%\n",
      "[BATCH 145/149] Loss_D: 0.8820 Loss_G: 0.8964 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.8819 Loss_G: 0.9032 acc: 46.9%\n",
      "[BATCH 147/149] Loss_D: 0.8686 Loss_G: 0.9081 acc: 40.6%\n",
      "[BATCH 148/149] Loss_D: 0.8710 Loss_G: 0.9051 acc: 50.0%\n",
      "[BATCH 149/149] Loss_D: 0.8264 Loss_G: 0.9041 acc: 48.4%\n",
      "-----THE [9/50] epoch end-----\n",
      "-----THE [10/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8524 Loss_G: 0.8988 acc: 54.7%\n",
      "[BATCH 2/149] Loss_D: 0.8277 Loss_G: 0.8915 acc: 54.7%\n",
      "[BATCH 3/149] Loss_D: 0.7802 Loss_G: 0.8800 acc: 51.6%\n",
      "[BATCH 4/149] Loss_D: 0.8147 Loss_G: 0.8717 acc: 53.1%\n",
      "[BATCH 5/149] Loss_D: 0.8229 Loss_G: 0.8779 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.8780 Loss_G: 0.8835 acc: 53.1%\n",
      "[BATCH 7/149] Loss_D: 0.7909 Loss_G: 0.8791 acc: 40.6%\n",
      "[BATCH 8/149] Loss_D: 0.8573 Loss_G: 0.8778 acc: 46.9%\n",
      "[BATCH 9/149] Loss_D: 0.8460 Loss_G: 0.8796 acc: 56.2%\n",
      "[EPOCH 1350] TEST ACC is : 54.3%\n",
      "[BATCH 10/149] Loss_D: 0.7886 Loss_G: 0.8739 acc: 45.3%\n",
      "[BATCH 11/149] Loss_D: 0.8186 Loss_G: 0.8741 acc: 51.6%\n",
      "[BATCH 12/149] Loss_D: 0.8401 Loss_G: 0.8788 acc: 53.1%\n",
      "[BATCH 13/149] Loss_D: 0.8680 Loss_G: 0.8912 acc: 48.4%\n",
      "[BATCH 14/149] Loss_D: 0.8414 Loss_G: 0.8974 acc: 53.1%\n",
      "[BATCH 15/149] Loss_D: 0.8933 Loss_G: 0.9034 acc: 51.6%\n",
      "[BATCH 16/149] Loss_D: 0.8114 Loss_G: 0.8931 acc: 54.7%\n",
      "[BATCH 17/149] Loss_D: 0.8335 Loss_G: 0.8818 acc: 56.2%\n",
      "[BATCH 18/149] Loss_D: 0.8614 Loss_G: 0.8806 acc: 56.2%\n",
      "[BATCH 19/149] Loss_D: 0.8563 Loss_G: 0.8790 acc: 50.0%\n",
      "[BATCH 20/149] Loss_D: 0.8453 Loss_G: 0.8825 acc: 51.6%\n",
      "[BATCH 21/149] Loss_D: 0.8026 Loss_G: 0.8803 acc: 51.6%\n",
      "[BATCH 22/149] Loss_D: 0.8003 Loss_G: 0.8741 acc: 59.4%\n",
      "[BATCH 23/149] Loss_D: 0.8265 Loss_G: 0.8737 acc: 45.3%\n",
      "[BATCH 24/149] Loss_D: 0.8785 Loss_G: 0.8847 acc: 45.3%\n",
      "[BATCH 25/149] Loss_D: 0.8263 Loss_G: 0.8931 acc: 51.6%\n",
      "[BATCH 26/149] Loss_D: 0.8317 Loss_G: 0.8898 acc: 50.0%\n",
      "[BATCH 27/149] Loss_D: 0.8426 Loss_G: 0.8840 acc: 54.7%\n",
      "[BATCH 28/149] Loss_D: 0.8444 Loss_G: 0.8872 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.8583 Loss_G: 0.8900 acc: 48.4%\n",
      "[BATCH 30/149] Loss_D: 0.8371 Loss_G: 0.8946 acc: 51.6%\n",
      "[BATCH 31/149] Loss_D: 0.8039 Loss_G: 0.8830 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.8256 Loss_G: 0.8825 acc: 45.3%\n",
      "[BATCH 33/149] Loss_D: 0.8834 Loss_G: 0.8919 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8007 Loss_G: 0.8898 acc: 60.9%\n",
      "[BATCH 35/149] Loss_D: 0.9005 Loss_G: 0.8973 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.9285 Loss_G: 0.9223 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.8048 Loss_G: 0.9101 acc: 48.4%\n",
      "[BATCH 38/149] Loss_D: 0.8612 Loss_G: 0.9069 acc: 45.3%\n",
      "[BATCH 39/149] Loss_D: 0.8361 Loss_G: 0.8938 acc: 45.3%\n",
      "[BATCH 40/149] Loss_D: 0.8303 Loss_G: 0.8901 acc: 46.9%\n",
      "[BATCH 41/149] Loss_D: 0.8795 Loss_G: 0.8998 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.8364 Loss_G: 0.8982 acc: 59.4%\n",
      "[BATCH 43/149] Loss_D: 0.8318 Loss_G: 0.8967 acc: 50.0%\n",
      "[BATCH 44/149] Loss_D: 0.8138 Loss_G: 0.8890 acc: 46.9%\n",
      "[BATCH 45/149] Loss_D: 0.8175 Loss_G: 0.8848 acc: 48.4%\n",
      "[BATCH 46/149] Loss_D: 0.8236 Loss_G: 0.8833 acc: 46.9%\n",
      "[BATCH 47/149] Loss_D: 0.8628 Loss_G: 0.8869 acc: 60.9%\n",
      "[BATCH 48/149] Loss_D: 0.8583 Loss_G: 0.8895 acc: 46.9%\n",
      "[BATCH 49/149] Loss_D: 0.8277 Loss_G: 0.8926 acc: 50.0%\n",
      "[BATCH 50/149] Loss_D: 0.8358 Loss_G: 0.8850 acc: 43.8%\n",
      "[BATCH 51/149] Loss_D: 0.8362 Loss_G: 0.8861 acc: 48.4%\n",
      "[BATCH 52/149] Loss_D: 0.8430 Loss_G: 0.8864 acc: 40.6%\n",
      "[BATCH 53/149] Loss_D: 0.8619 Loss_G: 0.8895 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.8324 Loss_G: 0.8943 acc: 51.6%\n",
      "[BATCH 55/149] Loss_D: 0.8192 Loss_G: 0.8884 acc: 42.2%\n",
      "[BATCH 56/149] Loss_D: 0.8652 Loss_G: 0.8859 acc: 54.7%\n",
      "[BATCH 57/149] Loss_D: 0.8172 Loss_G: 0.8903 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.8232 Loss_G: 0.8845 acc: 51.6%\n",
      "[BATCH 59/149] Loss_D: 0.8403 Loss_G: 0.8855 acc: 54.7%\n",
      "[EPOCH 1400] TEST ACC is : 54.5%\n",
      "[BATCH 60/149] Loss_D: 0.8761 Loss_G: 0.8959 acc: 54.7%\n",
      "[BATCH 61/149] Loss_D: 0.7893 Loss_G: 0.8829 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.8785 Loss_G: 0.8832 acc: 56.2%\n",
      "[BATCH 63/149] Loss_D: 0.8968 Loss_G: 0.8926 acc: 45.3%\n",
      "[BATCH 64/149] Loss_D: 0.8824 Loss_G: 0.9035 acc: 51.6%\n",
      "[BATCH 65/149] Loss_D: 0.8788 Loss_G: 0.9050 acc: 53.1%\n",
      "[BATCH 66/149] Loss_D: 0.8567 Loss_G: 0.9036 acc: 51.6%\n",
      "[BATCH 67/149] Loss_D: 0.8277 Loss_G: 0.8986 acc: 53.1%\n",
      "[BATCH 68/149] Loss_D: 0.8682 Loss_G: 0.9022 acc: 53.1%\n",
      "[BATCH 69/149] Loss_D: 0.8562 Loss_G: 0.8991 acc: 53.1%\n",
      "[BATCH 70/149] Loss_D: 0.8459 Loss_G: 0.8991 acc: 50.0%\n",
      "[BATCH 71/149] Loss_D: 0.9101 Loss_G: 0.9045 acc: 50.0%\n",
      "[BATCH 72/149] Loss_D: 0.8603 Loss_G: 0.9116 acc: 48.4%\n",
      "[BATCH 73/149] Loss_D: 0.9069 Loss_G: 0.9173 acc: 60.9%\n",
      "[BATCH 74/149] Loss_D: 0.8020 Loss_G: 0.9017 acc: 46.9%\n",
      "[BATCH 75/149] Loss_D: 0.7992 Loss_G: 0.8815 acc: 43.8%\n",
      "[BATCH 76/149] Loss_D: 0.8369 Loss_G: 0.8763 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.7980 Loss_G: 0.8753 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.8373 Loss_G: 0.8797 acc: 48.4%\n",
      "[BATCH 79/149] Loss_D: 0.8422 Loss_G: 0.8838 acc: 54.7%\n",
      "[BATCH 80/149] Loss_D: 0.8270 Loss_G: 0.8791 acc: 51.6%\n",
      "[BATCH 81/149] Loss_D: 0.7837 Loss_G: 0.8716 acc: 62.5%\n",
      "[BATCH 82/149] Loss_D: 0.8412 Loss_G: 0.8714 acc: 46.9%\n",
      "[BATCH 83/149] Loss_D: 0.8452 Loss_G: 0.8786 acc: 56.2%\n",
      "[BATCH 84/149] Loss_D: 0.8674 Loss_G: 0.8790 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.8759 Loss_G: 0.8885 acc: 53.1%\n",
      "[BATCH 86/149] Loss_D: 0.8803 Loss_G: 0.8931 acc: 53.1%\n",
      "[BATCH 87/149] Loss_D: 0.8751 Loss_G: 0.8954 acc: 40.6%\n",
      "[BATCH 88/149] Loss_D: 0.8524 Loss_G: 0.8922 acc: 45.3%\n",
      "[BATCH 89/149] Loss_D: 0.8407 Loss_G: 0.8875 acc: 51.6%\n",
      "[BATCH 90/149] Loss_D: 0.8413 Loss_G: 0.8912 acc: 46.9%\n",
      "[BATCH 91/149] Loss_D: 0.8365 Loss_G: 0.8948 acc: 43.8%\n",
      "[BATCH 92/149] Loss_D: 0.8343 Loss_G: 0.8926 acc: 51.6%\n",
      "[BATCH 93/149] Loss_D: 0.8250 Loss_G: 0.8956 acc: 39.1%\n",
      "[BATCH 94/149] Loss_D: 0.9090 Loss_G: 0.9124 acc: 45.3%\n",
      "[BATCH 95/149] Loss_D: 0.9241 Loss_G: 0.9235 acc: 54.7%\n",
      "[BATCH 96/149] Loss_D: 0.8596 Loss_G: 0.9311 acc: 48.4%\n",
      "[BATCH 97/149] Loss_D: 0.8999 Loss_G: 0.9239 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.8308 Loss_G: 0.9068 acc: 35.9%\n",
      "[BATCH 99/149] Loss_D: 0.8811 Loss_G: 0.9016 acc: 51.6%\n",
      "[BATCH 100/149] Loss_D: 0.8216 Loss_G: 0.8948 acc: 59.4%\n",
      "[BATCH 101/149] Loss_D: 0.8468 Loss_G: 0.8957 acc: 43.8%\n",
      "[BATCH 102/149] Loss_D: 0.8763 Loss_G: 0.9045 acc: 54.7%\n",
      "[BATCH 103/149] Loss_D: 0.8753 Loss_G: 0.9022 acc: 56.2%\n",
      "[BATCH 104/149] Loss_D: 0.8623 Loss_G: 0.9015 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.8622 Loss_G: 0.8961 acc: 54.7%\n",
      "[BATCH 106/149] Loss_D: 0.8399 Loss_G: 0.8913 acc: 51.6%\n",
      "[BATCH 107/149] Loss_D: 0.8894 Loss_G: 0.8955 acc: 46.9%\n",
      "[BATCH 108/149] Loss_D: 0.8856 Loss_G: 0.8975 acc: 42.2%\n",
      "[BATCH 109/149] Loss_D: 0.8587 Loss_G: 0.9042 acc: 53.1%\n",
      "[EPOCH 1450] TEST ACC is : 53.3%\n",
      "[BATCH 110/149] Loss_D: 0.8663 Loss_G: 0.8989 acc: 54.7%\n",
      "[BATCH 111/149] Loss_D: 0.9052 Loss_G: 0.9063 acc: 45.3%\n",
      "[BATCH 112/149] Loss_D: 0.7970 Loss_G: 0.8968 acc: 48.4%\n",
      "[BATCH 113/149] Loss_D: 0.8505 Loss_G: 0.8896 acc: 59.4%\n",
      "[BATCH 114/149] Loss_D: 0.8532 Loss_G: 0.8901 acc: 46.9%\n",
      "[BATCH 115/149] Loss_D: 0.8769 Loss_G: 0.8898 acc: 48.4%\n",
      "[BATCH 116/149] Loss_D: 0.7961 Loss_G: 0.8870 acc: 53.1%\n",
      "[BATCH 117/149] Loss_D: 0.8543 Loss_G: 0.8844 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.8493 Loss_G: 0.8876 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.8427 Loss_G: 0.8902 acc: 48.4%\n",
      "[BATCH 120/149] Loss_D: 0.8045 Loss_G: 0.8818 acc: 48.4%\n",
      "[BATCH 121/149] Loss_D: 0.8358 Loss_G: 0.8785 acc: 53.1%\n",
      "[BATCH 122/149] Loss_D: 0.8363 Loss_G: 0.8806 acc: 51.6%\n",
      "[BATCH 123/149] Loss_D: 0.8369 Loss_G: 0.8795 acc: 45.3%\n",
      "[BATCH 124/149] Loss_D: 0.8337 Loss_G: 0.8858 acc: 50.0%\n",
      "[BATCH 125/149] Loss_D: 0.9223 Loss_G: 0.9007 acc: 57.8%\n",
      "[BATCH 126/149] Loss_D: 0.8697 Loss_G: 0.9060 acc: 46.9%\n",
      "[BATCH 127/149] Loss_D: 0.8116 Loss_G: 0.8884 acc: 48.4%\n",
      "[BATCH 128/149] Loss_D: 0.8460 Loss_G: 0.8925 acc: 29.7%\n",
      "[BATCH 129/149] Loss_D: 0.8118 Loss_G: 0.8794 acc: 46.9%\n",
      "[BATCH 130/149] Loss_D: 0.8332 Loss_G: 0.8825 acc: 65.6%\n",
      "[BATCH 131/149] Loss_D: 0.8204 Loss_G: 0.8803 acc: 40.6%\n",
      "[BATCH 132/149] Loss_D: 0.8218 Loss_G: 0.8774 acc: 46.9%\n",
      "[BATCH 133/149] Loss_D: 0.9128 Loss_G: 0.8942 acc: 51.6%\n",
      "[BATCH 134/149] Loss_D: 0.8661 Loss_G: 0.8947 acc: 45.3%\n",
      "[BATCH 135/149] Loss_D: 0.8177 Loss_G: 0.8841 acc: 51.6%\n",
      "[BATCH 136/149] Loss_D: 0.8452 Loss_G: 0.8768 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8619 Loss_G: 0.8814 acc: 54.7%\n",
      "[BATCH 138/149] Loss_D: 0.8102 Loss_G: 0.8806 acc: 54.7%\n",
      "[BATCH 139/149] Loss_D: 0.9321 Loss_G: 0.8973 acc: 54.7%\n",
      "[BATCH 140/149] Loss_D: 0.8282 Loss_G: 0.8955 acc: 53.1%\n",
      "[BATCH 141/149] Loss_D: 0.8569 Loss_G: 0.8973 acc: 45.3%\n",
      "[BATCH 142/149] Loss_D: 0.8353 Loss_G: 0.8845 acc: 53.1%\n",
      "[BATCH 143/149] Loss_D: 0.8738 Loss_G: 0.8857 acc: 45.3%\n",
      "[BATCH 144/149] Loss_D: 0.8553 Loss_G: 0.8880 acc: 57.8%\n",
      "[BATCH 145/149] Loss_D: 0.8875 Loss_G: 0.8929 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.8241 Loss_G: 0.8835 acc: 54.7%\n",
      "[BATCH 147/149] Loss_D: 0.7830 Loss_G: 0.8751 acc: 53.1%\n",
      "[BATCH 148/149] Loss_D: 0.8767 Loss_G: 0.8794 acc: 56.2%\n",
      "[BATCH 149/149] Loss_D: 0.8414 Loss_G: 0.8887 acc: 48.4%\n",
      "-----THE [10/50] epoch end-----\n",
      "-----THE [11/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8515 Loss_G: 0.8836 acc: 57.8%\n",
      "[BATCH 2/149] Loss_D: 0.8539 Loss_G: 0.8907 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.8715 Loss_G: 0.8914 acc: 56.2%\n",
      "[BATCH 4/149] Loss_D: 0.7797 Loss_G: 0.8816 acc: 40.6%\n",
      "[BATCH 5/149] Loss_D: 0.7893 Loss_G: 0.8744 acc: 46.9%\n",
      "[BATCH 6/149] Loss_D: 0.9141 Loss_G: 0.8940 acc: 54.7%\n",
      "[BATCH 7/149] Loss_D: 0.8659 Loss_G: 0.8980 acc: 59.4%\n",
      "[BATCH 8/149] Loss_D: 0.8154 Loss_G: 0.8878 acc: 42.2%\n",
      "[BATCH 9/149] Loss_D: 0.8424 Loss_G: 0.8837 acc: 56.2%\n",
      "[BATCH 10/149] Loss_D: 0.8386 Loss_G: 0.8788 acc: 53.1%\n",
      "[EPOCH 1500] TEST ACC is : 55.1%\n",
      "[BATCH 11/149] Loss_D: 0.9020 Loss_G: 0.8912 acc: 48.4%\n",
      "[BATCH 12/149] Loss_D: 0.8690 Loss_G: 0.8930 acc: 45.3%\n",
      "[BATCH 13/149] Loss_D: 0.8794 Loss_G: 0.8977 acc: 54.7%\n",
      "[BATCH 14/149] Loss_D: 0.8451 Loss_G: 0.8875 acc: 48.4%\n",
      "[BATCH 15/149] Loss_D: 0.8378 Loss_G: 0.8843 acc: 51.6%\n",
      "[BATCH 16/149] Loss_D: 0.8316 Loss_G: 0.8804 acc: 50.0%\n",
      "[BATCH 17/149] Loss_D: 0.8595 Loss_G: 0.8827 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.8490 Loss_G: 0.8852 acc: 57.8%\n",
      "[BATCH 19/149] Loss_D: 0.8089 Loss_G: 0.8842 acc: 48.4%\n",
      "[BATCH 20/149] Loss_D: 0.8626 Loss_G: 0.8995 acc: 56.2%\n",
      "[BATCH 21/149] Loss_D: 0.7953 Loss_G: 0.8813 acc: 43.8%\n",
      "[BATCH 22/149] Loss_D: 0.8790 Loss_G: 0.8735 acc: 46.9%\n",
      "[BATCH 23/149] Loss_D: 0.8879 Loss_G: 0.8860 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.8654 Loss_G: 0.8895 acc: 53.1%\n",
      "[BATCH 25/149] Loss_D: 0.8335 Loss_G: 0.8849 acc: 48.4%\n",
      "[BATCH 26/149] Loss_D: 0.8561 Loss_G: 0.8867 acc: 51.6%\n",
      "[BATCH 27/149] Loss_D: 0.8134 Loss_G: 0.8832 acc: 45.3%\n",
      "[BATCH 28/149] Loss_D: 0.8777 Loss_G: 0.8874 acc: 51.6%\n",
      "[BATCH 29/149] Loss_D: 0.8662 Loss_G: 0.8948 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.8039 Loss_G: 0.8950 acc: 42.2%\n",
      "[BATCH 31/149] Loss_D: 0.7957 Loss_G: 0.8857 acc: 46.9%\n",
      "[BATCH 32/149] Loss_D: 0.8290 Loss_G: 0.8802 acc: 56.2%\n",
      "[BATCH 33/149] Loss_D: 0.8835 Loss_G: 0.8901 acc: 53.1%\n",
      "[BATCH 34/149] Loss_D: 0.8525 Loss_G: 0.8950 acc: 54.7%\n",
      "[BATCH 35/149] Loss_D: 0.8246 Loss_G: 0.8843 acc: 57.8%\n",
      "[BATCH 36/149] Loss_D: 0.8421 Loss_G: 0.8798 acc: 51.6%\n",
      "[BATCH 37/149] Loss_D: 0.8139 Loss_G: 0.8733 acc: 54.7%\n",
      "[BATCH 38/149] Loss_D: 0.8683 Loss_G: 0.8792 acc: 51.6%\n",
      "[BATCH 39/149] Loss_D: 0.7779 Loss_G: 0.8771 acc: 54.7%\n",
      "[BATCH 40/149] Loss_D: 0.8586 Loss_G: 0.8808 acc: 53.1%\n",
      "[BATCH 41/149] Loss_D: 0.8142 Loss_G: 0.8753 acc: 59.4%\n",
      "[BATCH 42/149] Loss_D: 0.8751 Loss_G: 0.8786 acc: 59.4%\n",
      "[BATCH 43/149] Loss_D: 0.8245 Loss_G: 0.8753 acc: 46.9%\n",
      "[BATCH 44/149] Loss_D: 0.8166 Loss_G: 0.8709 acc: 51.6%\n",
      "[BATCH 45/149] Loss_D: 0.8397 Loss_G: 0.8716 acc: 48.4%\n",
      "[BATCH 46/149] Loss_D: 0.8034 Loss_G: 0.8678 acc: 48.4%\n",
      "[BATCH 47/149] Loss_D: 0.8526 Loss_G: 0.8728 acc: 51.6%\n",
      "[BATCH 48/149] Loss_D: 0.8612 Loss_G: 0.8842 acc: 68.8%\n",
      "[BATCH 49/149] Loss_D: 0.8490 Loss_G: 0.8855 acc: 45.3%\n",
      "[BATCH 50/149] Loss_D: 0.8885 Loss_G: 0.8914 acc: 46.9%\n",
      "[BATCH 51/149] Loss_D: 0.8631 Loss_G: 0.8907 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.8198 Loss_G: 0.8810 acc: 50.0%\n",
      "[BATCH 53/149] Loss_D: 0.7930 Loss_G: 0.8731 acc: 54.7%\n",
      "[BATCH 54/149] Loss_D: 0.8758 Loss_G: 0.8761 acc: 50.0%\n",
      "[BATCH 55/149] Loss_D: 0.7930 Loss_G: 0.8732 acc: 51.6%\n",
      "[BATCH 56/149] Loss_D: 0.8320 Loss_G: 0.8708 acc: 60.9%\n",
      "[BATCH 57/149] Loss_D: 0.7927 Loss_G: 0.8687 acc: 54.7%\n",
      "[BATCH 58/149] Loss_D: 0.8751 Loss_G: 0.8783 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.8296 Loss_G: 0.8803 acc: 54.7%\n",
      "[BATCH 60/149] Loss_D: 0.8709 Loss_G: 0.8835 acc: 39.1%\n",
      "[EPOCH 1550] TEST ACC is : 54.5%\n",
      "[BATCH 61/149] Loss_D: 0.8241 Loss_G: 0.8804 acc: 53.1%\n",
      "[BATCH 62/149] Loss_D: 0.8743 Loss_G: 0.8876 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.8771 Loss_G: 0.8912 acc: 53.1%\n",
      "[BATCH 64/149] Loss_D: 0.9080 Loss_G: 0.8957 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8594 Loss_G: 0.8942 acc: 57.8%\n",
      "[BATCH 66/149] Loss_D: 0.8305 Loss_G: 0.8844 acc: 51.6%\n",
      "[BATCH 67/149] Loss_D: 0.7963 Loss_G: 0.8858 acc: 40.6%\n",
      "[BATCH 68/149] Loss_D: 0.8317 Loss_G: 0.8749 acc: 50.0%\n",
      "[BATCH 69/149] Loss_D: 0.8957 Loss_G: 0.8789 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.8508 Loss_G: 0.8803 acc: 57.8%\n",
      "[BATCH 71/149] Loss_D: 0.8153 Loss_G: 0.8808 acc: 54.7%\n",
      "[BATCH 72/149] Loss_D: 0.9393 Loss_G: 0.8958 acc: 54.7%\n",
      "[BATCH 73/149] Loss_D: 0.8451 Loss_G: 0.8949 acc: 40.6%\n",
      "[BATCH 74/149] Loss_D: 0.8579 Loss_G: 0.8889 acc: 57.8%\n",
      "[BATCH 75/149] Loss_D: 0.8492 Loss_G: 0.8836 acc: 53.1%\n",
      "[BATCH 76/149] Loss_D: 0.8491 Loss_G: 0.8810 acc: 48.4%\n",
      "[BATCH 77/149] Loss_D: 0.8510 Loss_G: 0.8783 acc: 54.7%\n",
      "[BATCH 78/149] Loss_D: 0.8255 Loss_G: 0.8731 acc: 56.2%\n",
      "[BATCH 79/149] Loss_D: 0.8530 Loss_G: 0.8787 acc: 53.1%\n",
      "[BATCH 80/149] Loss_D: 0.8250 Loss_G: 0.8768 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.8435 Loss_G: 0.8794 acc: 43.8%\n",
      "[BATCH 82/149] Loss_D: 0.9361 Loss_G: 0.8977 acc: 53.1%\n",
      "[BATCH 83/149] Loss_D: 0.8209 Loss_G: 0.8936 acc: 56.2%\n",
      "[BATCH 84/149] Loss_D: 0.8101 Loss_G: 0.8778 acc: 48.4%\n",
      "[BATCH 85/149] Loss_D: 0.8276 Loss_G: 0.8720 acc: 50.0%\n",
      "[BATCH 86/149] Loss_D: 0.8436 Loss_G: 0.8700 acc: 53.1%\n",
      "[BATCH 87/149] Loss_D: 0.9038 Loss_G: 0.8846 acc: 51.6%\n",
      "[BATCH 88/149] Loss_D: 0.8637 Loss_G: 0.8928 acc: 42.2%\n",
      "[BATCH 89/149] Loss_D: 0.8886 Loss_G: 0.8975 acc: 54.7%\n",
      "[BATCH 90/149] Loss_D: 0.8200 Loss_G: 0.8841 acc: 57.8%\n",
      "[BATCH 91/149] Loss_D: 0.8801 Loss_G: 0.8911 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.8432 Loss_G: 0.8823 acc: 48.4%\n",
      "[BATCH 93/149] Loss_D: 0.8195 Loss_G: 0.8860 acc: 54.7%\n",
      "[BATCH 94/149] Loss_D: 0.7957 Loss_G: 0.8750 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.8887 Loss_G: 0.8849 acc: 46.9%\n",
      "[BATCH 96/149] Loss_D: 0.8337 Loss_G: 0.8873 acc: 54.7%\n",
      "[BATCH 97/149] Loss_D: 0.8135 Loss_G: 0.8853 acc: 46.9%\n",
      "[BATCH 98/149] Loss_D: 0.8106 Loss_G: 0.8856 acc: 48.4%\n",
      "[BATCH 99/149] Loss_D: 0.8539 Loss_G: 0.8868 acc: 45.3%\n",
      "[BATCH 100/149] Loss_D: 0.8263 Loss_G: 0.8857 acc: 53.1%\n",
      "[BATCH 101/149] Loss_D: 0.8244 Loss_G: 0.8752 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.8214 Loss_G: 0.8690 acc: 53.1%\n",
      "[BATCH 103/149] Loss_D: 0.8613 Loss_G: 0.8755 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8118 Loss_G: 0.8728 acc: 48.4%\n",
      "[BATCH 105/149] Loss_D: 0.9164 Loss_G: 0.8902 acc: 56.2%\n",
      "[BATCH 106/149] Loss_D: 0.8871 Loss_G: 0.8960 acc: 54.7%\n",
      "[BATCH 107/149] Loss_D: 0.8402 Loss_G: 0.8916 acc: 54.7%\n",
      "[BATCH 108/149] Loss_D: 0.8949 Loss_G: 0.8971 acc: 51.6%\n",
      "[BATCH 109/149] Loss_D: 0.8881 Loss_G: 0.9058 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.8671 Loss_G: 0.8993 acc: 51.6%\n",
      "[EPOCH 1600] TEST ACC is : 53.7%\n",
      "[BATCH 111/149] Loss_D: 0.7982 Loss_G: 0.8836 acc: 51.6%\n",
      "[BATCH 112/149] Loss_D: 0.7948 Loss_G: 0.8718 acc: 39.1%\n",
      "[BATCH 113/149] Loss_D: 0.9015 Loss_G: 0.8801 acc: 48.4%\n",
      "[BATCH 114/149] Loss_D: 0.8085 Loss_G: 0.8779 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.9064 Loss_G: 0.8989 acc: 51.6%\n",
      "[BATCH 116/149] Loss_D: 0.8430 Loss_G: 0.8898 acc: 54.7%\n",
      "[BATCH 117/149] Loss_D: 0.8103 Loss_G: 0.8731 acc: 48.4%\n",
      "[BATCH 118/149] Loss_D: 0.8260 Loss_G: 0.8668 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.8230 Loss_G: 0.8706 acc: 46.9%\n",
      "[BATCH 120/149] Loss_D: 0.8876 Loss_G: 0.8795 acc: 57.8%\n",
      "[BATCH 121/149] Loss_D: 0.8577 Loss_G: 0.8916 acc: 46.9%\n",
      "[BATCH 122/149] Loss_D: 0.8224 Loss_G: 0.8838 acc: 48.4%\n",
      "[BATCH 123/149] Loss_D: 0.8069 Loss_G: 0.8786 acc: 53.1%\n",
      "[BATCH 124/149] Loss_D: 0.8810 Loss_G: 0.8844 acc: 54.7%\n",
      "[BATCH 125/149] Loss_D: 0.8560 Loss_G: 0.8868 acc: 53.1%\n",
      "[BATCH 126/149] Loss_D: 0.8163 Loss_G: 0.8809 acc: 54.7%\n",
      "[BATCH 127/149] Loss_D: 0.8351 Loss_G: 0.8792 acc: 50.0%\n",
      "[BATCH 128/149] Loss_D: 0.8049 Loss_G: 0.8720 acc: 48.4%\n",
      "[BATCH 129/149] Loss_D: 0.8201 Loss_G: 0.8675 acc: 53.1%\n",
      "[BATCH 130/149] Loss_D: 0.8259 Loss_G: 0.8671 acc: 48.4%\n",
      "[BATCH 131/149] Loss_D: 0.9148 Loss_G: 0.8823 acc: 54.7%\n",
      "[BATCH 132/149] Loss_D: 0.8293 Loss_G: 0.8809 acc: 50.0%\n",
      "[BATCH 133/149] Loss_D: 0.8202 Loss_G: 0.8792 acc: 48.4%\n",
      "[BATCH 134/149] Loss_D: 0.8603 Loss_G: 0.8845 acc: 50.0%\n",
      "[BATCH 135/149] Loss_D: 0.8732 Loss_G: 0.8935 acc: 50.0%\n",
      "[BATCH 136/149] Loss_D: 0.8391 Loss_G: 0.8893 acc: 50.0%\n",
      "[BATCH 137/149] Loss_D: 0.8227 Loss_G: 0.8819 acc: 50.0%\n",
      "[BATCH 138/149] Loss_D: 0.8072 Loss_G: 0.8778 acc: 45.3%\n",
      "[BATCH 139/149] Loss_D: 0.8057 Loss_G: 0.8759 acc: 59.4%\n",
      "[BATCH 140/149] Loss_D: 0.8014 Loss_G: 0.8737 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.8126 Loss_G: 0.8773 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8053 Loss_G: 0.8758 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.7946 Loss_G: 0.8769 acc: 56.2%\n",
      "[BATCH 144/149] Loss_D: 0.7915 Loss_G: 0.8748 acc: 45.3%\n",
      "[BATCH 145/149] Loss_D: 0.8467 Loss_G: 0.8785 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.8667 Loss_G: 0.8900 acc: 50.0%\n",
      "[BATCH 147/149] Loss_D: 0.8149 Loss_G: 0.8908 acc: 51.6%\n",
      "[BATCH 148/149] Loss_D: 0.8706 Loss_G: 0.8932 acc: 50.0%\n",
      "[BATCH 149/149] Loss_D: 0.8529 Loss_G: 0.8932 acc: 65.6%\n",
      "-----THE [11/50] epoch end-----\n",
      "-----THE [12/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8542 Loss_G: 0.8943 acc: 51.6%\n",
      "[BATCH 2/149] Loss_D: 0.8192 Loss_G: 0.8910 acc: 53.1%\n",
      "[BATCH 3/149] Loss_D: 0.7829 Loss_G: 0.8804 acc: 43.8%\n",
      "[BATCH 4/149] Loss_D: 0.9026 Loss_G: 0.8837 acc: 48.4%\n",
      "[BATCH 5/149] Loss_D: 0.8142 Loss_G: 0.8879 acc: 54.7%\n",
      "[BATCH 6/149] Loss_D: 0.7846 Loss_G: 0.8793 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.7890 Loss_G: 0.8691 acc: 46.9%\n",
      "[BATCH 8/149] Loss_D: 0.8404 Loss_G: 0.8683 acc: 51.6%\n",
      "[BATCH 9/149] Loss_D: 0.8235 Loss_G: 0.8754 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.8478 Loss_G: 0.8748 acc: 57.8%\n",
      "[BATCH 11/149] Loss_D: 0.8203 Loss_G: 0.8713 acc: 53.1%\n",
      "[EPOCH 1650] TEST ACC is : 53.7%\n",
      "[BATCH 12/149] Loss_D: 0.9011 Loss_G: 0.8813 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.8376 Loss_G: 0.8821 acc: 46.9%\n",
      "[BATCH 14/149] Loss_D: 0.8586 Loss_G: 0.8817 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.8630 Loss_G: 0.8825 acc: 54.7%\n",
      "[BATCH 16/149] Loss_D: 0.9128 Loss_G: 0.8960 acc: 50.0%\n",
      "[BATCH 17/149] Loss_D: 0.8918 Loss_G: 0.9015 acc: 50.0%\n",
      "[BATCH 18/149] Loss_D: 0.8016 Loss_G: 0.8932 acc: 56.2%\n",
      "[BATCH 19/149] Loss_D: 0.8101 Loss_G: 0.8787 acc: 43.8%\n",
      "[BATCH 20/149] Loss_D: 0.8178 Loss_G: 0.8721 acc: 50.0%\n",
      "[BATCH 21/149] Loss_D: 0.9061 Loss_G: 0.8783 acc: 56.2%\n",
      "[BATCH 22/149] Loss_D: 0.8311 Loss_G: 0.8802 acc: 56.2%\n",
      "[BATCH 23/149] Loss_D: 0.8173 Loss_G: 0.8732 acc: 50.0%\n",
      "[BATCH 24/149] Loss_D: 0.8156 Loss_G: 0.8676 acc: 42.2%\n",
      "[BATCH 25/149] Loss_D: 0.8522 Loss_G: 0.8724 acc: 53.1%\n",
      "[BATCH 26/149] Loss_D: 0.8261 Loss_G: 0.8707 acc: 45.3%\n",
      "[BATCH 27/149] Loss_D: 0.8041 Loss_G: 0.8703 acc: 53.1%\n",
      "[BATCH 28/149] Loss_D: 0.8547 Loss_G: 0.8738 acc: 39.1%\n",
      "[BATCH 29/149] Loss_D: 0.8318 Loss_G: 0.8759 acc: 48.4%\n",
      "[BATCH 30/149] Loss_D: 0.8290 Loss_G: 0.8784 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.8704 Loss_G: 0.8842 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.8048 Loss_G: 0.8783 acc: 46.9%\n",
      "[BATCH 33/149] Loss_D: 0.8051 Loss_G: 0.8775 acc: 48.4%\n",
      "[BATCH 34/149] Loss_D: 0.8114 Loss_G: 0.8679 acc: 43.8%\n",
      "[BATCH 35/149] Loss_D: 0.8591 Loss_G: 0.8766 acc: 54.7%\n",
      "[BATCH 36/149] Loss_D: 0.8636 Loss_G: 0.8776 acc: 56.2%\n",
      "[BATCH 37/149] Loss_D: 0.8532 Loss_G: 0.8831 acc: 57.8%\n",
      "[BATCH 38/149] Loss_D: 0.8368 Loss_G: 0.8746 acc: 54.7%\n",
      "[BATCH 39/149] Loss_D: 0.8648 Loss_G: 0.8772 acc: 53.1%\n",
      "[BATCH 40/149] Loss_D: 0.8694 Loss_G: 0.8804 acc: 48.4%\n",
      "[BATCH 41/149] Loss_D: 0.8600 Loss_G: 0.8787 acc: 46.9%\n",
      "[BATCH 42/149] Loss_D: 0.8265 Loss_G: 0.8739 acc: 50.0%\n",
      "[BATCH 43/149] Loss_D: 0.8347 Loss_G: 0.8674 acc: 51.6%\n",
      "[BATCH 44/149] Loss_D: 0.8441 Loss_G: 0.8680 acc: 46.9%\n",
      "[BATCH 45/149] Loss_D: 0.8252 Loss_G: 0.8732 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8577 Loss_G: 0.8762 acc: 53.1%\n",
      "[BATCH 47/149] Loss_D: 0.8446 Loss_G: 0.8800 acc: 46.9%\n",
      "[BATCH 48/149] Loss_D: 0.8429 Loss_G: 0.8761 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.8471 Loss_G: 0.8808 acc: 48.4%\n",
      "[BATCH 50/149] Loss_D: 0.8379 Loss_G: 0.8789 acc: 48.4%\n",
      "[BATCH 51/149] Loss_D: 0.8693 Loss_G: 0.8770 acc: 53.1%\n",
      "[BATCH 52/149] Loss_D: 0.8139 Loss_G: 0.8721 acc: 56.2%\n",
      "[BATCH 53/149] Loss_D: 0.8255 Loss_G: 0.8672 acc: 53.1%\n",
      "[BATCH 54/149] Loss_D: 0.8208 Loss_G: 0.8650 acc: 48.4%\n",
      "[BATCH 55/149] Loss_D: 0.8432 Loss_G: 0.8634 acc: 54.7%\n",
      "[BATCH 56/149] Loss_D: 0.9519 Loss_G: 0.8979 acc: 43.8%\n",
      "[BATCH 57/149] Loss_D: 0.9135 Loss_G: 0.9145 acc: 50.0%\n",
      "[BATCH 58/149] Loss_D: 0.7998 Loss_G: 0.9012 acc: 45.3%\n",
      "[BATCH 59/149] Loss_D: 0.8500 Loss_G: 0.8917 acc: 60.9%\n",
      "[BATCH 60/149] Loss_D: 0.8768 Loss_G: 0.8962 acc: 54.7%\n",
      "[BATCH 61/149] Loss_D: 0.8339 Loss_G: 0.8918 acc: 50.0%\n",
      "[EPOCH 1700] TEST ACC is : 54.7%\n",
      "[BATCH 62/149] Loss_D: 0.7957 Loss_G: 0.8843 acc: 54.7%\n",
      "[BATCH 63/149] Loss_D: 0.8886 Loss_G: 0.8886 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.8306 Loss_G: 0.8866 acc: 53.1%\n",
      "[BATCH 65/149] Loss_D: 0.8016 Loss_G: 0.8804 acc: 43.8%\n",
      "[BATCH 66/149] Loss_D: 0.8711 Loss_G: 0.8812 acc: 53.1%\n",
      "[BATCH 67/149] Loss_D: 0.8153 Loss_G: 0.8821 acc: 59.4%\n",
      "[BATCH 68/149] Loss_D: 0.8901 Loss_G: 0.8875 acc: 59.4%\n",
      "[BATCH 69/149] Loss_D: 0.8107 Loss_G: 0.8783 acc: 43.8%\n",
      "[BATCH 70/149] Loss_D: 0.8225 Loss_G: 0.8712 acc: 42.2%\n",
      "[BATCH 71/149] Loss_D: 0.8460 Loss_G: 0.8727 acc: 45.3%\n",
      "[BATCH 72/149] Loss_D: 0.8707 Loss_G: 0.8843 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.7896 Loss_G: 0.8805 acc: 51.6%\n",
      "[BATCH 74/149] Loss_D: 0.8338 Loss_G: 0.8785 acc: 53.1%\n",
      "[BATCH 75/149] Loss_D: 0.8341 Loss_G: 0.8817 acc: 59.4%\n",
      "[BATCH 76/149] Loss_D: 0.8053 Loss_G: 0.8861 acc: 43.8%\n",
      "[BATCH 77/149] Loss_D: 0.8342 Loss_G: 0.8789 acc: 51.6%\n",
      "[BATCH 78/149] Loss_D: 0.8702 Loss_G: 0.8869 acc: 48.4%\n",
      "[BATCH 79/149] Loss_D: 0.8476 Loss_G: 0.8839 acc: 48.4%\n",
      "[BATCH 80/149] Loss_D: 0.7681 Loss_G: 0.8713 acc: 56.2%\n",
      "[BATCH 81/149] Loss_D: 0.8348 Loss_G: 0.8689 acc: 50.0%\n",
      "[BATCH 82/149] Loss_D: 0.8905 Loss_G: 0.8798 acc: 50.0%\n",
      "[BATCH 83/149] Loss_D: 0.8195 Loss_G: 0.8794 acc: 50.0%\n",
      "[BATCH 84/149] Loss_D: 0.7776 Loss_G: 0.8730 acc: 42.2%\n",
      "[BATCH 85/149] Loss_D: 0.7928 Loss_G: 0.8685 acc: 39.1%\n",
      "[BATCH 86/149] Loss_D: 0.8125 Loss_G: 0.8680 acc: 43.8%\n",
      "[BATCH 87/149] Loss_D: 0.8721 Loss_G: 0.8734 acc: 43.8%\n",
      "[BATCH 88/149] Loss_D: 0.8527 Loss_G: 0.8791 acc: 56.2%\n",
      "[BATCH 89/149] Loss_D: 0.7638 Loss_G: 0.8692 acc: 53.1%\n",
      "[BATCH 90/149] Loss_D: 0.8262 Loss_G: 0.8684 acc: 60.9%\n",
      "[BATCH 91/149] Loss_D: 0.8578 Loss_G: 0.8770 acc: 68.8%\n",
      "[BATCH 92/149] Loss_D: 0.8843 Loss_G: 0.8801 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.8420 Loss_G: 0.8805 acc: 53.1%\n",
      "[BATCH 94/149] Loss_D: 0.8102 Loss_G: 0.8712 acc: 43.8%\n",
      "[BATCH 95/149] Loss_D: 0.9027 Loss_G: 0.8784 acc: 48.4%\n",
      "[BATCH 96/149] Loss_D: 0.8145 Loss_G: 0.8766 acc: 56.2%\n",
      "[BATCH 97/149] Loss_D: 0.8204 Loss_G: 0.8662 acc: 50.0%\n",
      "[BATCH 98/149] Loss_D: 0.8655 Loss_G: 0.8686 acc: 54.7%\n",
      "[BATCH 99/149] Loss_D: 0.7882 Loss_G: 0.8649 acc: 42.2%\n",
      "[BATCH 100/149] Loss_D: 0.8183 Loss_G: 0.8636 acc: 53.1%\n",
      "[BATCH 101/149] Loss_D: 0.8022 Loss_G: 0.8634 acc: 56.2%\n",
      "[BATCH 102/149] Loss_D: 0.8892 Loss_G: 0.8801 acc: 60.9%\n",
      "[BATCH 103/149] Loss_D: 0.8706 Loss_G: 0.8944 acc: 53.1%\n",
      "[BATCH 104/149] Loss_D: 0.8264 Loss_G: 0.8826 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.8339 Loss_G: 0.8747 acc: 37.5%\n",
      "[BATCH 106/149] Loss_D: 0.8585 Loss_G: 0.8674 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.8018 Loss_G: 0.8602 acc: 57.8%\n",
      "[BATCH 108/149] Loss_D: 0.8982 Loss_G: 0.8718 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.8678 Loss_G: 0.8822 acc: 54.7%\n",
      "[BATCH 110/149] Loss_D: 0.8469 Loss_G: 0.8781 acc: 51.6%\n",
      "[BATCH 111/149] Loss_D: 0.8409 Loss_G: 0.8723 acc: 45.3%\n",
      "[EPOCH 1750] TEST ACC is : 54.9%\n",
      "[BATCH 112/149] Loss_D: 0.8049 Loss_G: 0.8729 acc: 54.7%\n",
      "[BATCH 113/149] Loss_D: 0.8682 Loss_G: 0.8760 acc: 59.4%\n",
      "[BATCH 114/149] Loss_D: 0.8974 Loss_G: 0.8926 acc: 48.4%\n",
      "[BATCH 115/149] Loss_D: 0.8960 Loss_G: 0.9115 acc: 54.7%\n",
      "[BATCH 116/149] Loss_D: 0.8366 Loss_G: 0.9027 acc: 51.6%\n",
      "[BATCH 117/149] Loss_D: 0.8893 Loss_G: 0.9063 acc: 60.9%\n",
      "[BATCH 118/149] Loss_D: 0.8767 Loss_G: 0.9001 acc: 48.4%\n",
      "[BATCH 119/149] Loss_D: 0.8193 Loss_G: 0.8930 acc: 42.2%\n",
      "[BATCH 120/149] Loss_D: 0.9073 Loss_G: 0.8964 acc: 48.4%\n",
      "[BATCH 121/149] Loss_D: 0.8033 Loss_G: 0.8887 acc: 71.9%\n",
      "[BATCH 122/149] Loss_D: 0.8589 Loss_G: 0.8838 acc: 50.0%\n",
      "[BATCH 123/149] Loss_D: 0.8088 Loss_G: 0.8864 acc: 50.0%\n",
      "[BATCH 124/149] Loss_D: 0.8414 Loss_G: 0.8769 acc: 48.4%\n",
      "[BATCH 125/149] Loss_D: 0.8773 Loss_G: 0.8906 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8480 Loss_G: 0.8918 acc: 45.3%\n",
      "[BATCH 127/149] Loss_D: 0.8688 Loss_G: 0.8883 acc: 53.1%\n",
      "[BATCH 128/149] Loss_D: 0.8176 Loss_G: 0.8886 acc: 57.8%\n",
      "[BATCH 129/149] Loss_D: 0.7761 Loss_G: 0.8757 acc: 50.0%\n",
      "[BATCH 130/149] Loss_D: 0.8256 Loss_G: 0.8676 acc: 54.7%\n",
      "[BATCH 131/149] Loss_D: 0.8029 Loss_G: 0.8641 acc: 46.9%\n",
      "[BATCH 132/149] Loss_D: 0.8218 Loss_G: 0.8636 acc: 46.9%\n",
      "[BATCH 133/149] Loss_D: 0.8590 Loss_G: 0.8681 acc: 48.4%\n",
      "[BATCH 134/149] Loss_D: 0.8747 Loss_G: 0.8788 acc: 62.5%\n",
      "[BATCH 135/149] Loss_D: 0.8033 Loss_G: 0.8792 acc: 43.8%\n",
      "[BATCH 136/149] Loss_D: 0.8441 Loss_G: 0.8777 acc: 54.7%\n",
      "[BATCH 137/149] Loss_D: 0.8614 Loss_G: 0.8804 acc: 50.0%\n",
      "[BATCH 138/149] Loss_D: 0.8362 Loss_G: 0.8792 acc: 50.0%\n",
      "[BATCH 139/149] Loss_D: 0.8143 Loss_G: 0.8728 acc: 43.8%\n",
      "[BATCH 140/149] Loss_D: 0.8291 Loss_G: 0.8708 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.8430 Loss_G: 0.8782 acc: 56.2%\n",
      "[BATCH 142/149] Loss_D: 0.8012 Loss_G: 0.8726 acc: 48.4%\n",
      "[BATCH 143/149] Loss_D: 0.8530 Loss_G: 0.8738 acc: 60.9%\n",
      "[BATCH 144/149] Loss_D: 0.7635 Loss_G: 0.8669 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.9112 Loss_G: 0.8911 acc: 50.0%\n",
      "[BATCH 146/149] Loss_D: 0.8421 Loss_G: 0.8924 acc: 57.8%\n",
      "[BATCH 147/149] Loss_D: 0.8634 Loss_G: 0.8811 acc: 45.3%\n",
      "[BATCH 148/149] Loss_D: 0.8048 Loss_G: 0.8734 acc: 53.1%\n",
      "[BATCH 149/149] Loss_D: 0.8743 Loss_G: 0.8732 acc: 60.9%\n",
      "-----THE [12/50] epoch end-----\n",
      "-----THE [13/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.9077 Loss_G: 0.8889 acc: 48.4%\n",
      "[BATCH 2/149] Loss_D: 0.8457 Loss_G: 0.8907 acc: 48.4%\n",
      "[BATCH 3/149] Loss_D: 0.8110 Loss_G: 0.8935 acc: 50.0%\n",
      "[BATCH 4/149] Loss_D: 0.8257 Loss_G: 0.8913 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.8127 Loss_G: 0.8792 acc: 57.8%\n",
      "[BATCH 6/149] Loss_D: 0.8461 Loss_G: 0.8783 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8895 Loss_G: 0.8793 acc: 51.6%\n",
      "[BATCH 8/149] Loss_D: 0.8351 Loss_G: 0.8781 acc: 54.7%\n",
      "[BATCH 9/149] Loss_D: 0.7771 Loss_G: 0.8664 acc: 51.6%\n",
      "[BATCH 10/149] Loss_D: 0.8356 Loss_G: 0.8622 acc: 56.2%\n",
      "[BATCH 11/149] Loss_D: 0.8181 Loss_G: 0.8589 acc: 46.9%\n",
      "[BATCH 12/149] Loss_D: 0.8411 Loss_G: 0.8654 acc: 60.9%\n",
      "[EPOCH 1800] TEST ACC is : 54.9%\n",
      "[BATCH 13/149] Loss_D: 0.8796 Loss_G: 0.8787 acc: 50.0%\n",
      "[BATCH 14/149] Loss_D: 0.8447 Loss_G: 0.8844 acc: 51.6%\n",
      "[BATCH 15/149] Loss_D: 0.8316 Loss_G: 0.8876 acc: 50.0%\n",
      "[BATCH 16/149] Loss_D: 0.8477 Loss_G: 0.8833 acc: 56.2%\n",
      "[BATCH 17/149] Loss_D: 0.8423 Loss_G: 0.8817 acc: 53.1%\n",
      "[BATCH 18/149] Loss_D: 0.8248 Loss_G: 0.8771 acc: 46.9%\n",
      "[BATCH 19/149] Loss_D: 0.8426 Loss_G: 0.8764 acc: 40.6%\n",
      "[BATCH 20/149] Loss_D: 0.7908 Loss_G: 0.8641 acc: 46.9%\n",
      "[BATCH 21/149] Loss_D: 0.8220 Loss_G: 0.8666 acc: 64.1%\n",
      "[BATCH 22/149] Loss_D: 0.8424 Loss_G: 0.8716 acc: 50.0%\n",
      "[BATCH 23/149] Loss_D: 0.8160 Loss_G: 0.8667 acc: 53.1%\n",
      "[BATCH 24/149] Loss_D: 0.8414 Loss_G: 0.8635 acc: 40.6%\n",
      "[BATCH 25/149] Loss_D: 0.8890 Loss_G: 0.8715 acc: 54.7%\n",
      "[BATCH 26/149] Loss_D: 0.8680 Loss_G: 0.8817 acc: 43.8%\n",
      "[BATCH 27/149] Loss_D: 0.8395 Loss_G: 0.8806 acc: 43.8%\n",
      "[BATCH 28/149] Loss_D: 0.8444 Loss_G: 0.8776 acc: 54.7%\n",
      "[BATCH 29/149] Loss_D: 0.8943 Loss_G: 0.8919 acc: 50.0%\n",
      "[BATCH 30/149] Loss_D: 0.9604 Loss_G: 0.9221 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.8232 Loss_G: 0.9058 acc: 48.4%\n",
      "[BATCH 32/149] Loss_D: 0.8111 Loss_G: 0.8853 acc: 54.7%\n",
      "[BATCH 33/149] Loss_D: 0.8272 Loss_G: 0.8737 acc: 37.5%\n",
      "[BATCH 34/149] Loss_D: 0.7983 Loss_G: 0.8683 acc: 59.4%\n",
      "[BATCH 35/149] Loss_D: 0.8700 Loss_G: 0.8760 acc: 51.6%\n",
      "[BATCH 36/149] Loss_D: 0.8399 Loss_G: 0.8748 acc: 42.2%\n",
      "[BATCH 37/149] Loss_D: 0.8193 Loss_G: 0.8688 acc: 50.0%\n",
      "[BATCH 38/149] Loss_D: 0.8309 Loss_G: 0.8629 acc: 50.0%\n",
      "[BATCH 39/149] Loss_D: 0.9327 Loss_G: 0.8815 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.8478 Loss_G: 0.8776 acc: 59.4%\n",
      "[BATCH 41/149] Loss_D: 0.8671 Loss_G: 0.8777 acc: 54.7%\n",
      "[BATCH 42/149] Loss_D: 0.8220 Loss_G: 0.8724 acc: 57.8%\n",
      "[BATCH 43/149] Loss_D: 0.8486 Loss_G: 0.8762 acc: 53.1%\n",
      "[BATCH 44/149] Loss_D: 0.7925 Loss_G: 0.8726 acc: 57.8%\n",
      "[BATCH 45/149] Loss_D: 0.8514 Loss_G: 0.8791 acc: 42.2%\n",
      "[BATCH 46/149] Loss_D: 0.8870 Loss_G: 0.8848 acc: 40.6%\n",
      "[BATCH 47/149] Loss_D: 0.8227 Loss_G: 0.8884 acc: 54.7%\n",
      "[BATCH 48/149] Loss_D: 0.8176 Loss_G: 0.8737 acc: 46.9%\n",
      "[BATCH 49/149] Loss_D: 0.8307 Loss_G: 0.8683 acc: 57.8%\n",
      "[BATCH 50/149] Loss_D: 0.8101 Loss_G: 0.8652 acc: 51.6%\n",
      "[BATCH 51/149] Loss_D: 0.7639 Loss_G: 0.8566 acc: 54.7%\n",
      "[BATCH 52/149] Loss_D: 0.8647 Loss_G: 0.8612 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.8218 Loss_G: 0.8639 acc: 48.4%\n",
      "[BATCH 54/149] Loss_D: 0.8199 Loss_G: 0.8642 acc: 56.2%\n",
      "[BATCH 55/149] Loss_D: 0.8281 Loss_G: 0.8567 acc: 51.6%\n",
      "[BATCH 56/149] Loss_D: 0.8622 Loss_G: 0.8634 acc: 54.7%\n",
      "[BATCH 57/149] Loss_D: 0.8020 Loss_G: 0.8645 acc: 48.4%\n",
      "[BATCH 58/149] Loss_D: 0.8176 Loss_G: 0.8589 acc: 45.3%\n",
      "[BATCH 59/149] Loss_D: 0.9036 Loss_G: 0.8739 acc: 53.1%\n",
      "[BATCH 60/149] Loss_D: 0.8386 Loss_G: 0.8746 acc: 46.9%\n",
      "[BATCH 61/149] Loss_D: 0.8187 Loss_G: 0.8715 acc: 50.0%\n",
      "[BATCH 62/149] Loss_D: 0.8751 Loss_G: 0.8760 acc: 45.3%\n",
      "[EPOCH 1850] TEST ACC is : 58.8%\n",
      "[BATCH 63/149] Loss_D: 0.9174 Loss_G: 0.9003 acc: 51.6%\n",
      "[BATCH 64/149] Loss_D: 0.8604 Loss_G: 0.9095 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.7965 Loss_G: 0.8923 acc: 51.6%\n",
      "[BATCH 66/149] Loss_D: 0.8366 Loss_G: 0.8819 acc: 51.6%\n",
      "[BATCH 67/149] Loss_D: 0.8066 Loss_G: 0.8731 acc: 54.7%\n",
      "[BATCH 68/149] Loss_D: 0.8028 Loss_G: 0.8714 acc: 48.4%\n",
      "[BATCH 69/149] Loss_D: 0.8428 Loss_G: 0.8774 acc: 40.6%\n",
      "[BATCH 70/149] Loss_D: 0.8267 Loss_G: 0.8767 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.8678 Loss_G: 0.8872 acc: 51.6%\n",
      "[BATCH 72/149] Loss_D: 0.8135 Loss_G: 0.8825 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.8392 Loss_G: 0.8796 acc: 51.6%\n",
      "[BATCH 74/149] Loss_D: 0.8386 Loss_G: 0.8810 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.8491 Loss_G: 0.8819 acc: 56.2%\n",
      "[BATCH 76/149] Loss_D: 0.8108 Loss_G: 0.8778 acc: 54.7%\n",
      "[BATCH 77/149] Loss_D: 0.8084 Loss_G: 0.8706 acc: 53.1%\n",
      "[BATCH 78/149] Loss_D: 0.9204 Loss_G: 0.8881 acc: 39.1%\n",
      "[BATCH 79/149] Loss_D: 0.7860 Loss_G: 0.8842 acc: 56.2%\n",
      "[BATCH 80/149] Loss_D: 0.8597 Loss_G: 0.8863 acc: 53.1%\n",
      "[BATCH 81/149] Loss_D: 0.8174 Loss_G: 0.8750 acc: 53.1%\n",
      "[BATCH 82/149] Loss_D: 0.8787 Loss_G: 0.8824 acc: 54.7%\n",
      "[BATCH 83/149] Loss_D: 0.8966 Loss_G: 0.8918 acc: 57.8%\n",
      "[BATCH 84/149] Loss_D: 0.7914 Loss_G: 0.8777 acc: 53.1%\n",
      "[BATCH 85/149] Loss_D: 0.8718 Loss_G: 0.8772 acc: 50.0%\n",
      "[BATCH 86/149] Loss_D: 0.7968 Loss_G: 0.8696 acc: 48.4%\n",
      "[BATCH 87/149] Loss_D: 0.8097 Loss_G: 0.8632 acc: 46.9%\n",
      "[BATCH 88/149] Loss_D: 0.7972 Loss_G: 0.8583 acc: 51.6%\n",
      "[BATCH 89/149] Loss_D: 0.8167 Loss_G: 0.8623 acc: 59.4%\n",
      "[BATCH 90/149] Loss_D: 0.8342 Loss_G: 0.8622 acc: 54.7%\n",
      "[BATCH 91/149] Loss_D: 0.7848 Loss_G: 0.8603 acc: 57.8%\n",
      "[BATCH 92/149] Loss_D: 0.8043 Loss_G: 0.8545 acc: 54.7%\n",
      "[BATCH 93/149] Loss_D: 0.9049 Loss_G: 0.8632 acc: 50.0%\n",
      "[BATCH 94/149] Loss_D: 0.8535 Loss_G: 0.8735 acc: 46.9%\n",
      "[BATCH 95/149] Loss_D: 0.8207 Loss_G: 0.8793 acc: 53.1%\n",
      "[BATCH 96/149] Loss_D: 0.8407 Loss_G: 0.8729 acc: 50.0%\n",
      "[BATCH 97/149] Loss_D: 0.8113 Loss_G: 0.8658 acc: 54.7%\n",
      "[BATCH 98/149] Loss_D: 0.8837 Loss_G: 0.8736 acc: 57.8%\n",
      "[BATCH 99/149] Loss_D: 0.8611 Loss_G: 0.8787 acc: 45.3%\n",
      "[BATCH 100/149] Loss_D: 0.8001 Loss_G: 0.8742 acc: 59.4%\n",
      "[BATCH 101/149] Loss_D: 0.7947 Loss_G: 0.8659 acc: 50.0%\n",
      "[BATCH 102/149] Loss_D: 0.8340 Loss_G: 0.8688 acc: 51.6%\n",
      "[BATCH 103/149] Loss_D: 0.9399 Loss_G: 0.8861 acc: 45.3%\n",
      "[BATCH 104/149] Loss_D: 0.8019 Loss_G: 0.8852 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.8375 Loss_G: 0.8738 acc: 57.8%\n",
      "[BATCH 106/149] Loss_D: 0.8649 Loss_G: 0.8748 acc: 50.0%\n",
      "[BATCH 107/149] Loss_D: 0.8260 Loss_G: 0.8682 acc: 46.9%\n",
      "[BATCH 108/149] Loss_D: 0.8110 Loss_G: 0.8633 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.8959 Loss_G: 0.8871 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.8331 Loss_G: 0.8819 acc: 54.7%\n",
      "[BATCH 111/149] Loss_D: 0.9029 Loss_G: 0.8838 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.8726 Loss_G: 0.8880 acc: 59.4%\n",
      "[EPOCH 1900] TEST ACC is : 53.5%\n",
      "[BATCH 113/149] Loss_D: 0.7913 Loss_G: 0.8739 acc: 39.1%\n",
      "[BATCH 114/149] Loss_D: 0.8399 Loss_G: 0.8653 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.8132 Loss_G: 0.8634 acc: 54.7%\n",
      "[BATCH 116/149] Loss_D: 0.8187 Loss_G: 0.8588 acc: 56.2%\n",
      "[BATCH 117/149] Loss_D: 0.8723 Loss_G: 0.8680 acc: 56.2%\n",
      "[BATCH 118/149] Loss_D: 0.8505 Loss_G: 0.8769 acc: 54.7%\n",
      "[BATCH 119/149] Loss_D: 0.8185 Loss_G: 0.8771 acc: 54.7%\n",
      "[BATCH 120/149] Loss_D: 0.8330 Loss_G: 0.8687 acc: 53.1%\n",
      "[BATCH 121/149] Loss_D: 0.8802 Loss_G: 0.8718 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8763 Loss_G: 0.8788 acc: 51.6%\n",
      "[BATCH 123/149] Loss_D: 0.8112 Loss_G: 0.8750 acc: 56.2%\n",
      "[BATCH 124/149] Loss_D: 0.8871 Loss_G: 0.8748 acc: 51.6%\n",
      "[BATCH 125/149] Loss_D: 0.7740 Loss_G: 0.8630 acc: 51.6%\n",
      "[BATCH 126/149] Loss_D: 0.8271 Loss_G: 0.8627 acc: 51.6%\n",
      "[BATCH 127/149] Loss_D: 0.7747 Loss_G: 0.8565 acc: 53.1%\n",
      "[BATCH 128/149] Loss_D: 0.8250 Loss_G: 0.8556 acc: 53.1%\n",
      "[BATCH 129/149] Loss_D: 0.7911 Loss_G: 0.8534 acc: 54.7%\n",
      "[BATCH 130/149] Loss_D: 0.8872 Loss_G: 0.8705 acc: 50.0%\n",
      "[BATCH 131/149] Loss_D: 0.7994 Loss_G: 0.8645 acc: 51.6%\n",
      "[BATCH 132/149] Loss_D: 0.7743 Loss_G: 0.8494 acc: 40.6%\n",
      "[BATCH 133/149] Loss_D: 0.8381 Loss_G: 0.8567 acc: 46.9%\n",
      "[BATCH 134/149] Loss_D: 0.8749 Loss_G: 0.8615 acc: 56.2%\n",
      "[BATCH 135/149] Loss_D: 0.7807 Loss_G: 0.8605 acc: 51.6%\n",
      "[BATCH 136/149] Loss_D: 0.8450 Loss_G: 0.8674 acc: 45.3%\n",
      "[BATCH 137/149] Loss_D: 0.8564 Loss_G: 0.8749 acc: 57.8%\n",
      "[BATCH 138/149] Loss_D: 0.8409 Loss_G: 0.8812 acc: 54.7%\n",
      "[BATCH 139/149] Loss_D: 0.8430 Loss_G: 0.8753 acc: 53.1%\n",
      "[BATCH 140/149] Loss_D: 0.8789 Loss_G: 0.8786 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.7975 Loss_G: 0.8769 acc: 50.0%\n",
      "[BATCH 142/149] Loss_D: 0.8260 Loss_G: 0.8797 acc: 51.6%\n",
      "[BATCH 143/149] Loss_D: 0.8178 Loss_G: 0.8734 acc: 51.6%\n",
      "[BATCH 144/149] Loss_D: 0.7857 Loss_G: 0.8721 acc: 50.0%\n",
      "[BATCH 145/149] Loss_D: 0.8363 Loss_G: 0.8745 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.8157 Loss_G: 0.8735 acc: 54.7%\n",
      "[BATCH 147/149] Loss_D: 0.8608 Loss_G: 0.8744 acc: 51.6%\n",
      "[BATCH 148/149] Loss_D: 0.8544 Loss_G: 0.8780 acc: 60.9%\n",
      "[BATCH 149/149] Loss_D: 0.8388 Loss_G: 0.8812 acc: 70.3%\n",
      "-----THE [13/50] epoch end-----\n",
      "-----THE [14/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8809 Loss_G: 0.8764 acc: 57.8%\n",
      "[BATCH 2/149] Loss_D: 0.8700 Loss_G: 0.8771 acc: 45.3%\n",
      "[BATCH 3/149] Loss_D: 0.8649 Loss_G: 0.8785 acc: 45.3%\n",
      "[BATCH 4/149] Loss_D: 0.8596 Loss_G: 0.8769 acc: 51.6%\n",
      "[BATCH 5/149] Loss_D: 0.8378 Loss_G: 0.8777 acc: 59.4%\n",
      "[BATCH 6/149] Loss_D: 0.7924 Loss_G: 0.8618 acc: 64.1%\n",
      "[BATCH 7/149] Loss_D: 0.8911 Loss_G: 0.8682 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.8461 Loss_G: 0.8730 acc: 51.6%\n",
      "[BATCH 9/149] Loss_D: 0.8170 Loss_G: 0.8714 acc: 53.1%\n",
      "[BATCH 10/149] Loss_D: 0.8338 Loss_G: 0.8696 acc: 51.6%\n",
      "[BATCH 11/149] Loss_D: 0.8778 Loss_G: 0.8791 acc: 57.8%\n",
      "[BATCH 12/149] Loss_D: 0.8428 Loss_G: 0.8860 acc: 53.1%\n",
      "[BATCH 13/149] Loss_D: 0.8913 Loss_G: 0.9030 acc: 48.4%\n",
      "[EPOCH 1950] TEST ACC is : 53.5%\n",
      "[BATCH 14/149] Loss_D: 0.8379 Loss_G: 0.9016 acc: 43.8%\n",
      "[BATCH 15/149] Loss_D: 0.8509 Loss_G: 0.8831 acc: 53.1%\n",
      "[BATCH 16/149] Loss_D: 0.7951 Loss_G: 0.8668 acc: 40.6%\n",
      "[BATCH 17/149] Loss_D: 0.8475 Loss_G: 0.8599 acc: 50.0%\n",
      "[BATCH 18/149] Loss_D: 0.8711 Loss_G: 0.8677 acc: 50.0%\n",
      "[BATCH 19/149] Loss_D: 0.8581 Loss_G: 0.8794 acc: 56.2%\n",
      "[BATCH 20/149] Loss_D: 0.8239 Loss_G: 0.8759 acc: 48.4%\n",
      "[BATCH 21/149] Loss_D: 0.8458 Loss_G: 0.8727 acc: 59.4%\n",
      "[BATCH 22/149] Loss_D: 0.8158 Loss_G: 0.8703 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.9053 Loss_G: 0.8847 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.8240 Loss_G: 0.8866 acc: 48.4%\n",
      "[BATCH 25/149] Loss_D: 0.8466 Loss_G: 0.8794 acc: 46.9%\n",
      "[BATCH 26/149] Loss_D: 0.7834 Loss_G: 0.8672 acc: 60.9%\n",
      "[BATCH 27/149] Loss_D: 0.8625 Loss_G: 0.8677 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.8430 Loss_G: 0.8727 acc: 50.0%\n",
      "[BATCH 29/149] Loss_D: 0.8014 Loss_G: 0.8663 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.8300 Loss_G: 0.8661 acc: 57.8%\n",
      "[BATCH 31/149] Loss_D: 0.8550 Loss_G: 0.8742 acc: 54.7%\n",
      "[BATCH 32/149] Loss_D: 0.8002 Loss_G: 0.8690 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.8369 Loss_G: 0.8681 acc: 67.2%\n",
      "[BATCH 34/149] Loss_D: 0.8204 Loss_G: 0.8667 acc: 56.2%\n",
      "[BATCH 35/149] Loss_D: 0.8362 Loss_G: 0.8754 acc: 51.6%\n",
      "[BATCH 36/149] Loss_D: 0.8220 Loss_G: 0.8719 acc: 48.4%\n",
      "[BATCH 37/149] Loss_D: 0.7941 Loss_G: 0.8620 acc: 51.6%\n",
      "[BATCH 38/149] Loss_D: 0.8682 Loss_G: 0.8687 acc: 48.4%\n",
      "[BATCH 39/149] Loss_D: 0.8686 Loss_G: 0.8760 acc: 51.6%\n",
      "[BATCH 40/149] Loss_D: 0.8313 Loss_G: 0.8714 acc: 46.9%\n",
      "[BATCH 41/149] Loss_D: 0.8557 Loss_G: 0.8747 acc: 51.6%\n",
      "[BATCH 42/149] Loss_D: 0.7858 Loss_G: 0.8608 acc: 50.0%\n",
      "[BATCH 43/149] Loss_D: 0.7994 Loss_G: 0.8554 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.8282 Loss_G: 0.8610 acc: 46.9%\n",
      "[BATCH 45/149] Loss_D: 0.8259 Loss_G: 0.8597 acc: 53.1%\n",
      "[BATCH 46/149] Loss_D: 0.9015 Loss_G: 0.8743 acc: 43.8%\n",
      "[BATCH 47/149] Loss_D: 0.8349 Loss_G: 0.8799 acc: 46.9%\n",
      "[BATCH 48/149] Loss_D: 0.8324 Loss_G: 0.8782 acc: 57.8%\n",
      "[BATCH 49/149] Loss_D: 0.8376 Loss_G: 0.8771 acc: 57.8%\n",
      "[BATCH 50/149] Loss_D: 0.8390 Loss_G: 0.8754 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7919 Loss_G: 0.8683 acc: 50.0%\n",
      "[BATCH 52/149] Loss_D: 0.7893 Loss_G: 0.8662 acc: 48.4%\n",
      "[BATCH 53/149] Loss_D: 0.8076 Loss_G: 0.8625 acc: 46.9%\n",
      "[BATCH 54/149] Loss_D: 0.8874 Loss_G: 0.8746 acc: 51.6%\n",
      "[BATCH 55/149] Loss_D: 0.8315 Loss_G: 0.8691 acc: 40.6%\n",
      "[BATCH 56/149] Loss_D: 0.8424 Loss_G: 0.8688 acc: 51.6%\n",
      "[BATCH 57/149] Loss_D: 0.7779 Loss_G: 0.8564 acc: 48.4%\n",
      "[BATCH 58/149] Loss_D: 0.8042 Loss_G: 0.8509 acc: 51.6%\n",
      "[BATCH 59/149] Loss_D: 0.8356 Loss_G: 0.8538 acc: 54.7%\n",
      "[BATCH 60/149] Loss_D: 0.8681 Loss_G: 0.8684 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.8254 Loss_G: 0.8765 acc: 50.0%\n",
      "[BATCH 62/149] Loss_D: 0.8329 Loss_G: 0.8747 acc: 53.1%\n",
      "[BATCH 63/149] Loss_D: 0.8350 Loss_G: 0.8757 acc: 56.2%\n",
      "[EPOCH 2000] TEST ACC is : 55.3%\n",
      "[BATCH 64/149] Loss_D: 0.8506 Loss_G: 0.8754 acc: 53.1%\n",
      "[BATCH 65/149] Loss_D: 0.8555 Loss_G: 0.8727 acc: 46.9%\n",
      "[BATCH 66/149] Loss_D: 0.8384 Loss_G: 0.8735 acc: 51.6%\n",
      "[BATCH 67/149] Loss_D: 0.8345 Loss_G: 0.8703 acc: 64.1%\n",
      "[BATCH 68/149] Loss_D: 0.8420 Loss_G: 0.8721 acc: 57.8%\n",
      "[BATCH 69/149] Loss_D: 0.7713 Loss_G: 0.8625 acc: 40.6%\n",
      "[BATCH 70/149] Loss_D: 0.8055 Loss_G: 0.8588 acc: 45.3%\n",
      "[BATCH 71/149] Loss_D: 0.8744 Loss_G: 0.8629 acc: 54.7%\n",
      "[BATCH 72/149] Loss_D: 0.8166 Loss_G: 0.8632 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.7908 Loss_G: 0.8603 acc: 71.9%\n",
      "[BATCH 74/149] Loss_D: 0.7986 Loss_G: 0.8595 acc: 53.1%\n",
      "[BATCH 75/149] Loss_D: 0.7905 Loss_G: 0.8581 acc: 54.7%\n",
      "[BATCH 76/149] Loss_D: 0.8012 Loss_G: 0.8529 acc: 51.6%\n",
      "[BATCH 77/149] Loss_D: 0.8171 Loss_G: 0.8587 acc: 51.6%\n",
      "[BATCH 78/149] Loss_D: 0.8576 Loss_G: 0.8709 acc: 46.9%\n",
      "[BATCH 79/149] Loss_D: 0.8619 Loss_G: 0.8861 acc: 54.7%\n",
      "[BATCH 80/149] Loss_D: 0.8761 Loss_G: 0.9005 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.8057 Loss_G: 0.8825 acc: 50.0%\n",
      "[BATCH 82/149] Loss_D: 0.8640 Loss_G: 0.8709 acc: 48.4%\n",
      "[BATCH 83/149] Loss_D: 0.8573 Loss_G: 0.8779 acc: 51.6%\n",
      "[BATCH 84/149] Loss_D: 0.8737 Loss_G: 0.8739 acc: 54.7%\n",
      "[BATCH 85/149] Loss_D: 0.8206 Loss_G: 0.8693 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.8497 Loss_G: 0.8678 acc: 43.8%\n",
      "[BATCH 87/149] Loss_D: 0.8589 Loss_G: 0.8768 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.7908 Loss_G: 0.8684 acc: 56.2%\n",
      "[BATCH 89/149] Loss_D: 0.7730 Loss_G: 0.8548 acc: 53.1%\n",
      "[BATCH 90/149] Loss_D: 0.8656 Loss_G: 0.8633 acc: 54.7%\n",
      "[BATCH 91/149] Loss_D: 0.8468 Loss_G: 0.8674 acc: 53.1%\n",
      "[BATCH 92/149] Loss_D: 0.8693 Loss_G: 0.8771 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.8605 Loss_G: 0.8811 acc: 54.7%\n",
      "[BATCH 94/149] Loss_D: 0.7808 Loss_G: 0.8688 acc: 45.3%\n",
      "[BATCH 95/149] Loss_D: 0.8350 Loss_G: 0.8639 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.7928 Loss_G: 0.8576 acc: 57.8%\n",
      "[BATCH 97/149] Loss_D: 0.8172 Loss_G: 0.8553 acc: 50.0%\n",
      "[BATCH 98/149] Loss_D: 0.7921 Loss_G: 0.8518 acc: 42.2%\n",
      "[BATCH 99/149] Loss_D: 0.8137 Loss_G: 0.8536 acc: 56.2%\n",
      "[BATCH 100/149] Loss_D: 0.8126 Loss_G: 0.8559 acc: 51.6%\n",
      "[BATCH 101/149] Loss_D: 0.8654 Loss_G: 0.8672 acc: 54.7%\n",
      "[BATCH 102/149] Loss_D: 0.8785 Loss_G: 0.8791 acc: 42.2%\n",
      "[BATCH 103/149] Loss_D: 0.8346 Loss_G: 0.8750 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.8227 Loss_G: 0.8657 acc: 50.0%\n",
      "[BATCH 105/149] Loss_D: 0.8716 Loss_G: 0.8654 acc: 54.7%\n",
      "[BATCH 106/149] Loss_D: 0.8055 Loss_G: 0.8674 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7937 Loss_G: 0.8601 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.9033 Loss_G: 0.8716 acc: 53.1%\n",
      "[BATCH 109/149] Loss_D: 0.7850 Loss_G: 0.8732 acc: 53.1%\n",
      "[BATCH 110/149] Loss_D: 0.8339 Loss_G: 0.8626 acc: 59.4%\n",
      "[BATCH 111/149] Loss_D: 0.8014 Loss_G: 0.8560 acc: 46.9%\n",
      "[BATCH 112/149] Loss_D: 0.9436 Loss_G: 0.8821 acc: 59.4%\n",
      "[BATCH 113/149] Loss_D: 0.9236 Loss_G: 0.8878 acc: 46.9%\n",
      "[EPOCH 2050] TEST ACC is : 54.3%\n",
      "[BATCH 114/149] Loss_D: 0.8227 Loss_G: 0.8834 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.8718 Loss_G: 0.8809 acc: 51.6%\n",
      "[BATCH 116/149] Loss_D: 0.8426 Loss_G: 0.8748 acc: 48.4%\n",
      "[BATCH 117/149] Loss_D: 0.8589 Loss_G: 0.8740 acc: 45.3%\n",
      "[BATCH 118/149] Loss_D: 0.9056 Loss_G: 0.8858 acc: 53.1%\n",
      "[BATCH 119/149] Loss_D: 0.8664 Loss_G: 0.8895 acc: 56.2%\n",
      "[BATCH 120/149] Loss_D: 0.8020 Loss_G: 0.8823 acc: 53.1%\n",
      "[BATCH 121/149] Loss_D: 0.8528 Loss_G: 0.8748 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.8530 Loss_G: 0.8795 acc: 42.2%\n",
      "[BATCH 123/149] Loss_D: 0.8110 Loss_G: 0.8752 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.8445 Loss_G: 0.8803 acc: 50.0%\n",
      "[BATCH 125/149] Loss_D: 0.8155 Loss_G: 0.8740 acc: 40.6%\n",
      "[BATCH 126/149] Loss_D: 0.7980 Loss_G: 0.8664 acc: 48.4%\n",
      "[BATCH 127/149] Loss_D: 0.8117 Loss_G: 0.8595 acc: 43.8%\n",
      "[BATCH 128/149] Loss_D: 0.8022 Loss_G: 0.8578 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 0.7891 Loss_G: 0.8513 acc: 50.0%\n",
      "[BATCH 130/149] Loss_D: 0.8676 Loss_G: 0.8601 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.8024 Loss_G: 0.8608 acc: 45.3%\n",
      "[BATCH 132/149] Loss_D: 0.8529 Loss_G: 0.8602 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.8669 Loss_G: 0.8653 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.8232 Loss_G: 0.8659 acc: 48.4%\n",
      "[BATCH 135/149] Loss_D: 0.8821 Loss_G: 0.8650 acc: 40.6%\n",
      "[BATCH 136/149] Loss_D: 0.8492 Loss_G: 0.8682 acc: 53.1%\n",
      "[BATCH 137/149] Loss_D: 0.8354 Loss_G: 0.8697 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.8610 Loss_G: 0.8667 acc: 51.6%\n",
      "[BATCH 139/149] Loss_D: 0.8240 Loss_G: 0.8633 acc: 57.8%\n",
      "[BATCH 140/149] Loss_D: 0.7914 Loss_G: 0.8548 acc: 46.9%\n",
      "[BATCH 141/149] Loss_D: 0.8030 Loss_G: 0.8512 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7970 Loss_G: 0.8482 acc: 56.2%\n",
      "[BATCH 143/149] Loss_D: 0.8493 Loss_G: 0.8546 acc: 48.4%\n",
      "[BATCH 144/149] Loss_D: 0.7821 Loss_G: 0.8546 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.8469 Loss_G: 0.8581 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.8108 Loss_G: 0.8594 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.8367 Loss_G: 0.8605 acc: 53.1%\n",
      "[BATCH 148/149] Loss_D: 0.8190 Loss_G: 0.8647 acc: 59.4%\n",
      "[BATCH 149/149] Loss_D: 0.8242 Loss_G: 0.8633 acc: 54.7%\n",
      "-----THE [14/50] epoch end-----\n",
      "-----THE [15/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7739 Loss_G: 0.8607 acc: 45.3%\n",
      "[BATCH 2/149] Loss_D: 0.8622 Loss_G: 0.8609 acc: 56.2%\n",
      "[BATCH 3/149] Loss_D: 0.8607 Loss_G: 0.8736 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.8120 Loss_G: 0.8672 acc: 51.6%\n",
      "[BATCH 5/149] Loss_D: 0.9027 Loss_G: 0.8743 acc: 59.4%\n",
      "[BATCH 6/149] Loss_D: 0.8408 Loss_G: 0.8717 acc: 54.7%\n",
      "[BATCH 7/149] Loss_D: 0.8042 Loss_G: 0.8579 acc: 45.3%\n",
      "[BATCH 8/149] Loss_D: 0.8501 Loss_G: 0.8555 acc: 46.9%\n",
      "[BATCH 9/149] Loss_D: 0.8629 Loss_G: 0.8632 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7844 Loss_G: 0.8645 acc: 54.7%\n",
      "[BATCH 11/149] Loss_D: 0.8058 Loss_G: 0.8499 acc: 50.0%\n",
      "[BATCH 12/149] Loss_D: 0.9111 Loss_G: 0.8668 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.8252 Loss_G: 0.8678 acc: 48.4%\n",
      "[BATCH 14/149] Loss_D: 0.8356 Loss_G: 0.8645 acc: 46.9%\n",
      "[EPOCH 2100] TEST ACC is : 53.3%\n",
      "[BATCH 15/149] Loss_D: 0.8737 Loss_G: 0.8705 acc: 53.1%\n",
      "[BATCH 16/149] Loss_D: 0.8483 Loss_G: 0.8758 acc: 54.7%\n",
      "[BATCH 17/149] Loss_D: 0.8111 Loss_G: 0.8732 acc: 57.8%\n",
      "[BATCH 18/149] Loss_D: 0.8311 Loss_G: 0.8740 acc: 32.8%\n",
      "[BATCH 19/149] Loss_D: 0.7990 Loss_G: 0.8657 acc: 59.4%\n",
      "[BATCH 20/149] Loss_D: 0.7991 Loss_G: 0.8630 acc: 50.0%\n",
      "[BATCH 21/149] Loss_D: 0.8364 Loss_G: 0.8642 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.8239 Loss_G: 0.8708 acc: 48.4%\n",
      "[BATCH 23/149] Loss_D: 0.8312 Loss_G: 0.8723 acc: 57.8%\n",
      "[BATCH 24/149] Loss_D: 0.9112 Loss_G: 0.8914 acc: 59.4%\n",
      "[BATCH 25/149] Loss_D: 0.8610 Loss_G: 0.8870 acc: 59.4%\n",
      "[BATCH 26/149] Loss_D: 0.8996 Loss_G: 0.8906 acc: 43.8%\n",
      "[BATCH 27/149] Loss_D: 0.8489 Loss_G: 0.8895 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.8156 Loss_G: 0.8766 acc: 46.9%\n",
      "[BATCH 29/149] Loss_D: 0.8491 Loss_G: 0.8714 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.8667 Loss_G: 0.8776 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.8986 Loss_G: 0.9063 acc: 50.0%\n",
      "[BATCH 32/149] Loss_D: 0.9006 Loss_G: 0.9246 acc: 57.8%\n",
      "[BATCH 33/149] Loss_D: 0.8636 Loss_G: 0.9017 acc: 57.8%\n",
      "[BATCH 34/149] Loss_D: 0.8479 Loss_G: 0.8857 acc: 56.2%\n",
      "[BATCH 35/149] Loss_D: 0.8552 Loss_G: 0.8837 acc: 48.4%\n",
      "[BATCH 36/149] Loss_D: 0.8562 Loss_G: 0.8796 acc: 53.1%\n",
      "[BATCH 37/149] Loss_D: 0.8024 Loss_G: 0.8716 acc: 50.0%\n",
      "[BATCH 38/149] Loss_D: 0.7929 Loss_G: 0.8638 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.8014 Loss_G: 0.8604 acc: 45.3%\n",
      "[BATCH 40/149] Loss_D: 0.7939 Loss_G: 0.8558 acc: 48.4%\n",
      "[BATCH 41/149] Loss_D: 0.8210 Loss_G: 0.8593 acc: 51.6%\n",
      "[BATCH 42/149] Loss_D: 0.7887 Loss_G: 0.8556 acc: 50.0%\n",
      "[BATCH 43/149] Loss_D: 0.8645 Loss_G: 0.8647 acc: 51.6%\n",
      "[BATCH 44/149] Loss_D: 0.8450 Loss_G: 0.8654 acc: 56.2%\n",
      "[BATCH 45/149] Loss_D: 0.8381 Loss_G: 0.8657 acc: 46.9%\n",
      "[BATCH 46/149] Loss_D: 0.8488 Loss_G: 0.8707 acc: 51.6%\n",
      "[BATCH 47/149] Loss_D: 0.8453 Loss_G: 0.8754 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.7812 Loss_G: 0.8643 acc: 53.1%\n",
      "[BATCH 49/149] Loss_D: 0.7901 Loss_G: 0.8538 acc: 60.9%\n",
      "[BATCH 50/149] Loss_D: 0.8795 Loss_G: 0.8606 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.8091 Loss_G: 0.8595 acc: 56.2%\n",
      "[BATCH 52/149] Loss_D: 0.9019 Loss_G: 0.8731 acc: 56.2%\n",
      "[BATCH 53/149] Loss_D: 0.7933 Loss_G: 0.8615 acc: 51.6%\n",
      "[BATCH 54/149] Loss_D: 0.8630 Loss_G: 0.8652 acc: 51.6%\n",
      "[BATCH 55/149] Loss_D: 0.8567 Loss_G: 0.8613 acc: 59.4%\n",
      "[BATCH 56/149] Loss_D: 0.8058 Loss_G: 0.8666 acc: 50.0%\n",
      "[BATCH 57/149] Loss_D: 0.8619 Loss_G: 0.8706 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.8340 Loss_G: 0.8683 acc: 57.8%\n",
      "[BATCH 59/149] Loss_D: 0.8323 Loss_G: 0.8716 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.9065 Loss_G: 0.8799 acc: 57.8%\n",
      "[BATCH 61/149] Loss_D: 0.8215 Loss_G: 0.8787 acc: 64.1%\n",
      "[BATCH 62/149] Loss_D: 0.8634 Loss_G: 0.8861 acc: 54.7%\n",
      "[BATCH 63/149] Loss_D: 0.8384 Loss_G: 0.8765 acc: 43.8%\n",
      "[BATCH 64/149] Loss_D: 0.8385 Loss_G: 0.8699 acc: 53.1%\n",
      "[EPOCH 2150] TEST ACC is : 56.4%\n",
      "[BATCH 65/149] Loss_D: 0.8559 Loss_G: 0.8681 acc: 46.9%\n",
      "[BATCH 66/149] Loss_D: 0.8673 Loss_G: 0.8734 acc: 51.6%\n",
      "[BATCH 67/149] Loss_D: 0.8494 Loss_G: 0.8757 acc: 50.0%\n",
      "[BATCH 68/149] Loss_D: 0.8366 Loss_G: 0.8704 acc: 51.6%\n",
      "[BATCH 69/149] Loss_D: 0.8419 Loss_G: 0.8739 acc: 46.9%\n",
      "[BATCH 70/149] Loss_D: 0.8241 Loss_G: 0.8682 acc: 59.4%\n",
      "[BATCH 71/149] Loss_D: 0.8709 Loss_G: 0.8770 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.8521 Loss_G: 0.8784 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7993 Loss_G: 0.8721 acc: 45.3%\n",
      "[BATCH 74/149] Loss_D: 0.7878 Loss_G: 0.8637 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.8301 Loss_G: 0.8593 acc: 53.1%\n",
      "[BATCH 76/149] Loss_D: 0.7876 Loss_G: 0.8589 acc: 45.3%\n",
      "[BATCH 77/149] Loss_D: 0.8143 Loss_G: 0.8597 acc: 54.7%\n",
      "[BATCH 78/149] Loss_D: 0.8564 Loss_G: 0.8643 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.8112 Loss_G: 0.8611 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.8372 Loss_G: 0.8601 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.8638 Loss_G: 0.8642 acc: 56.2%\n",
      "[BATCH 82/149] Loss_D: 0.8453 Loss_G: 0.8683 acc: 54.7%\n",
      "[BATCH 83/149] Loss_D: 0.8072 Loss_G: 0.8633 acc: 46.9%\n",
      "[BATCH 84/149] Loss_D: 0.8224 Loss_G: 0.8624 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.7642 Loss_G: 0.8504 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.8808 Loss_G: 0.8587 acc: 56.2%\n",
      "[BATCH 87/149] Loss_D: 0.7968 Loss_G: 0.8571 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 0.8445 Loss_G: 0.8641 acc: 51.6%\n",
      "[BATCH 89/149] Loss_D: 0.8876 Loss_G: 0.8620 acc: 59.4%\n",
      "[BATCH 90/149] Loss_D: 0.8036 Loss_G: 0.8649 acc: 50.0%\n",
      "[BATCH 91/149] Loss_D: 0.8066 Loss_G: 0.8602 acc: 48.4%\n",
      "[BATCH 92/149] Loss_D: 0.8032 Loss_G: 0.8621 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.8172 Loss_G: 0.8656 acc: 56.2%\n",
      "[BATCH 94/149] Loss_D: 0.8687 Loss_G: 0.8782 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.8004 Loss_G: 0.8751 acc: 57.8%\n",
      "[BATCH 96/149] Loss_D: 0.7988 Loss_G: 0.8709 acc: 57.8%\n",
      "[BATCH 97/149] Loss_D: 0.8756 Loss_G: 0.8774 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.8606 Loss_G: 0.8937 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.8072 Loss_G: 0.8734 acc: 51.6%\n",
      "[BATCH 100/149] Loss_D: 0.7882 Loss_G: 0.8557 acc: 50.0%\n",
      "[BATCH 101/149] Loss_D: 0.8269 Loss_G: 0.8528 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.8327 Loss_G: 0.8605 acc: 50.0%\n",
      "[BATCH 103/149] Loss_D: 0.8521 Loss_G: 0.8611 acc: 54.7%\n",
      "[BATCH 104/149] Loss_D: 0.8307 Loss_G: 0.8614 acc: 51.6%\n",
      "[BATCH 105/149] Loss_D: 0.8051 Loss_G: 0.8657 acc: 53.1%\n",
      "[BATCH 106/149] Loss_D: 0.8567 Loss_G: 0.8677 acc: 50.0%\n",
      "[BATCH 107/149] Loss_D: 0.7691 Loss_G: 0.8553 acc: 51.6%\n",
      "[BATCH 108/149] Loss_D: 0.9064 Loss_G: 0.8743 acc: 42.2%\n",
      "[BATCH 109/149] Loss_D: 0.8148 Loss_G: 0.8682 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.8034 Loss_G: 0.8582 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.8201 Loss_G: 0.8584 acc: 40.6%\n",
      "[BATCH 112/149] Loss_D: 0.7741 Loss_G: 0.8579 acc: 53.1%\n",
      "[BATCH 113/149] Loss_D: 0.8153 Loss_G: 0.8569 acc: 51.6%\n",
      "[BATCH 114/149] Loss_D: 0.8517 Loss_G: 0.8686 acc: 46.9%\n",
      "[EPOCH 2200] TEST ACC is : 60.5%\n",
      "[BATCH 115/149] Loss_D: 0.8255 Loss_G: 0.8736 acc: 43.8%\n",
      "[BATCH 116/149] Loss_D: 0.7967 Loss_G: 0.8705 acc: 62.5%\n",
      "[BATCH 117/149] Loss_D: 0.7954 Loss_G: 0.8598 acc: 56.2%\n",
      "[BATCH 118/149] Loss_D: 0.8151 Loss_G: 0.8639 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.8115 Loss_G: 0.8556 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.8691 Loss_G: 0.8667 acc: 53.1%\n",
      "[BATCH 121/149] Loss_D: 0.8747 Loss_G: 0.8846 acc: 54.7%\n",
      "[BATCH 122/149] Loss_D: 0.8168 Loss_G: 0.8698 acc: 56.2%\n",
      "[BATCH 123/149] Loss_D: 0.8136 Loss_G: 0.8547 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.8743 Loss_G: 0.8586 acc: 53.1%\n",
      "[BATCH 125/149] Loss_D: 0.8094 Loss_G: 0.8507 acc: 46.9%\n",
      "[BATCH 126/149] Loss_D: 0.8176 Loss_G: 0.8555 acc: 53.1%\n",
      "[BATCH 127/149] Loss_D: 0.8181 Loss_G: 0.8545 acc: 51.6%\n",
      "[BATCH 128/149] Loss_D: 0.8108 Loss_G: 0.8508 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 0.8566 Loss_G: 0.8529 acc: 46.9%\n",
      "[BATCH 130/149] Loss_D: 0.8498 Loss_G: 0.8539 acc: 46.9%\n",
      "[BATCH 131/149] Loss_D: 0.8089 Loss_G: 0.8461 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.8117 Loss_G: 0.8465 acc: 53.1%\n",
      "[BATCH 133/149] Loss_D: 0.8652 Loss_G: 0.8538 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.7854 Loss_G: 0.8499 acc: 54.7%\n",
      "[BATCH 135/149] Loss_D: 0.8438 Loss_G: 0.8563 acc: 54.7%\n",
      "[BATCH 136/149] Loss_D: 0.8557 Loss_G: 0.8637 acc: 48.4%\n",
      "[BATCH 137/149] Loss_D: 0.8639 Loss_G: 0.8775 acc: 51.6%\n",
      "[BATCH 138/149] Loss_D: 0.7896 Loss_G: 0.8702 acc: 56.2%\n",
      "[BATCH 139/149] Loss_D: 0.7858 Loss_G: 0.8537 acc: 39.1%\n",
      "[BATCH 140/149] Loss_D: 0.8443 Loss_G: 0.8544 acc: 54.7%\n",
      "[BATCH 141/149] Loss_D: 0.7957 Loss_G: 0.8525 acc: 51.6%\n",
      "[BATCH 142/149] Loss_D: 0.8424 Loss_G: 0.8519 acc: 59.4%\n",
      "[BATCH 143/149] Loss_D: 0.8431 Loss_G: 0.8594 acc: 42.2%\n",
      "[BATCH 144/149] Loss_D: 0.8253 Loss_G: 0.8705 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.8149 Loss_G: 0.8694 acc: 51.6%\n",
      "[BATCH 146/149] Loss_D: 0.8181 Loss_G: 0.8574 acc: 51.6%\n",
      "[BATCH 147/149] Loss_D: 0.9016 Loss_G: 0.8710 acc: 45.3%\n",
      "[BATCH 148/149] Loss_D: 0.7588 Loss_G: 0.8591 acc: 60.9%\n",
      "[BATCH 149/149] Loss_D: 0.7945 Loss_G: 0.8545 acc: 50.0%\n",
      "-----THE [15/50] epoch end-----\n",
      "-----THE [16/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7761 Loss_G: 0.8455 acc: 51.6%\n",
      "[BATCH 2/149] Loss_D: 0.7959 Loss_G: 0.8426 acc: 62.5%\n",
      "[BATCH 3/149] Loss_D: 0.8336 Loss_G: 0.8510 acc: 45.3%\n",
      "[BATCH 4/149] Loss_D: 0.8538 Loss_G: 0.8684 acc: 51.6%\n",
      "[BATCH 5/149] Loss_D: 0.8260 Loss_G: 0.8675 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.8007 Loss_G: 0.8616 acc: 51.6%\n",
      "[BATCH 7/149] Loss_D: 0.8199 Loss_G: 0.8595 acc: 54.7%\n",
      "[BATCH 8/149] Loss_D: 0.8311 Loss_G: 0.8562 acc: 45.3%\n",
      "[BATCH 9/149] Loss_D: 0.7866 Loss_G: 0.8545 acc: 50.0%\n",
      "[BATCH 10/149] Loss_D: 0.8573 Loss_G: 0.8560 acc: 56.2%\n",
      "[BATCH 11/149] Loss_D: 0.7997 Loss_G: 0.8538 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.8211 Loss_G: 0.8441 acc: 51.6%\n",
      "[BATCH 13/149] Loss_D: 0.7974 Loss_G: 0.8465 acc: 65.6%\n",
      "[BATCH 14/149] Loss_D: 0.8319 Loss_G: 0.8496 acc: 53.1%\n",
      "[BATCH 15/149] Loss_D: 0.7797 Loss_G: 0.8453 acc: 51.6%\n",
      "[EPOCH 2250] TEST ACC is : 58.6%\n",
      "[BATCH 16/149] Loss_D: 0.7885 Loss_G: 0.8406 acc: 54.7%\n",
      "[BATCH 17/149] Loss_D: 0.9244 Loss_G: 0.8736 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.8509 Loss_G: 0.8891 acc: 50.0%\n",
      "[BATCH 19/149] Loss_D: 0.7736 Loss_G: 0.8631 acc: 54.7%\n",
      "[BATCH 20/149] Loss_D: 0.7790 Loss_G: 0.8445 acc: 56.2%\n",
      "[BATCH 21/149] Loss_D: 0.8892 Loss_G: 0.8544 acc: 59.4%\n",
      "[BATCH 22/149] Loss_D: 0.8180 Loss_G: 0.8632 acc: 50.0%\n",
      "[BATCH 23/149] Loss_D: 0.8014 Loss_G: 0.8641 acc: 46.9%\n",
      "[BATCH 24/149] Loss_D: 0.8549 Loss_G: 0.8710 acc: 54.7%\n",
      "[BATCH 25/149] Loss_D: 0.8051 Loss_G: 0.8657 acc: 51.6%\n",
      "[BATCH 26/149] Loss_D: 0.8075 Loss_G: 0.8630 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.8805 Loss_G: 0.8785 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.7988 Loss_G: 0.8724 acc: 43.8%\n",
      "[BATCH 29/149] Loss_D: 0.7843 Loss_G: 0.8700 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 0.8392 Loss_G: 0.8677 acc: 51.6%\n",
      "[BATCH 31/149] Loss_D: 0.7897 Loss_G: 0.8630 acc: 51.6%\n",
      "[BATCH 32/149] Loss_D: 0.8177 Loss_G: 0.8619 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.8435 Loss_G: 0.8596 acc: 48.4%\n",
      "[BATCH 34/149] Loss_D: 0.8251 Loss_G: 0.8609 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.8361 Loss_G: 0.8602 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.7909 Loss_G: 0.8561 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.8914 Loss_G: 0.8686 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.8716 Loss_G: 0.8751 acc: 59.4%\n",
      "[BATCH 39/149] Loss_D: 0.8094 Loss_G: 0.8629 acc: 60.9%\n",
      "[BATCH 40/149] Loss_D: 0.8083 Loss_G: 0.8492 acc: 54.7%\n",
      "[BATCH 41/149] Loss_D: 0.8064 Loss_G: 0.8449 acc: 50.0%\n",
      "[BATCH 42/149] Loss_D: 0.8170 Loss_G: 0.8439 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.7964 Loss_G: 0.8417 acc: 50.0%\n",
      "[BATCH 44/149] Loss_D: 0.8218 Loss_G: 0.8377 acc: 56.2%\n",
      "[BATCH 45/149] Loss_D: 0.7970 Loss_G: 0.8377 acc: 40.6%\n",
      "[BATCH 46/149] Loss_D: 0.8246 Loss_G: 0.8391 acc: 46.9%\n",
      "[BATCH 47/149] Loss_D: 0.8024 Loss_G: 0.8456 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.8212 Loss_G: 0.8480 acc: 56.2%\n",
      "[BATCH 49/149] Loss_D: 0.8360 Loss_G: 0.8540 acc: 57.8%\n",
      "[BATCH 50/149] Loss_D: 0.8390 Loss_G: 0.8607 acc: 48.4%\n",
      "[BATCH 51/149] Loss_D: 0.8116 Loss_G: 0.8613 acc: 45.3%\n",
      "[BATCH 52/149] Loss_D: 0.8007 Loss_G: 0.8553 acc: 51.6%\n",
      "[BATCH 53/149] Loss_D: 0.8548 Loss_G: 0.8597 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.8552 Loss_G: 0.8658 acc: 53.1%\n",
      "[BATCH 55/149] Loss_D: 0.8158 Loss_G: 0.8640 acc: 53.1%\n",
      "[BATCH 56/149] Loss_D: 0.8726 Loss_G: 0.8712 acc: 62.5%\n",
      "[BATCH 57/149] Loss_D: 0.8269 Loss_G: 0.8645 acc: 51.6%\n",
      "[BATCH 58/149] Loss_D: 0.8521 Loss_G: 0.8713 acc: 50.0%\n",
      "[BATCH 59/149] Loss_D: 0.8213 Loss_G: 0.8654 acc: 71.9%\n",
      "[BATCH 60/149] Loss_D: 0.7856 Loss_G: 0.8529 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.8315 Loss_G: 0.8524 acc: 50.0%\n",
      "[BATCH 62/149] Loss_D: 0.7910 Loss_G: 0.8541 acc: 48.4%\n",
      "[BATCH 63/149] Loss_D: 0.8070 Loss_G: 0.8610 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.8401 Loss_G: 0.8646 acc: 56.2%\n",
      "[BATCH 65/149] Loss_D: 0.8045 Loss_G: 0.8651 acc: 43.8%\n",
      "[EPOCH 2300] TEST ACC is : 57.8%\n",
      "[BATCH 66/149] Loss_D: 0.8242 Loss_G: 0.8572 acc: 56.2%\n",
      "[BATCH 67/149] Loss_D: 0.7925 Loss_G: 0.8515 acc: 50.0%\n",
      "[BATCH 68/149] Loss_D: 0.8586 Loss_G: 0.8488 acc: 54.7%\n",
      "[BATCH 69/149] Loss_D: 0.8887 Loss_G: 0.8731 acc: 51.6%\n",
      "[BATCH 70/149] Loss_D: 0.8068 Loss_G: 0.8733 acc: 45.3%\n",
      "[BATCH 71/149] Loss_D: 0.8456 Loss_G: 0.8706 acc: 51.6%\n",
      "[BATCH 72/149] Loss_D: 0.9052 Loss_G: 0.8748 acc: 50.0%\n",
      "[BATCH 73/149] Loss_D: 0.8272 Loss_G: 0.8641 acc: 60.9%\n",
      "[BATCH 74/149] Loss_D: 0.8082 Loss_G: 0.8554 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.8811 Loss_G: 0.8719 acc: 54.7%\n",
      "[BATCH 76/149] Loss_D: 0.8120 Loss_G: 0.8646 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.8378 Loss_G: 0.8611 acc: 54.7%\n",
      "[BATCH 78/149] Loss_D: 0.9583 Loss_G: 0.8887 acc: 53.1%\n",
      "[BATCH 79/149] Loss_D: 0.8463 Loss_G: 0.8911 acc: 53.1%\n",
      "[BATCH 80/149] Loss_D: 0.8054 Loss_G: 0.8699 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.8462 Loss_G: 0.8578 acc: 57.8%\n",
      "[BATCH 82/149] Loss_D: 0.8064 Loss_G: 0.8517 acc: 50.0%\n",
      "[BATCH 83/149] Loss_D: 0.8814 Loss_G: 0.8562 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.8240 Loss_G: 0.8529 acc: 59.4%\n",
      "[BATCH 85/149] Loss_D: 0.8095 Loss_G: 0.8523 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.8652 Loss_G: 0.8569 acc: 46.9%\n",
      "[BATCH 87/149] Loss_D: 0.8307 Loss_G: 0.8574 acc: 51.6%\n",
      "[BATCH 88/149] Loss_D: 0.7953 Loss_G: 0.8510 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.7579 Loss_G: 0.8446 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.8544 Loss_G: 0.8523 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.7744 Loss_G: 0.8528 acc: 43.8%\n",
      "[BATCH 92/149] Loss_D: 0.8216 Loss_G: 0.8501 acc: 51.6%\n",
      "[BATCH 93/149] Loss_D: 0.8169 Loss_G: 0.8501 acc: 45.3%\n",
      "[BATCH 94/149] Loss_D: 0.8193 Loss_G: 0.8526 acc: 50.0%\n",
      "[BATCH 95/149] Loss_D: 0.9001 Loss_G: 0.8799 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.8619 Loss_G: 0.9008 acc: 75.0%\n",
      "[BATCH 97/149] Loss_D: 0.8059 Loss_G: 0.8756 acc: 46.9%\n",
      "[BATCH 98/149] Loss_D: 0.8486 Loss_G: 0.8664 acc: 51.6%\n",
      "[BATCH 99/149] Loss_D: 0.8816 Loss_G: 0.8635 acc: 56.2%\n",
      "[BATCH 100/149] Loss_D: 0.9276 Loss_G: 0.8815 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.8612 Loss_G: 0.8862 acc: 51.6%\n",
      "[BATCH 102/149] Loss_D: 0.8126 Loss_G: 0.8730 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.8545 Loss_G: 0.8636 acc: 67.2%\n",
      "[BATCH 104/149] Loss_D: 0.7916 Loss_G: 0.8545 acc: 54.7%\n",
      "[BATCH 105/149] Loss_D: 0.8331 Loss_G: 0.8556 acc: 50.0%\n",
      "[BATCH 106/149] Loss_D: 0.8452 Loss_G: 0.8592 acc: 54.7%\n",
      "[BATCH 107/149] Loss_D: 0.8360 Loss_G: 0.8564 acc: 51.6%\n",
      "[BATCH 108/149] Loss_D: 0.8024 Loss_G: 0.8548 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.8134 Loss_G: 0.8507 acc: 53.1%\n",
      "[BATCH 110/149] Loss_D: 0.8105 Loss_G: 0.8518 acc: 56.2%\n",
      "[BATCH 111/149] Loss_D: 0.8764 Loss_G: 0.8610 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.8041 Loss_G: 0.8597 acc: 45.3%\n",
      "[BATCH 113/149] Loss_D: 0.7896 Loss_G: 0.8509 acc: 46.9%\n",
      "[BATCH 114/149] Loss_D: 0.9115 Loss_G: 0.8654 acc: 48.4%\n",
      "[BATCH 115/149] Loss_D: 0.8170 Loss_G: 0.8681 acc: 54.7%\n",
      "[EPOCH 2350] TEST ACC is : 55.3%\n",
      "[BATCH 116/149] Loss_D: 0.8818 Loss_G: 0.8719 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.8094 Loss_G: 0.8681 acc: 40.6%\n",
      "[BATCH 118/149] Loss_D: 0.8387 Loss_G: 0.8643 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.8691 Loss_G: 0.8787 acc: 54.7%\n",
      "[BATCH 120/149] Loss_D: 0.8255 Loss_G: 0.8687 acc: 42.2%\n",
      "[BATCH 121/149] Loss_D: 0.8677 Loss_G: 0.8646 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.7923 Loss_G: 0.8612 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.8701 Loss_G: 0.8672 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.8585 Loss_G: 0.8764 acc: 50.0%\n",
      "[BATCH 125/149] Loss_D: 0.8368 Loss_G: 0.8839 acc: 54.7%\n",
      "[BATCH 126/149] Loss_D: 0.7929 Loss_G: 0.8729 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.8672 Loss_G: 0.8763 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.9046 Loss_G: 0.8905 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.8197 Loss_G: 0.8746 acc: 45.3%\n",
      "[BATCH 130/149] Loss_D: 0.8553 Loss_G: 0.8646 acc: 51.6%\n",
      "[BATCH 131/149] Loss_D: 0.8214 Loss_G: 0.8683 acc: 50.0%\n",
      "[BATCH 132/149] Loss_D: 0.8565 Loss_G: 0.8695 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.7870 Loss_G: 0.8566 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.8827 Loss_G: 0.8670 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.8239 Loss_G: 0.8650 acc: 51.6%\n",
      "[BATCH 136/149] Loss_D: 0.8419 Loss_G: 0.8670 acc: 56.2%\n",
      "[BATCH 137/149] Loss_D: 0.8494 Loss_G: 0.8679 acc: 57.8%\n",
      "[BATCH 138/149] Loss_D: 0.8886 Loss_G: 0.8709 acc: 51.6%\n",
      "[BATCH 139/149] Loss_D: 0.8264 Loss_G: 0.8628 acc: 57.8%\n",
      "[BATCH 140/149] Loss_D: 0.8039 Loss_G: 0.8475 acc: 42.2%\n",
      "[BATCH 141/149] Loss_D: 0.8683 Loss_G: 0.8519 acc: 48.4%\n",
      "[BATCH 142/149] Loss_D: 0.8131 Loss_G: 0.8573 acc: 56.2%\n",
      "[BATCH 143/149] Loss_D: 0.8039 Loss_G: 0.8607 acc: 45.3%\n",
      "[BATCH 144/149] Loss_D: 0.8886 Loss_G: 0.8710 acc: 50.0%\n",
      "[BATCH 145/149] Loss_D: 0.7667 Loss_G: 0.8622 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.8196 Loss_G: 0.8541 acc: 53.1%\n",
      "[BATCH 147/149] Loss_D: 0.8379 Loss_G: 0.8518 acc: 50.0%\n",
      "[BATCH 148/149] Loss_D: 0.8226 Loss_G: 0.8564 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.8403 Loss_G: 0.8552 acc: 51.6%\n",
      "-----THE [16/50] epoch end-----\n",
      "-----THE [17/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8327 Loss_G: 0.8634 acc: 57.8%\n",
      "[BATCH 2/149] Loss_D: 0.7893 Loss_G: 0.8574 acc: 48.4%\n",
      "[BATCH 3/149] Loss_D: 0.8367 Loss_G: 0.8597 acc: 54.7%\n",
      "[BATCH 4/149] Loss_D: 0.8680 Loss_G: 0.8678 acc: 53.1%\n",
      "[BATCH 5/149] Loss_D: 0.8730 Loss_G: 0.8882 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.8445 Loss_G: 0.8854 acc: 48.4%\n",
      "[BATCH 7/149] Loss_D: 0.7799 Loss_G: 0.8722 acc: 54.7%\n",
      "[BATCH 8/149] Loss_D: 0.8665 Loss_G: 0.8655 acc: 54.7%\n",
      "[BATCH 9/149] Loss_D: 0.7782 Loss_G: 0.8541 acc: 48.4%\n",
      "[BATCH 10/149] Loss_D: 0.8336 Loss_G: 0.8501 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.8628 Loss_G: 0.8590 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.8334 Loss_G: 0.8661 acc: 48.4%\n",
      "[BATCH 13/149] Loss_D: 0.7969 Loss_G: 0.8647 acc: 57.8%\n",
      "[BATCH 14/149] Loss_D: 0.8253 Loss_G: 0.8709 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.8414 Loss_G: 0.8792 acc: 60.9%\n",
      "[BATCH 16/149] Loss_D: 0.8227 Loss_G: 0.8711 acc: 57.8%\n",
      "[EPOCH 2400] TEST ACC is : 54.7%\n",
      "[BATCH 17/149] Loss_D: 0.8059 Loss_G: 0.8568 acc: 48.4%\n",
      "[BATCH 18/149] Loss_D: 0.8259 Loss_G: 0.8513 acc: 53.1%\n",
      "[BATCH 19/149] Loss_D: 0.8200 Loss_G: 0.8535 acc: 56.2%\n",
      "[BATCH 20/149] Loss_D: 0.7985 Loss_G: 0.8485 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8106 Loss_G: 0.8415 acc: 56.2%\n",
      "[BATCH 22/149] Loss_D: 0.8316 Loss_G: 0.8442 acc: 53.1%\n",
      "[BATCH 23/149] Loss_D: 0.8304 Loss_G: 0.8515 acc: 60.9%\n",
      "[BATCH 24/149] Loss_D: 0.8117 Loss_G: 0.8509 acc: 48.4%\n",
      "[BATCH 25/149] Loss_D: 0.8310 Loss_G: 0.8543 acc: 53.1%\n",
      "[BATCH 26/149] Loss_D: 0.8236 Loss_G: 0.8470 acc: 54.7%\n",
      "[BATCH 27/149] Loss_D: 0.8075 Loss_G: 0.8508 acc: 60.9%\n",
      "[BATCH 28/149] Loss_D: 0.8211 Loss_G: 0.8454 acc: 37.5%\n",
      "[BATCH 29/149] Loss_D: 0.8369 Loss_G: 0.8536 acc: 62.5%\n",
      "[BATCH 30/149] Loss_D: 0.8207 Loss_G: 0.8542 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.9007 Loss_G: 0.8653 acc: 48.4%\n",
      "[BATCH 32/149] Loss_D: 0.8486 Loss_G: 0.8661 acc: 54.7%\n",
      "[BATCH 33/149] Loss_D: 0.8149 Loss_G: 0.8611 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7897 Loss_G: 0.8571 acc: 48.4%\n",
      "[BATCH 35/149] Loss_D: 0.8596 Loss_G: 0.8649 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.8094 Loss_G: 0.8635 acc: 53.1%\n",
      "[BATCH 37/149] Loss_D: 0.8561 Loss_G: 0.8729 acc: 53.1%\n",
      "[BATCH 38/149] Loss_D: 0.9134 Loss_G: 0.8942 acc: 54.7%\n",
      "[BATCH 39/149] Loss_D: 0.9118 Loss_G: 0.9164 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.8086 Loss_G: 0.8894 acc: 45.3%\n",
      "[BATCH 41/149] Loss_D: 0.8144 Loss_G: 0.8727 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.8311 Loss_G: 0.8692 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.8496 Loss_G: 0.8695 acc: 40.6%\n",
      "[BATCH 44/149] Loss_D: 0.8117 Loss_G: 0.8603 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7997 Loss_G: 0.8581 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8361 Loss_G: 0.8568 acc: 46.9%\n",
      "[BATCH 47/149] Loss_D: 0.8773 Loss_G: 0.8702 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.8349 Loss_G: 0.8735 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.8249 Loss_G: 0.8638 acc: 46.9%\n",
      "[BATCH 50/149] Loss_D: 0.8460 Loss_G: 0.8549 acc: 51.6%\n",
      "[BATCH 51/149] Loss_D: 0.7671 Loss_G: 0.8445 acc: 56.2%\n",
      "[BATCH 52/149] Loss_D: 0.8163 Loss_G: 0.8453 acc: 51.6%\n",
      "[BATCH 53/149] Loss_D: 0.8379 Loss_G: 0.8493 acc: 50.0%\n",
      "[BATCH 54/149] Loss_D: 0.8768 Loss_G: 0.8643 acc: 51.6%\n",
      "[BATCH 55/149] Loss_D: 0.8454 Loss_G: 0.8716 acc: 54.7%\n",
      "[BATCH 56/149] Loss_D: 0.8539 Loss_G: 0.8715 acc: 51.6%\n",
      "[BATCH 57/149] Loss_D: 0.8070 Loss_G: 0.8618 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.8218 Loss_G: 0.8585 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.7941 Loss_G: 0.8560 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7841 Loss_G: 0.8535 acc: 53.1%\n",
      "[BATCH 61/149] Loss_D: 0.8101 Loss_G: 0.8537 acc: 57.8%\n",
      "[BATCH 62/149] Loss_D: 0.8547 Loss_G: 0.8656 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.8015 Loss_G: 0.8642 acc: 42.2%\n",
      "[BATCH 64/149] Loss_D: 0.7741 Loss_G: 0.8520 acc: 51.6%\n",
      "[BATCH 65/149] Loss_D: 0.8929 Loss_G: 0.8634 acc: 57.8%\n",
      "[BATCH 66/149] Loss_D: 0.8250 Loss_G: 0.8634 acc: 50.0%\n",
      "[EPOCH 2450] TEST ACC is : 56.2%\n",
      "[BATCH 67/149] Loss_D: 0.8015 Loss_G: 0.8605 acc: 56.2%\n",
      "[BATCH 68/149] Loss_D: 0.8394 Loss_G: 0.8576 acc: 51.6%\n",
      "[BATCH 69/149] Loss_D: 0.7975 Loss_G: 0.8526 acc: 50.0%\n",
      "[BATCH 70/149] Loss_D: 0.8635 Loss_G: 0.8712 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.8025 Loss_G: 0.8728 acc: 50.0%\n",
      "[BATCH 72/149] Loss_D: 0.8568 Loss_G: 0.8588 acc: 43.8%\n",
      "[BATCH 73/149] Loss_D: 0.8719 Loss_G: 0.8583 acc: 53.1%\n",
      "[BATCH 74/149] Loss_D: 0.8500 Loss_G: 0.8630 acc: 57.8%\n",
      "[BATCH 75/149] Loss_D: 0.8001 Loss_G: 0.8611 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7884 Loss_G: 0.8584 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.8554 Loss_G: 0.8606 acc: 37.5%\n",
      "[BATCH 78/149] Loss_D: 0.7614 Loss_G: 0.8570 acc: 59.4%\n",
      "[BATCH 79/149] Loss_D: 0.8054 Loss_G: 0.8599 acc: 54.7%\n",
      "[BATCH 80/149] Loss_D: 0.8471 Loss_G: 0.8701 acc: 67.2%\n",
      "[BATCH 81/149] Loss_D: 0.8365 Loss_G: 0.8791 acc: 40.6%\n",
      "[BATCH 82/149] Loss_D: 0.8512 Loss_G: 0.8808 acc: 56.2%\n",
      "[BATCH 83/149] Loss_D: 0.8483 Loss_G: 0.8728 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.8094 Loss_G: 0.8714 acc: 46.9%\n",
      "[BATCH 85/149] Loss_D: 0.8388 Loss_G: 0.8575 acc: 53.1%\n",
      "[BATCH 86/149] Loss_D: 0.8318 Loss_G: 0.8593 acc: 45.3%\n",
      "[BATCH 87/149] Loss_D: 0.7913 Loss_G: 0.8524 acc: 53.1%\n",
      "[BATCH 88/149] Loss_D: 0.9252 Loss_G: 0.8702 acc: 56.2%\n",
      "[BATCH 89/149] Loss_D: 0.8206 Loss_G: 0.8689 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7716 Loss_G: 0.8499 acc: 40.6%\n",
      "[BATCH 91/149] Loss_D: 0.7998 Loss_G: 0.8436 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.8716 Loss_G: 0.8501 acc: 50.0%\n",
      "[BATCH 93/149] Loss_D: 0.8086 Loss_G: 0.8535 acc: 45.3%\n",
      "[BATCH 94/149] Loss_D: 0.8514 Loss_G: 0.8571 acc: 71.9%\n",
      "[BATCH 95/149] Loss_D: 0.8272 Loss_G: 0.8503 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.8292 Loss_G: 0.8511 acc: 71.9%\n",
      "[BATCH 97/149] Loss_D: 0.8374 Loss_G: 0.8492 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.8856 Loss_G: 0.8614 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.8505 Loss_G: 0.8683 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.7982 Loss_G: 0.8600 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.8578 Loss_G: 0.8626 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.7795 Loss_G: 0.8532 acc: 50.0%\n",
      "[BATCH 103/149] Loss_D: 0.8739 Loss_G: 0.8587 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.8211 Loss_G: 0.8662 acc: 51.6%\n",
      "[BATCH 105/149] Loss_D: 0.7962 Loss_G: 0.8534 acc: 53.1%\n",
      "[BATCH 106/149] Loss_D: 0.8297 Loss_G: 0.8484 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.8217 Loss_G: 0.8515 acc: 71.9%\n",
      "[BATCH 108/149] Loss_D: 0.8597 Loss_G: 0.8563 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.8182 Loss_G: 0.8609 acc: 56.2%\n",
      "[BATCH 110/149] Loss_D: 0.8545 Loss_G: 0.8593 acc: 45.3%\n",
      "[BATCH 111/149] Loss_D: 0.7894 Loss_G: 0.8478 acc: 57.8%\n",
      "[BATCH 112/149] Loss_D: 0.8340 Loss_G: 0.8499 acc: 45.3%\n",
      "[BATCH 113/149] Loss_D: 0.9036 Loss_G: 0.8696 acc: 56.2%\n",
      "[BATCH 114/149] Loss_D: 0.8301 Loss_G: 0.8739 acc: 43.8%\n",
      "[BATCH 115/149] Loss_D: 0.7839 Loss_G: 0.8569 acc: 56.2%\n",
      "[BATCH 116/149] Loss_D: 0.8316 Loss_G: 0.8583 acc: 56.2%\n",
      "[EPOCH 2500] TEST ACC is : 55.9%\n",
      "[BATCH 117/149] Loss_D: 0.8541 Loss_G: 0.8617 acc: 57.8%\n",
      "[BATCH 118/149] Loss_D: 0.7835 Loss_G: 0.8563 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.8001 Loss_G: 0.8486 acc: 54.7%\n",
      "[BATCH 120/149] Loss_D: 0.7633 Loss_G: 0.8448 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.8280 Loss_G: 0.8555 acc: 64.1%\n",
      "[BATCH 122/149] Loss_D: 0.8026 Loss_G: 0.8502 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.8402 Loss_G: 0.8604 acc: 53.1%\n",
      "[BATCH 124/149] Loss_D: 0.8347 Loss_G: 0.8597 acc: 48.4%\n",
      "[BATCH 125/149] Loss_D: 0.8015 Loss_G: 0.8546 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8193 Loss_G: 0.8479 acc: 64.1%\n",
      "[BATCH 127/149] Loss_D: 0.8331 Loss_G: 0.8526 acc: 50.0%\n",
      "[BATCH 128/149] Loss_D: 0.8318 Loss_G: 0.8517 acc: 46.9%\n",
      "[BATCH 129/149] Loss_D: 0.8401 Loss_G: 0.8641 acc: 48.4%\n",
      "[BATCH 130/149] Loss_D: 0.8832 Loss_G: 0.8822 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.8634 Loss_G: 0.8935 acc: 57.8%\n",
      "[BATCH 132/149] Loss_D: 0.8653 Loss_G: 0.8827 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.8007 Loss_G: 0.8664 acc: 57.8%\n",
      "[BATCH 134/149] Loss_D: 0.8126 Loss_G: 0.8581 acc: 46.9%\n",
      "[BATCH 135/149] Loss_D: 0.8310 Loss_G: 0.8583 acc: 56.2%\n",
      "[BATCH 136/149] Loss_D: 0.8123 Loss_G: 0.8653 acc: 51.6%\n",
      "[BATCH 137/149] Loss_D: 0.8015 Loss_G: 0.8634 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.8429 Loss_G: 0.8679 acc: 54.7%\n",
      "[BATCH 139/149] Loss_D: 0.7845 Loss_G: 0.8611 acc: 50.0%\n",
      "[BATCH 140/149] Loss_D: 0.8369 Loss_G: 0.8669 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.8400 Loss_G: 0.8740 acc: 56.2%\n",
      "[BATCH 142/149] Loss_D: 0.8450 Loss_G: 0.8726 acc: 53.1%\n",
      "[BATCH 143/149] Loss_D: 0.8160 Loss_G: 0.8703 acc: 50.0%\n",
      "[BATCH 144/149] Loss_D: 0.8094 Loss_G: 0.8611 acc: 56.2%\n",
      "[BATCH 145/149] Loss_D: 0.7942 Loss_G: 0.8514 acc: 43.8%\n",
      "[BATCH 146/149] Loss_D: 0.8095 Loss_G: 0.8428 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.8847 Loss_G: 0.8558 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.8506 Loss_G: 0.8644 acc: 54.7%\n",
      "[BATCH 149/149] Loss_D: 0.8977 Loss_G: 0.8881 acc: 60.9%\n",
      "-----THE [17/50] epoch end-----\n",
      "-----THE [18/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.9094 Loss_G: 0.8859 acc: 56.2%\n",
      "[BATCH 2/149] Loss_D: 0.8046 Loss_G: 0.8721 acc: 48.4%\n",
      "[BATCH 3/149] Loss_D: 0.8372 Loss_G: 0.8609 acc: 54.7%\n",
      "[BATCH 4/149] Loss_D: 0.8251 Loss_G: 0.8542 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.7965 Loss_G: 0.8538 acc: 50.0%\n",
      "[BATCH 6/149] Loss_D: 0.8332 Loss_G: 0.8493 acc: 57.8%\n",
      "[BATCH 7/149] Loss_D: 0.8402 Loss_G: 0.8583 acc: 56.2%\n",
      "[BATCH 8/149] Loss_D: 0.8950 Loss_G: 0.8694 acc: 62.5%\n",
      "[BATCH 9/149] Loss_D: 0.8558 Loss_G: 0.8839 acc: 57.8%\n",
      "[BATCH 10/149] Loss_D: 0.8742 Loss_G: 0.8748 acc: 53.1%\n",
      "[BATCH 11/149] Loss_D: 0.8215 Loss_G: 0.8688 acc: 53.1%\n",
      "[BATCH 12/149] Loss_D: 0.8413 Loss_G: 0.8635 acc: 50.0%\n",
      "[BATCH 13/149] Loss_D: 0.8728 Loss_G: 0.8671 acc: 59.4%\n",
      "[BATCH 14/149] Loss_D: 0.8435 Loss_G: 0.8728 acc: 51.6%\n",
      "[BATCH 15/149] Loss_D: 0.8292 Loss_G: 0.8709 acc: 54.7%\n",
      "[BATCH 16/149] Loss_D: 0.8154 Loss_G: 0.8614 acc: 54.7%\n",
      "[BATCH 17/149] Loss_D: 0.8662 Loss_G: 0.8620 acc: 50.0%\n",
      "[EPOCH 2550] TEST ACC is : 53.9%\n",
      "[BATCH 18/149] Loss_D: 0.7860 Loss_G: 0.8522 acc: 53.1%\n",
      "[BATCH 19/149] Loss_D: 0.7883 Loss_G: 0.8463 acc: 54.7%\n",
      "[BATCH 20/149] Loss_D: 0.8374 Loss_G: 0.8435 acc: 45.3%\n",
      "[BATCH 21/149] Loss_D: 0.8274 Loss_G: 0.8458 acc: 48.4%\n",
      "[BATCH 22/149] Loss_D: 0.8366 Loss_G: 0.8439 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.8201 Loss_G: 0.8417 acc: 70.3%\n",
      "[BATCH 24/149] Loss_D: 0.7964 Loss_G: 0.8380 acc: 57.8%\n",
      "[BATCH 25/149] Loss_D: 0.8106 Loss_G: 0.8410 acc: 53.1%\n",
      "[BATCH 26/149] Loss_D: 0.7752 Loss_G: 0.8424 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.7901 Loss_G: 0.8340 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.8617 Loss_G: 0.8443 acc: 48.4%\n",
      "[BATCH 29/149] Loss_D: 0.8147 Loss_G: 0.8457 acc: 53.1%\n",
      "[BATCH 30/149] Loss_D: 0.8176 Loss_G: 0.8457 acc: 46.9%\n",
      "[BATCH 31/149] Loss_D: 0.8266 Loss_G: 0.8432 acc: 42.2%\n",
      "[BATCH 32/149] Loss_D: 0.8109 Loss_G: 0.8445 acc: 60.9%\n",
      "[BATCH 33/149] Loss_D: 0.8122 Loss_G: 0.8480 acc: 57.8%\n",
      "[BATCH 34/149] Loss_D: 0.8788 Loss_G: 0.8563 acc: 62.5%\n",
      "[BATCH 35/149] Loss_D: 0.8536 Loss_G: 0.8634 acc: 45.3%\n",
      "[BATCH 36/149] Loss_D: 0.7969 Loss_G: 0.8551 acc: 43.8%\n",
      "[BATCH 37/149] Loss_D: 0.8424 Loss_G: 0.8523 acc: 43.8%\n",
      "[BATCH 38/149] Loss_D: 0.8280 Loss_G: 0.8536 acc: 51.6%\n",
      "[BATCH 39/149] Loss_D: 0.8503 Loss_G: 0.8571 acc: 70.3%\n",
      "[BATCH 40/149] Loss_D: 0.8238 Loss_G: 0.8619 acc: 53.1%\n",
      "[BATCH 41/149] Loss_D: 0.8498 Loss_G: 0.8624 acc: 50.0%\n",
      "[BATCH 42/149] Loss_D: 0.8259 Loss_G: 0.8637 acc: 70.3%\n",
      "[BATCH 43/149] Loss_D: 0.8229 Loss_G: 0.8587 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.8629 Loss_G: 0.8680 acc: 45.3%\n",
      "[BATCH 45/149] Loss_D: 0.8360 Loss_G: 0.8706 acc: 46.9%\n",
      "[BATCH 46/149] Loss_D: 0.7478 Loss_G: 0.8532 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.8574 Loss_G: 0.8583 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.8992 Loss_G: 0.8949 acc: 59.4%\n",
      "[BATCH 49/149] Loss_D: 0.8130 Loss_G: 0.8715 acc: 56.2%\n",
      "[BATCH 50/149] Loss_D: 0.8611 Loss_G: 0.8645 acc: 45.3%\n",
      "[BATCH 51/149] Loss_D: 0.8629 Loss_G: 0.8656 acc: 50.0%\n",
      "[BATCH 52/149] Loss_D: 0.8444 Loss_G: 0.8565 acc: 51.6%\n",
      "[BATCH 53/149] Loss_D: 0.7731 Loss_G: 0.8462 acc: 56.2%\n",
      "[BATCH 54/149] Loss_D: 0.8935 Loss_G: 0.8519 acc: 50.0%\n",
      "[BATCH 55/149] Loss_D: 0.8475 Loss_G: 0.8572 acc: 43.8%\n",
      "[BATCH 56/149] Loss_D: 0.8421 Loss_G: 0.8556 acc: 53.1%\n",
      "[BATCH 57/149] Loss_D: 0.7818 Loss_G: 0.8462 acc: 51.6%\n",
      "[BATCH 58/149] Loss_D: 0.7753 Loss_G: 0.8385 acc: 51.6%\n",
      "[BATCH 59/149] Loss_D: 0.8448 Loss_G: 0.8415 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.8110 Loss_G: 0.8470 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.8206 Loss_G: 0.8531 acc: 64.1%\n",
      "[BATCH 62/149] Loss_D: 0.7947 Loss_G: 0.8510 acc: 50.0%\n",
      "[BATCH 63/149] Loss_D: 0.8315 Loss_G: 0.8541 acc: 62.5%\n",
      "[BATCH 64/149] Loss_D: 0.8042 Loss_G: 0.8534 acc: 53.1%\n",
      "[BATCH 65/149] Loss_D: 0.8005 Loss_G: 0.8524 acc: 57.8%\n",
      "[BATCH 66/149] Loss_D: 0.8134 Loss_G: 0.8478 acc: 56.2%\n",
      "[BATCH 67/149] Loss_D: 0.9331 Loss_G: 0.8728 acc: 60.9%\n",
      "[EPOCH 2600] TEST ACC is : 59.4%\n",
      "[BATCH 68/149] Loss_D: 0.8200 Loss_G: 0.8750 acc: 53.1%\n",
      "[BATCH 69/149] Loss_D: 0.7955 Loss_G: 0.8601 acc: 54.7%\n",
      "[BATCH 70/149] Loss_D: 0.8772 Loss_G: 0.8599 acc: 50.0%\n",
      "[BATCH 71/149] Loss_D: 0.8566 Loss_G: 0.8589 acc: 57.8%\n",
      "[BATCH 72/149] Loss_D: 0.8174 Loss_G: 0.8558 acc: 59.4%\n",
      "[BATCH 73/149] Loss_D: 0.7984 Loss_G: 0.8528 acc: 54.7%\n",
      "[BATCH 74/149] Loss_D: 0.8695 Loss_G: 0.8565 acc: 57.8%\n",
      "[BATCH 75/149] Loss_D: 0.8469 Loss_G: 0.8516 acc: 50.0%\n",
      "[BATCH 76/149] Loss_D: 0.7939 Loss_G: 0.8488 acc: 51.6%\n",
      "[BATCH 77/149] Loss_D: 0.8023 Loss_G: 0.8422 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.8370 Loss_G: 0.8504 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.8353 Loss_G: 0.8623 acc: 53.1%\n",
      "[BATCH 80/149] Loss_D: 0.7866 Loss_G: 0.8472 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.8237 Loss_G: 0.8443 acc: 53.1%\n",
      "[BATCH 82/149] Loss_D: 0.8018 Loss_G: 0.8418 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8041 Loss_G: 0.8451 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 0.8204 Loss_G: 0.8523 acc: 51.6%\n",
      "[BATCH 85/149] Loss_D: 0.8001 Loss_G: 0.8493 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.7945 Loss_G: 0.8470 acc: 51.6%\n",
      "[BATCH 87/149] Loss_D: 0.7824 Loss_G: 0.8464 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 0.7433 Loss_G: 0.8388 acc: 50.0%\n",
      "[BATCH 89/149] Loss_D: 0.7636 Loss_G: 0.8302 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.9077 Loss_G: 0.8585 acc: 60.9%\n",
      "[BATCH 91/149] Loss_D: 0.8170 Loss_G: 0.8743 acc: 51.6%\n",
      "[BATCH 92/149] Loss_D: 0.8490 Loss_G: 0.8606 acc: 46.9%\n",
      "[BATCH 93/149] Loss_D: 0.8352 Loss_G: 0.8641 acc: 56.2%\n",
      "[BATCH 94/149] Loss_D: 0.8164 Loss_G: 0.8544 acc: 56.2%\n",
      "[BATCH 95/149] Loss_D: 0.8424 Loss_G: 0.8595 acc: 56.2%\n",
      "[BATCH 96/149] Loss_D: 0.7965 Loss_G: 0.8540 acc: 53.1%\n",
      "[BATCH 97/149] Loss_D: 0.7999 Loss_G: 0.8538 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.9031 Loss_G: 0.8764 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.8065 Loss_G: 0.8675 acc: 42.2%\n",
      "[BATCH 100/149] Loss_D: 0.8563 Loss_G: 0.8702 acc: 53.1%\n",
      "[BATCH 101/149] Loss_D: 0.7598 Loss_G: 0.8510 acc: 39.1%\n",
      "[BATCH 102/149] Loss_D: 0.8403 Loss_G: 0.8589 acc: 59.4%\n",
      "[BATCH 103/149] Loss_D: 0.8204 Loss_G: 0.8628 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.8557 Loss_G: 0.8728 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.8076 Loss_G: 0.8692 acc: 70.3%\n",
      "[BATCH 106/149] Loss_D: 0.8352 Loss_G: 0.8646 acc: 53.1%\n",
      "[BATCH 107/149] Loss_D: 0.8381 Loss_G: 0.8626 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7891 Loss_G: 0.8542 acc: 56.2%\n",
      "[BATCH 109/149] Loss_D: 0.8526 Loss_G: 0.8617 acc: 45.3%\n",
      "[BATCH 110/149] Loss_D: 0.8415 Loss_G: 0.8672 acc: 60.9%\n",
      "[BATCH 111/149] Loss_D: 0.8498 Loss_G: 0.8713 acc: 45.3%\n",
      "[BATCH 112/149] Loss_D: 0.8334 Loss_G: 0.8564 acc: 48.4%\n",
      "[BATCH 113/149] Loss_D: 0.8145 Loss_G: 0.8447 acc: 48.4%\n",
      "[BATCH 114/149] Loss_D: 0.8274 Loss_G: 0.8480 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.8949 Loss_G: 0.8605 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8265 Loss_G: 0.8614 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.8090 Loss_G: 0.8619 acc: 53.1%\n",
      "[EPOCH 2650] TEST ACC is : 58.6%\n",
      "[BATCH 118/149] Loss_D: 0.8182 Loss_G: 0.8597 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.8500 Loss_G: 0.8609 acc: 51.6%\n",
      "[BATCH 120/149] Loss_D: 0.8591 Loss_G: 0.8703 acc: 46.9%\n",
      "[BATCH 121/149] Loss_D: 0.8085 Loss_G: 0.8597 acc: 51.6%\n",
      "[BATCH 122/149] Loss_D: 0.8801 Loss_G: 0.8707 acc: 62.5%\n",
      "[BATCH 123/149] Loss_D: 0.8434 Loss_G: 0.8683 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.7871 Loss_G: 0.8543 acc: 53.1%\n",
      "[BATCH 125/149] Loss_D: 0.8498 Loss_G: 0.8582 acc: 59.4%\n",
      "[BATCH 126/149] Loss_D: 0.7933 Loss_G: 0.8545 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.8897 Loss_G: 0.8699 acc: 50.0%\n",
      "[BATCH 128/149] Loss_D: 0.8373 Loss_G: 0.8741 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.7746 Loss_G: 0.8518 acc: 54.7%\n",
      "[BATCH 130/149] Loss_D: 0.8005 Loss_G: 0.8450 acc: 50.0%\n",
      "[BATCH 131/149] Loss_D: 0.8139 Loss_G: 0.8345 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.8087 Loss_G: 0.8339 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.8059 Loss_G: 0.8369 acc: 54.7%\n",
      "[BATCH 134/149] Loss_D: 0.7897 Loss_G: 0.8401 acc: 51.6%\n",
      "[BATCH 135/149] Loss_D: 0.7832 Loss_G: 0.8338 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.8514 Loss_G: 0.8491 acc: 54.7%\n",
      "[BATCH 137/149] Loss_D: 0.8500 Loss_G: 0.8615 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.8069 Loss_G: 0.8542 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.8365 Loss_G: 0.8501 acc: 50.0%\n",
      "[BATCH 140/149] Loss_D: 0.7880 Loss_G: 0.8456 acc: 57.8%\n",
      "[BATCH 141/149] Loss_D: 0.8327 Loss_G: 0.8526 acc: 65.6%\n",
      "[BATCH 142/149] Loss_D: 0.8261 Loss_G: 0.8463 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.8261 Loss_G: 0.8533 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.8176 Loss_G: 0.8551 acc: 57.8%\n",
      "[BATCH 145/149] Loss_D: 0.8298 Loss_G: 0.8613 acc: 46.9%\n",
      "[BATCH 146/149] Loss_D: 0.8338 Loss_G: 0.8635 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7537 Loss_G: 0.8503 acc: 51.6%\n",
      "[BATCH 148/149] Loss_D: 0.8468 Loss_G: 0.8422 acc: 56.2%\n",
      "[BATCH 149/149] Loss_D: 0.8601 Loss_G: 0.8510 acc: 53.1%\n",
      "-----THE [18/50] epoch end-----\n",
      "-----THE [19/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8499 Loss_G: 0.8615 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.7681 Loss_G: 0.8414 acc: 46.9%\n",
      "[BATCH 3/149] Loss_D: 0.8180 Loss_G: 0.8425 acc: 65.6%\n",
      "[BATCH 4/149] Loss_D: 0.7903 Loss_G: 0.8363 acc: 54.7%\n",
      "[BATCH 5/149] Loss_D: 0.8075 Loss_G: 0.8376 acc: 50.0%\n",
      "[BATCH 6/149] Loss_D: 0.8356 Loss_G: 0.8478 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.7716 Loss_G: 0.8472 acc: 56.2%\n",
      "[BATCH 8/149] Loss_D: 0.8010 Loss_G: 0.8407 acc: 42.2%\n",
      "[BATCH 9/149] Loss_D: 0.8060 Loss_G: 0.8441 acc: 50.0%\n",
      "[BATCH 10/149] Loss_D: 0.8741 Loss_G: 0.8620 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.8124 Loss_G: 0.8611 acc: 54.7%\n",
      "[BATCH 12/149] Loss_D: 0.7984 Loss_G: 0.8617 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7616 Loss_G: 0.8447 acc: 50.0%\n",
      "[BATCH 14/149] Loss_D: 0.8316 Loss_G: 0.8474 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.8153 Loss_G: 0.8402 acc: 42.2%\n",
      "[BATCH 16/149] Loss_D: 0.8224 Loss_G: 0.8414 acc: 54.7%\n",
      "[BATCH 17/149] Loss_D: 0.8314 Loss_G: 0.8443 acc: 53.1%\n",
      "[BATCH 18/149] Loss_D: 0.7550 Loss_G: 0.8380 acc: 54.7%\n",
      "[EPOCH 2700] TEST ACC is : 62.9%\n",
      "[BATCH 19/149] Loss_D: 0.7995 Loss_G: 0.8355 acc: 56.2%\n",
      "[BATCH 20/149] Loss_D: 0.8132 Loss_G: 0.8364 acc: 48.4%\n",
      "[BATCH 21/149] Loss_D: 0.7882 Loss_G: 0.8390 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.8537 Loss_G: 0.8474 acc: 46.9%\n",
      "[BATCH 23/149] Loss_D: 0.8276 Loss_G: 0.8634 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.8557 Loss_G: 0.8646 acc: 57.8%\n",
      "[BATCH 25/149] Loss_D: 0.8270 Loss_G: 0.8577 acc: 57.8%\n",
      "[BATCH 26/149] Loss_D: 0.8402 Loss_G: 0.8501 acc: 48.4%\n",
      "[BATCH 27/149] Loss_D: 0.8319 Loss_G: 0.8474 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.8607 Loss_G: 0.8606 acc: 50.0%\n",
      "[BATCH 29/149] Loss_D: 0.8105 Loss_G: 0.8585 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.7659 Loss_G: 0.8455 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.8731 Loss_G: 0.8502 acc: 56.2%\n",
      "[BATCH 32/149] Loss_D: 0.8608 Loss_G: 0.8608 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7655 Loss_G: 0.8589 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.8514 Loss_G: 0.8582 acc: 59.4%\n",
      "[BATCH 35/149] Loss_D: 0.8093 Loss_G: 0.8599 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.8416 Loss_G: 0.8655 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7967 Loss_G: 0.8665 acc: 56.2%\n",
      "[BATCH 38/149] Loss_D: 0.8302 Loss_G: 0.8652 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.7809 Loss_G: 0.8447 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.8325 Loss_G: 0.8473 acc: 51.6%\n",
      "[BATCH 41/149] Loss_D: 0.8098 Loss_G: 0.8408 acc: 51.6%\n",
      "[BATCH 42/149] Loss_D: 0.8237 Loss_G: 0.8519 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.8640 Loss_G: 0.8574 acc: 50.0%\n",
      "[BATCH 44/149] Loss_D: 0.8054 Loss_G: 0.8547 acc: 57.8%\n",
      "[BATCH 45/149] Loss_D: 0.7838 Loss_G: 0.8521 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.8538 Loss_G: 0.8524 acc: 59.4%\n",
      "[BATCH 47/149] Loss_D: 0.8064 Loss_G: 0.8532 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.8225 Loss_G: 0.8436 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.8516 Loss_G: 0.8430 acc: 64.1%\n",
      "[BATCH 50/149] Loss_D: 0.7987 Loss_G: 0.8388 acc: 54.7%\n",
      "[BATCH 51/149] Loss_D: 0.8076 Loss_G: 0.8336 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.8129 Loss_G: 0.8380 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.8234 Loss_G: 0.8376 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.8012 Loss_G: 0.8359 acc: 57.8%\n",
      "[BATCH 55/149] Loss_D: 0.8640 Loss_G: 0.8469 acc: 56.2%\n",
      "[BATCH 56/149] Loss_D: 0.7899 Loss_G: 0.8481 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.8198 Loss_G: 0.8435 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.8423 Loss_G: 0.8447 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.8176 Loss_G: 0.8460 acc: 50.0%\n",
      "[BATCH 60/149] Loss_D: 0.8450 Loss_G: 0.8448 acc: 56.2%\n",
      "[BATCH 61/149] Loss_D: 0.8154 Loss_G: 0.8476 acc: 57.8%\n",
      "[BATCH 62/149] Loss_D: 0.8384 Loss_G: 0.8568 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.8569 Loss_G: 0.8662 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.8068 Loss_G: 0.8626 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8519 Loss_G: 0.8701 acc: 54.7%\n",
      "[BATCH 66/149] Loss_D: 0.7933 Loss_G: 0.8674 acc: 57.8%\n",
      "[BATCH 67/149] Loss_D: 0.7939 Loss_G: 0.8580 acc: 53.1%\n",
      "[BATCH 68/149] Loss_D: 0.7925 Loss_G: 0.8544 acc: 56.2%\n",
      "[EPOCH 2750] TEST ACC is : 62.9%\n",
      "[BATCH 69/149] Loss_D: 0.8335 Loss_G: 0.8520 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.8589 Loss_G: 0.8574 acc: 54.7%\n",
      "[BATCH 71/149] Loss_D: 0.7975 Loss_G: 0.8509 acc: 37.5%\n",
      "[BATCH 72/149] Loss_D: 0.8758 Loss_G: 0.8596 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.8020 Loss_G: 0.8590 acc: 57.8%\n",
      "[BATCH 74/149] Loss_D: 0.8291 Loss_G: 0.8516 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.8110 Loss_G: 0.8463 acc: 53.1%\n",
      "[BATCH 76/149] Loss_D: 0.8111 Loss_G: 0.8409 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.7758 Loss_G: 0.8348 acc: 54.7%\n",
      "[BATCH 78/149] Loss_D: 0.8475 Loss_G: 0.8395 acc: 50.0%\n",
      "[BATCH 79/149] Loss_D: 0.9033 Loss_G: 0.8631 acc: 57.8%\n",
      "[BATCH 80/149] Loss_D: 0.8256 Loss_G: 0.8747 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.8795 Loss_G: 0.8631 acc: 51.6%\n",
      "[BATCH 82/149] Loss_D: 0.8950 Loss_G: 0.8615 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8178 Loss_G: 0.8533 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.8733 Loss_G: 0.8630 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.7981 Loss_G: 0.8613 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.8110 Loss_G: 0.8497 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.8668 Loss_G: 0.8562 acc: 70.3%\n",
      "[BATCH 88/149] Loss_D: 0.8042 Loss_G: 0.8503 acc: 57.8%\n",
      "[BATCH 89/149] Loss_D: 0.8139 Loss_G: 0.8508 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.8121 Loss_G: 0.8498 acc: 57.8%\n",
      "[BATCH 91/149] Loss_D: 0.7879 Loss_G: 0.8406 acc: 59.4%\n",
      "[BATCH 92/149] Loss_D: 0.8654 Loss_G: 0.8463 acc: 45.3%\n",
      "[BATCH 93/149] Loss_D: 0.8032 Loss_G: 0.8455 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7767 Loss_G: 0.8470 acc: 59.4%\n",
      "[BATCH 95/149] Loss_D: 0.8019 Loss_G: 0.8388 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.8403 Loss_G: 0.8475 acc: 48.4%\n",
      "[BATCH 97/149] Loss_D: 0.8650 Loss_G: 0.8540 acc: 57.8%\n",
      "[BATCH 98/149] Loss_D: 0.8618 Loss_G: 0.8610 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.8558 Loss_G: 0.8629 acc: 68.8%\n",
      "[BATCH 100/149] Loss_D: 0.8628 Loss_G: 0.8632 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.8626 Loss_G: 0.8597 acc: 48.4%\n",
      "[BATCH 102/149] Loss_D: 0.8984 Loss_G: 0.8660 acc: 57.8%\n",
      "[BATCH 103/149] Loss_D: 0.8159 Loss_G: 0.8566 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.7970 Loss_G: 0.8444 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.7854 Loss_G: 0.8373 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.7914 Loss_G: 0.8364 acc: 54.7%\n",
      "[BATCH 107/149] Loss_D: 0.8712 Loss_G: 0.8426 acc: 51.6%\n",
      "[BATCH 108/149] Loss_D: 0.8512 Loss_G: 0.8545 acc: 51.6%\n",
      "[BATCH 109/149] Loss_D: 0.8764 Loss_G: 0.8617 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.8138 Loss_G: 0.8491 acc: 50.0%\n",
      "[BATCH 111/149] Loss_D: 0.7835 Loss_G: 0.8370 acc: 56.2%\n",
      "[BATCH 112/149] Loss_D: 0.8414 Loss_G: 0.8366 acc: 57.8%\n",
      "[BATCH 113/149] Loss_D: 0.8927 Loss_G: 0.8511 acc: 48.4%\n",
      "[BATCH 114/149] Loss_D: 0.8219 Loss_G: 0.8575 acc: 56.2%\n",
      "[BATCH 115/149] Loss_D: 0.8335 Loss_G: 0.8574 acc: 54.7%\n",
      "[BATCH 116/149] Loss_D: 0.8495 Loss_G: 0.8638 acc: 51.6%\n",
      "[BATCH 117/149] Loss_D: 0.8526 Loss_G: 0.8605 acc: 51.6%\n",
      "[BATCH 118/149] Loss_D: 0.8596 Loss_G: 0.8622 acc: 56.2%\n",
      "[EPOCH 2800] TEST ACC is : 59.6%\n",
      "[BATCH 119/149] Loss_D: 0.8400 Loss_G: 0.8624 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.8126 Loss_G: 0.8569 acc: 43.8%\n",
      "[BATCH 121/149] Loss_D: 0.8211 Loss_G: 0.8494 acc: 65.6%\n",
      "[BATCH 122/149] Loss_D: 0.8541 Loss_G: 0.8543 acc: 53.1%\n",
      "[BATCH 123/149] Loss_D: 0.7809 Loss_G: 0.8477 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.8267 Loss_G: 0.8508 acc: 51.6%\n",
      "[BATCH 125/149] Loss_D: 0.8171 Loss_G: 0.8503 acc: 53.1%\n",
      "[BATCH 126/149] Loss_D: 0.8580 Loss_G: 0.8572 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.8103 Loss_G: 0.8634 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7957 Loss_G: 0.8534 acc: 51.6%\n",
      "[BATCH 129/149] Loss_D: 0.8228 Loss_G: 0.8534 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.8435 Loss_G: 0.8557 acc: 42.2%\n",
      "[BATCH 131/149] Loss_D: 0.8694 Loss_G: 0.8718 acc: 54.7%\n",
      "[BATCH 132/149] Loss_D: 0.8470 Loss_G: 0.8703 acc: 53.1%\n",
      "[BATCH 133/149] Loss_D: 0.8190 Loss_G: 0.8624 acc: 64.1%\n",
      "[BATCH 134/149] Loss_D: 0.8406 Loss_G: 0.8595 acc: 51.6%\n",
      "[BATCH 135/149] Loss_D: 0.8219 Loss_G: 0.8642 acc: 42.2%\n",
      "[BATCH 136/149] Loss_D: 0.8316 Loss_G: 0.8607 acc: 56.2%\n",
      "[BATCH 137/149] Loss_D: 0.8007 Loss_G: 0.8576 acc: 54.7%\n",
      "[BATCH 138/149] Loss_D: 0.8425 Loss_G: 0.8536 acc: 57.8%\n",
      "[BATCH 139/149] Loss_D: 0.8249 Loss_G: 0.8503 acc: 51.6%\n",
      "[BATCH 140/149] Loss_D: 0.8627 Loss_G: 0.8506 acc: 48.4%\n",
      "[BATCH 141/149] Loss_D: 0.7809 Loss_G: 0.8533 acc: 54.7%\n",
      "[BATCH 142/149] Loss_D: 0.8170 Loss_G: 0.8409 acc: 51.6%\n",
      "[BATCH 143/149] Loss_D: 0.7855 Loss_G: 0.8397 acc: 56.2%\n",
      "[BATCH 144/149] Loss_D: 0.8311 Loss_G: 0.8364 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.7922 Loss_G: 0.8333 acc: 56.2%\n",
      "[BATCH 146/149] Loss_D: 0.8326 Loss_G: 0.8460 acc: 57.8%\n",
      "[BATCH 147/149] Loss_D: 0.8622 Loss_G: 0.8597 acc: 51.6%\n",
      "[BATCH 148/149] Loss_D: 0.8292 Loss_G: 0.8494 acc: 51.6%\n",
      "[BATCH 149/149] Loss_D: 0.8521 Loss_G: 0.8533 acc: 64.1%\n",
      "-----THE [19/50] epoch end-----\n",
      "-----THE [20/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8213 Loss_G: 0.8529 acc: 56.2%\n",
      "[BATCH 2/149] Loss_D: 0.8340 Loss_G: 0.8484 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.7687 Loss_G: 0.8408 acc: 53.1%\n",
      "[BATCH 4/149] Loss_D: 0.8270 Loss_G: 0.8417 acc: 45.3%\n",
      "[BATCH 5/149] Loss_D: 0.7999 Loss_G: 0.8422 acc: 53.1%\n",
      "[BATCH 6/149] Loss_D: 0.8067 Loss_G: 0.8410 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8284 Loss_G: 0.8422 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7504 Loss_G: 0.8352 acc: 54.7%\n",
      "[BATCH 9/149] Loss_D: 0.8525 Loss_G: 0.8399 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7653 Loss_G: 0.8364 acc: 51.6%\n",
      "[BATCH 11/149] Loss_D: 0.8546 Loss_G: 0.8512 acc: 54.7%\n",
      "[BATCH 12/149] Loss_D: 0.8369 Loss_G: 0.8729 acc: 57.8%\n",
      "[BATCH 13/149] Loss_D: 0.7963 Loss_G: 0.8547 acc: 46.9%\n",
      "[BATCH 14/149] Loss_D: 0.8738 Loss_G: 0.8586 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7945 Loss_G: 0.8546 acc: 51.6%\n",
      "[BATCH 16/149] Loss_D: 0.7959 Loss_G: 0.8573 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.8236 Loss_G: 0.8515 acc: 51.6%\n",
      "[BATCH 18/149] Loss_D: 0.8098 Loss_G: 0.8445 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.7841 Loss_G: 0.8414 acc: 65.6%\n",
      "[EPOCH 2850] TEST ACC is : 57.6%\n",
      "[BATCH 20/149] Loss_D: 0.8345 Loss_G: 0.8460 acc: 64.1%\n",
      "[BATCH 21/149] Loss_D: 0.8136 Loss_G: 0.8452 acc: 56.2%\n",
      "[BATCH 22/149] Loss_D: 0.8257 Loss_G: 0.8468 acc: 65.6%\n",
      "[BATCH 23/149] Loss_D: 0.8687 Loss_G: 0.8607 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.7694 Loss_G: 0.8539 acc: 56.2%\n",
      "[BATCH 25/149] Loss_D: 0.7954 Loss_G: 0.8443 acc: 40.6%\n",
      "[BATCH 26/149] Loss_D: 0.7924 Loss_G: 0.8419 acc: 53.1%\n",
      "[BATCH 27/149] Loss_D: 0.8521 Loss_G: 0.8508 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.9066 Loss_G: 0.8804 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.7896 Loss_G: 0.8698 acc: 67.2%\n",
      "[BATCH 30/149] Loss_D: 0.8282 Loss_G: 0.8528 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.7955 Loss_G: 0.8481 acc: 48.4%\n",
      "[BATCH 32/149] Loss_D: 0.9107 Loss_G: 0.8679 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.8418 Loss_G: 0.8619 acc: 59.4%\n",
      "[BATCH 34/149] Loss_D: 0.8208 Loss_G: 0.8559 acc: 56.2%\n",
      "[BATCH 35/149] Loss_D: 0.8546 Loss_G: 0.8526 acc: 53.1%\n",
      "[BATCH 36/149] Loss_D: 0.8825 Loss_G: 0.8565 acc: 51.6%\n",
      "[BATCH 37/149] Loss_D: 0.8232 Loss_G: 0.8525 acc: 60.9%\n",
      "[BATCH 38/149] Loss_D: 0.8552 Loss_G: 0.8557 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.8074 Loss_G: 0.8552 acc: 54.7%\n",
      "[BATCH 40/149] Loss_D: 0.8340 Loss_G: 0.8553 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.8035 Loss_G: 0.8593 acc: 56.2%\n",
      "[BATCH 42/149] Loss_D: 0.8670 Loss_G: 0.8593 acc: 59.4%\n",
      "[BATCH 43/149] Loss_D: 0.8353 Loss_G: 0.8579 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.8521 Loss_G: 0.8631 acc: 59.4%\n",
      "[BATCH 45/149] Loss_D: 0.8247 Loss_G: 0.8570 acc: 56.2%\n",
      "[BATCH 46/149] Loss_D: 0.7745 Loss_G: 0.8499 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.7972 Loss_G: 0.8438 acc: 57.8%\n",
      "[BATCH 48/149] Loss_D: 0.8345 Loss_G: 0.8545 acc: 53.1%\n",
      "[BATCH 49/149] Loss_D: 0.8604 Loss_G: 0.8648 acc: 57.8%\n",
      "[BATCH 50/149] Loss_D: 0.8116 Loss_G: 0.8570 acc: 54.7%\n",
      "[BATCH 51/149] Loss_D: 0.8580 Loss_G: 0.8672 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.8622 Loss_G: 0.8655 acc: 57.8%\n",
      "[BATCH 53/149] Loss_D: 0.8197 Loss_G: 0.8654 acc: 51.6%\n",
      "[BATCH 54/149] Loss_D: 0.7755 Loss_G: 0.8501 acc: 53.1%\n",
      "[BATCH 55/149] Loss_D: 0.8203 Loss_G: 0.8392 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.8318 Loss_G: 0.8475 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.8021 Loss_G: 0.8487 acc: 54.7%\n",
      "[BATCH 58/149] Loss_D: 0.7587 Loss_G: 0.8367 acc: 54.7%\n",
      "[BATCH 59/149] Loss_D: 0.7917 Loss_G: 0.8370 acc: 53.1%\n",
      "[BATCH 60/149] Loss_D: 0.8358 Loss_G: 0.8397 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.8555 Loss_G: 0.8466 acc: 60.9%\n",
      "[BATCH 62/149] Loss_D: 0.8653 Loss_G: 0.8599 acc: 50.0%\n",
      "[BATCH 63/149] Loss_D: 0.8647 Loss_G: 0.8604 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.7651 Loss_G: 0.8443 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8795 Loss_G: 0.8442 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7895 Loss_G: 0.8398 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.8311 Loss_G: 0.8401 acc: 40.6%\n",
      "[BATCH 68/149] Loss_D: 0.7866 Loss_G: 0.8425 acc: 62.5%\n",
      "[BATCH 69/149] Loss_D: 0.8430 Loss_G: 0.8525 acc: 59.4%\n",
      "[EPOCH 2900] TEST ACC is : 59.8%\n",
      "[BATCH 70/149] Loss_D: 0.8440 Loss_G: 0.8566 acc: 54.7%\n",
      "[BATCH 71/149] Loss_D: 0.8246 Loss_G: 0.8582 acc: 45.3%\n",
      "[BATCH 72/149] Loss_D: 0.8745 Loss_G: 0.8589 acc: 45.3%\n",
      "[BATCH 73/149] Loss_D: 0.8082 Loss_G: 0.8519 acc: 40.6%\n",
      "[BATCH 74/149] Loss_D: 0.8049 Loss_G: 0.8456 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.8058 Loss_G: 0.8420 acc: 54.7%\n",
      "[BATCH 76/149] Loss_D: 0.8943 Loss_G: 0.8580 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.8309 Loss_G: 0.8573 acc: 56.2%\n",
      "[BATCH 78/149] Loss_D: 0.8048 Loss_G: 0.8510 acc: 54.7%\n",
      "[BATCH 79/149] Loss_D: 0.8653 Loss_G: 0.8662 acc: 46.9%\n",
      "[BATCH 80/149] Loss_D: 0.7939 Loss_G: 0.8597 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.8710 Loss_G: 0.8640 acc: 51.6%\n",
      "[BATCH 82/149] Loss_D: 0.8539 Loss_G: 0.8683 acc: 45.3%\n",
      "[BATCH 83/149] Loss_D: 0.8734 Loss_G: 0.8775 acc: 53.1%\n",
      "[BATCH 84/149] Loss_D: 0.8704 Loss_G: 0.8816 acc: 51.6%\n",
      "[BATCH 85/149] Loss_D: 0.7962 Loss_G: 0.8597 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.8447 Loss_G: 0.8564 acc: 50.0%\n",
      "[BATCH 87/149] Loss_D: 0.8306 Loss_G: 0.8497 acc: 51.6%\n",
      "[BATCH 88/149] Loss_D: 0.7870 Loss_G: 0.8373 acc: 48.4%\n",
      "[BATCH 89/149] Loss_D: 0.7833 Loss_G: 0.8326 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.7928 Loss_G: 0.8287 acc: 46.9%\n",
      "[BATCH 91/149] Loss_D: 0.8178 Loss_G: 0.8320 acc: 56.2%\n",
      "[BATCH 92/149] Loss_D: 0.8009 Loss_G: 0.8328 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.8738 Loss_G: 0.8442 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.8011 Loss_G: 0.8420 acc: 48.4%\n",
      "[BATCH 95/149] Loss_D: 0.7625 Loss_G: 0.8305 acc: 73.4%\n",
      "[BATCH 96/149] Loss_D: 0.8271 Loss_G: 0.8342 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.7941 Loss_G: 0.8327 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.8016 Loss_G: 0.8363 acc: 51.6%\n",
      "[BATCH 99/149] Loss_D: 0.7972 Loss_G: 0.8327 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.8451 Loss_G: 0.8548 acc: 59.4%\n",
      "[BATCH 101/149] Loss_D: 0.8164 Loss_G: 0.8500 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.8063 Loss_G: 0.8416 acc: 56.2%\n",
      "[BATCH 103/149] Loss_D: 0.8022 Loss_G: 0.8385 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8361 Loss_G: 0.8401 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.8518 Loss_G: 0.8492 acc: 54.7%\n",
      "[BATCH 106/149] Loss_D: 0.8087 Loss_G: 0.8507 acc: 54.7%\n",
      "[BATCH 107/149] Loss_D: 0.8532 Loss_G: 0.8525 acc: 56.2%\n",
      "[BATCH 108/149] Loss_D: 0.8692 Loss_G: 0.8612 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.9015 Loss_G: 0.8778 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.8351 Loss_G: 0.8654 acc: 54.7%\n",
      "[BATCH 111/149] Loss_D: 0.8201 Loss_G: 0.8589 acc: 56.2%\n",
      "[BATCH 112/149] Loss_D: 0.7886 Loss_G: 0.8438 acc: 45.3%\n",
      "[BATCH 113/149] Loss_D: 0.8362 Loss_G: 0.8481 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7589 Loss_G: 0.8361 acc: 51.6%\n",
      "[BATCH 115/149] Loss_D: 0.7819 Loss_G: 0.8331 acc: 50.0%\n",
      "[BATCH 116/149] Loss_D: 0.7923 Loss_G: 0.8340 acc: 54.7%\n",
      "[BATCH 117/149] Loss_D: 0.7824 Loss_G: 0.8308 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.7891 Loss_G: 0.8280 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.8305 Loss_G: 0.8329 acc: 65.6%\n",
      "[EPOCH 2950] TEST ACC is : 58.8%\n",
      "[BATCH 120/149] Loss_D: 0.8355 Loss_G: 0.8312 acc: 60.9%\n",
      "[BATCH 121/149] Loss_D: 0.7974 Loss_G: 0.8384 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.8460 Loss_G: 0.8483 acc: 54.7%\n",
      "[BATCH 123/149] Loss_D: 0.8162 Loss_G: 0.8485 acc: 50.0%\n",
      "[BATCH 124/149] Loss_D: 0.7629 Loss_G: 0.8425 acc: 59.4%\n",
      "[BATCH 125/149] Loss_D: 0.7844 Loss_G: 0.8315 acc: 59.4%\n",
      "[BATCH 126/149] Loss_D: 0.8742 Loss_G: 0.8457 acc: 51.6%\n",
      "[BATCH 127/149] Loss_D: 0.7905 Loss_G: 0.8413 acc: 50.0%\n",
      "[BATCH 128/149] Loss_D: 0.8447 Loss_G: 0.8441 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.8112 Loss_G: 0.8429 acc: 46.9%\n",
      "[BATCH 130/149] Loss_D: 0.7896 Loss_G: 0.8393 acc: 54.7%\n",
      "[BATCH 131/149] Loss_D: 0.7802 Loss_G: 0.8327 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.8299 Loss_G: 0.8395 acc: 51.6%\n",
      "[BATCH 133/149] Loss_D: 0.8414 Loss_G: 0.8486 acc: 51.6%\n",
      "[BATCH 134/149] Loss_D: 0.8781 Loss_G: 0.8658 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.9209 Loss_G: 0.9015 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.8314 Loss_G: 0.8909 acc: 54.7%\n",
      "[BATCH 137/149] Loss_D: 0.7878 Loss_G: 0.8663 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.8389 Loss_G: 0.8583 acc: 48.4%\n",
      "[BATCH 139/149] Loss_D: 0.8539 Loss_G: 0.8590 acc: 57.8%\n",
      "[BATCH 140/149] Loss_D: 0.8272 Loss_G: 0.8595 acc: 54.7%\n",
      "[BATCH 141/149] Loss_D: 0.8254 Loss_G: 0.8513 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.8489 Loss_G: 0.8610 acc: 65.6%\n",
      "[BATCH 143/149] Loss_D: 0.8381 Loss_G: 0.8703 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7997 Loss_G: 0.8567 acc: 57.8%\n",
      "[BATCH 145/149] Loss_D: 0.8664 Loss_G: 0.8637 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.8576 Loss_G: 0.8653 acc: 59.4%\n",
      "[BATCH 147/149] Loss_D: 0.8037 Loss_G: 0.8574 acc: 53.1%\n",
      "[BATCH 148/149] Loss_D: 0.8250 Loss_G: 0.8508 acc: 56.2%\n",
      "[BATCH 149/149] Loss_D: 0.8342 Loss_G: 0.8443 acc: 60.9%\n",
      "-----THE [20/50] epoch end-----\n",
      "-----THE [21/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8611 Loss_G: 0.8681 acc: 71.9%\n",
      "[BATCH 2/149] Loss_D: 0.8136 Loss_G: 0.8680 acc: 62.5%\n",
      "[BATCH 3/149] Loss_D: 0.8478 Loss_G: 0.8567 acc: 50.0%\n",
      "[BATCH 4/149] Loss_D: 0.8869 Loss_G: 0.8527 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.8699 Loss_G: 0.8589 acc: 62.5%\n",
      "[BATCH 6/149] Loss_D: 0.8073 Loss_G: 0.8572 acc: 59.4%\n",
      "[BATCH 7/149] Loss_D: 0.8047 Loss_G: 0.8501 acc: 45.3%\n",
      "[BATCH 8/149] Loss_D: 0.8410 Loss_G: 0.8529 acc: 56.2%\n",
      "[BATCH 9/149] Loss_D: 0.8162 Loss_G: 0.8432 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7950 Loss_G: 0.8427 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.8353 Loss_G: 0.8460 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.8330 Loss_G: 0.8482 acc: 57.8%\n",
      "[BATCH 13/149] Loss_D: 0.8582 Loss_G: 0.8538 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.8326 Loss_G: 0.8505 acc: 53.1%\n",
      "[BATCH 15/149] Loss_D: 0.8487 Loss_G: 0.8539 acc: 57.8%\n",
      "[BATCH 16/149] Loss_D: 0.8125 Loss_G: 0.8438 acc: 57.8%\n",
      "[BATCH 17/149] Loss_D: 0.7920 Loss_G: 0.8419 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.8831 Loss_G: 0.8509 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.8009 Loss_G: 0.8527 acc: 71.9%\n",
      "[BATCH 20/149] Loss_D: 0.8028 Loss_G: 0.8390 acc: 54.7%\n",
      "[EPOCH 3000] TEST ACC is : 56.8%\n",
      "[BATCH 21/149] Loss_D: 0.8306 Loss_G: 0.8403 acc: 59.4%\n",
      "[BATCH 22/149] Loss_D: 0.7889 Loss_G: 0.8388 acc: 51.6%\n",
      "[BATCH 23/149] Loss_D: 0.7585 Loss_G: 0.8340 acc: 51.6%\n",
      "[BATCH 24/149] Loss_D: 0.7906 Loss_G: 0.8332 acc: 39.1%\n",
      "[BATCH 25/149] Loss_D: 0.8379 Loss_G: 0.8425 acc: 57.8%\n",
      "[BATCH 26/149] Loss_D: 0.8732 Loss_G: 0.8551 acc: 50.0%\n",
      "[BATCH 27/149] Loss_D: 0.8244 Loss_G: 0.8524 acc: 54.7%\n",
      "[BATCH 28/149] Loss_D: 0.8314 Loss_G: 0.8584 acc: 50.0%\n",
      "[BATCH 29/149] Loss_D: 0.8039 Loss_G: 0.8565 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 0.7978 Loss_G: 0.8440 acc: 53.1%\n",
      "[BATCH 31/149] Loss_D: 0.8735 Loss_G: 0.8569 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.7983 Loss_G: 0.8528 acc: 53.1%\n",
      "[BATCH 33/149] Loss_D: 0.7778 Loss_G: 0.8459 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8059 Loss_G: 0.8407 acc: 62.5%\n",
      "[BATCH 35/149] Loss_D: 0.8813 Loss_G: 0.8596 acc: 51.6%\n",
      "[BATCH 36/149] Loss_D: 0.8012 Loss_G: 0.8564 acc: 53.1%\n",
      "[BATCH 37/149] Loss_D: 0.8086 Loss_G: 0.8534 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.8303 Loss_G: 0.8496 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.8083 Loss_G: 0.8532 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.8123 Loss_G: 0.8404 acc: 50.0%\n",
      "[BATCH 41/149] Loss_D: 0.8007 Loss_G: 0.8362 acc: 59.4%\n",
      "[BATCH 42/149] Loss_D: 0.8174 Loss_G: 0.8382 acc: 56.2%\n",
      "[BATCH 43/149] Loss_D: 0.7770 Loss_G: 0.8373 acc: 51.6%\n",
      "[BATCH 44/149] Loss_D: 0.8279 Loss_G: 0.8453 acc: 54.7%\n",
      "[BATCH 45/149] Loss_D: 0.8261 Loss_G: 0.8495 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.7895 Loss_G: 0.8515 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.7829 Loss_G: 0.8471 acc: 53.1%\n",
      "[BATCH 48/149] Loss_D: 0.7965 Loss_G: 0.8323 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.8198 Loss_G: 0.8329 acc: 56.2%\n",
      "[BATCH 50/149] Loss_D: 0.8538 Loss_G: 0.8368 acc: 56.2%\n",
      "[BATCH 51/149] Loss_D: 0.8180 Loss_G: 0.8407 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.8751 Loss_G: 0.8462 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.8170 Loss_G: 0.8503 acc: 56.2%\n",
      "[BATCH 54/149] Loss_D: 0.8108 Loss_G: 0.8524 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.8338 Loss_G: 0.8536 acc: 59.4%\n",
      "[BATCH 56/149] Loss_D: 0.7985 Loss_G: 0.8571 acc: 53.1%\n",
      "[BATCH 57/149] Loss_D: 0.8126 Loss_G: 0.8547 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.8533 Loss_G: 0.8567 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.8351 Loss_G: 0.8582 acc: 42.2%\n",
      "[BATCH 60/149] Loss_D: 0.8162 Loss_G: 0.8461 acc: 54.7%\n",
      "[BATCH 61/149] Loss_D: 0.7675 Loss_G: 0.8358 acc: 57.8%\n",
      "[BATCH 62/149] Loss_D: 0.7840 Loss_G: 0.8330 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.7945 Loss_G: 0.8310 acc: 62.5%\n",
      "[BATCH 64/149] Loss_D: 0.7627 Loss_G: 0.8307 acc: 59.4%\n",
      "[BATCH 65/149] Loss_D: 0.9072 Loss_G: 0.8480 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.7992 Loss_G: 0.8415 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.8034 Loss_G: 0.8406 acc: 57.8%\n",
      "[BATCH 68/149] Loss_D: 0.8033 Loss_G: 0.8355 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.8496 Loss_G: 0.8404 acc: 68.8%\n",
      "[BATCH 70/149] Loss_D: 0.8605 Loss_G: 0.8539 acc: 59.4%\n",
      "[EPOCH 3050] TEST ACC is : 58.2%\n",
      "[BATCH 71/149] Loss_D: 0.8245 Loss_G: 0.8527 acc: 51.6%\n",
      "[BATCH 72/149] Loss_D: 0.7881 Loss_G: 0.8449 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.8186 Loss_G: 0.8487 acc: 56.2%\n",
      "[BATCH 74/149] Loss_D: 0.8969 Loss_G: 0.8628 acc: 67.2%\n",
      "[BATCH 75/149] Loss_D: 0.8367 Loss_G: 0.8802 acc: 57.8%\n",
      "[BATCH 76/149] Loss_D: 0.8048 Loss_G: 0.8543 acc: 46.9%\n",
      "[BATCH 77/149] Loss_D: 0.8450 Loss_G: 0.8532 acc: 46.9%\n",
      "[BATCH 78/149] Loss_D: 0.7937 Loss_G: 0.8425 acc: 51.6%\n",
      "[BATCH 79/149] Loss_D: 0.7719 Loss_G: 0.8312 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.7737 Loss_G: 0.8248 acc: 68.8%\n",
      "[BATCH 81/149] Loss_D: 0.8310 Loss_G: 0.8299 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.8576 Loss_G: 0.8435 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7759 Loss_G: 0.8373 acc: 56.2%\n",
      "[BATCH 84/149] Loss_D: 0.7986 Loss_G: 0.8340 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.8492 Loss_G: 0.8392 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.8260 Loss_G: 0.8393 acc: 62.5%\n",
      "[BATCH 87/149] Loss_D: 0.8580 Loss_G: 0.8463 acc: 54.7%\n",
      "[BATCH 88/149] Loss_D: 0.8530 Loss_G: 0.8513 acc: 56.2%\n",
      "[BATCH 89/149] Loss_D: 0.8440 Loss_G: 0.8573 acc: 51.6%\n",
      "[BATCH 90/149] Loss_D: 0.8350 Loss_G: 0.8569 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.8414 Loss_G: 0.8556 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.8138 Loss_G: 0.8544 acc: 54.7%\n",
      "[BATCH 93/149] Loss_D: 0.7624 Loss_G: 0.8376 acc: 48.4%\n",
      "[BATCH 94/149] Loss_D: 0.8801 Loss_G: 0.8435 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.8082 Loss_G: 0.8440 acc: 45.3%\n",
      "[BATCH 96/149] Loss_D: 0.8046 Loss_G: 0.8390 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.8400 Loss_G: 0.8396 acc: 62.5%\n",
      "[BATCH 98/149] Loss_D: 0.8555 Loss_G: 0.8513 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.8261 Loss_G: 0.8499 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.8468 Loss_G: 0.8488 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.8477 Loss_G: 0.8532 acc: 51.6%\n",
      "[BATCH 102/149] Loss_D: 0.8289 Loss_G: 0.8533 acc: 60.9%\n",
      "[BATCH 103/149] Loss_D: 0.8262 Loss_G: 0.8521 acc: 56.2%\n",
      "[BATCH 104/149] Loss_D: 0.7913 Loss_G: 0.8431 acc: 53.1%\n",
      "[BATCH 105/149] Loss_D: 0.8049 Loss_G: 0.8462 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.8019 Loss_G: 0.8365 acc: 45.3%\n",
      "[BATCH 107/149] Loss_D: 0.8608 Loss_G: 0.8519 acc: 57.8%\n",
      "[BATCH 108/149] Loss_D: 0.7926 Loss_G: 0.8604 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.8119 Loss_G: 0.8486 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.8664 Loss_G: 0.8574 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.8068 Loss_G: 0.8552 acc: 53.1%\n",
      "[BATCH 112/149] Loss_D: 0.8656 Loss_G: 0.8520 acc: 50.0%\n",
      "[BATCH 113/149] Loss_D: 0.8972 Loss_G: 0.8758 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.8435 Loss_G: 0.8803 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.8515 Loss_G: 0.8742 acc: 50.0%\n",
      "[BATCH 116/149] Loss_D: 0.8240 Loss_G: 0.8589 acc: 59.4%\n",
      "[BATCH 117/149] Loss_D: 0.8039 Loss_G: 0.8503 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.8139 Loss_G: 0.8452 acc: 53.1%\n",
      "[BATCH 119/149] Loss_D: 0.7895 Loss_G: 0.8415 acc: 53.1%\n",
      "[BATCH 120/149] Loss_D: 0.8602 Loss_G: 0.8473 acc: 54.7%\n",
      "[EPOCH 3100] TEST ACC is : 57.8%\n",
      "[BATCH 121/149] Loss_D: 0.8152 Loss_G: 0.8472 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8037 Loss_G: 0.8451 acc: 54.7%\n",
      "[BATCH 123/149] Loss_D: 0.8301 Loss_G: 0.8445 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.8223 Loss_G: 0.8466 acc: 51.6%\n",
      "[BATCH 125/149] Loss_D: 0.8680 Loss_G: 0.8535 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8109 Loss_G: 0.8541 acc: 65.6%\n",
      "[BATCH 127/149] Loss_D: 0.8010 Loss_G: 0.8457 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.8371 Loss_G: 0.8367 acc: 54.7%\n",
      "[BATCH 129/149] Loss_D: 0.8095 Loss_G: 0.8419 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.8206 Loss_G: 0.8397 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.8235 Loss_G: 0.8433 acc: 54.7%\n",
      "[BATCH 132/149] Loss_D: 0.8363 Loss_G: 0.8434 acc: 56.2%\n",
      "[BATCH 133/149] Loss_D: 0.8201 Loss_G: 0.8445 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.8279 Loss_G: 0.8361 acc: 50.0%\n",
      "[BATCH 135/149] Loss_D: 0.8091 Loss_G: 0.8361 acc: 67.2%\n",
      "[BATCH 136/149] Loss_D: 0.8300 Loss_G: 0.8393 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8132 Loss_G: 0.8396 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.8168 Loss_G: 0.8381 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7533 Loss_G: 0.8299 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.8273 Loss_G: 0.8283 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.8229 Loss_G: 0.8382 acc: 56.2%\n",
      "[BATCH 142/149] Loss_D: 0.8535 Loss_G: 0.8485 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.8236 Loss_G: 0.8492 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.8761 Loss_G: 0.8560 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.8044 Loss_G: 0.8491 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.8115 Loss_G: 0.8433 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.8061 Loss_G: 0.8416 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.8061 Loss_G: 0.8459 acc: 60.9%\n",
      "[BATCH 149/149] Loss_D: 0.8088 Loss_G: 0.8462 acc: 59.4%\n",
      "-----THE [21/50] epoch end-----\n",
      "-----THE [22/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8356 Loss_G: 0.8495 acc: 56.2%\n",
      "[BATCH 2/149] Loss_D: 0.8066 Loss_G: 0.8504 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.8186 Loss_G: 0.8545 acc: 59.4%\n",
      "[BATCH 4/149] Loss_D: 0.8410 Loss_G: 0.8579 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.8341 Loss_G: 0.8574 acc: 75.0%\n",
      "[BATCH 6/149] Loss_D: 0.8203 Loss_G: 0.8521 acc: 48.4%\n",
      "[BATCH 7/149] Loss_D: 0.8004 Loss_G: 0.8439 acc: 57.8%\n",
      "[BATCH 8/149] Loss_D: 0.8397 Loss_G: 0.8425 acc: 54.7%\n",
      "[BATCH 9/149] Loss_D: 0.8753 Loss_G: 0.8518 acc: 51.6%\n",
      "[BATCH 10/149] Loss_D: 0.7563 Loss_G: 0.8442 acc: 48.4%\n",
      "[BATCH 11/149] Loss_D: 0.8529 Loss_G: 0.8459 acc: 62.5%\n",
      "[BATCH 12/149] Loss_D: 0.8072 Loss_G: 0.8457 acc: 57.8%\n",
      "[BATCH 13/149] Loss_D: 0.8219 Loss_G: 0.8500 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.8229 Loss_G: 0.8418 acc: 68.8%\n",
      "[BATCH 15/149] Loss_D: 0.7987 Loss_G: 0.8439 acc: 57.8%\n",
      "[BATCH 16/149] Loss_D: 0.8333 Loss_G: 0.8499 acc: 56.2%\n",
      "[BATCH 17/149] Loss_D: 0.7624 Loss_G: 0.8386 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.8399 Loss_G: 0.8408 acc: 53.1%\n",
      "[BATCH 19/149] Loss_D: 0.8066 Loss_G: 0.8422 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.8534 Loss_G: 0.8485 acc: 54.7%\n",
      "[BATCH 21/149] Loss_D: 0.8089 Loss_G: 0.8467 acc: 54.7%\n",
      "[EPOCH 3150] TEST ACC is : 61.9%\n",
      "[BATCH 22/149] Loss_D: 0.7678 Loss_G: 0.8394 acc: 53.1%\n",
      "[BATCH 23/149] Loss_D: 0.7944 Loss_G: 0.8370 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.8293 Loss_G: 0.8459 acc: 53.1%\n",
      "[BATCH 25/149] Loss_D: 0.8393 Loss_G: 0.8473 acc: 57.8%\n",
      "[BATCH 26/149] Loss_D: 0.8333 Loss_G: 0.8476 acc: 60.9%\n",
      "[BATCH 27/149] Loss_D: 0.7897 Loss_G: 0.8466 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7971 Loss_G: 0.8377 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.8534 Loss_G: 0.8426 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 0.8619 Loss_G: 0.8537 acc: 53.1%\n",
      "[BATCH 31/149] Loss_D: 0.8068 Loss_G: 0.8583 acc: 54.7%\n",
      "[BATCH 32/149] Loss_D: 0.8197 Loss_G: 0.8478 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.7991 Loss_G: 0.8409 acc: 56.2%\n",
      "[BATCH 34/149] Loss_D: 0.8303 Loss_G: 0.8427 acc: 48.4%\n",
      "[BATCH 35/149] Loss_D: 0.7837 Loss_G: 0.8376 acc: 56.2%\n",
      "[BATCH 36/149] Loss_D: 0.8100 Loss_G: 0.8335 acc: 46.9%\n",
      "[BATCH 37/149] Loss_D: 0.8426 Loss_G: 0.8387 acc: 57.8%\n",
      "[BATCH 38/149] Loss_D: 0.8282 Loss_G: 0.8499 acc: 67.2%\n",
      "[BATCH 39/149] Loss_D: 0.8346 Loss_G: 0.8513 acc: 59.4%\n",
      "[BATCH 40/149] Loss_D: 0.7756 Loss_G: 0.8409 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.8171 Loss_G: 0.8403 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.8049 Loss_G: 0.8413 acc: 65.6%\n",
      "[BATCH 43/149] Loss_D: 0.8283 Loss_G: 0.8474 acc: 51.6%\n",
      "[BATCH 44/149] Loss_D: 0.8268 Loss_G: 0.8479 acc: 51.6%\n",
      "[BATCH 45/149] Loss_D: 0.8091 Loss_G: 0.8610 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.8225 Loss_G: 0.8510 acc: 53.1%\n",
      "[BATCH 47/149] Loss_D: 0.8519 Loss_G: 0.8463 acc: 54.7%\n",
      "[BATCH 48/149] Loss_D: 0.7836 Loss_G: 0.8385 acc: 50.0%\n",
      "[BATCH 49/149] Loss_D: 0.8591 Loss_G: 0.8460 acc: 53.1%\n",
      "[BATCH 50/149] Loss_D: 0.7872 Loss_G: 0.8403 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.7703 Loss_G: 0.8337 acc: 54.7%\n",
      "[BATCH 52/149] Loss_D: 0.8107 Loss_G: 0.8336 acc: 59.4%\n",
      "[BATCH 53/149] Loss_D: 0.8602 Loss_G: 0.8529 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.8854 Loss_G: 0.8788 acc: 71.9%\n",
      "[BATCH 55/149] Loss_D: 0.8163 Loss_G: 0.8832 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7996 Loss_G: 0.8513 acc: 46.9%\n",
      "[BATCH 57/149] Loss_D: 0.8531 Loss_G: 0.8439 acc: 50.0%\n",
      "[BATCH 58/149] Loss_D: 0.7610 Loss_G: 0.8358 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.8186 Loss_G: 0.8328 acc: 57.8%\n",
      "[BATCH 60/149] Loss_D: 0.7747 Loss_G: 0.8321 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.8375 Loss_G: 0.8354 acc: 51.6%\n",
      "[BATCH 62/149] Loss_D: 0.8169 Loss_G: 0.8335 acc: 46.9%\n",
      "[BATCH 63/149] Loss_D: 0.8202 Loss_G: 0.8371 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.8221 Loss_G: 0.8419 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.8335 Loss_G: 0.8466 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.8015 Loss_G: 0.8437 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.8452 Loss_G: 0.8460 acc: 48.4%\n",
      "[BATCH 68/149] Loss_D: 0.8088 Loss_G: 0.8450 acc: 57.8%\n",
      "[BATCH 69/149] Loss_D: 0.7705 Loss_G: 0.8427 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.8262 Loss_G: 0.8389 acc: 62.5%\n",
      "[BATCH 71/149] Loss_D: 0.7850 Loss_G: 0.8402 acc: 59.4%\n",
      "[EPOCH 3200] TEST ACC is : 66.0%\n",
      "[BATCH 72/149] Loss_D: 0.7935 Loss_G: 0.8341 acc: 56.2%\n",
      "[BATCH 73/149] Loss_D: 0.8458 Loss_G: 0.8451 acc: 54.7%\n",
      "[BATCH 74/149] Loss_D: 0.8001 Loss_G: 0.8422 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.7939 Loss_G: 0.8361 acc: 45.3%\n",
      "[BATCH 76/149] Loss_D: 0.8495 Loss_G: 0.8400 acc: 62.5%\n",
      "[BATCH 77/149] Loss_D: 0.7922 Loss_G: 0.8404 acc: 50.0%\n",
      "[BATCH 78/149] Loss_D: 0.8086 Loss_G: 0.8487 acc: 59.4%\n",
      "[BATCH 79/149] Loss_D: 0.8297 Loss_G: 0.8472 acc: 45.3%\n",
      "[BATCH 80/149] Loss_D: 0.8099 Loss_G: 0.8359 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.8271 Loss_G: 0.8383 acc: 56.2%\n",
      "[BATCH 82/149] Loss_D: 0.8048 Loss_G: 0.8327 acc: 40.6%\n",
      "[BATCH 83/149] Loss_D: 0.9048 Loss_G: 0.8561 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 0.8231 Loss_G: 0.8529 acc: 57.8%\n",
      "[BATCH 85/149] Loss_D: 0.8105 Loss_G: 0.8431 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.8246 Loss_G: 0.8401 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.8083 Loss_G: 0.8408 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.8033 Loss_G: 0.8403 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.7931 Loss_G: 0.8315 acc: 59.4%\n",
      "[BATCH 90/149] Loss_D: 0.8194 Loss_G: 0.8291 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.7931 Loss_G: 0.8306 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.8411 Loss_G: 0.8390 acc: 54.7%\n",
      "[BATCH 93/149] Loss_D: 0.8195 Loss_G: 0.8404 acc: 56.2%\n",
      "[BATCH 94/149] Loss_D: 0.8148 Loss_G: 0.8390 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.8213 Loss_G: 0.8415 acc: 56.2%\n",
      "[BATCH 96/149] Loss_D: 0.8252 Loss_G: 0.8389 acc: 51.6%\n",
      "[BATCH 97/149] Loss_D: 0.8510 Loss_G: 0.8461 acc: 54.7%\n",
      "[BATCH 98/149] Loss_D: 0.7917 Loss_G: 0.8384 acc: 56.2%\n",
      "[BATCH 99/149] Loss_D: 0.8208 Loss_G: 0.8338 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.8215 Loss_G: 0.8390 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7780 Loss_G: 0.8332 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.8625 Loss_G: 0.8411 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.8051 Loss_G: 0.8436 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8100 Loss_G: 0.8409 acc: 51.6%\n",
      "[BATCH 105/149] Loss_D: 0.8787 Loss_G: 0.8488 acc: 46.9%\n",
      "[BATCH 106/149] Loss_D: 0.8235 Loss_G: 0.8476 acc: 53.1%\n",
      "[BATCH 107/149] Loss_D: 0.7977 Loss_G: 0.8373 acc: 53.1%\n",
      "[BATCH 108/149] Loss_D: 0.8428 Loss_G: 0.8440 acc: 56.2%\n",
      "[BATCH 109/149] Loss_D: 0.8277 Loss_G: 0.8474 acc: 56.2%\n",
      "[BATCH 110/149] Loss_D: 0.8317 Loss_G: 0.8503 acc: 59.4%\n",
      "[BATCH 111/149] Loss_D: 0.8118 Loss_G: 0.8435 acc: 56.2%\n",
      "[BATCH 112/149] Loss_D: 0.7556 Loss_G: 0.8361 acc: 54.7%\n",
      "[BATCH 113/149] Loss_D: 0.8295 Loss_G: 0.8379 acc: 50.0%\n",
      "[BATCH 114/149] Loss_D: 0.8844 Loss_G: 0.8536 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.7984 Loss_G: 0.8501 acc: 57.8%\n",
      "[BATCH 116/149] Loss_D: 0.7793 Loss_G: 0.8364 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.8331 Loss_G: 0.8354 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.8083 Loss_G: 0.8314 acc: 57.8%\n",
      "[BATCH 119/149] Loss_D: 0.8006 Loss_G: 0.8268 acc: 56.2%\n",
      "[BATCH 120/149] Loss_D: 0.8384 Loss_G: 0.8466 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.8292 Loss_G: 0.8493 acc: 71.9%\n",
      "[EPOCH 3250] TEST ACC is : 62.7%\n",
      "[BATCH 122/149] Loss_D: 0.8622 Loss_G: 0.8519 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.7999 Loss_G: 0.8512 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.8457 Loss_G: 0.8509 acc: 59.4%\n",
      "[BATCH 125/149] Loss_D: 0.8341 Loss_G: 0.8452 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.7796 Loss_G: 0.8301 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.8180 Loss_G: 0.8378 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.8562 Loss_G: 0.8483 acc: 53.1%\n",
      "[BATCH 129/149] Loss_D: 0.8340 Loss_G: 0.8480 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.8488 Loss_G: 0.8541 acc: 62.5%\n",
      "[BATCH 131/149] Loss_D: 0.8017 Loss_G: 0.8477 acc: 51.6%\n",
      "[BATCH 132/149] Loss_D: 0.8375 Loss_G: 0.8565 acc: 64.1%\n",
      "[BATCH 133/149] Loss_D: 0.7652 Loss_G: 0.8410 acc: 45.3%\n",
      "[BATCH 134/149] Loss_D: 0.8561 Loss_G: 0.8526 acc: 46.9%\n",
      "[BATCH 135/149] Loss_D: 0.8339 Loss_G: 0.8609 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.8065 Loss_G: 0.8483 acc: 43.8%\n",
      "[BATCH 137/149] Loss_D: 0.8347 Loss_G: 0.8424 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.8361 Loss_G: 0.8499 acc: 46.9%\n",
      "[BATCH 139/149] Loss_D: 0.7937 Loss_G: 0.8462 acc: 53.1%\n",
      "[BATCH 140/149] Loss_D: 0.7753 Loss_G: 0.8350 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.8304 Loss_G: 0.8349 acc: 50.0%\n",
      "[BATCH 142/149] Loss_D: 0.8998 Loss_G: 0.8561 acc: 57.8%\n",
      "[BATCH 143/149] Loss_D: 0.8761 Loss_G: 0.8736 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.8630 Loss_G: 0.8652 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.8505 Loss_G: 0.8498 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.9017 Loss_G: 0.8568 acc: 56.2%\n",
      "[BATCH 147/149] Loss_D: 0.8446 Loss_G: 0.8585 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.7788 Loss_G: 0.8413 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.8488 Loss_G: 0.8451 acc: 53.1%\n",
      "-----THE [22/50] epoch end-----\n",
      "-----THE [23/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7822 Loss_G: 0.8369 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.8155 Loss_G: 0.8381 acc: 62.5%\n",
      "[BATCH 3/149] Loss_D: 0.8432 Loss_G: 0.8438 acc: 51.6%\n",
      "[BATCH 4/149] Loss_D: 0.8428 Loss_G: 0.8517 acc: 51.6%\n",
      "[BATCH 5/149] Loss_D: 0.8262 Loss_G: 0.8520 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7557 Loss_G: 0.8392 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8400 Loss_G: 0.8405 acc: 59.4%\n",
      "[BATCH 8/149] Loss_D: 0.8211 Loss_G: 0.8477 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.8353 Loss_G: 0.8460 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8125 Loss_G: 0.8450 acc: 56.2%\n",
      "[BATCH 11/149] Loss_D: 0.8312 Loss_G: 0.8420 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.8047 Loss_G: 0.8356 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.8291 Loss_G: 0.8382 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.8704 Loss_G: 0.8503 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.8741 Loss_G: 0.8818 acc: 56.2%\n",
      "[BATCH 16/149] Loss_D: 0.8280 Loss_G: 0.8802 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.7990 Loss_G: 0.8540 acc: 53.1%\n",
      "[BATCH 18/149] Loss_D: 0.8620 Loss_G: 0.8524 acc: 53.1%\n",
      "[BATCH 19/149] Loss_D: 0.8217 Loss_G: 0.8495 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.8266 Loss_G: 0.8502 acc: 65.6%\n",
      "[BATCH 21/149] Loss_D: 0.8830 Loss_G: 0.8647 acc: 56.2%\n",
      "[BATCH 22/149] Loss_D: 0.7838 Loss_G: 0.8575 acc: 70.3%\n",
      "[EPOCH 3300] TEST ACC is : 61.1%\n",
      "[BATCH 23/149] Loss_D: 0.8668 Loss_G: 0.8639 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7807 Loss_G: 0.8554 acc: 53.1%\n",
      "[BATCH 25/149] Loss_D: 0.8182 Loss_G: 0.8477 acc: 62.5%\n",
      "[BATCH 26/149] Loss_D: 0.8125 Loss_G: 0.8393 acc: 54.7%\n",
      "[BATCH 27/149] Loss_D: 0.8182 Loss_G: 0.8430 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.7761 Loss_G: 0.8359 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.8086 Loss_G: 0.8332 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 0.8113 Loss_G: 0.8327 acc: 60.9%\n",
      "[BATCH 31/149] Loss_D: 0.8296 Loss_G: 0.8424 acc: 56.2%\n",
      "[BATCH 32/149] Loss_D: 0.8480 Loss_G: 0.8583 acc: 51.6%\n",
      "[BATCH 33/149] Loss_D: 0.8373 Loss_G: 0.8514 acc: 48.4%\n",
      "[BATCH 34/149] Loss_D: 0.7995 Loss_G: 0.8332 acc: 46.9%\n",
      "[BATCH 35/149] Loss_D: 0.7692 Loss_G: 0.8230 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.7798 Loss_G: 0.8211 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.8241 Loss_G: 0.8305 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7980 Loss_G: 0.8313 acc: 54.7%\n",
      "[BATCH 39/149] Loss_D: 0.8237 Loss_G: 0.8452 acc: 54.7%\n",
      "[BATCH 40/149] Loss_D: 0.8402 Loss_G: 0.8430 acc: 56.2%\n",
      "[BATCH 41/149] Loss_D: 0.8058 Loss_G: 0.8389 acc: 73.4%\n",
      "[BATCH 42/149] Loss_D: 0.7757 Loss_G: 0.8377 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.8640 Loss_G: 0.8372 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.7966 Loss_G: 0.8392 acc: 59.4%\n",
      "[BATCH 45/149] Loss_D: 0.8641 Loss_G: 0.8531 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.8775 Loss_G: 0.8605 acc: 51.6%\n",
      "[BATCH 47/149] Loss_D: 0.9230 Loss_G: 0.8886 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.7943 Loss_G: 0.8659 acc: 59.4%\n",
      "[BATCH 49/149] Loss_D: 0.8603 Loss_G: 0.8558 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.7969 Loss_G: 0.8420 acc: 56.2%\n",
      "[BATCH 51/149] Loss_D: 0.7992 Loss_G: 0.8336 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.8367 Loss_G: 0.8322 acc: 54.7%\n",
      "[BATCH 53/149] Loss_D: 0.8754 Loss_G: 0.8457 acc: 50.0%\n",
      "[BATCH 54/149] Loss_D: 0.7794 Loss_G: 0.8444 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.8357 Loss_G: 0.8348 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.8396 Loss_G: 0.8310 acc: 59.4%\n",
      "[BATCH 57/149] Loss_D: 0.8128 Loss_G: 0.8352 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.8312 Loss_G: 0.8391 acc: 54.7%\n",
      "[BATCH 59/149] Loss_D: 0.8595 Loss_G: 0.8520 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.8519 Loss_G: 0.8623 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.7917 Loss_G: 0.8534 acc: 53.1%\n",
      "[BATCH 62/149] Loss_D: 0.8156 Loss_G: 0.8393 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.7884 Loss_G: 0.8382 acc: 62.5%\n",
      "[BATCH 64/149] Loss_D: 0.7592 Loss_G: 0.8289 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.8937 Loss_G: 0.8457 acc: 71.9%\n",
      "[BATCH 66/149] Loss_D: 0.8183 Loss_G: 0.8510 acc: 56.2%\n",
      "[BATCH 67/149] Loss_D: 0.8015 Loss_G: 0.8514 acc: 59.4%\n",
      "[BATCH 68/149] Loss_D: 0.8477 Loss_G: 0.8469 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.7990 Loss_G: 0.8349 acc: 48.4%\n",
      "[BATCH 70/149] Loss_D: 0.8265 Loss_G: 0.8342 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7700 Loss_G: 0.8273 acc: 51.6%\n",
      "[BATCH 72/149] Loss_D: 0.8034 Loss_G: 0.8316 acc: 46.9%\n",
      "[EPOCH 3350] TEST ACC is : 63.7%\n",
      "[BATCH 73/149] Loss_D: 0.8557 Loss_G: 0.8477 acc: 75.0%\n",
      "[BATCH 74/149] Loss_D: 0.8149 Loss_G: 0.8504 acc: 54.7%\n",
      "[BATCH 75/149] Loss_D: 0.8576 Loss_G: 0.8563 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.8069 Loss_G: 0.8444 acc: 48.4%\n",
      "[BATCH 77/149] Loss_D: 0.8019 Loss_G: 0.8413 acc: 51.6%\n",
      "[BATCH 78/149] Loss_D: 0.7642 Loss_G: 0.8302 acc: 46.9%\n",
      "[BATCH 79/149] Loss_D: 0.8112 Loss_G: 0.8244 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.8187 Loss_G: 0.8308 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.8203 Loss_G: 0.8352 acc: 54.7%\n",
      "[BATCH 82/149] Loss_D: 0.8269 Loss_G: 0.8422 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.8297 Loss_G: 0.8374 acc: 56.2%\n",
      "[BATCH 84/149] Loss_D: 0.8273 Loss_G: 0.8494 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.8121 Loss_G: 0.8497 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.8041 Loss_G: 0.8389 acc: 42.2%\n",
      "[BATCH 87/149] Loss_D: 0.8008 Loss_G: 0.8350 acc: 45.3%\n",
      "[BATCH 88/149] Loss_D: 0.7880 Loss_G: 0.8327 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.8095 Loss_G: 0.8345 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.8230 Loss_G: 0.8428 acc: 48.4%\n",
      "[BATCH 91/149] Loss_D: 0.7727 Loss_G: 0.8495 acc: 54.7%\n",
      "[BATCH 92/149] Loss_D: 0.8843 Loss_G: 0.8546 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.8634 Loss_G: 0.8560 acc: 46.9%\n",
      "[BATCH 94/149] Loss_D: 0.8180 Loss_G: 0.8541 acc: 59.4%\n",
      "[BATCH 95/149] Loss_D: 0.8272 Loss_G: 0.8514 acc: 54.7%\n",
      "[BATCH 96/149] Loss_D: 0.7706 Loss_G: 0.8413 acc: 56.2%\n",
      "[BATCH 97/149] Loss_D: 0.8130 Loss_G: 0.8335 acc: 48.4%\n",
      "[BATCH 98/149] Loss_D: 0.8393 Loss_G: 0.8388 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.7855 Loss_G: 0.8363 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.8307 Loss_G: 0.8397 acc: 67.2%\n",
      "[BATCH 101/149] Loss_D: 0.8091 Loss_G: 0.8445 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.8485 Loss_G: 0.8589 acc: 51.6%\n",
      "[BATCH 103/149] Loss_D: 0.7865 Loss_G: 0.8560 acc: 59.4%\n",
      "[BATCH 104/149] Loss_D: 0.8091 Loss_G: 0.8482 acc: 54.7%\n",
      "[BATCH 105/149] Loss_D: 0.7744 Loss_G: 0.8309 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.8960 Loss_G: 0.8481 acc: 51.6%\n",
      "[BATCH 107/149] Loss_D: 0.8181 Loss_G: 0.8507 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8221 Loss_G: 0.8388 acc: 54.7%\n",
      "[BATCH 109/149] Loss_D: 0.7898 Loss_G: 0.8283 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.8043 Loss_G: 0.8238 acc: 57.8%\n",
      "[BATCH 111/149] Loss_D: 0.8107 Loss_G: 0.8275 acc: 57.8%\n",
      "[BATCH 112/149] Loss_D: 0.7885 Loss_G: 0.8230 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.7698 Loss_G: 0.8202 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.8186 Loss_G: 0.8196 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.8370 Loss_G: 0.8301 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.8152 Loss_G: 0.8380 acc: 56.2%\n",
      "[BATCH 117/149] Loss_D: 0.7983 Loss_G: 0.8415 acc: 57.8%\n",
      "[BATCH 118/149] Loss_D: 0.8920 Loss_G: 0.8583 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.8370 Loss_G: 0.8736 acc: 75.0%\n",
      "[BATCH 120/149] Loss_D: 0.7820 Loss_G: 0.8511 acc: 65.6%\n",
      "[BATCH 121/149] Loss_D: 0.8052 Loss_G: 0.8475 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8042 Loss_G: 0.8445 acc: 59.4%\n",
      "[EPOCH 3400] TEST ACC is : 61.7%\n",
      "[BATCH 123/149] Loss_D: 0.7840 Loss_G: 0.8412 acc: 46.9%\n",
      "[BATCH 124/149] Loss_D: 0.8218 Loss_G: 0.8424 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.8055 Loss_G: 0.8491 acc: 43.8%\n",
      "[BATCH 126/149] Loss_D: 0.7734 Loss_G: 0.8464 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.8121 Loss_G: 0.8495 acc: 57.8%\n",
      "[BATCH 128/149] Loss_D: 0.8134 Loss_G: 0.8547 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.8757 Loss_G: 0.8672 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.8482 Loss_G: 0.8638 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.7956 Loss_G: 0.8471 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.8445 Loss_G: 0.8445 acc: 64.1%\n",
      "[BATCH 133/149] Loss_D: 0.9089 Loss_G: 0.8767 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.8142 Loss_G: 0.8652 acc: 50.0%\n",
      "[BATCH 135/149] Loss_D: 0.8293 Loss_G: 0.8546 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.8025 Loss_G: 0.8383 acc: 50.0%\n",
      "[BATCH 137/149] Loss_D: 0.8582 Loss_G: 0.8406 acc: 50.0%\n",
      "[BATCH 138/149] Loss_D: 0.8055 Loss_G: 0.8321 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.8090 Loss_G: 0.8289 acc: 50.0%\n",
      "[BATCH 140/149] Loss_D: 0.8426 Loss_G: 0.8409 acc: 50.0%\n",
      "[BATCH 141/149] Loss_D: 0.8146 Loss_G: 0.8401 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.8318 Loss_G: 0.8336 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.8006 Loss_G: 0.8298 acc: 54.7%\n",
      "[BATCH 144/149] Loss_D: 0.8041 Loss_G: 0.8328 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.7779 Loss_G: 0.8284 acc: 64.1%\n",
      "[BATCH 146/149] Loss_D: 0.8630 Loss_G: 0.8352 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.7762 Loss_G: 0.8283 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.8370 Loss_G: 0.8292 acc: 57.8%\n",
      "[BATCH 149/149] Loss_D: 0.8351 Loss_G: 0.8352 acc: 51.6%\n",
      "-----THE [23/50] epoch end-----\n",
      "-----THE [24/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7801 Loss_G: 0.8287 acc: 57.8%\n",
      "[BATCH 2/149] Loss_D: 0.8480 Loss_G: 0.8361 acc: 54.7%\n",
      "[BATCH 3/149] Loss_D: 0.8281 Loss_G: 0.8436 acc: 65.6%\n",
      "[BATCH 4/149] Loss_D: 0.8685 Loss_G: 0.8548 acc: 64.1%\n",
      "[BATCH 5/149] Loss_D: 0.8196 Loss_G: 0.8519 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.8180 Loss_G: 0.8542 acc: 53.1%\n",
      "[BATCH 7/149] Loss_D: 0.8123 Loss_G: 0.8431 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.7977 Loss_G: 0.8417 acc: 68.8%\n",
      "[BATCH 9/149] Loss_D: 0.8099 Loss_G: 0.8408 acc: 56.2%\n",
      "[BATCH 10/149] Loss_D: 0.8234 Loss_G: 0.8481 acc: 59.4%\n",
      "[BATCH 11/149] Loss_D: 0.8101 Loss_G: 0.8391 acc: 59.4%\n",
      "[BATCH 12/149] Loss_D: 0.8272 Loss_G: 0.8423 acc: 48.4%\n",
      "[BATCH 13/149] Loss_D: 0.8106 Loss_G: 0.8395 acc: 60.9%\n",
      "[BATCH 14/149] Loss_D: 0.8269 Loss_G: 0.8450 acc: 50.0%\n",
      "[BATCH 15/149] Loss_D: 0.7963 Loss_G: 0.8447 acc: 59.4%\n",
      "[BATCH 16/149] Loss_D: 0.8454 Loss_G: 0.8485 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.7808 Loss_G: 0.8445 acc: 59.4%\n",
      "[BATCH 18/149] Loss_D: 0.8088 Loss_G: 0.8387 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.8339 Loss_G: 0.8527 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.8349 Loss_G: 0.8527 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8036 Loss_G: 0.8431 acc: 59.4%\n",
      "[BATCH 22/149] Loss_D: 0.7771 Loss_G: 0.8311 acc: 59.4%\n",
      "[BATCH 23/149] Loss_D: 0.8506 Loss_G: 0.8364 acc: 57.8%\n",
      "[EPOCH 3450] TEST ACC is : 67.6%\n",
      "[BATCH 24/149] Loss_D: 0.7942 Loss_G: 0.8358 acc: 59.4%\n",
      "[BATCH 25/149] Loss_D: 0.7743 Loss_G: 0.8256 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.8295 Loss_G: 0.8334 acc: 60.9%\n",
      "[BATCH 27/149] Loss_D: 0.8996 Loss_G: 0.8658 acc: 57.8%\n",
      "[BATCH 28/149] Loss_D: 0.8529 Loss_G: 0.8709 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.8047 Loss_G: 0.8534 acc: 50.0%\n",
      "[BATCH 30/149] Loss_D: 0.7463 Loss_G: 0.8329 acc: 50.0%\n",
      "[BATCH 31/149] Loss_D: 0.7865 Loss_G: 0.8284 acc: 54.7%\n",
      "[BATCH 32/149] Loss_D: 0.7893 Loss_G: 0.8237 acc: 60.9%\n",
      "[BATCH 33/149] Loss_D: 0.7985 Loss_G: 0.8252 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.7943 Loss_G: 0.8307 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7873 Loss_G: 0.8321 acc: 50.0%\n",
      "[BATCH 36/149] Loss_D: 0.8148 Loss_G: 0.8268 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7976 Loss_G: 0.8305 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.8244 Loss_G: 0.8361 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8398 Loss_G: 0.8509 acc: 53.1%\n",
      "[BATCH 40/149] Loss_D: 0.8270 Loss_G: 0.8537 acc: 59.4%\n",
      "[BATCH 41/149] Loss_D: 0.7753 Loss_G: 0.8376 acc: 59.4%\n",
      "[BATCH 42/149] Loss_D: 0.7969 Loss_G: 0.8336 acc: 57.8%\n",
      "[BATCH 43/149] Loss_D: 0.8490 Loss_G: 0.8446 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.8217 Loss_G: 0.8591 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.8628 Loss_G: 0.8632 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8204 Loss_G: 0.8507 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.8033 Loss_G: 0.8401 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.8842 Loss_G: 0.8492 acc: 43.8%\n",
      "[BATCH 49/149] Loss_D: 0.7798 Loss_G: 0.8392 acc: 60.9%\n",
      "[BATCH 50/149] Loss_D: 0.7966 Loss_G: 0.8392 acc: 45.3%\n",
      "[BATCH 51/149] Loss_D: 0.8429 Loss_G: 0.8425 acc: 46.9%\n",
      "[BATCH 52/149] Loss_D: 0.8008 Loss_G: 0.8451 acc: 54.7%\n",
      "[BATCH 53/149] Loss_D: 0.8298 Loss_G: 0.8451 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.7906 Loss_G: 0.8408 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.8255 Loss_G: 0.8432 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.8119 Loss_G: 0.8351 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.8124 Loss_G: 0.8380 acc: 51.6%\n",
      "[BATCH 58/149] Loss_D: 0.7875 Loss_G: 0.8304 acc: 53.1%\n",
      "[BATCH 59/149] Loss_D: 0.8208 Loss_G: 0.8321 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.9220 Loss_G: 0.8601 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.8066 Loss_G: 0.8533 acc: 54.7%\n",
      "[BATCH 62/149] Loss_D: 0.7891 Loss_G: 0.8454 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.8393 Loss_G: 0.8400 acc: 56.2%\n",
      "[BATCH 64/149] Loss_D: 0.8328 Loss_G: 0.8478 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.8295 Loss_G: 0.8540 acc: 73.4%\n",
      "[BATCH 66/149] Loss_D: 0.7959 Loss_G: 0.8459 acc: 54.7%\n",
      "[BATCH 67/149] Loss_D: 0.7969 Loss_G: 0.8358 acc: 62.5%\n",
      "[BATCH 68/149] Loss_D: 0.7870 Loss_G: 0.8264 acc: 54.7%\n",
      "[BATCH 69/149] Loss_D: 0.8367 Loss_G: 0.8336 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.8118 Loss_G: 0.8406 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.8122 Loss_G: 0.8433 acc: 60.9%\n",
      "[BATCH 72/149] Loss_D: 0.8169 Loss_G: 0.8358 acc: 56.2%\n",
      "[BATCH 73/149] Loss_D: 0.8037 Loss_G: 0.8371 acc: 67.2%\n",
      "[EPOCH 3500] TEST ACC is : 66.0%\n",
      "[BATCH 74/149] Loss_D: 0.9325 Loss_G: 0.8698 acc: 51.6%\n",
      "[BATCH 75/149] Loss_D: 0.7845 Loss_G: 0.8563 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.8235 Loss_G: 0.8399 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.7845 Loss_G: 0.8259 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.7803 Loss_G: 0.8248 acc: 57.8%\n",
      "[BATCH 79/149] Loss_D: 0.8438 Loss_G: 0.8352 acc: 54.7%\n",
      "[BATCH 80/149] Loss_D: 0.8109 Loss_G: 0.8469 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.8188 Loss_G: 0.8431 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.8587 Loss_G: 0.8546 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.8178 Loss_G: 0.8522 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.8227 Loss_G: 0.8466 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.7996 Loss_G: 0.8418 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.8352 Loss_G: 0.8421 acc: 57.8%\n",
      "[BATCH 87/149] Loss_D: 0.8125 Loss_G: 0.8578 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.9043 Loss_G: 0.8687 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.8032 Loss_G: 0.8645 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.8285 Loss_G: 0.8524 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.8550 Loss_G: 0.8579 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.7749 Loss_G: 0.8466 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.7962 Loss_G: 0.8422 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 0.7700 Loss_G: 0.8361 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.8578 Loss_G: 0.8450 acc: 53.1%\n",
      "[BATCH 96/149] Loss_D: 0.7830 Loss_G: 0.8394 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.8529 Loss_G: 0.8478 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.8409 Loss_G: 0.8699 acc: 71.9%\n",
      "[BATCH 99/149] Loss_D: 0.7670 Loss_G: 0.8388 acc: 51.6%\n",
      "[BATCH 100/149] Loss_D: 0.7806 Loss_G: 0.8324 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.8262 Loss_G: 0.8294 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.8164 Loss_G: 0.8301 acc: 54.7%\n",
      "[BATCH 103/149] Loss_D: 0.9017 Loss_G: 0.8558 acc: 70.3%\n",
      "[BATCH 104/149] Loss_D: 0.8543 Loss_G: 0.8639 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.8313 Loss_G: 0.8551 acc: 46.9%\n",
      "[BATCH 106/149] Loss_D: 0.8595 Loss_G: 0.8475 acc: 54.7%\n",
      "[BATCH 107/149] Loss_D: 0.8028 Loss_G: 0.8387 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.7785 Loss_G: 0.8286 acc: 56.2%\n",
      "[BATCH 109/149] Loss_D: 0.8912 Loss_G: 0.8393 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.8200 Loss_G: 0.8474 acc: 57.8%\n",
      "[BATCH 111/149] Loss_D: 0.8291 Loss_G: 0.8400 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8011 Loss_G: 0.8376 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.7941 Loss_G: 0.8333 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7804 Loss_G: 0.8326 acc: 60.9%\n",
      "[BATCH 115/149] Loss_D: 0.7682 Loss_G: 0.8313 acc: 59.4%\n",
      "[BATCH 116/149] Loss_D: 0.8260 Loss_G: 0.8373 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.8647 Loss_G: 0.8484 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.8148 Loss_G: 0.8559 acc: 51.6%\n",
      "[BATCH 119/149] Loss_D: 0.8453 Loss_G: 0.8546 acc: 56.2%\n",
      "[BATCH 120/149] Loss_D: 0.7956 Loss_G: 0.8428 acc: 54.7%\n",
      "[BATCH 121/149] Loss_D: 0.8617 Loss_G: 0.8524 acc: 51.6%\n",
      "[BATCH 122/149] Loss_D: 0.7921 Loss_G: 0.8473 acc: 57.8%\n",
      "[BATCH 123/149] Loss_D: 0.8329 Loss_G: 0.8543 acc: 54.7%\n",
      "[EPOCH 3550] TEST ACC is : 64.3%\n",
      "[BATCH 124/149] Loss_D: 0.8137 Loss_G: 0.8486 acc: 50.0%\n",
      "[BATCH 125/149] Loss_D: 0.7740 Loss_G: 0.8425 acc: 60.9%\n",
      "[BATCH 126/149] Loss_D: 0.8111 Loss_G: 0.8422 acc: 57.8%\n",
      "[BATCH 127/149] Loss_D: 0.8398 Loss_G: 0.8458 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.8512 Loss_G: 0.8560 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.8110 Loss_G: 0.8541 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.8261 Loss_G: 0.8447 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.8360 Loss_G: 0.8364 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.8375 Loss_G: 0.8387 acc: 64.1%\n",
      "[BATCH 133/149] Loss_D: 0.8305 Loss_G: 0.8421 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.8684 Loss_G: 0.8621 acc: 56.2%\n",
      "[BATCH 135/149] Loss_D: 0.8234 Loss_G: 0.8681 acc: 67.2%\n",
      "[BATCH 136/149] Loss_D: 0.8325 Loss_G: 0.8499 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.7979 Loss_G: 0.8374 acc: 67.2%\n",
      "[BATCH 138/149] Loss_D: 0.7848 Loss_G: 0.8283 acc: 50.0%\n",
      "[BATCH 139/149] Loss_D: 0.7990 Loss_G: 0.8270 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.8084 Loss_G: 0.8326 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.7827 Loss_G: 0.8277 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.7800 Loss_G: 0.8213 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.8623 Loss_G: 0.8394 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.8636 Loss_G: 0.8519 acc: 53.1%\n",
      "[BATCH 145/149] Loss_D: 0.8559 Loss_G: 0.8445 acc: 53.1%\n",
      "[BATCH 146/149] Loss_D: 0.7990 Loss_G: 0.8354 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.8579 Loss_G: 0.8392 acc: 56.2%\n",
      "[BATCH 148/149] Loss_D: 0.8142 Loss_G: 0.8455 acc: 50.0%\n",
      "[BATCH 149/149] Loss_D: 0.7972 Loss_G: 0.8394 acc: 60.9%\n",
      "-----THE [24/50] epoch end-----\n",
      "-----THE [25/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8031 Loss_G: 0.8339 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.7919 Loss_G: 0.8251 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.8394 Loss_G: 0.8299 acc: 71.9%\n",
      "[BATCH 4/149] Loss_D: 0.7622 Loss_G: 0.8290 acc: 62.5%\n",
      "[BATCH 5/149] Loss_D: 0.8244 Loss_G: 0.8305 acc: 62.5%\n",
      "[BATCH 6/149] Loss_D: 0.7889 Loss_G: 0.8292 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8072 Loss_G: 0.8320 acc: 57.8%\n",
      "[BATCH 8/149] Loss_D: 0.8092 Loss_G: 0.8377 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.8087 Loss_G: 0.8388 acc: 50.0%\n",
      "[BATCH 10/149] Loss_D: 0.7683 Loss_G: 0.8292 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.7805 Loss_G: 0.8275 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.9436 Loss_G: 0.8570 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.8115 Loss_G: 0.8673 acc: 50.0%\n",
      "[BATCH 14/149] Loss_D: 0.8069 Loss_G: 0.8464 acc: 59.4%\n",
      "[BATCH 15/149] Loss_D: 0.8449 Loss_G: 0.8390 acc: 50.0%\n",
      "[BATCH 16/149] Loss_D: 0.8043 Loss_G: 0.8343 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.8154 Loss_G: 0.8312 acc: 48.4%\n",
      "[BATCH 18/149] Loss_D: 0.7855 Loss_G: 0.8265 acc: 53.1%\n",
      "[BATCH 19/149] Loss_D: 0.7914 Loss_G: 0.8234 acc: 57.8%\n",
      "[BATCH 20/149] Loss_D: 0.7721 Loss_G: 0.8260 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8520 Loss_G: 0.8373 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.8116 Loss_G: 0.8503 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.8080 Loss_G: 0.8331 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.8393 Loss_G: 0.8391 acc: 68.8%\n",
      "[EPOCH 3600] TEST ACC is : 67.6%\n",
      "[BATCH 25/149] Loss_D: 0.8281 Loss_G: 0.8434 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.8778 Loss_G: 0.8546 acc: 51.6%\n",
      "[BATCH 27/149] Loss_D: 0.7869 Loss_G: 0.8460 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.8265 Loss_G: 0.8422 acc: 54.7%\n",
      "[BATCH 29/149] Loss_D: 0.7381 Loss_G: 0.8294 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.8306 Loss_G: 0.8408 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.8174 Loss_G: 0.8429 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.8333 Loss_G: 0.8454 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.7738 Loss_G: 0.8444 acc: 56.2%\n",
      "[BATCH 34/149] Loss_D: 0.8240 Loss_G: 0.8377 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.8148 Loss_G: 0.8501 acc: 57.8%\n",
      "[BATCH 36/149] Loss_D: 0.8027 Loss_G: 0.8369 acc: 68.8%\n",
      "[BATCH 37/149] Loss_D: 0.8380 Loss_G: 0.8454 acc: 56.2%\n",
      "[BATCH 38/149] Loss_D: 0.8184 Loss_G: 0.8546 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.7935 Loss_G: 0.8280 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.8559 Loss_G: 0.8349 acc: 62.5%\n",
      "[BATCH 41/149] Loss_D: 0.8519 Loss_G: 0.8455 acc: 59.4%\n",
      "[BATCH 42/149] Loss_D: 0.7951 Loss_G: 0.8435 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.8135 Loss_G: 0.8401 acc: 57.8%\n",
      "[BATCH 44/149] Loss_D: 0.8420 Loss_G: 0.8458 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.8143 Loss_G: 0.8316 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.8468 Loss_G: 0.8443 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.8200 Loss_G: 0.8436 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.8286 Loss_G: 0.8464 acc: 53.1%\n",
      "[BATCH 49/149] Loss_D: 0.7759 Loss_G: 0.8417 acc: 54.7%\n",
      "[BATCH 50/149] Loss_D: 0.8437 Loss_G: 0.8497 acc: 60.9%\n",
      "[BATCH 51/149] Loss_D: 0.7890 Loss_G: 0.8479 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.8187 Loss_G: 0.8483 acc: 59.4%\n",
      "[BATCH 53/149] Loss_D: 0.8337 Loss_G: 0.8501 acc: 50.0%\n",
      "[BATCH 54/149] Loss_D: 0.8498 Loss_G: 0.8540 acc: 73.4%\n",
      "[BATCH 55/149] Loss_D: 0.9196 Loss_G: 0.8780 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.8097 Loss_G: 0.8687 acc: 53.1%\n",
      "[BATCH 57/149] Loss_D: 0.7851 Loss_G: 0.8429 acc: 56.2%\n",
      "[BATCH 58/149] Loss_D: 0.8041 Loss_G: 0.8268 acc: 57.8%\n",
      "[BATCH 59/149] Loss_D: 0.7885 Loss_G: 0.8211 acc: 54.7%\n",
      "[BATCH 60/149] Loss_D: 0.8195 Loss_G: 0.8230 acc: 68.8%\n",
      "[BATCH 61/149] Loss_D: 0.7853 Loss_G: 0.8226 acc: 53.1%\n",
      "[BATCH 62/149] Loss_D: 0.8502 Loss_G: 0.8270 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.8254 Loss_G: 0.8391 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.7855 Loss_G: 0.8315 acc: 46.9%\n",
      "[BATCH 65/149] Loss_D: 0.8705 Loss_G: 0.8407 acc: 56.2%\n",
      "[BATCH 66/149] Loss_D: 0.8799 Loss_G: 0.8491 acc: 45.3%\n",
      "[BATCH 67/149] Loss_D: 0.8493 Loss_G: 0.8519 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7909 Loss_G: 0.8447 acc: 53.1%\n",
      "[BATCH 69/149] Loss_D: 0.8068 Loss_G: 0.8369 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.8048 Loss_G: 0.8320 acc: 56.2%\n",
      "[BATCH 71/149] Loss_D: 0.8405 Loss_G: 0.8357 acc: 51.6%\n",
      "[BATCH 72/149] Loss_D: 0.8641 Loss_G: 0.8497 acc: 48.4%\n",
      "[BATCH 73/149] Loss_D: 0.8040 Loss_G: 0.8434 acc: 54.7%\n",
      "[BATCH 74/149] Loss_D: 0.8133 Loss_G: 0.8382 acc: 43.8%\n",
      "[EPOCH 3650] TEST ACC is : 62.1%\n",
      "[BATCH 75/149] Loss_D: 0.8689 Loss_G: 0.8432 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.8020 Loss_G: 0.8330 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.7951 Loss_G: 0.8179 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.8022 Loss_G: 0.8151 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.8254 Loss_G: 0.8185 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7614 Loss_G: 0.8218 acc: 53.1%\n",
      "[BATCH 81/149] Loss_D: 0.8366 Loss_G: 0.8366 acc: 67.2%\n",
      "[BATCH 82/149] Loss_D: 0.8022 Loss_G: 0.8332 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.7737 Loss_G: 0.8255 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 0.8372 Loss_G: 0.8316 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.7798 Loss_G: 0.8294 acc: 59.4%\n",
      "[BATCH 86/149] Loss_D: 0.8226 Loss_G: 0.8406 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.8590 Loss_G: 0.8519 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7915 Loss_G: 0.8424 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.9207 Loss_G: 0.8569 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.7996 Loss_G: 0.8497 acc: 51.6%\n",
      "[BATCH 91/149] Loss_D: 0.8046 Loss_G: 0.8367 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.7958 Loss_G: 0.8379 acc: 51.6%\n",
      "[BATCH 93/149] Loss_D: 0.8203 Loss_G: 0.8366 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.7804 Loss_G: 0.8542 acc: 57.8%\n",
      "[BATCH 95/149] Loss_D: 0.8798 Loss_G: 0.8712 acc: 54.7%\n",
      "[BATCH 96/149] Loss_D: 0.7958 Loss_G: 0.8555 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.8136 Loss_G: 0.8404 acc: 62.5%\n",
      "[BATCH 98/149] Loss_D: 0.7823 Loss_G: 0.8320 acc: 54.7%\n",
      "[BATCH 99/149] Loss_D: 0.7925 Loss_G: 0.8274 acc: 59.4%\n",
      "[BATCH 100/149] Loss_D: 0.8921 Loss_G: 0.8398 acc: 53.1%\n",
      "[BATCH 101/149] Loss_D: 0.7497 Loss_G: 0.8298 acc: 57.8%\n",
      "[BATCH 102/149] Loss_D: 0.7715 Loss_G: 0.8270 acc: 59.4%\n",
      "[BATCH 103/149] Loss_D: 0.8265 Loss_G: 0.8228 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.7801 Loss_G: 0.8210 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.7985 Loss_G: 0.8256 acc: 56.2%\n",
      "[BATCH 106/149] Loss_D: 0.8589 Loss_G: 0.8386 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.8143 Loss_G: 0.8412 acc: 62.5%\n",
      "[BATCH 108/149] Loss_D: 0.7824 Loss_G: 0.8357 acc: 43.8%\n",
      "[BATCH 109/149] Loss_D: 0.8672 Loss_G: 0.8503 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.8164 Loss_G: 0.8450 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.8628 Loss_G: 0.8575 acc: 65.6%\n",
      "[BATCH 112/149] Loss_D: 0.8345 Loss_G: 0.8622 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.8217 Loss_G: 0.8540 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.8449 Loss_G: 0.8509 acc: 60.9%\n",
      "[BATCH 115/149] Loss_D: 0.8434 Loss_G: 0.8551 acc: 62.5%\n",
      "[BATCH 116/149] Loss_D: 0.8240 Loss_G: 0.8621 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.8089 Loss_G: 0.8441 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7960 Loss_G: 0.8414 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.8118 Loss_G: 0.8389 acc: 56.2%\n",
      "[BATCH 120/149] Loss_D: 0.8797 Loss_G: 0.8538 acc: 57.8%\n",
      "[BATCH 121/149] Loss_D: 0.8034 Loss_G: 0.8561 acc: 46.9%\n",
      "[BATCH 122/149] Loss_D: 0.8796 Loss_G: 0.8546 acc: 56.2%\n",
      "[BATCH 123/149] Loss_D: 0.8219 Loss_G: 0.8401 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.8345 Loss_G: 0.8353 acc: 51.6%\n",
      "[EPOCH 3700] TEST ACC is : 68.0%\n",
      "[BATCH 125/149] Loss_D: 0.8078 Loss_G: 0.8265 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.9008 Loss_G: 0.8424 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.8170 Loss_G: 0.8394 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7852 Loss_G: 0.8346 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.8071 Loss_G: 0.8282 acc: 62.5%\n",
      "[BATCH 130/149] Loss_D: 0.8024 Loss_G: 0.8254 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.8355 Loss_G: 0.8292 acc: 57.8%\n",
      "[BATCH 132/149] Loss_D: 0.8161 Loss_G: 0.8327 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.8280 Loss_G: 0.8398 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.7834 Loss_G: 0.8358 acc: 73.4%\n",
      "[BATCH 135/149] Loss_D: 0.8384 Loss_G: 0.8375 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.8349 Loss_G: 0.8420 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7611 Loss_G: 0.8357 acc: 54.7%\n",
      "[BATCH 138/149] Loss_D: 0.8299 Loss_G: 0.8343 acc: 67.2%\n",
      "[BATCH 139/149] Loss_D: 0.8041 Loss_G: 0.8330 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.8688 Loss_G: 0.8456 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.8153 Loss_G: 0.8466 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.7802 Loss_G: 0.8363 acc: 56.2%\n",
      "[BATCH 143/149] Loss_D: 0.8052 Loss_G: 0.8294 acc: 54.7%\n",
      "[BATCH 144/149] Loss_D: 0.7766 Loss_G: 0.8241 acc: 62.5%\n",
      "[BATCH 145/149] Loss_D: 0.8397 Loss_G: 0.8249 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7837 Loss_G: 0.8317 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.8181 Loss_G: 0.8341 acc: 59.4%\n",
      "[BATCH 148/149] Loss_D: 0.7885 Loss_G: 0.8308 acc: 50.0%\n",
      "[BATCH 149/149] Loss_D: 0.8072 Loss_G: 0.8299 acc: 60.9%\n",
      "-----THE [25/50] epoch end-----\n",
      "-----THE [26/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8327 Loss_G: 0.8360 acc: 56.2%\n",
      "[BATCH 2/149] Loss_D: 0.7922 Loss_G: 0.8354 acc: 57.8%\n",
      "[BATCH 3/149] Loss_D: 0.7717 Loss_G: 0.8196 acc: 56.2%\n",
      "[BATCH 4/149] Loss_D: 0.8118 Loss_G: 0.8240 acc: 59.4%\n",
      "[BATCH 5/149] Loss_D: 0.7934 Loss_G: 0.8242 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7972 Loss_G: 0.8252 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8468 Loss_G: 0.8423 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.8181 Loss_G: 0.8492 acc: 57.8%\n",
      "[BATCH 9/149] Loss_D: 0.7696 Loss_G: 0.8388 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.8290 Loss_G: 0.8410 acc: 57.8%\n",
      "[BATCH 11/149] Loss_D: 0.8542 Loss_G: 0.8556 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.8762 Loss_G: 0.8776 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.8348 Loss_G: 0.8735 acc: 64.1%\n",
      "[BATCH 14/149] Loss_D: 0.8129 Loss_G: 0.8627 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.7967 Loss_G: 0.8443 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.8807 Loss_G: 0.8473 acc: 45.3%\n",
      "[BATCH 17/149] Loss_D: 0.8303 Loss_G: 0.8471 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.8098 Loss_G: 0.8350 acc: 53.1%\n",
      "[BATCH 19/149] Loss_D: 0.8257 Loss_G: 0.8369 acc: 56.2%\n",
      "[BATCH 20/149] Loss_D: 0.8144 Loss_G: 0.8347 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.7755 Loss_G: 0.8310 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.8729 Loss_G: 0.8482 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.8185 Loss_G: 0.8452 acc: 71.9%\n",
      "[BATCH 24/149] Loss_D: 0.8314 Loss_G: 0.8447 acc: 67.2%\n",
      "[BATCH 25/149] Loss_D: 0.7947 Loss_G: 0.8340 acc: 65.6%\n",
      "[EPOCH 3750] TEST ACC is : 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.8106 Loss_G: 0.8317 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.8172 Loss_G: 0.8386 acc: 51.6%\n",
      "[BATCH 28/149] Loss_D: 0.8468 Loss_G: 0.8518 acc: 76.6%\n",
      "[BATCH 29/149] Loss_D: 0.7829 Loss_G: 0.8464 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.8148 Loss_G: 0.8415 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.8995 Loss_G: 0.8654 acc: 53.1%\n",
      "[BATCH 32/149] Loss_D: 0.7966 Loss_G: 0.8394 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.8203 Loss_G: 0.8382 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.7940 Loss_G: 0.8323 acc: 62.5%\n",
      "[BATCH 35/149] Loss_D: 0.8408 Loss_G: 0.8391 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.8168 Loss_G: 0.8427 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.7897 Loss_G: 0.8383 acc: 62.5%\n",
      "[BATCH 38/149] Loss_D: 0.7925 Loss_G: 0.8329 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.8476 Loss_G: 0.8400 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.8360 Loss_G: 0.8570 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.8500 Loss_G: 0.8418 acc: 59.4%\n",
      "[BATCH 42/149] Loss_D: 0.7732 Loss_G: 0.8374 acc: 53.1%\n",
      "[BATCH 43/149] Loss_D: 0.8066 Loss_G: 0.8388 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.8199 Loss_G: 0.8435 acc: 59.4%\n",
      "[BATCH 45/149] Loss_D: 0.8297 Loss_G: 0.8402 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.7950 Loss_G: 0.8382 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7740 Loss_G: 0.8333 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.8623 Loss_G: 0.8503 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.8988 Loss_G: 0.8650 acc: 65.6%\n",
      "[BATCH 50/149] Loss_D: 0.8141 Loss_G: 0.8629 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7974 Loss_G: 0.8457 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.7711 Loss_G: 0.8315 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.8284 Loss_G: 0.8252 acc: 56.2%\n",
      "[BATCH 54/149] Loss_D: 0.7963 Loss_G: 0.8221 acc: 54.7%\n",
      "[BATCH 55/149] Loss_D: 0.8014 Loss_G: 0.8267 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.8154 Loss_G: 0.8278 acc: 53.1%\n",
      "[BATCH 57/149] Loss_D: 0.8339 Loss_G: 0.8277 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.8281 Loss_G: 0.8344 acc: 57.8%\n",
      "[BATCH 59/149] Loss_D: 0.8476 Loss_G: 0.8440 acc: 56.2%\n",
      "[BATCH 60/149] Loss_D: 0.8214 Loss_G: 0.8427 acc: 54.7%\n",
      "[BATCH 61/149] Loss_D: 0.8226 Loss_G: 0.8487 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.8068 Loss_G: 0.8451 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.8558 Loss_G: 0.8480 acc: 48.4%\n",
      "[BATCH 64/149] Loss_D: 0.7727 Loss_G: 0.8394 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.7849 Loss_G: 0.8312 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.8154 Loss_G: 0.8334 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7822 Loss_G: 0.8305 acc: 62.5%\n",
      "[BATCH 68/149] Loss_D: 0.8419 Loss_G: 0.8397 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.7929 Loss_G: 0.8341 acc: 54.7%\n",
      "[BATCH 70/149] Loss_D: 0.8310 Loss_G: 0.8284 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.8297 Loss_G: 0.8331 acc: 60.9%\n",
      "[BATCH 72/149] Loss_D: 0.7929 Loss_G: 0.8347 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7656 Loss_G: 0.8326 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.8124 Loss_G: 0.8298 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.8473 Loss_G: 0.8369 acc: 65.6%\n",
      "[EPOCH 3800] TEST ACC is : 68.0%\n",
      "[BATCH 76/149] Loss_D: 0.8166 Loss_G: 0.8499 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.8235 Loss_G: 0.8497 acc: 50.0%\n",
      "[BATCH 78/149] Loss_D: 0.8300 Loss_G: 0.8493 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.7936 Loss_G: 0.8468 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.8705 Loss_G: 0.8560 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7907 Loss_G: 0.8366 acc: 46.9%\n",
      "[BATCH 82/149] Loss_D: 0.8149 Loss_G: 0.8337 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8449 Loss_G: 0.8347 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.8381 Loss_G: 0.8450 acc: 68.8%\n",
      "[BATCH 85/149] Loss_D: 0.7968 Loss_G: 0.8419 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.8119 Loss_G: 0.8358 acc: 57.8%\n",
      "[BATCH 87/149] Loss_D: 0.8667 Loss_G: 0.8467 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7649 Loss_G: 0.8348 acc: 62.5%\n",
      "[BATCH 89/149] Loss_D: 0.7846 Loss_G: 0.8234 acc: 65.6%\n",
      "[BATCH 90/149] Loss_D: 0.8577 Loss_G: 0.8311 acc: 51.6%\n",
      "[BATCH 91/149] Loss_D: 0.8383 Loss_G: 0.8428 acc: 73.4%\n",
      "[BATCH 92/149] Loss_D: 0.8101 Loss_G: 0.8407 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.8105 Loss_G: 0.8366 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 0.8751 Loss_G: 0.8474 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.8227 Loss_G: 0.8493 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.8303 Loss_G: 0.8352 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.8305 Loss_G: 0.8360 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.7934 Loss_G: 0.8338 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.7646 Loss_G: 0.8316 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.7919 Loss_G: 0.8264 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.8321 Loss_G: 0.8277 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.8263 Loss_G: 0.8378 acc: 56.2%\n",
      "[BATCH 103/149] Loss_D: 0.7998 Loss_G: 0.8393 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.8207 Loss_G: 0.8335 acc: 51.6%\n",
      "[BATCH 105/149] Loss_D: 0.7551 Loss_G: 0.8285 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.8212 Loss_G: 0.8284 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.7805 Loss_G: 0.8250 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8103 Loss_G: 0.8306 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.8532 Loss_G: 0.8364 acc: 56.2%\n",
      "[BATCH 110/149] Loss_D: 0.8215 Loss_G: 0.8450 acc: 50.0%\n",
      "[BATCH 111/149] Loss_D: 0.8421 Loss_G: 0.8478 acc: 65.6%\n",
      "[BATCH 112/149] Loss_D: 0.7831 Loss_G: 0.8483 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.8180 Loss_G: 0.8473 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.8156 Loss_G: 0.8428 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.7691 Loss_G: 0.8337 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.8096 Loss_G: 0.8320 acc: 68.8%\n",
      "[BATCH 117/149] Loss_D: 0.8037 Loss_G: 0.8350 acc: 67.2%\n",
      "[BATCH 118/149] Loss_D: 0.7739 Loss_G: 0.8294 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.8028 Loss_G: 0.8330 acc: 57.8%\n",
      "[BATCH 120/149] Loss_D: 0.7987 Loss_G: 0.8329 acc: 51.6%\n",
      "[BATCH 121/149] Loss_D: 0.8051 Loss_G: 0.8335 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8021 Loss_G: 0.8251 acc: 54.7%\n",
      "[BATCH 123/149] Loss_D: 0.8376 Loss_G: 0.8417 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.8375 Loss_G: 0.8522 acc: 53.1%\n",
      "[BATCH 125/149] Loss_D: 0.8401 Loss_G: 0.8517 acc: 57.8%\n",
      "[EPOCH 3850] TEST ACC is : 62.1%\n",
      "[BATCH 126/149] Loss_D: 0.8285 Loss_G: 0.8497 acc: 54.7%\n",
      "[BATCH 127/149] Loss_D: 0.8767 Loss_G: 0.8532 acc: 56.2%\n",
      "[BATCH 128/149] Loss_D: 0.7848 Loss_G: 0.8323 acc: 42.2%\n",
      "[BATCH 129/149] Loss_D: 0.7990 Loss_G: 0.8303 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.8071 Loss_G: 0.8293 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.7830 Loss_G: 0.8284 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.8318 Loss_G: 0.8360 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.7801 Loss_G: 0.8373 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.8087 Loss_G: 0.8309 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.8199 Loss_G: 0.8329 acc: 67.2%\n",
      "[BATCH 136/149] Loss_D: 0.7772 Loss_G: 0.8280 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8277 Loss_G: 0.8256 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7681 Loss_G: 0.8233 acc: 64.1%\n",
      "[BATCH 139/149] Loss_D: 0.8608 Loss_G: 0.8388 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.8589 Loss_G: 0.8569 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.8496 Loss_G: 0.8479 acc: 57.8%\n",
      "[BATCH 142/149] Loss_D: 0.8500 Loss_G: 0.8510 acc: 57.8%\n",
      "[BATCH 143/149] Loss_D: 0.7826 Loss_G: 0.8330 acc: 59.4%\n",
      "[BATCH 144/149] Loss_D: 0.8200 Loss_G: 0.8267 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.7914 Loss_G: 0.8257 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.8581 Loss_G: 0.8378 acc: 51.6%\n",
      "[BATCH 147/149] Loss_D: 0.8101 Loss_G: 0.8386 acc: 59.4%\n",
      "[BATCH 148/149] Loss_D: 0.8363 Loss_G: 0.8370 acc: 53.1%\n",
      "[BATCH 149/149] Loss_D: 0.8330 Loss_G: 0.8383 acc: 62.5%\n",
      "-----THE [26/50] epoch end-----\n",
      "-----THE [27/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8359 Loss_G: 0.8456 acc: 59.4%\n",
      "[BATCH 2/149] Loss_D: 0.7956 Loss_G: 0.8360 acc: 56.2%\n",
      "[BATCH 3/149] Loss_D: 0.8314 Loss_G: 0.8331 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.7965 Loss_G: 0.8343 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.7648 Loss_G: 0.8276 acc: 54.7%\n",
      "[BATCH 6/149] Loss_D: 0.8819 Loss_G: 0.8458 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.7919 Loss_G: 0.8481 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7823 Loss_G: 0.8256 acc: 53.1%\n",
      "[BATCH 9/149] Loss_D: 0.8582 Loss_G: 0.8321 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8248 Loss_G: 0.8356 acc: 48.4%\n",
      "[BATCH 11/149] Loss_D: 0.8123 Loss_G: 0.8357 acc: 67.2%\n",
      "[BATCH 12/149] Loss_D: 0.8272 Loss_G: 0.8403 acc: 62.5%\n",
      "[BATCH 13/149] Loss_D: 0.7895 Loss_G: 0.8334 acc: 60.9%\n",
      "[BATCH 14/149] Loss_D: 0.8125 Loss_G: 0.8259 acc: 56.2%\n",
      "[BATCH 15/149] Loss_D: 0.8121 Loss_G: 0.8287 acc: 54.7%\n",
      "[BATCH 16/149] Loss_D: 0.7949 Loss_G: 0.8326 acc: 59.4%\n",
      "[BATCH 17/149] Loss_D: 0.7806 Loss_G: 0.8284 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.8214 Loss_G: 0.8355 acc: 76.6%\n",
      "[BATCH 19/149] Loss_D: 0.7627 Loss_G: 0.8290 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.8496 Loss_G: 0.8393 acc: 56.2%\n",
      "[BATCH 21/149] Loss_D: 0.8138 Loss_G: 0.8436 acc: 56.2%\n",
      "[BATCH 22/149] Loss_D: 0.8987 Loss_G: 0.8659 acc: 59.4%\n",
      "[BATCH 23/149] Loss_D: 0.7949 Loss_G: 0.8641 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.7951 Loss_G: 0.8413 acc: 56.2%\n",
      "[BATCH 25/149] Loss_D: 0.7607 Loss_G: 0.8244 acc: 50.0%\n",
      "[BATCH 26/149] Loss_D: 0.7892 Loss_G: 0.8198 acc: 65.6%\n",
      "[EPOCH 3900] TEST ACC is : 69.3%\n",
      "[BATCH 27/149] Loss_D: 0.7962 Loss_G: 0.8232 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7686 Loss_G: 0.8222 acc: 64.1%\n",
      "[BATCH 29/149] Loss_D: 0.8544 Loss_G: 0.8338 acc: 57.8%\n",
      "[BATCH 30/149] Loss_D: 0.7917 Loss_G: 0.8373 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.7846 Loss_G: 0.8216 acc: 50.0%\n",
      "[BATCH 32/149] Loss_D: 0.8516 Loss_G: 0.8224 acc: 56.2%\n",
      "[BATCH 33/149] Loss_D: 0.8662 Loss_G: 0.8354 acc: 60.9%\n",
      "[BATCH 34/149] Loss_D: 0.8314 Loss_G: 0.8447 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.8346 Loss_G: 0.8474 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.7809 Loss_G: 0.8362 acc: 56.2%\n",
      "[BATCH 37/149] Loss_D: 0.7985 Loss_G: 0.8244 acc: 53.1%\n",
      "[BATCH 38/149] Loss_D: 0.8763 Loss_G: 0.8357 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8072 Loss_G: 0.8371 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.8351 Loss_G: 0.8406 acc: 57.8%\n",
      "[BATCH 41/149] Loss_D: 0.8459 Loss_G: 0.8419 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.8446 Loss_G: 0.8429 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.8321 Loss_G: 0.8465 acc: 57.8%\n",
      "[BATCH 44/149] Loss_D: 0.8411 Loss_G: 0.8545 acc: 62.5%\n",
      "[BATCH 45/149] Loss_D: 0.7470 Loss_G: 0.8319 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.8088 Loss_G: 0.8278 acc: 59.4%\n",
      "[BATCH 47/149] Loss_D: 0.7913 Loss_G: 0.8240 acc: 54.7%\n",
      "[BATCH 48/149] Loss_D: 0.8374 Loss_G: 0.8362 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.8189 Loss_G: 0.8420 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.8452 Loss_G: 0.8497 acc: 71.9%\n",
      "[BATCH 51/149] Loss_D: 0.8499 Loss_G: 0.8453 acc: 56.2%\n",
      "[BATCH 52/149] Loss_D: 0.7758 Loss_G: 0.8308 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.8349 Loss_G: 0.8358 acc: 73.4%\n",
      "[BATCH 54/149] Loss_D: 0.7834 Loss_G: 0.8333 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.8570 Loss_G: 0.8455 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7856 Loss_G: 0.8491 acc: 56.2%\n",
      "[BATCH 57/149] Loss_D: 0.8039 Loss_G: 0.8477 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.8614 Loss_G: 0.8629 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.8751 Loss_G: 0.8522 acc: 51.6%\n",
      "[BATCH 60/149] Loss_D: 0.8394 Loss_G: 0.8425 acc: 54.7%\n",
      "[BATCH 61/149] Loss_D: 0.7823 Loss_G: 0.8320 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.8243 Loss_G: 0.8336 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.8654 Loss_G: 0.8450 acc: 67.2%\n",
      "[BATCH 64/149] Loss_D: 0.7936 Loss_G: 0.8401 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.8375 Loss_G: 0.8352 acc: 54.7%\n",
      "[BATCH 66/149] Loss_D: 0.7642 Loss_G: 0.8293 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.8158 Loss_G: 0.8250 acc: 51.6%\n",
      "[BATCH 68/149] Loss_D: 0.8498 Loss_G: 0.8372 acc: 59.4%\n",
      "[BATCH 69/149] Loss_D: 0.7828 Loss_G: 0.8325 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.7803 Loss_G: 0.8307 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.8156 Loss_G: 0.8257 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.8775 Loss_G: 0.8594 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.8633 Loss_G: 0.8637 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7796 Loss_G: 0.8488 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.7765 Loss_G: 0.8346 acc: 62.5%\n",
      "[BATCH 76/149] Loss_D: 0.7918 Loss_G: 0.8317 acc: 68.8%\n",
      "[EPOCH 3950] TEST ACC is : 67.8%\n",
      "[BATCH 77/149] Loss_D: 0.8228 Loss_G: 0.8331 acc: 59.4%\n",
      "[BATCH 78/149] Loss_D: 0.7999 Loss_G: 0.8243 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.7748 Loss_G: 0.8237 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.8450 Loss_G: 0.8318 acc: 53.1%\n",
      "[BATCH 81/149] Loss_D: 0.8279 Loss_G: 0.8430 acc: 62.5%\n",
      "[BATCH 82/149] Loss_D: 0.8136 Loss_G: 0.8444 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.8660 Loss_G: 0.8523 acc: 76.6%\n",
      "[BATCH 84/149] Loss_D: 0.7976 Loss_G: 0.8495 acc: 56.2%\n",
      "[BATCH 85/149] Loss_D: 0.8028 Loss_G: 0.8345 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.8185 Loss_G: 0.8246 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.8618 Loss_G: 0.8396 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.8148 Loss_G: 0.8353 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.8199 Loss_G: 0.8318 acc: 65.6%\n",
      "[BATCH 90/149] Loss_D: 0.8128 Loss_G: 0.8349 acc: 71.9%\n",
      "[BATCH 91/149] Loss_D: 0.9100 Loss_G: 0.8564 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.8132 Loss_G: 0.8640 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.8352 Loss_G: 0.8662 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.8344 Loss_G: 0.8566 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.8077 Loss_G: 0.8453 acc: 54.7%\n",
      "[BATCH 96/149] Loss_D: 0.8026 Loss_G: 0.8391 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.8030 Loss_G: 0.8359 acc: 53.1%\n",
      "[BATCH 98/149] Loss_D: 0.8219 Loss_G: 0.8332 acc: 57.8%\n",
      "[BATCH 99/149] Loss_D: 0.7958 Loss_G: 0.8317 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.8381 Loss_G: 0.8427 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.7655 Loss_G: 0.8235 acc: 56.2%\n",
      "[BATCH 102/149] Loss_D: 0.8340 Loss_G: 0.8241 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.8188 Loss_G: 0.8238 acc: 51.6%\n",
      "[BATCH 104/149] Loss_D: 0.7934 Loss_G: 0.8279 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7869 Loss_G: 0.8263 acc: 73.4%\n",
      "[BATCH 106/149] Loss_D: 0.8532 Loss_G: 0.8365 acc: 57.8%\n",
      "[BATCH 107/149] Loss_D: 0.8314 Loss_G: 0.8491 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.8085 Loss_G: 0.8419 acc: 56.2%\n",
      "[BATCH 109/149] Loss_D: 0.7972 Loss_G: 0.8328 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.8072 Loss_G: 0.8303 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.8213 Loss_G: 0.8340 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7880 Loss_G: 0.8368 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7950 Loss_G: 0.8343 acc: 59.4%\n",
      "[BATCH 114/149] Loss_D: 0.7948 Loss_G: 0.8277 acc: 53.1%\n",
      "[BATCH 115/149] Loss_D: 0.8350 Loss_G: 0.8365 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8248 Loss_G: 0.8416 acc: 56.2%\n",
      "[BATCH 117/149] Loss_D: 0.7756 Loss_G: 0.8303 acc: 45.3%\n",
      "[BATCH 118/149] Loss_D: 0.8546 Loss_G: 0.8388 acc: 57.8%\n",
      "[BATCH 119/149] Loss_D: 0.8064 Loss_G: 0.8312 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.8017 Loss_G: 0.8256 acc: 67.2%\n",
      "[BATCH 121/149] Loss_D: 0.8267 Loss_G: 0.8324 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.7884 Loss_G: 0.8260 acc: 73.4%\n",
      "[BATCH 123/149] Loss_D: 0.7986 Loss_G: 0.8276 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.7601 Loss_G: 0.8236 acc: 62.5%\n",
      "[BATCH 125/149] Loss_D: 0.8122 Loss_G: 0.8198 acc: 60.9%\n",
      "[BATCH 126/149] Loss_D: 0.8392 Loss_G: 0.8329 acc: 67.2%\n",
      "[EPOCH 4000] TEST ACC is : 68.6%\n",
      "[BATCH 127/149] Loss_D: 0.7878 Loss_G: 0.8308 acc: 54.7%\n",
      "[BATCH 128/149] Loss_D: 0.8194 Loss_G: 0.8317 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.8140 Loss_G: 0.8317 acc: 68.8%\n",
      "[BATCH 130/149] Loss_D: 0.8329 Loss_G: 0.8405 acc: 43.8%\n",
      "[BATCH 131/149] Loss_D: 0.8468 Loss_G: 0.8416 acc: 48.4%\n",
      "[BATCH 132/149] Loss_D: 0.8046 Loss_G: 0.8323 acc: 59.4%\n",
      "[BATCH 133/149] Loss_D: 0.8229 Loss_G: 0.8402 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.8151 Loss_G: 0.8378 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.8704 Loss_G: 0.8526 acc: 62.5%\n",
      "[BATCH 136/149] Loss_D: 0.8578 Loss_G: 0.8520 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.7899 Loss_G: 0.8396 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.8080 Loss_G: 0.8365 acc: 60.9%\n",
      "[BATCH 139/149] Loss_D: 0.7675 Loss_G: 0.8221 acc: 59.4%\n",
      "[BATCH 140/149] Loss_D: 0.7999 Loss_G: 0.8218 acc: 76.6%\n",
      "[BATCH 141/149] Loss_D: 0.8200 Loss_G: 0.8306 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.7916 Loss_G: 0.8285 acc: 48.4%\n",
      "[BATCH 143/149] Loss_D: 0.8530 Loss_G: 0.8381 acc: 56.2%\n",
      "[BATCH 144/149] Loss_D: 0.7928 Loss_G: 0.8262 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.7891 Loss_G: 0.8256 acc: 53.1%\n",
      "[BATCH 146/149] Loss_D: 0.8187 Loss_G: 0.8283 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7984 Loss_G: 0.8304 acc: 64.1%\n",
      "[BATCH 148/149] Loss_D: 0.8525 Loss_G: 0.8354 acc: 65.6%\n",
      "[BATCH 149/149] Loss_D: 0.7762 Loss_G: 0.8305 acc: 62.5%\n",
      "-----THE [27/50] epoch end-----\n",
      "-----THE [28/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8099 Loss_G: 0.8255 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.7823 Loss_G: 0.8238 acc: 54.7%\n",
      "[BATCH 3/149] Loss_D: 0.7603 Loss_G: 0.8249 acc: 71.9%\n",
      "[BATCH 4/149] Loss_D: 0.8064 Loss_G: 0.8242 acc: 71.9%\n",
      "[BATCH 5/149] Loss_D: 0.7942 Loss_G: 0.8252 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.8029 Loss_G: 0.8307 acc: 57.8%\n",
      "[BATCH 7/149] Loss_D: 0.8144 Loss_G: 0.8389 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.8255 Loss_G: 0.8393 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7810 Loss_G: 0.8320 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7871 Loss_G: 0.8277 acc: 59.4%\n",
      "[BATCH 11/149] Loss_D: 0.7837 Loss_G: 0.8293 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.8715 Loss_G: 0.8487 acc: 60.9%\n",
      "[BATCH 13/149] Loss_D: 0.8487 Loss_G: 0.8789 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.8404 Loss_G: 0.8581 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.8075 Loss_G: 0.8371 acc: 57.8%\n",
      "[BATCH 16/149] Loss_D: 0.8025 Loss_G: 0.8300 acc: 59.4%\n",
      "[BATCH 17/149] Loss_D: 0.8163 Loss_G: 0.8291 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.8319 Loss_G: 0.8392 acc: 59.4%\n",
      "[BATCH 19/149] Loss_D: 0.8243 Loss_G: 0.8355 acc: 53.1%\n",
      "[BATCH 20/149] Loss_D: 0.7981 Loss_G: 0.8312 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.8966 Loss_G: 0.8486 acc: 53.1%\n",
      "[BATCH 22/149] Loss_D: 0.7794 Loss_G: 0.8467 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.7940 Loss_G: 0.8384 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.8169 Loss_G: 0.8353 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.8218 Loss_G: 0.8420 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.7976 Loss_G: 0.8326 acc: 71.9%\n",
      "[BATCH 27/149] Loss_D: 0.8216 Loss_G: 0.8299 acc: 64.1%\n",
      "[EPOCH 4050] TEST ACC is : 67.4%\n",
      "[BATCH 28/149] Loss_D: 0.7934 Loss_G: 0.8253 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.8056 Loss_G: 0.8319 acc: 53.1%\n",
      "[BATCH 30/149] Loss_D: 0.7920 Loss_G: 0.8253 acc: 57.8%\n",
      "[BATCH 31/149] Loss_D: 0.7467 Loss_G: 0.8158 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7823 Loss_G: 0.8199 acc: 54.7%\n",
      "[BATCH 33/149] Loss_D: 0.7970 Loss_G: 0.8233 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.7893 Loss_G: 0.8262 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.8109 Loss_G: 0.8261 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.8061 Loss_G: 0.8263 acc: 59.4%\n",
      "[BATCH 37/149] Loss_D: 0.7972 Loss_G: 0.8252 acc: 60.9%\n",
      "[BATCH 38/149] Loss_D: 0.8406 Loss_G: 0.8396 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.7850 Loss_G: 0.8327 acc: 48.4%\n",
      "[BATCH 40/149] Loss_D: 0.7939 Loss_G: 0.8233 acc: 54.7%\n",
      "[BATCH 41/149] Loss_D: 0.7795 Loss_G: 0.8137 acc: 53.1%\n",
      "[BATCH 42/149] Loss_D: 0.8267 Loss_G: 0.8223 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.8226 Loss_G: 0.8339 acc: 73.4%\n",
      "[BATCH 44/149] Loss_D: 0.7554 Loss_G: 0.8294 acc: 54.7%\n",
      "[BATCH 45/149] Loss_D: 0.8149 Loss_G: 0.8265 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.7866 Loss_G: 0.8279 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.8812 Loss_G: 0.8570 acc: 57.8%\n",
      "[BATCH 48/149] Loss_D: 0.8340 Loss_G: 0.8622 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.8269 Loss_G: 0.8503 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.7939 Loss_G: 0.8448 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.8193 Loss_G: 0.8426 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.8001 Loss_G: 0.8415 acc: 62.5%\n",
      "[BATCH 53/149] Loss_D: 0.8449 Loss_G: 0.8523 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.8652 Loss_G: 0.8556 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.7943 Loss_G: 0.8555 acc: 57.8%\n",
      "[BATCH 56/149] Loss_D: 0.7782 Loss_G: 0.8348 acc: 53.1%\n",
      "[BATCH 57/149] Loss_D: 0.7904 Loss_G: 0.8250 acc: 56.2%\n",
      "[BATCH 58/149] Loss_D: 0.8278 Loss_G: 0.8330 acc: 57.8%\n",
      "[BATCH 59/149] Loss_D: 0.8767 Loss_G: 0.8508 acc: 73.4%\n",
      "[BATCH 60/149] Loss_D: 0.8083 Loss_G: 0.8480 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7882 Loss_G: 0.8290 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.8191 Loss_G: 0.8301 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.8101 Loss_G: 0.8231 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.8106 Loss_G: 0.8270 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.8474 Loss_G: 0.8390 acc: 67.2%\n",
      "[BATCH 66/149] Loss_D: 0.7637 Loss_G: 0.8270 acc: 59.4%\n",
      "[BATCH 67/149] Loss_D: 0.8900 Loss_G: 0.8375 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.8177 Loss_G: 0.8419 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.8139 Loss_G: 0.8400 acc: 68.8%\n",
      "[BATCH 70/149] Loss_D: 0.8138 Loss_G: 0.8354 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.7744 Loss_G: 0.8229 acc: 54.7%\n",
      "[BATCH 72/149] Loss_D: 0.8281 Loss_G: 0.8244 acc: 65.6%\n",
      "[BATCH 73/149] Loss_D: 0.7731 Loss_G: 0.8217 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.8233 Loss_G: 0.8245 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.7817 Loss_G: 0.8246 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.8924 Loss_G: 0.8436 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.8475 Loss_G: 0.8492 acc: 62.5%\n",
      "[EPOCH 4100] TEST ACC is : 63.5%\n",
      "[BATCH 78/149] Loss_D: 0.9156 Loss_G: 0.8694 acc: 51.6%\n",
      "[BATCH 79/149] Loss_D: 0.8400 Loss_G: 0.8556 acc: 51.6%\n",
      "[BATCH 80/149] Loss_D: 0.8111 Loss_G: 0.8411 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.8224 Loss_G: 0.8347 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.8661 Loss_G: 0.8423 acc: 56.2%\n",
      "[BATCH 83/149] Loss_D: 0.7685 Loss_G: 0.8302 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 0.8173 Loss_G: 0.8247 acc: 70.3%\n",
      "[BATCH 85/149] Loss_D: 0.8133 Loss_G: 0.8210 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.8205 Loss_G: 0.8260 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.8436 Loss_G: 0.8339 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 0.7559 Loss_G: 0.8262 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.7659 Loss_G: 0.8149 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.8106 Loss_G: 0.8176 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7648 Loss_G: 0.8208 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.8130 Loss_G: 0.8278 acc: 54.7%\n",
      "[BATCH 93/149] Loss_D: 0.7855 Loss_G: 0.8335 acc: 71.9%\n",
      "[BATCH 94/149] Loss_D: 0.8613 Loss_G: 0.8470 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.7868 Loss_G: 0.8444 acc: 60.9%\n",
      "[BATCH 96/149] Loss_D: 0.8073 Loss_G: 0.8267 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.8126 Loss_G: 0.8238 acc: 53.1%\n",
      "[BATCH 98/149] Loss_D: 0.7442 Loss_G: 0.8187 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.8440 Loss_G: 0.8274 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.8010 Loss_G: 0.8323 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.8033 Loss_G: 0.8310 acc: 54.7%\n",
      "[BATCH 102/149] Loss_D: 0.8813 Loss_G: 0.8515 acc: 59.4%\n",
      "[BATCH 103/149] Loss_D: 0.8060 Loss_G: 0.8491 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.8477 Loss_G: 0.8480 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.8305 Loss_G: 0.8426 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.8345 Loss_G: 0.8450 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.8886 Loss_G: 0.8607 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.8310 Loss_G: 0.8678 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.8292 Loss_G: 0.8481 acc: 43.8%\n",
      "[BATCH 110/149] Loss_D: 0.8395 Loss_G: 0.8531 acc: 57.8%\n",
      "[BATCH 111/149] Loss_D: 0.8270 Loss_G: 0.8543 acc: 56.2%\n",
      "[BATCH 112/149] Loss_D: 0.8158 Loss_G: 0.8495 acc: 54.7%\n",
      "[BATCH 113/149] Loss_D: 0.8918 Loss_G: 0.8612 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.7995 Loss_G: 0.8472 acc: 57.8%\n",
      "[BATCH 115/149] Loss_D: 0.8176 Loss_G: 0.8383 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7600 Loss_G: 0.8236 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.8509 Loss_G: 0.8372 acc: 71.9%\n",
      "[BATCH 118/149] Loss_D: 0.8129 Loss_G: 0.8332 acc: 57.8%\n",
      "[BATCH 119/149] Loss_D: 0.7818 Loss_G: 0.8270 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.8602 Loss_G: 0.8405 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.8524 Loss_G: 0.8503 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.8345 Loss_G: 0.8464 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.8270 Loss_G: 0.8420 acc: 56.2%\n",
      "[BATCH 124/149] Loss_D: 0.8218 Loss_G: 0.8453 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.7645 Loss_G: 0.8349 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.8026 Loss_G: 0.8265 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7846 Loss_G: 0.8226 acc: 64.1%\n",
      "[EPOCH 4150] TEST ACC is : 71.3%\n",
      "[BATCH 128/149] Loss_D: 0.7739 Loss_G: 0.8183 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.8510 Loss_G: 0.8323 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7949 Loss_G: 0.8335 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.7959 Loss_G: 0.8414 acc: 51.6%\n",
      "[BATCH 132/149] Loss_D: 0.8613 Loss_G: 0.8300 acc: 54.7%\n",
      "[BATCH 133/149] Loss_D: 0.7593 Loss_G: 0.8219 acc: 57.8%\n",
      "[BATCH 134/149] Loss_D: 0.8491 Loss_G: 0.8282 acc: 57.8%\n",
      "[BATCH 135/149] Loss_D: 0.8578 Loss_G: 0.8483 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.8354 Loss_G: 0.8496 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.8334 Loss_G: 0.8452 acc: 53.1%\n",
      "[BATCH 138/149] Loss_D: 0.7937 Loss_G: 0.8377 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7538 Loss_G: 0.8151 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7807 Loss_G: 0.8120 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.8409 Loss_G: 0.8170 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8374 Loss_G: 0.8329 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.8293 Loss_G: 0.8408 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.8122 Loss_G: 0.8366 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.8219 Loss_G: 0.8402 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.8052 Loss_G: 0.8346 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.7834 Loss_G: 0.8347 acc: 67.2%\n",
      "[BATCH 148/149] Loss_D: 0.8267 Loss_G: 0.8460 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.8112 Loss_G: 0.8437 acc: 54.7%\n",
      "-----THE [28/50] epoch end-----\n",
      "-----THE [29/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7512 Loss_G: 0.8307 acc: 64.1%\n",
      "[BATCH 2/149] Loss_D: 0.8208 Loss_G: 0.8327 acc: 60.9%\n",
      "[BATCH 3/149] Loss_D: 0.7968 Loss_G: 0.8318 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.8002 Loss_G: 0.8363 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.8059 Loss_G: 0.8312 acc: 71.9%\n",
      "[BATCH 6/149] Loss_D: 0.8127 Loss_G: 0.8361 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.7828 Loss_G: 0.8256 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.7993 Loss_G: 0.8309 acc: 59.4%\n",
      "[BATCH 9/149] Loss_D: 0.8176 Loss_G: 0.8350 acc: 60.9%\n",
      "[BATCH 10/149] Loss_D: 0.8183 Loss_G: 0.8448 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.8186 Loss_G: 0.8360 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.7737 Loss_G: 0.8268 acc: 53.1%\n",
      "[BATCH 13/149] Loss_D: 0.8013 Loss_G: 0.8232 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.8342 Loss_G: 0.8392 acc: 64.1%\n",
      "[BATCH 15/149] Loss_D: 0.8053 Loss_G: 0.8343 acc: 59.4%\n",
      "[BATCH 16/149] Loss_D: 0.7957 Loss_G: 0.8308 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.8346 Loss_G: 0.8355 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.7813 Loss_G: 0.8296 acc: 56.2%\n",
      "[BATCH 19/149] Loss_D: 0.8151 Loss_G: 0.8195 acc: 45.3%\n",
      "[BATCH 20/149] Loss_D: 0.8091 Loss_G: 0.8184 acc: 54.7%\n",
      "[BATCH 21/149] Loss_D: 0.8121 Loss_G: 0.8182 acc: 51.6%\n",
      "[BATCH 22/149] Loss_D: 0.8251 Loss_G: 0.8229 acc: 56.2%\n",
      "[BATCH 23/149] Loss_D: 0.7973 Loss_G: 0.8285 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.8246 Loss_G: 0.8315 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.8879 Loss_G: 0.8424 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.8247 Loss_G: 0.8399 acc: 50.0%\n",
      "[BATCH 27/149] Loss_D: 0.8011 Loss_G: 0.8338 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7899 Loss_G: 0.8317 acc: 57.8%\n",
      "[EPOCH 4200] TEST ACC is : 70.7%\n",
      "[BATCH 29/149] Loss_D: 0.8139 Loss_G: 0.8342 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.8287 Loss_G: 0.8422 acc: 73.4%\n",
      "[BATCH 31/149] Loss_D: 0.7891 Loss_G: 0.8346 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.8026 Loss_G: 0.8308 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.8156 Loss_G: 0.8364 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8210 Loss_G: 0.8495 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7801 Loss_G: 0.8281 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.8605 Loss_G: 0.8326 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.7766 Loss_G: 0.8255 acc: 57.8%\n",
      "[BATCH 38/149] Loss_D: 0.8597 Loss_G: 0.8349 acc: 62.5%\n",
      "[BATCH 39/149] Loss_D: 0.8367 Loss_G: 0.8463 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.7839 Loss_G: 0.8361 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.8071 Loss_G: 0.8326 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.7668 Loss_G: 0.8217 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.8355 Loss_G: 0.8314 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.8606 Loss_G: 0.8444 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.8036 Loss_G: 0.8474 acc: 67.2%\n",
      "[BATCH 46/149] Loss_D: 0.8583 Loss_G: 0.8443 acc: 54.7%\n",
      "[BATCH 47/149] Loss_D: 0.7972 Loss_G: 0.8389 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.7980 Loss_G: 0.8337 acc: 57.8%\n",
      "[BATCH 49/149] Loss_D: 0.7713 Loss_G: 0.8266 acc: 54.7%\n",
      "[BATCH 50/149] Loss_D: 0.8104 Loss_G: 0.8296 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7643 Loss_G: 0.8280 acc: 53.1%\n",
      "[BATCH 52/149] Loss_D: 0.8291 Loss_G: 0.8482 acc: 56.2%\n",
      "[BATCH 53/149] Loss_D: 0.8691 Loss_G: 0.8610 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.8744 Loss_G: 0.8529 acc: 54.7%\n",
      "[BATCH 55/149] Loss_D: 0.7958 Loss_G: 0.8388 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.8052 Loss_G: 0.8353 acc: 46.9%\n",
      "[BATCH 57/149] Loss_D: 0.8026 Loss_G: 0.8363 acc: 57.8%\n",
      "[BATCH 58/149] Loss_D: 0.8258 Loss_G: 0.8382 acc: 51.6%\n",
      "[BATCH 59/149] Loss_D: 0.8723 Loss_G: 0.8504 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.8374 Loss_G: 0.8571 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.8367 Loss_G: 0.8476 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.7766 Loss_G: 0.8388 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.8089 Loss_G: 0.8438 acc: 53.1%\n",
      "[BATCH 64/149] Loss_D: 0.8132 Loss_G: 0.8432 acc: 56.2%\n",
      "[BATCH 65/149] Loss_D: 0.7956 Loss_G: 0.8388 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.8270 Loss_G: 0.8384 acc: 59.4%\n",
      "[BATCH 67/149] Loss_D: 0.8535 Loss_G: 0.8506 acc: 62.5%\n",
      "[BATCH 68/149] Loss_D: 0.7984 Loss_G: 0.8390 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.8026 Loss_G: 0.8299 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.7809 Loss_G: 0.8230 acc: 54.7%\n",
      "[BATCH 71/149] Loss_D: 0.8329 Loss_G: 0.8298 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8353 Loss_G: 0.8416 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.8416 Loss_G: 0.8391 acc: 68.8%\n",
      "[BATCH 74/149] Loss_D: 0.8151 Loss_G: 0.8338 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.7792 Loss_G: 0.8221 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.8166 Loss_G: 0.8234 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.8141 Loss_G: 0.8298 acc: 59.4%\n",
      "[BATCH 78/149] Loss_D: 0.8209 Loss_G: 0.8403 acc: 57.8%\n",
      "[EPOCH 4250] TEST ACC is : 59.8%\n",
      "[BATCH 79/149] Loss_D: 0.7975 Loss_G: 0.8355 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.8028 Loss_G: 0.8337 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.7511 Loss_G: 0.8211 acc: 54.7%\n",
      "[BATCH 82/149] Loss_D: 0.8282 Loss_G: 0.8242 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8104 Loss_G: 0.8256 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.8125 Loss_G: 0.8172 acc: 48.4%\n",
      "[BATCH 85/149] Loss_D: 0.8633 Loss_G: 0.8336 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.8384 Loss_G: 0.8352 acc: 56.2%\n",
      "[BATCH 87/149] Loss_D: 0.8636 Loss_G: 0.8486 acc: 54.7%\n",
      "[BATCH 88/149] Loss_D: 0.7871 Loss_G: 0.8427 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.8335 Loss_G: 0.8351 acc: 64.1%\n",
      "[BATCH 90/149] Loss_D: 0.7930 Loss_G: 0.8281 acc: 51.6%\n",
      "[BATCH 91/149] Loss_D: 0.8816 Loss_G: 0.8369 acc: 75.0%\n",
      "[BATCH 92/149] Loss_D: 0.7914 Loss_G: 0.8406 acc: 75.0%\n",
      "[BATCH 93/149] Loss_D: 0.7946 Loss_G: 0.8325 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7939 Loss_G: 0.8337 acc: 57.8%\n",
      "[BATCH 95/149] Loss_D: 0.8132 Loss_G: 0.8296 acc: 50.0%\n",
      "[BATCH 96/149] Loss_D: 0.7788 Loss_G: 0.8180 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.8306 Loss_G: 0.8238 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.7744 Loss_G: 0.8302 acc: 51.6%\n",
      "[BATCH 99/149] Loss_D: 0.8109 Loss_G: 0.8298 acc: 73.4%\n",
      "[BATCH 100/149] Loss_D: 0.8289 Loss_G: 0.8343 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.8135 Loss_G: 0.8344 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.8027 Loss_G: 0.8353 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7751 Loss_G: 0.8210 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7998 Loss_G: 0.8125 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.8179 Loss_G: 0.8175 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.7500 Loss_G: 0.8098 acc: 50.0%\n",
      "[BATCH 107/149] Loss_D: 0.7859 Loss_G: 0.8105 acc: 56.2%\n",
      "[BATCH 108/149] Loss_D: 0.8347 Loss_G: 0.8236 acc: 62.5%\n",
      "[BATCH 109/149] Loss_D: 0.7932 Loss_G: 0.8309 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.8264 Loss_G: 0.8383 acc: 60.9%\n",
      "[BATCH 111/149] Loss_D: 0.9216 Loss_G: 0.8660 acc: 57.8%\n",
      "[BATCH 112/149] Loss_D: 0.7854 Loss_G: 0.8499 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.8369 Loss_G: 0.8395 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7968 Loss_G: 0.8362 acc: 60.9%\n",
      "[BATCH 115/149] Loss_D: 0.8408 Loss_G: 0.8412 acc: 57.8%\n",
      "[BATCH 116/149] Loss_D: 0.7940 Loss_G: 0.8364 acc: 51.6%\n",
      "[BATCH 117/149] Loss_D: 0.7862 Loss_G: 0.8294 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7835 Loss_G: 0.8234 acc: 57.8%\n",
      "[BATCH 119/149] Loss_D: 0.8299 Loss_G: 0.8260 acc: 53.1%\n",
      "[BATCH 120/149] Loss_D: 0.8004 Loss_G: 0.8322 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.8351 Loss_G: 0.8335 acc: 54.7%\n",
      "[BATCH 122/149] Loss_D: 0.7474 Loss_G: 0.8214 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.8339 Loss_G: 0.8216 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.8240 Loss_G: 0.8317 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.8060 Loss_G: 0.8358 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8739 Loss_G: 0.8458 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.7883 Loss_G: 0.8342 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.8139 Loss_G: 0.8266 acc: 67.2%\n",
      "[EPOCH 4300] TEST ACC is : 69.5%\n",
      "[BATCH 129/149] Loss_D: 0.8762 Loss_G: 0.8499 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.8264 Loss_G: 0.8678 acc: 71.9%\n",
      "[BATCH 131/149] Loss_D: 0.8313 Loss_G: 0.8688 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7847 Loss_G: 0.8445 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.7776 Loss_G: 0.8497 acc: 71.9%\n",
      "[BATCH 134/149] Loss_D: 0.8186 Loss_G: 0.8371 acc: 48.4%\n",
      "[BATCH 135/149] Loss_D: 0.7619 Loss_G: 0.8287 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.8296 Loss_G: 0.8255 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.8156 Loss_G: 0.8337 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.8513 Loss_G: 0.8343 acc: 60.9%\n",
      "[BATCH 139/149] Loss_D: 0.8289 Loss_G: 0.8368 acc: 70.3%\n",
      "[BATCH 140/149] Loss_D: 0.8650 Loss_G: 0.8361 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.7379 Loss_G: 0.8193 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.8351 Loss_G: 0.8222 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.8982 Loss_G: 0.8407 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7822 Loss_G: 0.8402 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.8488 Loss_G: 0.8385 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.7958 Loss_G: 0.8299 acc: 70.3%\n",
      "[BATCH 147/149] Loss_D: 0.8549 Loss_G: 0.8361 acc: 64.1%\n",
      "[BATCH 148/149] Loss_D: 0.8224 Loss_G: 0.8353 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7724 Loss_G: 0.8317 acc: 67.2%\n",
      "-----THE [29/50] epoch end-----\n",
      "-----THE [30/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7847 Loss_G: 0.8241 acc: 51.6%\n",
      "[BATCH 2/149] Loss_D: 0.8653 Loss_G: 0.8362 acc: 57.8%\n",
      "[BATCH 3/149] Loss_D: 0.8098 Loss_G: 0.8392 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.7634 Loss_G: 0.8300 acc: 56.2%\n",
      "[BATCH 5/149] Loss_D: 0.7904 Loss_G: 0.8232 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.8077 Loss_G: 0.8258 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.8488 Loss_G: 0.8356 acc: 45.3%\n",
      "[BATCH 8/149] Loss_D: 0.7696 Loss_G: 0.8219 acc: 57.8%\n",
      "[BATCH 9/149] Loss_D: 0.8297 Loss_G: 0.8213 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7500 Loss_G: 0.8147 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.8615 Loss_G: 0.8295 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.8372 Loss_G: 0.8381 acc: 54.7%\n",
      "[BATCH 13/149] Loss_D: 0.7828 Loss_G: 0.8266 acc: 73.4%\n",
      "[BATCH 14/149] Loss_D: 0.8208 Loss_G: 0.8255 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.8437 Loss_G: 0.8337 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7538 Loss_G: 0.8353 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.8261 Loss_G: 0.8323 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.8338 Loss_G: 0.8373 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.8300 Loss_G: 0.8438 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.7920 Loss_G: 0.8380 acc: 54.7%\n",
      "[BATCH 21/149] Loss_D: 0.7918 Loss_G: 0.8367 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.8328 Loss_G: 0.8411 acc: 53.1%\n",
      "[BATCH 23/149] Loss_D: 0.7922 Loss_G: 0.8337 acc: 60.9%\n",
      "[BATCH 24/149] Loss_D: 0.8022 Loss_G: 0.8334 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.7870 Loss_G: 0.8262 acc: 67.2%\n",
      "[BATCH 26/149] Loss_D: 0.8087 Loss_G: 0.8286 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.8137 Loss_G: 0.8324 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.8583 Loss_G: 0.8520 acc: 57.8%\n",
      "[BATCH 29/149] Loss_D: 0.8159 Loss_G: 0.8489 acc: 51.6%\n",
      "[EPOCH 4350] TEST ACC is : 70.5%\n",
      "[BATCH 30/149] Loss_D: 0.7940 Loss_G: 0.8405 acc: 60.9%\n",
      "[BATCH 31/149] Loss_D: 0.8206 Loss_G: 0.8330 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.7973 Loss_G: 0.8335 acc: 71.9%\n",
      "[BATCH 33/149] Loss_D: 0.8212 Loss_G: 0.8432 acc: 54.7%\n",
      "[BATCH 34/149] Loss_D: 0.7810 Loss_G: 0.8390 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.8276 Loss_G: 0.8454 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.8928 Loss_G: 0.8594 acc: 51.6%\n",
      "[BATCH 37/149] Loss_D: 0.8895 Loss_G: 0.8665 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7575 Loss_G: 0.8344 acc: 59.4%\n",
      "[BATCH 39/149] Loss_D: 0.8127 Loss_G: 0.8300 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.9317 Loss_G: 0.8467 acc: 73.4%\n",
      "[BATCH 41/149] Loss_D: 0.7633 Loss_G: 0.8409 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7883 Loss_G: 0.8239 acc: 56.2%\n",
      "[BATCH 43/149] Loss_D: 0.7760 Loss_G: 0.8173 acc: 54.7%\n",
      "[BATCH 44/149] Loss_D: 0.7986 Loss_G: 0.8151 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.8180 Loss_G: 0.8182 acc: 71.9%\n",
      "[BATCH 46/149] Loss_D: 0.8091 Loss_G: 0.8204 acc: 60.9%\n",
      "[BATCH 47/149] Loss_D: 0.8104 Loss_G: 0.8230 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7826 Loss_G: 0.8208 acc: 60.9%\n",
      "[BATCH 49/149] Loss_D: 0.8231 Loss_G: 0.8269 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7768 Loss_G: 0.8238 acc: 60.9%\n",
      "[BATCH 51/149] Loss_D: 0.8295 Loss_G: 0.8225 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.7896 Loss_G: 0.8287 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.8356 Loss_G: 0.8368 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.8279 Loss_G: 0.8281 acc: 57.8%\n",
      "[BATCH 55/149] Loss_D: 0.7667 Loss_G: 0.8244 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.7908 Loss_G: 0.8194 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.8353 Loss_G: 0.8270 acc: 70.3%\n",
      "[BATCH 58/149] Loss_D: 0.7642 Loss_G: 0.8258 acc: 62.5%\n",
      "[BATCH 59/149] Loss_D: 0.8861 Loss_G: 0.8366 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.8655 Loss_G: 0.8509 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.8037 Loss_G: 0.8417 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.7803 Loss_G: 0.8310 acc: 57.8%\n",
      "[BATCH 63/149] Loss_D: 0.7980 Loss_G: 0.8333 acc: 64.1%\n",
      "[BATCH 64/149] Loss_D: 0.8437 Loss_G: 0.8514 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8303 Loss_G: 0.8510 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.8919 Loss_G: 0.8649 acc: 67.2%\n",
      "[BATCH 67/149] Loss_D: 0.8088 Loss_G: 0.8545 acc: 70.3%\n",
      "[BATCH 68/149] Loss_D: 0.7980 Loss_G: 0.8536 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.8178 Loss_G: 0.8397 acc: 53.1%\n",
      "[BATCH 70/149] Loss_D: 0.7755 Loss_G: 0.8310 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.8216 Loss_G: 0.8318 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8104 Loss_G: 0.8290 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.8571 Loss_G: 0.8342 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.8521 Loss_G: 0.8500 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.8055 Loss_G: 0.8413 acc: 48.4%\n",
      "[BATCH 76/149] Loss_D: 0.7918 Loss_G: 0.8343 acc: 62.5%\n",
      "[BATCH 77/149] Loss_D: 0.8051 Loss_G: 0.8279 acc: 56.2%\n",
      "[BATCH 78/149] Loss_D: 0.7927 Loss_G: 0.8291 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.7795 Loss_G: 0.8237 acc: 70.3%\n",
      "[EPOCH 4400] TEST ACC is : 72.7%\n",
      "[BATCH 80/149] Loss_D: 0.7917 Loss_G: 0.8283 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.8184 Loss_G: 0.8313 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.8315 Loss_G: 0.8310 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.7918 Loss_G: 0.8251 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.8430 Loss_G: 0.8330 acc: 59.4%\n",
      "[BATCH 85/149] Loss_D: 0.8300 Loss_G: 0.8342 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.8038 Loss_G: 0.8212 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.8336 Loss_G: 0.8308 acc: 54.7%\n",
      "[BATCH 88/149] Loss_D: 0.7718 Loss_G: 0.8262 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.8032 Loss_G: 0.8244 acc: 57.8%\n",
      "[BATCH 90/149] Loss_D: 0.8302 Loss_G: 0.8338 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.8310 Loss_G: 0.8366 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.7905 Loss_G: 0.8341 acc: 64.1%\n",
      "[BATCH 93/149] Loss_D: 0.8236 Loss_G: 0.8304 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7884 Loss_G: 0.8302 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7756 Loss_G: 0.8283 acc: 70.3%\n",
      "[BATCH 96/149] Loss_D: 0.7725 Loss_G: 0.8202 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7594 Loss_G: 0.8179 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.8662 Loss_G: 0.8296 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.8297 Loss_G: 0.8437 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.7913 Loss_G: 0.8295 acc: 71.9%\n",
      "[BATCH 101/149] Loss_D: 0.7752 Loss_G: 0.8158 acc: 57.8%\n",
      "[BATCH 102/149] Loss_D: 0.8198 Loss_G: 0.8157 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.8292 Loss_G: 0.8371 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.7854 Loss_G: 0.8280 acc: 53.1%\n",
      "[BATCH 105/149] Loss_D: 0.8053 Loss_G: 0.8281 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.7626 Loss_G: 0.8288 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.8232 Loss_G: 0.8375 acc: 59.4%\n",
      "[BATCH 108/149] Loss_D: 0.8012 Loss_G: 0.8335 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.8312 Loss_G: 0.8327 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.8394 Loss_G: 0.8367 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.8444 Loss_G: 0.8291 acc: 53.1%\n",
      "[BATCH 112/149] Loss_D: 0.8174 Loss_G: 0.8267 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.7832 Loss_G: 0.8211 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7588 Loss_G: 0.8314 acc: 51.6%\n",
      "[BATCH 115/149] Loss_D: 0.7472 Loss_G: 0.8128 acc: 59.4%\n",
      "[BATCH 116/149] Loss_D: 0.7863 Loss_G: 0.8124 acc: 68.8%\n",
      "[BATCH 117/149] Loss_D: 0.8265 Loss_G: 0.8204 acc: 53.1%\n",
      "[BATCH 118/149] Loss_D: 0.8259 Loss_G: 0.8332 acc: 59.4%\n",
      "[BATCH 119/149] Loss_D: 0.8151 Loss_G: 0.8354 acc: 53.1%\n",
      "[BATCH 120/149] Loss_D: 0.8495 Loss_G: 0.8486 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.7789 Loss_G: 0.8310 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.8140 Loss_G: 0.8310 acc: 57.8%\n",
      "[BATCH 123/149] Loss_D: 0.7745 Loss_G: 0.8265 acc: 54.7%\n",
      "[BATCH 124/149] Loss_D: 0.8674 Loss_G: 0.8455 acc: 56.2%\n",
      "[BATCH 125/149] Loss_D: 0.7899 Loss_G: 0.8504 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.7889 Loss_G: 0.8384 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.8077 Loss_G: 0.8350 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.7912 Loss_G: 0.8359 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.8268 Loss_G: 0.8376 acc: 60.9%\n",
      "[EPOCH 4450] TEST ACC is : 71.5%\n",
      "[BATCH 130/149] Loss_D: 0.8133 Loss_G: 0.8420 acc: 67.2%\n",
      "[BATCH 131/149] Loss_D: 0.8065 Loss_G: 0.8421 acc: 59.4%\n",
      "[BATCH 132/149] Loss_D: 0.8581 Loss_G: 0.8540 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.8182 Loss_G: 0.8476 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.8174 Loss_G: 0.8457 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.8438 Loss_G: 0.8530 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.7913 Loss_G: 0.8363 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.8567 Loss_G: 0.8348 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.8558 Loss_G: 0.8518 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.8067 Loss_G: 0.8483 acc: 59.4%\n",
      "[BATCH 140/149] Loss_D: 0.8284 Loss_G: 0.8377 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.8544 Loss_G: 0.8428 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.8107 Loss_G: 0.8353 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.8410 Loss_G: 0.8431 acc: 71.9%\n",
      "[BATCH 144/149] Loss_D: 0.8455 Loss_G: 0.8506 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.8090 Loss_G: 0.8413 acc: 71.9%\n",
      "[BATCH 146/149] Loss_D: 0.8018 Loss_G: 0.8328 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7596 Loss_G: 0.8221 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.8512 Loss_G: 0.8279 acc: 60.9%\n",
      "[BATCH 149/149] Loss_D: 0.8355 Loss_G: 0.8391 acc: 54.7%\n",
      "-----THE [30/50] epoch end-----\n",
      "-----THE [31/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8154 Loss_G: 0.8311 acc: 59.4%\n",
      "[BATCH 2/149] Loss_D: 0.7799 Loss_G: 0.8238 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.7951 Loss_G: 0.8206 acc: 56.2%\n",
      "[BATCH 4/149] Loss_D: 0.7790 Loss_G: 0.8158 acc: 67.2%\n",
      "[BATCH 5/149] Loss_D: 0.8611 Loss_G: 0.8301 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.7704 Loss_G: 0.8283 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8659 Loss_G: 0.8534 acc: 56.2%\n",
      "[BATCH 8/149] Loss_D: 0.8215 Loss_G: 0.8393 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.7904 Loss_G: 0.8388 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.8066 Loss_G: 0.8281 acc: 54.7%\n",
      "[BATCH 11/149] Loss_D: 0.7860 Loss_G: 0.8239 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7839 Loss_G: 0.8233 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.8555 Loss_G: 0.8353 acc: 59.4%\n",
      "[BATCH 14/149] Loss_D: 0.8329 Loss_G: 0.8511 acc: 68.8%\n",
      "[BATCH 15/149] Loss_D: 0.8252 Loss_G: 0.8485 acc: 59.4%\n",
      "[BATCH 16/149] Loss_D: 0.8110 Loss_G: 0.8417 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.8071 Loss_G: 0.8318 acc: 54.7%\n",
      "[BATCH 18/149] Loss_D: 0.7675 Loss_G: 0.8199 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7774 Loss_G: 0.8186 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.8276 Loss_G: 0.8265 acc: 51.6%\n",
      "[BATCH 21/149] Loss_D: 0.7906 Loss_G: 0.8275 acc: 71.9%\n",
      "[BATCH 22/149] Loss_D: 0.8409 Loss_G: 0.8390 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.8690 Loss_G: 0.8511 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.7971 Loss_G: 0.8392 acc: 71.9%\n",
      "[BATCH 25/149] Loss_D: 0.7980 Loss_G: 0.8333 acc: 75.0%\n",
      "[BATCH 26/149] Loss_D: 0.8354 Loss_G: 0.8352 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.7779 Loss_G: 0.8291 acc: 60.9%\n",
      "[BATCH 28/149] Loss_D: 0.7749 Loss_G: 0.8244 acc: 64.1%\n",
      "[BATCH 29/149] Loss_D: 0.8082 Loss_G: 0.8258 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.8232 Loss_G: 0.8305 acc: 59.4%\n",
      "[EPOCH 4500] TEST ACC is : 71.9%\n",
      "[BATCH 31/149] Loss_D: 0.8076 Loss_G: 0.8340 acc: 60.9%\n",
      "[BATCH 32/149] Loss_D: 0.7580 Loss_G: 0.8279 acc: 71.9%\n",
      "[BATCH 33/149] Loss_D: 0.8508 Loss_G: 0.8294 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8367 Loss_G: 0.8383 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7906 Loss_G: 0.8391 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.8266 Loss_G: 0.8318 acc: 50.0%\n",
      "[BATCH 37/149] Loss_D: 0.7965 Loss_G: 0.8278 acc: 70.3%\n",
      "[BATCH 38/149] Loss_D: 0.8134 Loss_G: 0.8276 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.8077 Loss_G: 0.8249 acc: 65.6%\n",
      "[BATCH 40/149] Loss_D: 0.7761 Loss_G: 0.8264 acc: 53.1%\n",
      "[BATCH 41/149] Loss_D: 0.8308 Loss_G: 0.8323 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.8002 Loss_G: 0.8362 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.8386 Loss_G: 0.8366 acc: 65.6%\n",
      "[BATCH 44/149] Loss_D: 0.8097 Loss_G: 0.8281 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.8694 Loss_G: 0.8461 acc: 68.8%\n",
      "[BATCH 46/149] Loss_D: 0.8563 Loss_G: 0.8442 acc: 57.8%\n",
      "[BATCH 47/149] Loss_D: 0.7881 Loss_G: 0.8209 acc: 53.1%\n",
      "[BATCH 48/149] Loss_D: 0.7989 Loss_G: 0.8228 acc: 57.8%\n",
      "[BATCH 49/149] Loss_D: 0.8016 Loss_G: 0.8209 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.8311 Loss_G: 0.8266 acc: 60.9%\n",
      "[BATCH 51/149] Loss_D: 0.7928 Loss_G: 0.8267 acc: 54.7%\n",
      "[BATCH 52/149] Loss_D: 0.8219 Loss_G: 0.8300 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.7696 Loss_G: 0.8222 acc: 67.2%\n",
      "[BATCH 54/149] Loss_D: 0.7644 Loss_G: 0.8173 acc: 57.8%\n",
      "[BATCH 55/149] Loss_D: 0.7595 Loss_G: 0.8095 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7929 Loss_G: 0.8101 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.8522 Loss_G: 0.8348 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.7611 Loss_G: 0.8296 acc: 57.8%\n",
      "[BATCH 59/149] Loss_D: 0.8316 Loss_G: 0.8475 acc: 56.2%\n",
      "[BATCH 60/149] Loss_D: 0.9016 Loss_G: 0.8552 acc: 51.6%\n",
      "[BATCH 61/149] Loss_D: 0.8322 Loss_G: 0.8626 acc: 57.8%\n",
      "[BATCH 62/149] Loss_D: 0.8355 Loss_G: 0.8492 acc: 50.0%\n",
      "[BATCH 63/149] Loss_D: 0.7791 Loss_G: 0.8290 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.8484 Loss_G: 0.8277 acc: 64.1%\n",
      "[BATCH 65/149] Loss_D: 0.7819 Loss_G: 0.8254 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7754 Loss_G: 0.8242 acc: 67.2%\n",
      "[BATCH 67/149] Loss_D: 0.8325 Loss_G: 0.8318 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7993 Loss_G: 0.8283 acc: 68.8%\n",
      "[BATCH 69/149] Loss_D: 0.9065 Loss_G: 0.8392 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.8148 Loss_G: 0.8417 acc: 51.6%\n",
      "[BATCH 71/149] Loss_D: 0.8024 Loss_G: 0.8354 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8022 Loss_G: 0.8323 acc: 65.6%\n",
      "[BATCH 73/149] Loss_D: 0.7732 Loss_G: 0.8278 acc: 57.8%\n",
      "[BATCH 74/149] Loss_D: 0.8049 Loss_G: 0.8274 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.8196 Loss_G: 0.8262 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.8514 Loss_G: 0.8448 acc: 67.2%\n",
      "[BATCH 77/149] Loss_D: 0.8356 Loss_G: 0.8456 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.8145 Loss_G: 0.8413 acc: 70.3%\n",
      "[BATCH 79/149] Loss_D: 0.7805 Loss_G: 0.8309 acc: 57.8%\n",
      "[BATCH 80/149] Loss_D: 0.7861 Loss_G: 0.8237 acc: 54.7%\n",
      "[EPOCH 4550] TEST ACC is : 73.6%\n",
      "[BATCH 81/149] Loss_D: 0.8684 Loss_G: 0.8330 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.8041 Loss_G: 0.8400 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7936 Loss_G: 0.8257 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.8035 Loss_G: 0.8202 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.7897 Loss_G: 0.8199 acc: 64.1%\n",
      "[BATCH 86/149] Loss_D: 0.8287 Loss_G: 0.8229 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.8197 Loss_G: 0.8300 acc: 67.2%\n",
      "[BATCH 88/149] Loss_D: 0.8902 Loss_G: 0.8654 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.7922 Loss_G: 0.8635 acc: 46.9%\n",
      "[BATCH 90/149] Loss_D: 0.7671 Loss_G: 0.8346 acc: 60.9%\n",
      "[BATCH 91/149] Loss_D: 0.8019 Loss_G: 0.8228 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.8086 Loss_G: 0.8278 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.8147 Loss_G: 0.8269 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.8420 Loss_G: 0.8406 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7816 Loss_G: 0.8406 acc: 65.6%\n",
      "[BATCH 96/149] Loss_D: 0.8234 Loss_G: 0.8341 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7673 Loss_G: 0.8258 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.8001 Loss_G: 0.8271 acc: 56.2%\n",
      "[BATCH 99/149] Loss_D: 0.8595 Loss_G: 0.8365 acc: 59.4%\n",
      "[BATCH 100/149] Loss_D: 0.8394 Loss_G: 0.8445 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.8487 Loss_G: 0.8458 acc: 53.1%\n",
      "[BATCH 102/149] Loss_D: 0.8183 Loss_G: 0.8380 acc: 53.1%\n",
      "[BATCH 103/149] Loss_D: 0.8076 Loss_G: 0.8293 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8297 Loss_G: 0.8285 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.7897 Loss_G: 0.8258 acc: 56.2%\n",
      "[BATCH 106/149] Loss_D: 0.7827 Loss_G: 0.8236 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.8000 Loss_G: 0.8256 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.8398 Loss_G: 0.8507 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.8165 Loss_G: 0.8361 acc: 53.1%\n",
      "[BATCH 110/149] Loss_D: 0.7835 Loss_G: 0.8190 acc: 53.1%\n",
      "[BATCH 111/149] Loss_D: 0.7720 Loss_G: 0.8126 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.8197 Loss_G: 0.8181 acc: 73.4%\n",
      "[BATCH 113/149] Loss_D: 0.8360 Loss_G: 0.8230 acc: 75.0%\n",
      "[BATCH 114/149] Loss_D: 0.8038 Loss_G: 0.8319 acc: 67.2%\n",
      "[BATCH 115/149] Loss_D: 0.7714 Loss_G: 0.8170 acc: 50.0%\n",
      "[BATCH 116/149] Loss_D: 0.7929 Loss_G: 0.8173 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.8339 Loss_G: 0.8205 acc: 67.2%\n",
      "[BATCH 118/149] Loss_D: 0.7785 Loss_G: 0.8185 acc: 51.6%\n",
      "[BATCH 119/149] Loss_D: 0.8216 Loss_G: 0.8252 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7710 Loss_G: 0.8233 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.7977 Loss_G: 0.8254 acc: 51.6%\n",
      "[BATCH 122/149] Loss_D: 0.8011 Loss_G: 0.8294 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.8254 Loss_G: 0.8428 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.8553 Loss_G: 0.8554 acc: 71.9%\n",
      "[BATCH 125/149] Loss_D: 0.8350 Loss_G: 0.8512 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.8209 Loss_G: 0.8479 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.8484 Loss_G: 0.8407 acc: 75.0%\n",
      "[BATCH 128/149] Loss_D: 0.8032 Loss_G: 0.8324 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.8436 Loss_G: 0.8318 acc: 62.5%\n",
      "[BATCH 130/149] Loss_D: 0.7961 Loss_G: 0.8301 acc: 68.8%\n",
      "[EPOCH 4600] TEST ACC is : 67.8%\n",
      "[BATCH 131/149] Loss_D: 0.8072 Loss_G: 0.8268 acc: 65.6%\n",
      "[BATCH 132/149] Loss_D: 0.8246 Loss_G: 0.8223 acc: 70.3%\n",
      "[BATCH 133/149] Loss_D: 0.8928 Loss_G: 0.8437 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7700 Loss_G: 0.8330 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.8750 Loss_G: 0.8388 acc: 48.4%\n",
      "[BATCH 136/149] Loss_D: 0.8220 Loss_G: 0.8451 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.8009 Loss_G: 0.8321 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.7782 Loss_G: 0.8280 acc: 53.1%\n",
      "[BATCH 139/149] Loss_D: 0.7754 Loss_G: 0.8201 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.8323 Loss_G: 0.8284 acc: 65.6%\n",
      "[BATCH 141/149] Loss_D: 0.7653 Loss_G: 0.8278 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.8223 Loss_G: 0.8196 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7961 Loss_G: 0.8250 acc: 56.2%\n",
      "[BATCH 144/149] Loss_D: 0.8216 Loss_G: 0.8278 acc: 62.5%\n",
      "[BATCH 145/149] Loss_D: 0.8423 Loss_G: 0.8386 acc: 64.1%\n",
      "[BATCH 146/149] Loss_D: 0.8220 Loss_G: 0.8352 acc: 53.1%\n",
      "[BATCH 147/149] Loss_D: 0.7585 Loss_G: 0.8225 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.8002 Loss_G: 0.8180 acc: 60.9%\n",
      "[BATCH 149/149] Loss_D: 0.8446 Loss_G: 0.8333 acc: 57.8%\n",
      "-----THE [31/50] epoch end-----\n",
      "-----THE [32/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8102 Loss_G: 0.8208 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.8619 Loss_G: 0.8354 acc: 70.3%\n",
      "[BATCH 3/149] Loss_D: 0.7619 Loss_G: 0.8301 acc: 71.9%\n",
      "[BATCH 4/149] Loss_D: 0.7992 Loss_G: 0.8235 acc: 59.4%\n",
      "[BATCH 5/149] Loss_D: 0.8340 Loss_G: 0.8256 acc: 59.4%\n",
      "[BATCH 6/149] Loss_D: 0.7931 Loss_G: 0.8257 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.8247 Loss_G: 0.8321 acc: 65.6%\n",
      "[BATCH 8/149] Loss_D: 0.7630 Loss_G: 0.8317 acc: 62.5%\n",
      "[BATCH 9/149] Loss_D: 0.7653 Loss_G: 0.8144 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7797 Loss_G: 0.8113 acc: 71.9%\n",
      "[BATCH 11/149] Loss_D: 0.8029 Loss_G: 0.8138 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7846 Loss_G: 0.8143 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.7524 Loss_G: 0.8080 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7848 Loss_G: 0.8074 acc: 54.7%\n",
      "[BATCH 15/149] Loss_D: 0.8326 Loss_G: 0.8116 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.7517 Loss_G: 0.8069 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.8338 Loss_G: 0.8104 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.8224 Loss_G: 0.8138 acc: 59.4%\n",
      "[BATCH 19/149] Loss_D: 0.7698 Loss_G: 0.8072 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.8499 Loss_G: 0.8226 acc: 64.1%\n",
      "[BATCH 21/149] Loss_D: 0.7826 Loss_G: 0.8251 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.7936 Loss_G: 0.8198 acc: 53.1%\n",
      "[BATCH 23/149] Loss_D: 0.7542 Loss_G: 0.8119 acc: 68.8%\n",
      "[BATCH 24/149] Loss_D: 0.7762 Loss_G: 0.8071 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.8196 Loss_G: 0.8152 acc: 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.8330 Loss_G: 0.8337 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7999 Loss_G: 0.8453 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.7815 Loss_G: 0.8398 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.8462 Loss_G: 0.8342 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.8104 Loss_G: 0.8244 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.8121 Loss_G: 0.8224 acc: 56.2%\n",
      "[EPOCH 4650] TEST ACC is : 70.1%\n",
      "[BATCH 32/149] Loss_D: 0.7390 Loss_G: 0.8148 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.8078 Loss_G: 0.8162 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.8156 Loss_G: 0.8195 acc: 57.8%\n",
      "[BATCH 35/149] Loss_D: 0.7837 Loss_G: 0.8189 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.8016 Loss_G: 0.8243 acc: 67.2%\n",
      "[BATCH 37/149] Loss_D: 0.7890 Loss_G: 0.8151 acc: 60.9%\n",
      "[BATCH 38/149] Loss_D: 0.8297 Loss_G: 0.8225 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.8915 Loss_G: 0.8446 acc: 45.3%\n",
      "[BATCH 40/149] Loss_D: 0.7814 Loss_G: 0.8399 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.8374 Loss_G: 0.8385 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.7971 Loss_G: 0.8328 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.7820 Loss_G: 0.8277 acc: 65.6%\n",
      "[BATCH 44/149] Loss_D: 0.8268 Loss_G: 0.8248 acc: 50.0%\n",
      "[BATCH 45/149] Loss_D: 0.8284 Loss_G: 0.8254 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8143 Loss_G: 0.8318 acc: 60.9%\n",
      "[BATCH 47/149] Loss_D: 0.7829 Loss_G: 0.8267 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.8489 Loss_G: 0.8241 acc: 73.4%\n",
      "[BATCH 49/149] Loss_D: 0.8405 Loss_G: 0.8315 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.8349 Loss_G: 0.8420 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.7965 Loss_G: 0.8426 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.8658 Loss_G: 0.8551 acc: 67.2%\n",
      "[BATCH 53/149] Loss_D: 0.7715 Loss_G: 0.8440 acc: 64.1%\n",
      "[BATCH 54/149] Loss_D: 0.8049 Loss_G: 0.8398 acc: 46.9%\n",
      "[BATCH 55/149] Loss_D: 0.8344 Loss_G: 0.8415 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.8713 Loss_G: 0.8609 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.7862 Loss_G: 0.8493 acc: 57.8%\n",
      "[BATCH 58/149] Loss_D: 0.9230 Loss_G: 0.8743 acc: 56.2%\n",
      "[BATCH 59/149] Loss_D: 0.8263 Loss_G: 0.8877 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.8090 Loss_G: 0.8567 acc: 70.3%\n",
      "[BATCH 61/149] Loss_D: 0.7733 Loss_G: 0.8424 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.7913 Loss_G: 0.8323 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.8047 Loss_G: 0.8232 acc: 54.7%\n",
      "[BATCH 64/149] Loss_D: 0.8125 Loss_G: 0.8232 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.8024 Loss_G: 0.8250 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.7898 Loss_G: 0.8215 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.8542 Loss_G: 0.8323 acc: 56.2%\n",
      "[BATCH 68/149] Loss_D: 0.8372 Loss_G: 0.8409 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.7857 Loss_G: 0.8391 acc: 54.7%\n",
      "[BATCH 70/149] Loss_D: 0.8500 Loss_G: 0.8466 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.7814 Loss_G: 0.8278 acc: 53.1%\n",
      "[BATCH 72/149] Loss_D: 0.7889 Loss_G: 0.8254 acc: 50.0%\n",
      "[BATCH 73/149] Loss_D: 0.8784 Loss_G: 0.8506 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.7669 Loss_G: 0.8426 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.8507 Loss_G: 0.8446 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.8093 Loss_G: 0.8365 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.8331 Loss_G: 0.8476 acc: 79.7%\n",
      "[BATCH 78/149] Loss_D: 0.7835 Loss_G: 0.8329 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.8085 Loss_G: 0.8260 acc: 56.2%\n",
      "[BATCH 80/149] Loss_D: 0.8167 Loss_G: 0.8321 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.8340 Loss_G: 0.8395 acc: 67.2%\n",
      "[EPOCH 4700] TEST ACC is : 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.7952 Loss_G: 0.8438 acc: 46.9%\n",
      "[BATCH 83/149] Loss_D: 0.7992 Loss_G: 0.8287 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.7720 Loss_G: 0.8165 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.8339 Loss_G: 0.8232 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.7912 Loss_G: 0.8245 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.8546 Loss_G: 0.8323 acc: 50.0%\n",
      "[BATCH 88/149] Loss_D: 0.8082 Loss_G: 0.8299 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.7889 Loss_G: 0.8259 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7904 Loss_G: 0.8141 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.8113 Loss_G: 0.8206 acc: 53.1%\n",
      "[BATCH 92/149] Loss_D: 0.8127 Loss_G: 0.8257 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.8386 Loss_G: 0.8374 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.7973 Loss_G: 0.8315 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.7965 Loss_G: 0.8337 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.8070 Loss_G: 0.8308 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.8079 Loss_G: 0.8233 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.7980 Loss_G: 0.8204 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.8113 Loss_G: 0.8232 acc: 75.0%\n",
      "[BATCH 100/149] Loss_D: 0.8222 Loss_G: 0.8331 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.7909 Loss_G: 0.8330 acc: 57.8%\n",
      "[BATCH 102/149] Loss_D: 0.8337 Loss_G: 0.8331 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.8108 Loss_G: 0.8320 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.7902 Loss_G: 0.8189 acc: 59.4%\n",
      "[BATCH 105/149] Loss_D: 0.8254 Loss_G: 0.8139 acc: 71.9%\n",
      "[BATCH 106/149] Loss_D: 0.8143 Loss_G: 0.8232 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.8187 Loss_G: 0.8233 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8733 Loss_G: 0.8385 acc: 71.9%\n",
      "[BATCH 109/149] Loss_D: 0.8211 Loss_G: 0.8436 acc: 73.4%\n",
      "[BATCH 110/149] Loss_D: 0.8224 Loss_G: 0.8468 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.8252 Loss_G: 0.8458 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.7884 Loss_G: 0.8309 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.8256 Loss_G: 0.8395 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.8210 Loss_G: 0.8409 acc: 73.4%\n",
      "[BATCH 115/149] Loss_D: 0.8201 Loss_G: 0.8471 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8388 Loss_G: 0.8502 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.7888 Loss_G: 0.8414 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7760 Loss_G: 0.8293 acc: 62.5%\n",
      "[BATCH 119/149] Loss_D: 0.8004 Loss_G: 0.8253 acc: 54.7%\n",
      "[BATCH 120/149] Loss_D: 0.8361 Loss_G: 0.8285 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.8401 Loss_G: 0.8386 acc: 60.9%\n",
      "[BATCH 122/149] Loss_D: 0.7926 Loss_G: 0.8441 acc: 64.1%\n",
      "[BATCH 123/149] Loss_D: 0.8196 Loss_G: 0.8299 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.7975 Loss_G: 0.8299 acc: 62.5%\n",
      "[BATCH 125/149] Loss_D: 0.8310 Loss_G: 0.8332 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.8564 Loss_G: 0.8430 acc: 65.6%\n",
      "[BATCH 127/149] Loss_D: 0.7704 Loss_G: 0.8399 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.8179 Loss_G: 0.8452 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 0.7902 Loss_G: 0.8337 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7991 Loss_G: 0.8270 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.8119 Loss_G: 0.8295 acc: 64.1%\n",
      "[EPOCH 4750] TEST ACC is : 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.8091 Loss_G: 0.8321 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.8002 Loss_G: 0.8272 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.8289 Loss_G: 0.8229 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7475 Loss_G: 0.8185 acc: 73.4%\n",
      "[BATCH 136/149] Loss_D: 0.7778 Loss_G: 0.8175 acc: 56.2%\n",
      "[BATCH 137/149] Loss_D: 0.8253 Loss_G: 0.8176 acc: 57.8%\n",
      "[BATCH 138/149] Loss_D: 0.7969 Loss_G: 0.8253 acc: 70.3%\n",
      "[BATCH 139/149] Loss_D: 0.7836 Loss_G: 0.8192 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.7593 Loss_G: 0.8061 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.8220 Loss_G: 0.8103 acc: 48.4%\n",
      "[BATCH 142/149] Loss_D: 0.9111 Loss_G: 0.8362 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.8050 Loss_G: 0.8597 acc: 57.8%\n",
      "[BATCH 144/149] Loss_D: 0.8412 Loss_G: 0.8535 acc: 68.8%\n",
      "[BATCH 145/149] Loss_D: 0.8125 Loss_G: 0.8380 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.8242 Loss_G: 0.8415 acc: 43.8%\n",
      "[BATCH 147/149] Loss_D: 0.8585 Loss_G: 0.8508 acc: 67.2%\n",
      "[BATCH 148/149] Loss_D: 0.8007 Loss_G: 0.8416 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.8164 Loss_G: 0.8377 acc: 48.4%\n",
      "-----THE [32/50] epoch end-----\n",
      "-----THE [33/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8075 Loss_G: 0.8368 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.8347 Loss_G: 0.8375 acc: 53.1%\n",
      "[BATCH 3/149] Loss_D: 0.8229 Loss_G: 0.8339 acc: 57.8%\n",
      "[BATCH 4/149] Loss_D: 0.8089 Loss_G: 0.8335 acc: 64.1%\n",
      "[BATCH 5/149] Loss_D: 0.7973 Loss_G: 0.8342 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.8040 Loss_G: 0.8343 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.7918 Loss_G: 0.8277 acc: 67.2%\n",
      "[BATCH 8/149] Loss_D: 0.7853 Loss_G: 0.8195 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.7942 Loss_G: 0.8149 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8166 Loss_G: 0.8200 acc: 57.8%\n",
      "[BATCH 11/149] Loss_D: 0.8105 Loss_G: 0.8267 acc: 59.4%\n",
      "[BATCH 12/149] Loss_D: 0.8533 Loss_G: 0.8340 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.8041 Loss_G: 0.8317 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.8373 Loss_G: 0.8331 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.8157 Loss_G: 0.8302 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.7977 Loss_G: 0.8314 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.7683 Loss_G: 0.8247 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.8391 Loss_G: 0.8260 acc: 71.9%\n",
      "[BATCH 19/149] Loss_D: 0.7855 Loss_G: 0.8219 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7866 Loss_G: 0.8169 acc: 62.5%\n",
      "[BATCH 21/149] Loss_D: 0.7616 Loss_G: 0.8087 acc: 54.7%\n",
      "[BATCH 22/149] Loss_D: 0.7764 Loss_G: 0.8079 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7757 Loss_G: 0.8051 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.8173 Loss_G: 0.8066 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.8628 Loss_G: 0.8430 acc: 59.4%\n",
      "[BATCH 26/149] Loss_D: 0.8295 Loss_G: 0.8622 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.7765 Loss_G: 0.8320 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.8100 Loss_G: 0.8258 acc: 56.2%\n",
      "[BATCH 29/149] Loss_D: 0.8675 Loss_G: 0.8396 acc: 57.8%\n",
      "[BATCH 30/149] Loss_D: 0.8641 Loss_G: 0.8612 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.8089 Loss_G: 0.8740 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.8111 Loss_G: 0.8501 acc: 64.1%\n",
      "[EPOCH 4800] TEST ACC is : 69.7%\n",
      "[BATCH 33/149] Loss_D: 0.8244 Loss_G: 0.8405 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.8292 Loss_G: 0.8377 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7572 Loss_G: 0.8272 acc: 76.6%\n",
      "[BATCH 36/149] Loss_D: 0.7723 Loss_G: 0.8159 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7993 Loss_G: 0.8220 acc: 70.3%\n",
      "[BATCH 38/149] Loss_D: 0.7933 Loss_G: 0.8158 acc: 59.4%\n",
      "[BATCH 39/149] Loss_D: 0.8370 Loss_G: 0.8259 acc: 60.9%\n",
      "[BATCH 40/149] Loss_D: 0.8031 Loss_G: 0.8394 acc: 62.5%\n",
      "[BATCH 41/149] Loss_D: 0.8206 Loss_G: 0.8426 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.8113 Loss_G: 0.8327 acc: 56.2%\n",
      "[BATCH 43/149] Loss_D: 0.8030 Loss_G: 0.8241 acc: 65.6%\n",
      "[BATCH 44/149] Loss_D: 0.8010 Loss_G: 0.8218 acc: 59.4%\n",
      "[BATCH 45/149] Loss_D: 0.8070 Loss_G: 0.8286 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.7778 Loss_G: 0.8317 acc: 70.3%\n",
      "[BATCH 47/149] Loss_D: 0.8315 Loss_G: 0.8371 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.8482 Loss_G: 0.8388 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.7874 Loss_G: 0.8369 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.8121 Loss_G: 0.8308 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.8137 Loss_G: 0.8347 acc: 48.4%\n",
      "[BATCH 52/149] Loss_D: 0.8477 Loss_G: 0.8374 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.8160 Loss_G: 0.8415 acc: 64.1%\n",
      "[BATCH 54/149] Loss_D: 0.7991 Loss_G: 0.8371 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.8560 Loss_G: 0.8383 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7624 Loss_G: 0.8231 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.7853 Loss_G: 0.8187 acc: 57.8%\n",
      "[BATCH 58/149] Loss_D: 0.8020 Loss_G: 0.8205 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.8284 Loss_G: 0.8289 acc: 57.8%\n",
      "[BATCH 60/149] Loss_D: 0.8362 Loss_G: 0.8344 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.8324 Loss_G: 0.8346 acc: 50.0%\n",
      "[BATCH 62/149] Loss_D: 0.8304 Loss_G: 0.8362 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.7994 Loss_G: 0.8324 acc: 73.4%\n",
      "[BATCH 64/149] Loss_D: 0.7969 Loss_G: 0.8317 acc: 56.2%\n",
      "[BATCH 65/149] Loss_D: 0.8051 Loss_G: 0.8225 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.8429 Loss_G: 0.8385 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.8040 Loss_G: 0.8334 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7968 Loss_G: 0.8216 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.7763 Loss_G: 0.8144 acc: 57.8%\n",
      "[BATCH 70/149] Loss_D: 0.7722 Loss_G: 0.8137 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.8221 Loss_G: 0.8177 acc: 73.4%\n",
      "[BATCH 72/149] Loss_D: 0.8264 Loss_G: 0.8302 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.8053 Loss_G: 0.8385 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.7744 Loss_G: 0.8266 acc: 67.2%\n",
      "[BATCH 75/149] Loss_D: 0.8184 Loss_G: 0.8318 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.8519 Loss_G: 0.8422 acc: 54.7%\n",
      "[BATCH 77/149] Loss_D: 0.8389 Loss_G: 0.8585 acc: 57.8%\n",
      "[BATCH 78/149] Loss_D: 0.7940 Loss_G: 0.8530 acc: 71.9%\n",
      "[BATCH 79/149] Loss_D: 0.7691 Loss_G: 0.8341 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.7809 Loss_G: 0.8214 acc: 57.8%\n",
      "[BATCH 81/149] Loss_D: 0.7539 Loss_G: 0.8182 acc: 56.2%\n",
      "[BATCH 82/149] Loss_D: 0.8477 Loss_G: 0.8333 acc: 60.9%\n",
      "[EPOCH 4850] TEST ACC is : 69.3%\n",
      "[BATCH 83/149] Loss_D: 0.7670 Loss_G: 0.8287 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.8441 Loss_G: 0.8355 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.7678 Loss_G: 0.8186 acc: 64.1%\n",
      "[BATCH 86/149] Loss_D: 0.7579 Loss_G: 0.8071 acc: 50.0%\n",
      "[BATCH 87/149] Loss_D: 0.7817 Loss_G: 0.8031 acc: 56.2%\n",
      "[BATCH 88/149] Loss_D: 0.7987 Loss_G: 0.8120 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.8147 Loss_G: 0.8214 acc: 64.1%\n",
      "[BATCH 90/149] Loss_D: 0.8352 Loss_G: 0.8386 acc: 51.6%\n",
      "[BATCH 91/149] Loss_D: 0.8075 Loss_G: 0.8432 acc: 50.0%\n",
      "[BATCH 92/149] Loss_D: 0.8389 Loss_G: 0.8512 acc: 64.1%\n",
      "[BATCH 93/149] Loss_D: 0.8298 Loss_G: 0.8473 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7710 Loss_G: 0.8338 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.8030 Loss_G: 0.8260 acc: 73.4%\n",
      "[BATCH 96/149] Loss_D: 0.8258 Loss_G: 0.8335 acc: 57.8%\n",
      "[BATCH 97/149] Loss_D: 0.8431 Loss_G: 0.8426 acc: 57.8%\n",
      "[BATCH 98/149] Loss_D: 0.7815 Loss_G: 0.8301 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.8385 Loss_G: 0.8329 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.8052 Loss_G: 0.8295 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.8233 Loss_G: 0.8302 acc: 56.2%\n",
      "[BATCH 102/149] Loss_D: 0.8927 Loss_G: 0.8586 acc: 57.8%\n",
      "[BATCH 103/149] Loss_D: 0.8077 Loss_G: 0.8559 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.7645 Loss_G: 0.8378 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.8467 Loss_G: 0.8369 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.7847 Loss_G: 0.8308 acc: 54.7%\n",
      "[BATCH 107/149] Loss_D: 0.7971 Loss_G: 0.8249 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.8078 Loss_G: 0.8275 acc: 56.2%\n",
      "[BATCH 109/149] Loss_D: 0.7836 Loss_G: 0.8222 acc: 51.6%\n",
      "[BATCH 110/149] Loss_D: 0.8057 Loss_G: 0.8268 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7523 Loss_G: 0.8173 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8485 Loss_G: 0.8208 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.8293 Loss_G: 0.8335 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.8195 Loss_G: 0.8337 acc: 71.9%\n",
      "[BATCH 115/149] Loss_D: 0.8009 Loss_G: 0.8288 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8290 Loss_G: 0.8262 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7600 Loss_G: 0.8193 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.8113 Loss_G: 0.8202 acc: 50.0%\n",
      "[BATCH 119/149] Loss_D: 0.8424 Loss_G: 0.8301 acc: 50.0%\n",
      "[BATCH 120/149] Loss_D: 0.7643 Loss_G: 0.8208 acc: 70.3%\n",
      "[BATCH 121/149] Loss_D: 0.8159 Loss_G: 0.8194 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.7971 Loss_G: 0.8210 acc: 75.0%\n",
      "[BATCH 123/149] Loss_D: 0.8303 Loss_G: 0.8228 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.8099 Loss_G: 0.8277 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.7730 Loss_G: 0.8211 acc: 57.8%\n",
      "[BATCH 126/149] Loss_D: 0.8081 Loss_G: 0.8268 acc: 68.8%\n",
      "[BATCH 127/149] Loss_D: 0.8422 Loss_G: 0.8308 acc: 56.2%\n",
      "[BATCH 128/149] Loss_D: 0.8696 Loss_G: 0.8578 acc: 65.6%\n",
      "[BATCH 129/149] Loss_D: 0.7932 Loss_G: 0.8358 acc: 67.2%\n",
      "[BATCH 130/149] Loss_D: 0.7891 Loss_G: 0.8276 acc: 51.6%\n",
      "[BATCH 131/149] Loss_D: 0.8272 Loss_G: 0.8322 acc: 57.8%\n",
      "[BATCH 132/149] Loss_D: 0.8576 Loss_G: 0.8361 acc: 62.5%\n",
      "[EPOCH 4900] TEST ACC is : 68.4%\n",
      "[BATCH 133/149] Loss_D: 0.8287 Loss_G: 0.8390 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.8268 Loss_G: 0.8351 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.8078 Loss_G: 0.8341 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.8187 Loss_G: 0.8341 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.8121 Loss_G: 0.8311 acc: 67.2%\n",
      "[BATCH 138/149] Loss_D: 0.8316 Loss_G: 0.8358 acc: 54.7%\n",
      "[BATCH 139/149] Loss_D: 0.8640 Loss_G: 0.8475 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.8057 Loss_G: 0.8485 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.8314 Loss_G: 0.8568 acc: 46.9%\n",
      "[BATCH 142/149] Loss_D: 0.7949 Loss_G: 0.8446 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.7963 Loss_G: 0.8294 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7814 Loss_G: 0.8188 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.8429 Loss_G: 0.8261 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7719 Loss_G: 0.8268 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.8650 Loss_G: 0.8421 acc: 71.9%\n",
      "[BATCH 148/149] Loss_D: 0.8222 Loss_G: 0.8562 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7735 Loss_G: 0.8397 acc: 64.1%\n",
      "-----THE [33/50] epoch end-----\n",
      "-----THE [34/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7973 Loss_G: 0.8231 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7985 Loss_G: 0.8196 acc: 62.5%\n",
      "[BATCH 3/149] Loss_D: 0.8450 Loss_G: 0.8316 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.8000 Loss_G: 0.8292 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.8016 Loss_G: 0.8295 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7914 Loss_G: 0.8249 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.8292 Loss_G: 0.8322 acc: 54.7%\n",
      "[BATCH 8/149] Loss_D: 0.8358 Loss_G: 0.8371 acc: 53.1%\n",
      "[BATCH 9/149] Loss_D: 0.7934 Loss_G: 0.8296 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.8284 Loss_G: 0.8462 acc: 54.7%\n",
      "[BATCH 11/149] Loss_D: 0.8110 Loss_G: 0.8427 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7952 Loss_G: 0.8234 acc: 50.0%\n",
      "[BATCH 13/149] Loss_D: 0.7626 Loss_G: 0.8133 acc: 57.8%\n",
      "[BATCH 14/149] Loss_D: 0.8155 Loss_G: 0.8150 acc: 56.2%\n",
      "[BATCH 15/149] Loss_D: 0.7824 Loss_G: 0.8152 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7995 Loss_G: 0.8174 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7503 Loss_G: 0.8123 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.7887 Loss_G: 0.8065 acc: 48.4%\n",
      "[BATCH 19/149] Loss_D: 0.8294 Loss_G: 0.8261 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.8241 Loss_G: 0.8270 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8039 Loss_G: 0.8247 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7997 Loss_G: 0.8236 acc: 65.6%\n",
      "[BATCH 23/149] Loss_D: 0.8108 Loss_G: 0.8286 acc: 68.8%\n",
      "[BATCH 24/149] Loss_D: 0.8143 Loss_G: 0.8340 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.8203 Loss_G: 0.8324 acc: 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.7976 Loss_G: 0.8308 acc: 60.9%\n",
      "[BATCH 27/149] Loss_D: 0.7623 Loss_G: 0.8243 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.8156 Loss_G: 0.8198 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.8499 Loss_G: 0.8339 acc: 67.2%\n",
      "[BATCH 30/149] Loss_D: 0.8378 Loss_G: 0.8546 acc: 75.0%\n",
      "[BATCH 31/149] Loss_D: 0.8309 Loss_G: 0.8448 acc: 60.9%\n",
      "[BATCH 32/149] Loss_D: 0.8117 Loss_G: 0.8401 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.8017 Loss_G: 0.8296 acc: 56.2%\n",
      "[EPOCH 4950] TEST ACC is : 71.7%\n",
      "[BATCH 34/149] Loss_D: 0.8396 Loss_G: 0.8376 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7775 Loss_G: 0.8286 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.7783 Loss_G: 0.8193 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7748 Loss_G: 0.8142 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.8200 Loss_G: 0.8183 acc: 62.5%\n",
      "[BATCH 39/149] Loss_D: 0.8142 Loss_G: 0.8273 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.8163 Loss_G: 0.8286 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.7781 Loss_G: 0.8183 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.8048 Loss_G: 0.8248 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.8549 Loss_G: 0.8409 acc: 54.7%\n",
      "[BATCH 44/149] Loss_D: 0.8480 Loss_G: 0.8366 acc: 48.4%\n",
      "[BATCH 45/149] Loss_D: 0.8736 Loss_G: 0.8608 acc: 54.7%\n",
      "[BATCH 46/149] Loss_D: 0.8018 Loss_G: 0.8436 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.8234 Loss_G: 0.8385 acc: 60.9%\n",
      "[BATCH 48/149] Loss_D: 0.8171 Loss_G: 0.8335 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.8344 Loss_G: 0.8315 acc: 57.8%\n",
      "[BATCH 50/149] Loss_D: 0.7493 Loss_G: 0.8190 acc: 60.9%\n",
      "[BATCH 51/149] Loss_D: 0.8172 Loss_G: 0.8302 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.8065 Loss_G: 0.8175 acc: 56.2%\n",
      "[BATCH 53/149] Loss_D: 0.7908 Loss_G: 0.8101 acc: 57.8%\n",
      "[BATCH 54/149] Loss_D: 0.7938 Loss_G: 0.8071 acc: 59.4%\n",
      "[BATCH 55/149] Loss_D: 0.8193 Loss_G: 0.8078 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.8548 Loss_G: 0.8245 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.8118 Loss_G: 0.8348 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.7844 Loss_G: 0.8345 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.8175 Loss_G: 0.8312 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.8677 Loss_G: 0.8363 acc: 71.9%\n",
      "[BATCH 61/149] Loss_D: 0.8255 Loss_G: 0.8389 acc: 76.6%\n",
      "[BATCH 62/149] Loss_D: 0.7849 Loss_G: 0.8326 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.8332 Loss_G: 0.8276 acc: 64.1%\n",
      "[BATCH 64/149] Loss_D: 0.7871 Loss_G: 0.8227 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.7733 Loss_G: 0.8140 acc: 67.2%\n",
      "[BATCH 66/149] Loss_D: 0.7891 Loss_G: 0.8196 acc: 51.6%\n",
      "[BATCH 67/149] Loss_D: 0.7478 Loss_G: 0.8102 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.8508 Loss_G: 0.8237 acc: 56.2%\n",
      "[BATCH 69/149] Loss_D: 0.8175 Loss_G: 0.8346 acc: 56.2%\n",
      "[BATCH 70/149] Loss_D: 0.7891 Loss_G: 0.8279 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.8484 Loss_G: 0.8388 acc: 60.9%\n",
      "[BATCH 72/149] Loss_D: 0.7835 Loss_G: 0.8311 acc: 53.1%\n",
      "[BATCH 73/149] Loss_D: 0.8143 Loss_G: 0.8229 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.8527 Loss_G: 0.8400 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.7739 Loss_G: 0.8310 acc: 59.4%\n",
      "[BATCH 76/149] Loss_D: 0.8796 Loss_G: 0.8434 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.7987 Loss_G: 0.8527 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.7651 Loss_G: 0.8325 acc: 54.7%\n",
      "[BATCH 79/149] Loss_D: 0.7785 Loss_G: 0.8193 acc: 78.1%\n",
      "[BATCH 80/149] Loss_D: 0.8037 Loss_G: 0.8116 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.8270 Loss_G: 0.8181 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.8086 Loss_G: 0.8269 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.8050 Loss_G: 0.8294 acc: 59.4%\n",
      "[EPOCH 5000] TEST ACC is : 70.9%\n",
      "[BATCH 84/149] Loss_D: 0.8122 Loss_G: 0.8372 acc: 76.6%\n",
      "[BATCH 85/149] Loss_D: 0.8041 Loss_G: 0.8195 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.8180 Loss_G: 0.8214 acc: 75.0%\n",
      "[BATCH 87/149] Loss_D: 0.7759 Loss_G: 0.8131 acc: 65.6%\n",
      "[BATCH 88/149] Loss_D: 0.8132 Loss_G: 0.8210 acc: 57.8%\n",
      "[BATCH 89/149] Loss_D: 0.8031 Loss_G: 0.8187 acc: 57.8%\n",
      "[BATCH 90/149] Loss_D: 0.7617 Loss_G: 0.8166 acc: 76.6%\n",
      "[BATCH 91/149] Loss_D: 0.8099 Loss_G: 0.8185 acc: 56.2%\n",
      "[BATCH 92/149] Loss_D: 0.7812 Loss_G: 0.8133 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.8348 Loss_G: 0.8232 acc: 59.4%\n",
      "[BATCH 94/149] Loss_D: 0.8975 Loss_G: 0.8544 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.7754 Loss_G: 0.8530 acc: 53.1%\n",
      "[BATCH 96/149] Loss_D: 0.8129 Loss_G: 0.8487 acc: 53.1%\n",
      "[BATCH 97/149] Loss_D: 0.7989 Loss_G: 0.8384 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.8264 Loss_G: 0.8300 acc: 65.6%\n",
      "[BATCH 99/149] Loss_D: 0.8370 Loss_G: 0.8351 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.8065 Loss_G: 0.8478 acc: 54.7%\n",
      "[BATCH 101/149] Loss_D: 0.7776 Loss_G: 0.8302 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.8290 Loss_G: 0.8311 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.8322 Loss_G: 0.8331 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8400 Loss_G: 0.8357 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.8053 Loss_G: 0.8351 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.8621 Loss_G: 0.8526 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.7984 Loss_G: 0.8352 acc: 56.2%\n",
      "[BATCH 108/149] Loss_D: 0.8008 Loss_G: 0.8332 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.8230 Loss_G: 0.8349 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.8198 Loss_G: 0.8399 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.7707 Loss_G: 0.8301 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.7660 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.8118 Loss_G: 0.8227 acc: 75.0%\n",
      "[BATCH 114/149] Loss_D: 0.8503 Loss_G: 0.8393 acc: 71.9%\n",
      "[BATCH 115/149] Loss_D: 0.8050 Loss_G: 0.8298 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.8714 Loss_G: 0.8327 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7836 Loss_G: 0.8223 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.8124 Loss_G: 0.8228 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.8053 Loss_G: 0.8251 acc: 56.2%\n",
      "[BATCH 120/149] Loss_D: 0.7907 Loss_G: 0.8229 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.8077 Loss_G: 0.8181 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.8501 Loss_G: 0.8298 acc: 64.1%\n",
      "[BATCH 123/149] Loss_D: 0.7718 Loss_G: 0.8238 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.7977 Loss_G: 0.8160 acc: 56.2%\n",
      "[BATCH 125/149] Loss_D: 0.7786 Loss_G: 0.8145 acc: 54.7%\n",
      "[BATCH 126/149] Loss_D: 0.8343 Loss_G: 0.8217 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.7551 Loss_G: 0.8220 acc: 60.9%\n",
      "[BATCH 128/149] Loss_D: 0.7884 Loss_G: 0.8200 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.8343 Loss_G: 0.8282 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.7667 Loss_G: 0.8239 acc: 62.5%\n",
      "[BATCH 131/149] Loss_D: 0.8556 Loss_G: 0.8393 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.7532 Loss_G: 0.8303 acc: 57.8%\n",
      "[BATCH 133/149] Loss_D: 0.7770 Loss_G: 0.8226 acc: 65.6%\n",
      "[EPOCH 5050] TEST ACC is : 72.7%\n",
      "[BATCH 134/149] Loss_D: 0.8014 Loss_G: 0.8128 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.8338 Loss_G: 0.8210 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.8171 Loss_G: 0.8288 acc: 48.4%\n",
      "[BATCH 137/149] Loss_D: 0.8215 Loss_G: 0.8328 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.7774 Loss_G: 0.8199 acc: 56.2%\n",
      "[BATCH 139/149] Loss_D: 0.8621 Loss_G: 0.8318 acc: 56.2%\n",
      "[BATCH 140/149] Loss_D: 0.8050 Loss_G: 0.8325 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.8256 Loss_G: 0.8346 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.8215 Loss_G: 0.8397 acc: 51.6%\n",
      "[BATCH 143/149] Loss_D: 0.8178 Loss_G: 0.8361 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7723 Loss_G: 0.8233 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.8139 Loss_G: 0.8237 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.8114 Loss_G: 0.8218 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7925 Loss_G: 0.8207 acc: 75.0%\n",
      "[BATCH 148/149] Loss_D: 0.8428 Loss_G: 0.8314 acc: 59.4%\n",
      "[BATCH 149/149] Loss_D: 0.8125 Loss_G: 0.8370 acc: 67.2%\n",
      "-----THE [34/50] epoch end-----\n",
      "-----THE [35/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8118 Loss_G: 0.8390 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.7831 Loss_G: 0.8296 acc: 60.9%\n",
      "[BATCH 3/149] Loss_D: 0.8178 Loss_G: 0.8279 acc: 54.7%\n",
      "[BATCH 4/149] Loss_D: 0.7864 Loss_G: 0.8239 acc: 71.9%\n",
      "[BATCH 5/149] Loss_D: 0.7981 Loss_G: 0.8252 acc: 71.9%\n",
      "[BATCH 6/149] Loss_D: 0.8451 Loss_G: 0.8327 acc: 62.5%\n",
      "[BATCH 7/149] Loss_D: 0.7717 Loss_G: 0.8182 acc: 65.6%\n",
      "[BATCH 8/149] Loss_D: 0.8222 Loss_G: 0.8191 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.7636 Loss_G: 0.8138 acc: 75.0%\n",
      "[BATCH 10/149] Loss_D: 0.7940 Loss_G: 0.8132 acc: 50.0%\n",
      "[BATCH 11/149] Loss_D: 0.8083 Loss_G: 0.8216 acc: 62.5%\n",
      "[BATCH 12/149] Loss_D: 0.8098 Loss_G: 0.8238 acc: 56.2%\n",
      "[BATCH 13/149] Loss_D: 0.8415 Loss_G: 0.8314 acc: 51.6%\n",
      "[BATCH 14/149] Loss_D: 0.8088 Loss_G: 0.8265 acc: 54.7%\n",
      "[BATCH 15/149] Loss_D: 0.8212 Loss_G: 0.8185 acc: 75.0%\n",
      "[BATCH 16/149] Loss_D: 0.8056 Loss_G: 0.8201 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.8451 Loss_G: 0.8310 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.8577 Loss_G: 0.8427 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.8458 Loss_G: 0.8528 acc: 54.7%\n",
      "[BATCH 20/149] Loss_D: 0.7945 Loss_G: 0.8481 acc: 70.3%\n",
      "[BATCH 21/149] Loss_D: 0.7912 Loss_G: 0.8342 acc: 64.1%\n",
      "[BATCH 22/149] Loss_D: 0.8019 Loss_G: 0.8246 acc: 68.8%\n",
      "[BATCH 23/149] Loss_D: 0.8321 Loss_G: 0.8294 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.8338 Loss_G: 0.8525 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.8162 Loss_G: 0.8516 acc: 78.1%\n",
      "[BATCH 26/149] Loss_D: 0.7776 Loss_G: 0.8309 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.7813 Loss_G: 0.8217 acc: 71.9%\n",
      "[BATCH 28/149] Loss_D: 0.8105 Loss_G: 0.8212 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.8033 Loss_G: 0.8240 acc: 67.2%\n",
      "[BATCH 30/149] Loss_D: 0.8804 Loss_G: 0.8454 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7951 Loss_G: 0.8475 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.7966 Loss_G: 0.8360 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.8155 Loss_G: 0.8308 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.8151 Loss_G: 0.8285 acc: 70.3%\n",
      "[EPOCH 5100] TEST ACC is : 76.0%\n",
      "[BATCH 35/149] Loss_D: 0.8178 Loss_G: 0.8281 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.7652 Loss_G: 0.8248 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7494 Loss_G: 0.8150 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.8215 Loss_G: 0.8146 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.7912 Loss_G: 0.8227 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.8013 Loss_G: 0.8176 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.8060 Loss_G: 0.8182 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.7783 Loss_G: 0.8151 acc: 76.6%\n",
      "[BATCH 43/149] Loss_D: 0.8285 Loss_G: 0.8331 acc: 48.4%\n",
      "[BATCH 44/149] Loss_D: 0.8449 Loss_G: 0.8410 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.8508 Loss_G: 0.8370 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.8045 Loss_G: 0.8316 acc: 57.8%\n",
      "[BATCH 47/149] Loss_D: 0.8556 Loss_G: 0.8371 acc: 56.2%\n",
      "[BATCH 48/149] Loss_D: 0.7969 Loss_G: 0.8314 acc: 73.4%\n",
      "[BATCH 49/149] Loss_D: 0.8231 Loss_G: 0.8301 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.8083 Loss_G: 0.8259 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.8444 Loss_G: 0.8312 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.8111 Loss_G: 0.8336 acc: 71.9%\n",
      "[BATCH 53/149] Loss_D: 0.8245 Loss_G: 0.8436 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.8039 Loss_G: 0.8346 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.8044 Loss_G: 0.8240 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.8350 Loss_G: 0.8178 acc: 57.8%\n",
      "[BATCH 57/149] Loss_D: 0.8062 Loss_G: 0.8164 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.7672 Loss_G: 0.8136 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.7750 Loss_G: 0.8093 acc: 71.9%\n",
      "[BATCH 60/149] Loss_D: 0.7907 Loss_G: 0.8138 acc: 76.6%\n",
      "[BATCH 61/149] Loss_D: 0.8147 Loss_G: 0.8288 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.7620 Loss_G: 0.8128 acc: 57.8%\n",
      "[BATCH 63/149] Loss_D: 0.8970 Loss_G: 0.8325 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.7942 Loss_G: 0.8346 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8520 Loss_G: 0.8485 acc: 51.6%\n",
      "[BATCH 66/149] Loss_D: 0.7927 Loss_G: 0.8284 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.7994 Loss_G: 0.8292 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.7969 Loss_G: 0.8281 acc: 56.2%\n",
      "[BATCH 69/149] Loss_D: 0.8430 Loss_G: 0.8350 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.8088 Loss_G: 0.8379 acc: 53.1%\n",
      "[BATCH 71/149] Loss_D: 0.8255 Loss_G: 0.8391 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8134 Loss_G: 0.8304 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.7526 Loss_G: 0.8170 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.8266 Loss_G: 0.8154 acc: 51.6%\n",
      "[BATCH 75/149] Loss_D: 0.8099 Loss_G: 0.8228 acc: 54.7%\n",
      "[BATCH 76/149] Loss_D: 0.8595 Loss_G: 0.8402 acc: 54.7%\n",
      "[BATCH 77/149] Loss_D: 0.8185 Loss_G: 0.8387 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.7985 Loss_G: 0.8286 acc: 51.6%\n",
      "[BATCH 79/149] Loss_D: 0.8144 Loss_G: 0.8285 acc: 57.8%\n",
      "[BATCH 80/149] Loss_D: 0.7871 Loss_G: 0.8236 acc: 73.4%\n",
      "[BATCH 81/149] Loss_D: 0.7892 Loss_G: 0.8234 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.7907 Loss_G: 0.8228 acc: 51.6%\n",
      "[BATCH 83/149] Loss_D: 0.7738 Loss_G: 0.8177 acc: 71.9%\n",
      "[BATCH 84/149] Loss_D: 0.7843 Loss_G: 0.8168 acc: 60.9%\n",
      "[EPOCH 5150] TEST ACC is : 74.4%\n",
      "[BATCH 85/149] Loss_D: 0.7438 Loss_G: 0.8085 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.8036 Loss_G: 0.8112 acc: 64.1%\n",
      "[BATCH 87/149] Loss_D: 0.8286 Loss_G: 0.8364 acc: 57.8%\n",
      "[BATCH 88/149] Loss_D: 0.7705 Loss_G: 0.8231 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.7832 Loss_G: 0.8206 acc: 59.4%\n",
      "[BATCH 90/149] Loss_D: 0.7714 Loss_G: 0.8231 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.8413 Loss_G: 0.8282 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.8180 Loss_G: 0.8397 acc: 54.7%\n",
      "[BATCH 93/149] Loss_D: 0.7836 Loss_G: 0.8366 acc: 65.6%\n",
      "[BATCH 94/149] Loss_D: 0.7824 Loss_G: 0.8216 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.8625 Loss_G: 0.8313 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7702 Loss_G: 0.8271 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.8416 Loss_G: 0.8271 acc: 75.0%\n",
      "[BATCH 98/149] Loss_D: 0.7783 Loss_G: 0.8211 acc: 54.7%\n",
      "[BATCH 99/149] Loss_D: 0.7708 Loss_G: 0.8180 acc: 57.8%\n",
      "[BATCH 100/149] Loss_D: 0.8370 Loss_G: 0.8231 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.7849 Loss_G: 0.8252 acc: 57.8%\n",
      "[BATCH 102/149] Loss_D: 0.7905 Loss_G: 0.8221 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.8557 Loss_G: 0.8296 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.8157 Loss_G: 0.8303 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7838 Loss_G: 0.8293 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.7827 Loss_G: 0.8191 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.8417 Loss_G: 0.8282 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.8523 Loss_G: 0.8463 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.7780 Loss_G: 0.8308 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.7985 Loss_G: 0.8307 acc: 59.4%\n",
      "[BATCH 111/149] Loss_D: 0.7993 Loss_G: 0.8251 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7905 Loss_G: 0.8293 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.8151 Loss_G: 0.8291 acc: 54.7%\n",
      "[BATCH 114/149] Loss_D: 0.7907 Loss_G: 0.8313 acc: 67.2%\n",
      "[BATCH 115/149] Loss_D: 0.7993 Loss_G: 0.8254 acc: 53.1%\n",
      "[BATCH 116/149] Loss_D: 0.7918 Loss_G: 0.8209 acc: 76.6%\n",
      "[BATCH 117/149] Loss_D: 0.7839 Loss_G: 0.8151 acc: 73.4%\n",
      "[BATCH 118/149] Loss_D: 0.8038 Loss_G: 0.8180 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.8717 Loss_G: 0.8332 acc: 50.0%\n",
      "[BATCH 120/149] Loss_D: 0.7822 Loss_G: 0.8338 acc: 56.2%\n",
      "[BATCH 121/149] Loss_D: 0.8235 Loss_G: 0.8371 acc: 60.9%\n",
      "[BATCH 122/149] Loss_D: 0.7542 Loss_G: 0.8230 acc: 64.1%\n",
      "[BATCH 123/149] Loss_D: 0.7919 Loss_G: 0.8204 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.7774 Loss_G: 0.8167 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.8255 Loss_G: 0.8251 acc: 70.3%\n",
      "[BATCH 126/149] Loss_D: 0.8206 Loss_G: 0.8308 acc: 78.1%\n",
      "[BATCH 127/149] Loss_D: 0.7951 Loss_G: 0.8243 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.8853 Loss_G: 0.8336 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 0.8186 Loss_G: 0.8482 acc: 60.9%\n",
      "[BATCH 130/149] Loss_D: 0.7812 Loss_G: 0.8390 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.7740 Loss_G: 0.8231 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.8008 Loss_G: 0.8217 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.8743 Loss_G: 0.8318 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.7469 Loss_G: 0.8194 acc: 67.2%\n",
      "[EPOCH 5200] TEST ACC is : 73.0%\n",
      "[BATCH 135/149] Loss_D: 0.8591 Loss_G: 0.8256 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.8146 Loss_G: 0.8335 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8032 Loss_G: 0.8254 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.7810 Loss_G: 0.8205 acc: 64.1%\n",
      "[BATCH 139/149] Loss_D: 0.8194 Loss_G: 0.8224 acc: 53.1%\n",
      "[BATCH 140/149] Loss_D: 0.7814 Loss_G: 0.8227 acc: 54.7%\n",
      "[BATCH 141/149] Loss_D: 0.8094 Loss_G: 0.8247 acc: 70.3%\n",
      "[BATCH 142/149] Loss_D: 0.7875 Loss_G: 0.8248 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7924 Loss_G: 0.8239 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.7736 Loss_G: 0.8198 acc: 56.2%\n",
      "[BATCH 145/149] Loss_D: 0.8736 Loss_G: 0.8296 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.8541 Loss_G: 0.8578 acc: 59.4%\n",
      "[BATCH 147/149] Loss_D: 0.8155 Loss_G: 0.8367 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.8830 Loss_G: 0.8545 acc: 64.1%\n",
      "[BATCH 149/149] Loss_D: 0.8143 Loss_G: 0.8420 acc: 65.6%\n",
      "-----THE [35/50] epoch end-----\n",
      "-----THE [36/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7704 Loss_G: 0.8240 acc: 76.6%\n",
      "[BATCH 2/149] Loss_D: 0.8129 Loss_G: 0.8248 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.8395 Loss_G: 0.8304 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.8196 Loss_G: 0.8283 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.8022 Loss_G: 0.8229 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.7835 Loss_G: 0.8168 acc: 57.8%\n",
      "[BATCH 7/149] Loss_D: 0.7943 Loss_G: 0.8191 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.7919 Loss_G: 0.8217 acc: 62.5%\n",
      "[BATCH 9/149] Loss_D: 0.8045 Loss_G: 0.8289 acc: 57.8%\n",
      "[BATCH 10/149] Loss_D: 0.8532 Loss_G: 0.8299 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.7477 Loss_G: 0.8208 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.8361 Loss_G: 0.8255 acc: 71.9%\n",
      "[BATCH 13/149] Loss_D: 0.7470 Loss_G: 0.8201 acc: 67.2%\n",
      "[BATCH 14/149] Loss_D: 0.8308 Loss_G: 0.8243 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.7715 Loss_G: 0.8189 acc: 73.4%\n",
      "[BATCH 16/149] Loss_D: 0.8726 Loss_G: 0.8279 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.8151 Loss_G: 0.8231 acc: 56.2%\n",
      "[BATCH 18/149] Loss_D: 0.8693 Loss_G: 0.8336 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.8526 Loss_G: 0.8453 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.7729 Loss_G: 0.8282 acc: 67.2%\n",
      "[BATCH 21/149] Loss_D: 0.8616 Loss_G: 0.8320 acc: 64.1%\n",
      "[BATCH 22/149] Loss_D: 0.8890 Loss_G: 0.8453 acc: 71.9%\n",
      "[BATCH 23/149] Loss_D: 0.8463 Loss_G: 0.8537 acc: 57.8%\n",
      "[BATCH 24/149] Loss_D: 0.8288 Loss_G: 0.8482 acc: 70.3%\n",
      "[BATCH 25/149] Loss_D: 0.8129 Loss_G: 0.8387 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7979 Loss_G: 0.8324 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7797 Loss_G: 0.8263 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7880 Loss_G: 0.8282 acc: 73.4%\n",
      "[BATCH 29/149] Loss_D: 0.8411 Loss_G: 0.8345 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.7599 Loss_G: 0.8246 acc: 60.9%\n",
      "[BATCH 31/149] Loss_D: 0.8006 Loss_G: 0.8164 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.8127 Loss_G: 0.8115 acc: 51.6%\n",
      "[BATCH 33/149] Loss_D: 0.7828 Loss_G: 0.8108 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.7485 Loss_G: 0.8081 acc: 73.4%\n",
      "[BATCH 35/149] Loss_D: 0.7776 Loss_G: 0.8072 acc: 59.4%\n",
      "[EPOCH 5250] TEST ACC is : 73.0%\n",
      "[BATCH 36/149] Loss_D: 0.7752 Loss_G: 0.8103 acc: 57.8%\n",
      "[BATCH 37/149] Loss_D: 0.7781 Loss_G: 0.8100 acc: 53.1%\n",
      "[BATCH 38/149] Loss_D: 0.8617 Loss_G: 0.8185 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.8010 Loss_G: 0.8186 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.8744 Loss_G: 0.8288 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.7802 Loss_G: 0.8288 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7972 Loss_G: 0.8262 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.7757 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.7832 Loss_G: 0.8153 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.8460 Loss_G: 0.8347 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.7725 Loss_G: 0.8305 acc: 70.3%\n",
      "[BATCH 47/149] Loss_D: 0.7520 Loss_G: 0.8180 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.8003 Loss_G: 0.8149 acc: 54.7%\n",
      "[BATCH 49/149] Loss_D: 0.8177 Loss_G: 0.8230 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.8053 Loss_G: 0.8261 acc: 56.2%\n",
      "[BATCH 51/149] Loss_D: 0.8254 Loss_G: 0.8269 acc: 56.2%\n",
      "[BATCH 52/149] Loss_D: 0.7964 Loss_G: 0.8232 acc: 62.5%\n",
      "[BATCH 53/149] Loss_D: 0.7851 Loss_G: 0.8165 acc: 56.2%\n",
      "[BATCH 54/149] Loss_D: 0.7951 Loss_G: 0.8144 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.7652 Loss_G: 0.8103 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.7881 Loss_G: 0.8144 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.8494 Loss_G: 0.8313 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.8022 Loss_G: 0.8321 acc: 62.5%\n",
      "[BATCH 59/149] Loss_D: 0.8304 Loss_G: 0.8327 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7931 Loss_G: 0.8292 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7923 Loss_G: 0.8260 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.7469 Loss_G: 0.8153 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.8142 Loss_G: 0.8186 acc: 62.5%\n",
      "[BATCH 64/149] Loss_D: 0.7722 Loss_G: 0.8200 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8263 Loss_G: 0.8273 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.8417 Loss_G: 0.8370 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.8143 Loss_G: 0.8381 acc: 54.7%\n",
      "[BATCH 68/149] Loss_D: 0.8303 Loss_G: 0.8289 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.7692 Loss_G: 0.8209 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.7361 Loss_G: 0.8081 acc: 54.7%\n",
      "[BATCH 71/149] Loss_D: 0.8136 Loss_G: 0.8148 acc: 60.9%\n",
      "[BATCH 72/149] Loss_D: 0.8067 Loss_G: 0.8186 acc: 65.6%\n",
      "[BATCH 73/149] Loss_D: 0.8213 Loss_G: 0.8268 acc: 54.7%\n",
      "[BATCH 74/149] Loss_D: 0.8059 Loss_G: 0.8376 acc: 57.8%\n",
      "[BATCH 75/149] Loss_D: 0.8285 Loss_G: 0.8322 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7875 Loss_G: 0.8232 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.8286 Loss_G: 0.8247 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.8178 Loss_G: 0.8269 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.8019 Loss_G: 0.8306 acc: 65.6%\n",
      "[BATCH 80/149] Loss_D: 0.7812 Loss_G: 0.8250 acc: 57.8%\n",
      "[BATCH 81/149] Loss_D: 0.7871 Loss_G: 0.8204 acc: 75.0%\n",
      "[BATCH 82/149] Loss_D: 0.7896 Loss_G: 0.8215 acc: 59.4%\n",
      "[BATCH 83/149] Loss_D: 0.8496 Loss_G: 0.8251 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.7677 Loss_G: 0.8333 acc: 56.2%\n",
      "[BATCH 85/149] Loss_D: 0.8505 Loss_G: 0.8424 acc: 56.2%\n",
      "[EPOCH 5300] TEST ACC is : 71.1%\n",
      "[BATCH 86/149] Loss_D: 0.8769 Loss_G: 0.8547 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.8102 Loss_G: 0.8375 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.8318 Loss_G: 0.8361 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7926 Loss_G: 0.8316 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.8732 Loss_G: 0.8405 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.8218 Loss_G: 0.8355 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.8198 Loss_G: 0.8314 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.8510 Loss_G: 0.8349 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 0.8004 Loss_G: 0.8429 acc: 56.2%\n",
      "[BATCH 95/149] Loss_D: 0.8119 Loss_G: 0.8437 acc: 60.9%\n",
      "[BATCH 96/149] Loss_D: 0.8594 Loss_G: 0.8537 acc: 75.0%\n",
      "[BATCH 97/149] Loss_D: 0.7854 Loss_G: 0.8453 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.8019 Loss_G: 0.8325 acc: 57.8%\n",
      "[BATCH 99/149] Loss_D: 0.7965 Loss_G: 0.8253 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.8261 Loss_G: 0.8323 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.8543 Loss_G: 0.8530 acc: 65.6%\n",
      "[BATCH 102/149] Loss_D: 0.8531 Loss_G: 0.8570 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.7878 Loss_G: 0.8354 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.7908 Loss_G: 0.8209 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.7650 Loss_G: 0.8178 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.8110 Loss_G: 0.8179 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.7541 Loss_G: 0.8101 acc: 56.2%\n",
      "[BATCH 108/149] Loss_D: 0.8249 Loss_G: 0.8211 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.8264 Loss_G: 0.8313 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7733 Loss_G: 0.8236 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.7906 Loss_G: 0.8143 acc: 67.2%\n",
      "[BATCH 112/149] Loss_D: 0.7568 Loss_G: 0.8042 acc: 56.2%\n",
      "[BATCH 113/149] Loss_D: 0.7879 Loss_G: 0.8039 acc: 68.8%\n",
      "[BATCH 114/149] Loss_D: 0.8636 Loss_G: 0.8222 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.8834 Loss_G: 0.8678 acc: 53.1%\n",
      "[BATCH 116/149] Loss_D: 0.8208 Loss_G: 0.8596 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.8250 Loss_G: 0.8413 acc: 70.3%\n",
      "[BATCH 118/149] Loss_D: 0.8560 Loss_G: 0.8453 acc: 68.8%\n",
      "[BATCH 119/149] Loss_D: 0.8078 Loss_G: 0.8380 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7929 Loss_G: 0.8331 acc: 67.2%\n",
      "[BATCH 121/149] Loss_D: 0.7679 Loss_G: 0.8214 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8067 Loss_G: 0.8204 acc: 64.1%\n",
      "[BATCH 123/149] Loss_D: 0.8429 Loss_G: 0.8267 acc: 65.6%\n",
      "[BATCH 124/149] Loss_D: 0.7952 Loss_G: 0.8239 acc: 73.4%\n",
      "[BATCH 125/149] Loss_D: 0.8498 Loss_G: 0.8318 acc: 78.1%\n",
      "[BATCH 126/149] Loss_D: 0.8051 Loss_G: 0.8297 acc: 68.8%\n",
      "[BATCH 127/149] Loss_D: 0.7982 Loss_G: 0.8289 acc: 56.2%\n",
      "[BATCH 128/149] Loss_D: 0.7762 Loss_G: 0.8210 acc: 68.8%\n",
      "[BATCH 129/149] Loss_D: 0.7977 Loss_G: 0.8131 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.7701 Loss_G: 0.8079 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.8155 Loss_G: 0.8164 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.7821 Loss_G: 0.8219 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.7719 Loss_G: 0.8070 acc: 73.4%\n",
      "[BATCH 134/149] Loss_D: 0.7713 Loss_G: 0.8012 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7902 Loss_G: 0.8049 acc: 64.1%\n",
      "[EPOCH 5350] TEST ACC is : 70.9%\n",
      "[BATCH 136/149] Loss_D: 0.8219 Loss_G: 0.8196 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8369 Loss_G: 0.8333 acc: 68.8%\n",
      "[BATCH 138/149] Loss_D: 0.7928 Loss_G: 0.8346 acc: 75.0%\n",
      "[BATCH 139/149] Loss_D: 0.7727 Loss_G: 0.8151 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7981 Loss_G: 0.8121 acc: 65.6%\n",
      "[BATCH 141/149] Loss_D: 0.7734 Loss_G: 0.8146 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8155 Loss_G: 0.8199 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.8182 Loss_G: 0.8234 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.8187 Loss_G: 0.8352 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.8271 Loss_G: 0.8387 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.8666 Loss_G: 0.8504 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.8201 Loss_G: 0.8437 acc: 60.9%\n",
      "[BATCH 148/149] Loss_D: 0.7852 Loss_G: 0.8326 acc: 57.8%\n",
      "[BATCH 149/149] Loss_D: 0.8185 Loss_G: 0.8303 acc: 62.5%\n",
      "-----THE [36/50] epoch end-----\n",
      "-----THE [37/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7963 Loss_G: 0.8214 acc: 75.0%\n",
      "[BATCH 2/149] Loss_D: 0.7805 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.7999 Loss_G: 0.8116 acc: 65.6%\n",
      "[BATCH 4/149] Loss_D: 0.7936 Loss_G: 0.8176 acc: 59.4%\n",
      "[BATCH 5/149] Loss_D: 0.8553 Loss_G: 0.8284 acc: 57.8%\n",
      "[BATCH 6/149] Loss_D: 0.7945 Loss_G: 0.8238 acc: 65.6%\n",
      "[BATCH 7/149] Loss_D: 0.7936 Loss_G: 0.8211 acc: 65.6%\n",
      "[BATCH 8/149] Loss_D: 0.8074 Loss_G: 0.8247 acc: 56.2%\n",
      "[BATCH 9/149] Loss_D: 0.8082 Loss_G: 0.8186 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.8637 Loss_G: 0.8313 acc: 54.7%\n",
      "[BATCH 11/149] Loss_D: 0.8171 Loss_G: 0.8324 acc: 57.8%\n",
      "[BATCH 12/149] Loss_D: 0.7825 Loss_G: 0.8194 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.8029 Loss_G: 0.8175 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.7712 Loss_G: 0.8119 acc: 56.2%\n",
      "[BATCH 15/149] Loss_D: 0.8051 Loss_G: 0.8192 acc: 67.2%\n",
      "[BATCH 16/149] Loss_D: 0.8199 Loss_G: 0.8209 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.7774 Loss_G: 0.8138 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.8491 Loss_G: 0.8215 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.7551 Loss_G: 0.8116 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.8203 Loss_G: 0.8065 acc: 62.5%\n",
      "[BATCH 21/149] Loss_D: 0.7782 Loss_G: 0.8085 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.8419 Loss_G: 0.8268 acc: 71.9%\n",
      "[BATCH 23/149] Loss_D: 0.8061 Loss_G: 0.8271 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.8174 Loss_G: 0.8275 acc: 68.8%\n",
      "[BATCH 25/149] Loss_D: 0.7985 Loss_G: 0.8323 acc: 59.4%\n",
      "[BATCH 26/149] Loss_D: 0.8648 Loss_G: 0.8351 acc: 59.4%\n",
      "[BATCH 27/149] Loss_D: 0.7873 Loss_G: 0.8306 acc: 57.8%\n",
      "[BATCH 28/149] Loss_D: 0.7732 Loss_G: 0.8176 acc: 73.4%\n",
      "[BATCH 29/149] Loss_D: 0.7677 Loss_G: 0.8129 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.7858 Loss_G: 0.8069 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.7980 Loss_G: 0.8094 acc: 71.9%\n",
      "[BATCH 32/149] Loss_D: 0.7831 Loss_G: 0.8116 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.7709 Loss_G: 0.8175 acc: 51.6%\n",
      "[BATCH 34/149] Loss_D: 0.8173 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7621 Loss_G: 0.8206 acc: 53.1%\n",
      "[BATCH 36/149] Loss_D: 0.8090 Loss_G: 0.8199 acc: 70.3%\n",
      "[EPOCH 5400] TEST ACC is : 72.7%\n",
      "[BATCH 37/149] Loss_D: 0.7906 Loss_G: 0.8251 acc: 53.1%\n",
      "[BATCH 38/149] Loss_D: 0.8120 Loss_G: 0.8295 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8322 Loss_G: 0.8349 acc: 68.8%\n",
      "[BATCH 40/149] Loss_D: 0.8151 Loss_G: 0.8432 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7792 Loss_G: 0.8331 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.8056 Loss_G: 0.8404 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.8022 Loss_G: 0.8345 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7945 Loss_G: 0.8292 acc: 57.8%\n",
      "[BATCH 45/149] Loss_D: 0.7771 Loss_G: 0.8235 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.8117 Loss_G: 0.8308 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.7784 Loss_G: 0.8353 acc: 60.9%\n",
      "[BATCH 48/149] Loss_D: 0.7547 Loss_G: 0.8222 acc: 60.9%\n",
      "[BATCH 49/149] Loss_D: 0.8511 Loss_G: 0.8398 acc: 56.2%\n",
      "[BATCH 50/149] Loss_D: 0.8204 Loss_G: 0.8459 acc: 65.6%\n",
      "[BATCH 51/149] Loss_D: 0.8185 Loss_G: 0.8333 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.7935 Loss_G: 0.8294 acc: 62.5%\n",
      "[BATCH 53/149] Loss_D: 0.7617 Loss_G: 0.8265 acc: 56.2%\n",
      "[BATCH 54/149] Loss_D: 0.7778 Loss_G: 0.8116 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.8373 Loss_G: 0.8204 acc: 57.8%\n",
      "[BATCH 56/149] Loss_D: 0.7864 Loss_G: 0.8232 acc: 78.1%\n",
      "[BATCH 57/149] Loss_D: 0.7853 Loss_G: 0.8199 acc: 56.2%\n",
      "[BATCH 58/149] Loss_D: 0.8175 Loss_G: 0.8257 acc: 56.2%\n",
      "[BATCH 59/149] Loss_D: 0.8412 Loss_G: 0.8382 acc: 57.8%\n",
      "[BATCH 60/149] Loss_D: 0.7857 Loss_G: 0.8281 acc: 56.2%\n",
      "[BATCH 61/149] Loss_D: 0.8235 Loss_G: 0.8304 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.7955 Loss_G: 0.8274 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.7951 Loss_G: 0.8278 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.7501 Loss_G: 0.8136 acc: 68.8%\n",
      "[BATCH 65/149] Loss_D: 0.8364 Loss_G: 0.8176 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.8216 Loss_G: 0.8233 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.8104 Loss_G: 0.8236 acc: 64.1%\n",
      "[BATCH 68/149] Loss_D: 0.7730 Loss_G: 0.8165 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.8173 Loss_G: 0.8216 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.8089 Loss_G: 0.8195 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.8119 Loss_G: 0.8274 acc: 70.3%\n",
      "[BATCH 72/149] Loss_D: 0.8314 Loss_G: 0.8358 acc: 59.4%\n",
      "[BATCH 73/149] Loss_D: 0.8166 Loss_G: 0.8269 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.7800 Loss_G: 0.8180 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.7994 Loss_G: 0.8205 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.8054 Loss_G: 0.8308 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.8692 Loss_G: 0.8482 acc: 59.4%\n",
      "[BATCH 78/149] Loss_D: 0.8063 Loss_G: 0.8400 acc: 57.8%\n",
      "[BATCH 79/149] Loss_D: 0.8207 Loss_G: 0.8330 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7976 Loss_G: 0.8213 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.7699 Loss_G: 0.8168 acc: 62.5%\n",
      "[BATCH 82/149] Loss_D: 0.8157 Loss_G: 0.8212 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.7731 Loss_G: 0.8179 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.8494 Loss_G: 0.8286 acc: 64.1%\n",
      "[BATCH 85/149] Loss_D: 0.8341 Loss_G: 0.8376 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.8151 Loss_G: 0.8533 acc: 56.2%\n",
      "[EPOCH 5450] TEST ACC is : 73.2%\n",
      "[BATCH 87/149] Loss_D: 0.7496 Loss_G: 0.8309 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.8152 Loss_G: 0.8198 acc: 62.5%\n",
      "[BATCH 89/149] Loss_D: 0.8314 Loss_G: 0.8185 acc: 53.1%\n",
      "[BATCH 90/149] Loss_D: 0.8300 Loss_G: 0.8287 acc: 51.6%\n",
      "[BATCH 91/149] Loss_D: 0.8103 Loss_G: 0.8217 acc: 57.8%\n",
      "[BATCH 92/149] Loss_D: 0.8410 Loss_G: 0.8230 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.7844 Loss_G: 0.8219 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.8414 Loss_G: 0.8291 acc: 65.6%\n",
      "[BATCH 95/149] Loss_D: 0.7574 Loss_G: 0.8139 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.8033 Loss_G: 0.8169 acc: 57.8%\n",
      "[BATCH 97/149] Loss_D: 0.7874 Loss_G: 0.8149 acc: 78.1%\n",
      "[BATCH 98/149] Loss_D: 0.7628 Loss_G: 0.8092 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.7912 Loss_G: 0.8067 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.7560 Loss_G: 0.8100 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7958 Loss_G: 0.8150 acc: 67.2%\n",
      "[BATCH 102/149] Loss_D: 0.8368 Loss_G: 0.8296 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.8594 Loss_G: 0.8526 acc: 75.0%\n",
      "[BATCH 104/149] Loss_D: 0.7887 Loss_G: 0.8424 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7904 Loss_G: 0.8377 acc: 76.6%\n",
      "[BATCH 106/149] Loss_D: 0.8036 Loss_G: 0.8339 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.7768 Loss_G: 0.8283 acc: 62.5%\n",
      "[BATCH 108/149] Loss_D: 0.7851 Loss_G: 0.8220 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.8373 Loss_G: 0.8230 acc: 56.2%\n",
      "[BATCH 110/149] Loss_D: 0.7811 Loss_G: 0.8362 acc: 70.3%\n",
      "[BATCH 111/149] Loss_D: 0.8018 Loss_G: 0.8238 acc: 51.6%\n",
      "[BATCH 112/149] Loss_D: 0.7882 Loss_G: 0.8161 acc: 59.4%\n",
      "[BATCH 113/149] Loss_D: 0.8326 Loss_G: 0.8191 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.8163 Loss_G: 0.8216 acc: 73.4%\n",
      "[BATCH 115/149] Loss_D: 0.8390 Loss_G: 0.8304 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8113 Loss_G: 0.8279 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.8026 Loss_G: 0.8151 acc: 57.8%\n",
      "[BATCH 118/149] Loss_D: 0.8190 Loss_G: 0.8209 acc: 73.4%\n",
      "[BATCH 119/149] Loss_D: 0.8348 Loss_G: 0.8325 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.8623 Loss_G: 0.8470 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.8213 Loss_G: 0.8422 acc: 56.2%\n",
      "[BATCH 122/149] Loss_D: 0.7862 Loss_G: 0.8247 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.7926 Loss_G: 0.8150 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.8653 Loss_G: 0.8190 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.8061 Loss_G: 0.8275 acc: 60.9%\n",
      "[BATCH 126/149] Loss_D: 0.7942 Loss_G: 0.8370 acc: 64.1%\n",
      "[BATCH 127/149] Loss_D: 0.8493 Loss_G: 0.8395 acc: 56.2%\n",
      "[BATCH 128/149] Loss_D: 0.8280 Loss_G: 0.8480 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 0.8074 Loss_G: 0.8318 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7950 Loss_G: 0.8264 acc: 67.2%\n",
      "[BATCH 131/149] Loss_D: 0.8404 Loss_G: 0.8366 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.7898 Loss_G: 0.8274 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.8221 Loss_G: 0.8391 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.8475 Loss_G: 0.8485 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.8597 Loss_G: 0.8580 acc: 53.1%\n",
      "[BATCH 136/149] Loss_D: 0.8355 Loss_G: 0.8682 acc: 59.4%\n",
      "[EPOCH 5500] TEST ACC is : 74.0%\n",
      "[BATCH 137/149] Loss_D: 0.8016 Loss_G: 0.8574 acc: 73.4%\n",
      "[BATCH 138/149] Loss_D: 0.7941 Loss_G: 0.8389 acc: 53.1%\n",
      "[BATCH 139/149] Loss_D: 0.8219 Loss_G: 0.8369 acc: 71.9%\n",
      "[BATCH 140/149] Loss_D: 0.8102 Loss_G: 0.8383 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.8266 Loss_G: 0.8335 acc: 57.8%\n",
      "[BATCH 142/149] Loss_D: 0.8089 Loss_G: 0.8262 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.8100 Loss_G: 0.8195 acc: 59.4%\n",
      "[BATCH 144/149] Loss_D: 0.7741 Loss_G: 0.8183 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.8337 Loss_G: 0.8198 acc: 53.1%\n",
      "[BATCH 146/149] Loss_D: 0.7875 Loss_G: 0.8138 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.8429 Loss_G: 0.8275 acc: 50.0%\n",
      "[BATCH 148/149] Loss_D: 0.7819 Loss_G: 0.8321 acc: 57.8%\n",
      "[BATCH 149/149] Loss_D: 0.8144 Loss_G: 0.8175 acc: 70.3%\n",
      "-----THE [37/50] epoch end-----\n",
      "-----THE [38/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7607 Loss_G: 0.8055 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.8162 Loss_G: 0.8084 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.9040 Loss_G: 0.8426 acc: 56.2%\n",
      "[BATCH 4/149] Loss_D: 0.8073 Loss_G: 0.8419 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.8452 Loss_G: 0.8297 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7961 Loss_G: 0.8184 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.8674 Loss_G: 0.8282 acc: 73.4%\n",
      "[BATCH 8/149] Loss_D: 0.7914 Loss_G: 0.8236 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.8137 Loss_G: 0.8250 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.8275 Loss_G: 0.8330 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.7802 Loss_G: 0.8242 acc: 62.5%\n",
      "[BATCH 12/149] Loss_D: 0.7756 Loss_G: 0.8203 acc: 57.8%\n",
      "[BATCH 13/149] Loss_D: 0.7916 Loss_G: 0.8200 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.7959 Loss_G: 0.8224 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.8312 Loss_G: 0.8305 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7913 Loss_G: 0.8318 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.7867 Loss_G: 0.8334 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.8115 Loss_G: 0.8319 acc: 64.1%\n",
      "[BATCH 19/149] Loss_D: 0.8039 Loss_G: 0.8222 acc: 73.4%\n",
      "[BATCH 20/149] Loss_D: 0.7850 Loss_G: 0.8198 acc: 53.1%\n",
      "[BATCH 21/149] Loss_D: 0.7800 Loss_G: 0.8163 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.8778 Loss_G: 0.8462 acc: 54.7%\n",
      "[BATCH 23/149] Loss_D: 0.7761 Loss_G: 0.8306 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7957 Loss_G: 0.8166 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.7624 Loss_G: 0.8134 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.8077 Loss_G: 0.8230 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.8254 Loss_G: 0.8310 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7815 Loss_G: 0.8283 acc: 65.6%\n",
      "[BATCH 29/149] Loss_D: 0.8120 Loss_G: 0.8216 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.7846 Loss_G: 0.8270 acc: 57.8%\n",
      "[BATCH 31/149] Loss_D: 0.7749 Loss_G: 0.8205 acc: 60.9%\n",
      "[BATCH 32/149] Loss_D: 0.8161 Loss_G: 0.8184 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7879 Loss_G: 0.8099 acc: 67.2%\n",
      "[BATCH 34/149] Loss_D: 0.8318 Loss_G: 0.8191 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7951 Loss_G: 0.8251 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.8122 Loss_G: 0.8266 acc: 51.6%\n",
      "[BATCH 37/149] Loss_D: 0.7691 Loss_G: 0.8265 acc: 59.4%\n",
      "[EPOCH 5550] TEST ACC is : 72.1%\n",
      "[BATCH 38/149] Loss_D: 0.7766 Loss_G: 0.8233 acc: 70.3%\n",
      "[BATCH 39/149] Loss_D: 0.8006 Loss_G: 0.8255 acc: 54.7%\n",
      "[BATCH 40/149] Loss_D: 0.7853 Loss_G: 0.8343 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.8470 Loss_G: 0.8494 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.8015 Loss_G: 0.8336 acc: 70.3%\n",
      "[BATCH 43/149] Loss_D: 0.7973 Loss_G: 0.8268 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.8754 Loss_G: 0.8327 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7809 Loss_G: 0.8279 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.7721 Loss_G: 0.8208 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.8139 Loss_G: 0.8141 acc: 51.6%\n",
      "[BATCH 48/149] Loss_D: 0.7773 Loss_G: 0.8144 acc: 68.8%\n",
      "[BATCH 49/149] Loss_D: 0.7668 Loss_G: 0.8073 acc: 64.1%\n",
      "[BATCH 50/149] Loss_D: 0.8457 Loss_G: 0.8071 acc: 70.3%\n",
      "[BATCH 51/149] Loss_D: 0.7795 Loss_G: 0.8110 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7873 Loss_G: 0.8139 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.8111 Loss_G: 0.8168 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.8213 Loss_G: 0.8302 acc: 59.4%\n",
      "[BATCH 55/149] Loss_D: 0.7762 Loss_G: 0.8275 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.7741 Loss_G: 0.8185 acc: 70.3%\n",
      "[BATCH 57/149] Loss_D: 0.7968 Loss_G: 0.8217 acc: 67.2%\n",
      "[BATCH 58/149] Loss_D: 0.8523 Loss_G: 0.8412 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.8158 Loss_G: 0.8483 acc: 73.4%\n",
      "[BATCH 60/149] Loss_D: 0.8411 Loss_G: 0.8429 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.7629 Loss_G: 0.8385 acc: 60.9%\n",
      "[BATCH 62/149] Loss_D: 0.8025 Loss_G: 0.8338 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7846 Loss_G: 0.8244 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.8105 Loss_G: 0.8300 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.8323 Loss_G: 0.8344 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.8394 Loss_G: 0.8419 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7636 Loss_G: 0.8216 acc: 56.2%\n",
      "[BATCH 68/149] Loss_D: 0.8141 Loss_G: 0.8299 acc: 62.5%\n",
      "[BATCH 69/149] Loss_D: 0.7733 Loss_G: 0.8157 acc: 56.2%\n",
      "[BATCH 70/149] Loss_D: 0.8065 Loss_G: 0.8138 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.8813 Loss_G: 0.8489 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.8710 Loss_G: 0.8474 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.8047 Loss_G: 0.8340 acc: 48.4%\n",
      "[BATCH 74/149] Loss_D: 0.8071 Loss_G: 0.8143 acc: 54.7%\n",
      "[BATCH 75/149] Loss_D: 0.8286 Loss_G: 0.8157 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.8131 Loss_G: 0.8238 acc: 75.0%\n",
      "[BATCH 77/149] Loss_D: 0.8025 Loss_G: 0.8197 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.8359 Loss_G: 0.8270 acc: 56.2%\n",
      "[BATCH 79/149] Loss_D: 0.8086 Loss_G: 0.8364 acc: 65.6%\n",
      "[BATCH 80/149] Loss_D: 0.8359 Loss_G: 0.8374 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7929 Loss_G: 0.8339 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.8041 Loss_G: 0.8246 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8013 Loss_G: 0.8284 acc: 73.4%\n",
      "[BATCH 84/149] Loss_D: 0.8432 Loss_G: 0.8451 acc: 54.7%\n",
      "[BATCH 85/149] Loss_D: 0.8026 Loss_G: 0.8411 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.8130 Loss_G: 0.8449 acc: 73.4%\n",
      "[BATCH 87/149] Loss_D: 0.8279 Loss_G: 0.8294 acc: 60.9%\n",
      "[EPOCH 5600] TEST ACC is : 69.5%\n",
      "[BATCH 88/149] Loss_D: 0.8101 Loss_G: 0.8226 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.8067 Loss_G: 0.8266 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.7795 Loss_G: 0.8201 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.8195 Loss_G: 0.8231 acc: 62.5%\n",
      "[BATCH 92/149] Loss_D: 0.8008 Loss_G: 0.8275 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.7791 Loss_G: 0.8226 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 0.8268 Loss_G: 0.8280 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.8199 Loss_G: 0.8322 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7879 Loss_G: 0.8200 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7985 Loss_G: 0.8114 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.8306 Loss_G: 0.8159 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7925 Loss_G: 0.8182 acc: 50.0%\n",
      "[BATCH 100/149] Loss_D: 0.8285 Loss_G: 0.8206 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.8091 Loss_G: 0.8201 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.8136 Loss_G: 0.8246 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.7586 Loss_G: 0.8192 acc: 67.2%\n",
      "[BATCH 104/149] Loss_D: 0.7840 Loss_G: 0.8159 acc: 73.4%\n",
      "[BATCH 105/149] Loss_D: 0.8033 Loss_G: 0.8251 acc: 57.8%\n",
      "[BATCH 106/149] Loss_D: 0.7946 Loss_G: 0.8302 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.8573 Loss_G: 0.8510 acc: 57.8%\n",
      "[BATCH 108/149] Loss_D: 0.7922 Loss_G: 0.8487 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.7694 Loss_G: 0.8421 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.8608 Loss_G: 0.8493 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.8635 Loss_G: 0.8636 acc: 65.6%\n",
      "[BATCH 112/149] Loss_D: 0.8213 Loss_G: 0.8497 acc: 76.6%\n",
      "[BATCH 113/149] Loss_D: 0.8126 Loss_G: 0.8311 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.8244 Loss_G: 0.8382 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.7943 Loss_G: 0.8387 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.8186 Loss_G: 0.8598 acc: 71.9%\n",
      "[BATCH 117/149] Loss_D: 0.7419 Loss_G: 0.8258 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.8193 Loss_G: 0.8281 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.8435 Loss_G: 0.8215 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.8647 Loss_G: 0.8331 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.7751 Loss_G: 0.8246 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.8058 Loss_G: 0.8229 acc: 53.1%\n",
      "[BATCH 123/149] Loss_D: 0.8038 Loss_G: 0.8283 acc: 75.0%\n",
      "[BATCH 124/149] Loss_D: 0.8257 Loss_G: 0.8269 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7826 Loss_G: 0.8301 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8139 Loss_G: 0.8237 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.8036 Loss_G: 0.8187 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7777 Loss_G: 0.8124 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.7729 Loss_G: 0.8133 acc: 68.8%\n",
      "[BATCH 130/149] Loss_D: 0.7845 Loss_G: 0.8111 acc: 68.8%\n",
      "[BATCH 131/149] Loss_D: 0.7781 Loss_G: 0.8136 acc: 75.0%\n",
      "[BATCH 132/149] Loss_D: 0.7902 Loss_G: 0.8181 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.8306 Loss_G: 0.8312 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.8134 Loss_G: 0.8248 acc: 56.2%\n",
      "[BATCH 135/149] Loss_D: 0.8691 Loss_G: 0.8279 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.8054 Loss_G: 0.8146 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7818 Loss_G: 0.8061 acc: 70.3%\n",
      "[EPOCH 5650] TEST ACC is : 73.6%\n",
      "[BATCH 138/149] Loss_D: 0.7872 Loss_G: 0.8059 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7828 Loss_G: 0.8059 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.8280 Loss_G: 0.8169 acc: 54.7%\n",
      "[BATCH 141/149] Loss_D: 0.7950 Loss_G: 0.8210 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.8073 Loss_G: 0.8108 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.7979 Loss_G: 0.8076 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.7967 Loss_G: 0.8079 acc: 73.4%\n",
      "[BATCH 145/149] Loss_D: 0.7956 Loss_G: 0.8173 acc: 57.8%\n",
      "[BATCH 146/149] Loss_D: 0.7555 Loss_G: 0.8104 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7676 Loss_G: 0.8062 acc: 54.7%\n",
      "[BATCH 148/149] Loss_D: 0.7922 Loss_G: 0.8094 acc: 68.8%\n",
      "[BATCH 149/149] Loss_D: 0.7963 Loss_G: 0.8181 acc: 75.0%\n",
      "-----THE [38/50] epoch end-----\n",
      "-----THE [39/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7882 Loss_G: 0.8286 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.8579 Loss_G: 0.8525 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.8240 Loss_G: 0.8493 acc: 65.6%\n",
      "[BATCH 4/149] Loss_D: 0.7854 Loss_G: 0.8307 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.8084 Loss_G: 0.8212 acc: 54.7%\n",
      "[BATCH 6/149] Loss_D: 0.8026 Loss_G: 0.8198 acc: 59.4%\n",
      "[BATCH 7/149] Loss_D: 0.8431 Loss_G: 0.8321 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.8293 Loss_G: 0.8492 acc: 56.2%\n",
      "[BATCH 9/149] Loss_D: 0.8504 Loss_G: 0.8362 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.7667 Loss_G: 0.8187 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.8255 Loss_G: 0.8184 acc: 70.3%\n",
      "[BATCH 12/149] Loss_D: 0.8514 Loss_G: 0.8379 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.8095 Loss_G: 0.8363 acc: 59.4%\n",
      "[BATCH 14/149] Loss_D: 0.8234 Loss_G: 0.8351 acc: 76.6%\n",
      "[BATCH 15/149] Loss_D: 0.8363 Loss_G: 0.8402 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7963 Loss_G: 0.8266 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.7980 Loss_G: 0.8293 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.8003 Loss_G: 0.8265 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.7654 Loss_G: 0.8133 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7761 Loss_G: 0.8066 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.8075 Loss_G: 0.8168 acc: 71.9%\n",
      "[BATCH 22/149] Loss_D: 0.7932 Loss_G: 0.8123 acc: 56.2%\n",
      "[BATCH 23/149] Loss_D: 0.8253 Loss_G: 0.8232 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.8065 Loss_G: 0.8152 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.7695 Loss_G: 0.8089 acc: 45.3%\n",
      "[BATCH 26/149] Loss_D: 0.7659 Loss_G: 0.8018 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.8025 Loss_G: 0.8091 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.8038 Loss_G: 0.8238 acc: 53.1%\n",
      "[BATCH 29/149] Loss_D: 0.7534 Loss_G: 0.8234 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.8010 Loss_G: 0.8208 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.8281 Loss_G: 0.8400 acc: 79.7%\n",
      "[BATCH 32/149] Loss_D: 0.7845 Loss_G: 0.8427 acc: 60.9%\n",
      "[BATCH 33/149] Loss_D: 0.8236 Loss_G: 0.8373 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7730 Loss_G: 0.8311 acc: 51.6%\n",
      "[BATCH 35/149] Loss_D: 0.7780 Loss_G: 0.8212 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.8057 Loss_G: 0.8171 acc: 56.2%\n",
      "[BATCH 37/149] Loss_D: 0.8002 Loss_G: 0.8122 acc: 68.8%\n",
      "[BATCH 38/149] Loss_D: 0.7721 Loss_G: 0.8091 acc: 64.1%\n",
      "[EPOCH 5700] TEST ACC is : 72.9%\n",
      "[BATCH 39/149] Loss_D: 0.8049 Loss_G: 0.8038 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.8510 Loss_G: 0.8194 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7816 Loss_G: 0.8251 acc: 50.0%\n",
      "[BATCH 42/149] Loss_D: 0.8152 Loss_G: 0.8190 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.7756 Loss_G: 0.8139 acc: 57.8%\n",
      "[BATCH 44/149] Loss_D: 0.7755 Loss_G: 0.8083 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7535 Loss_G: 0.8042 acc: 54.7%\n",
      "[BATCH 46/149] Loss_D: 0.7588 Loss_G: 0.7989 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.8094 Loss_G: 0.8086 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.8491 Loss_G: 0.8350 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7621 Loss_G: 0.8310 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.7954 Loss_G: 0.8138 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.8331 Loss_G: 0.8187 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.8357 Loss_G: 0.8287 acc: 67.2%\n",
      "[BATCH 53/149] Loss_D: 0.8690 Loss_G: 0.8569 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.7936 Loss_G: 0.8462 acc: 57.8%\n",
      "[BATCH 55/149] Loss_D: 0.7623 Loss_G: 0.8369 acc: 71.9%\n",
      "[BATCH 56/149] Loss_D: 0.8286 Loss_G: 0.8294 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.8173 Loss_G: 0.8328 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.8207 Loss_G: 0.8299 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.8131 Loss_G: 0.8362 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.7941 Loss_G: 0.8291 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.8192 Loss_G: 0.8342 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.8227 Loss_G: 0.8406 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.8619 Loss_G: 0.8510 acc: 71.9%\n",
      "[BATCH 64/149] Loss_D: 0.8208 Loss_G: 0.8358 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.7971 Loss_G: 0.8325 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.8664 Loss_G: 0.8381 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.8429 Loss_G: 0.8420 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.7957 Loss_G: 0.8417 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.8101 Loss_G: 0.8291 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.8526 Loss_G: 0.8283 acc: 59.4%\n",
      "[BATCH 71/149] Loss_D: 0.8097 Loss_G: 0.8233 acc: 59.4%\n",
      "[BATCH 72/149] Loss_D: 0.8147 Loss_G: 0.8186 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.8185 Loss_G: 0.8134 acc: 65.6%\n",
      "[BATCH 74/149] Loss_D: 0.7986 Loss_G: 0.8169 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.8449 Loss_G: 0.8236 acc: 62.5%\n",
      "[BATCH 76/149] Loss_D: 0.8180 Loss_G: 0.8250 acc: 64.1%\n",
      "[BATCH 77/149] Loss_D: 0.7869 Loss_G: 0.8267 acc: 48.4%\n",
      "[BATCH 78/149] Loss_D: 0.8257 Loss_G: 0.8229 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.8539 Loss_G: 0.8375 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7769 Loss_G: 0.8337 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7830 Loss_G: 0.8156 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.8541 Loss_G: 0.8265 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.8428 Loss_G: 0.8359 acc: 57.8%\n",
      "[BATCH 84/149] Loss_D: 0.8040 Loss_G: 0.8305 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.8034 Loss_G: 0.8252 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.8088 Loss_G: 0.8196 acc: 64.1%\n",
      "[BATCH 87/149] Loss_D: 0.7869 Loss_G: 0.8187 acc: 57.8%\n",
      "[BATCH 88/149] Loss_D: 0.7902 Loss_G: 0.8212 acc: 71.9%\n",
      "[EPOCH 5750] TEST ACC is : 69.5%\n",
      "[BATCH 89/149] Loss_D: 0.8251 Loss_G: 0.8269 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.8680 Loss_G: 0.8479 acc: 73.4%\n",
      "[BATCH 91/149] Loss_D: 0.7821 Loss_G: 0.8299 acc: 59.4%\n",
      "[BATCH 92/149] Loss_D: 0.7955 Loss_G: 0.8184 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.7810 Loss_G: 0.8164 acc: 53.1%\n",
      "[BATCH 94/149] Loss_D: 0.7825 Loss_G: 0.8000 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7997 Loss_G: 0.8081 acc: 65.6%\n",
      "[BATCH 96/149] Loss_D: 0.7782 Loss_G: 0.8079 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7966 Loss_G: 0.8108 acc: 62.5%\n",
      "[BATCH 98/149] Loss_D: 0.8220 Loss_G: 0.8254 acc: 56.2%\n",
      "[BATCH 99/149] Loss_D: 0.8233 Loss_G: 0.8157 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.8081 Loss_G: 0.8206 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.8301 Loss_G: 0.8268 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.7853 Loss_G: 0.8297 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.8057 Loss_G: 0.8341 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7718 Loss_G: 0.8248 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7974 Loss_G: 0.8211 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.8740 Loss_G: 0.8501 acc: 54.7%\n",
      "[BATCH 107/149] Loss_D: 0.7643 Loss_G: 0.8325 acc: 59.4%\n",
      "[BATCH 108/149] Loss_D: 0.7706 Loss_G: 0.8144 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.8008 Loss_G: 0.8161 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.7791 Loss_G: 0.8177 acc: 59.4%\n",
      "[BATCH 111/149] Loss_D: 0.7753 Loss_G: 0.8143 acc: 57.8%\n",
      "[BATCH 112/149] Loss_D: 0.7861 Loss_G: 0.8206 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.7856 Loss_G: 0.8127 acc: 73.4%\n",
      "[BATCH 114/149] Loss_D: 0.7672 Loss_G: 0.8012 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.8150 Loss_G: 0.8140 acc: 68.8%\n",
      "[BATCH 116/149] Loss_D: 0.7869 Loss_G: 0.8121 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.8050 Loss_G: 0.8179 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.7655 Loss_G: 0.8037 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.8135 Loss_G: 0.8094 acc: 57.8%\n",
      "[BATCH 120/149] Loss_D: 0.7935 Loss_G: 0.8167 acc: 71.9%\n",
      "[BATCH 121/149] Loss_D: 0.7490 Loss_G: 0.8092 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.8015 Loss_G: 0.8134 acc: 53.1%\n",
      "[BATCH 123/149] Loss_D: 0.7938 Loss_G: 0.8180 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.7790 Loss_G: 0.8205 acc: 71.9%\n",
      "[BATCH 125/149] Loss_D: 0.8082 Loss_G: 0.8182 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8636 Loss_G: 0.8395 acc: 78.1%\n",
      "[BATCH 127/149] Loss_D: 0.8282 Loss_G: 0.8411 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.8216 Loss_G: 0.8421 acc: 71.9%\n",
      "[BATCH 129/149] Loss_D: 0.8123 Loss_G: 0.8327 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.7990 Loss_G: 0.8301 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.8006 Loss_G: 0.8253 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.7755 Loss_G: 0.8185 acc: 59.4%\n",
      "[BATCH 133/149] Loss_D: 0.8086 Loss_G: 0.8232 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7779 Loss_G: 0.8128 acc: 54.7%\n",
      "[BATCH 135/149] Loss_D: 0.8591 Loss_G: 0.8292 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.7905 Loss_G: 0.8305 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.8974 Loss_G: 0.8484 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.8026 Loss_G: 0.8413 acc: 71.9%\n",
      "[EPOCH 5800] TEST ACC is : 72.9%\n",
      "[BATCH 139/149] Loss_D: 0.7866 Loss_G: 0.8319 acc: 59.4%\n",
      "[BATCH 140/149] Loss_D: 0.7729 Loss_G: 0.8108 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7805 Loss_G: 0.8085 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8685 Loss_G: 0.8179 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.8096 Loss_G: 0.8341 acc: 54.7%\n",
      "[BATCH 144/149] Loss_D: 0.8184 Loss_G: 0.8469 acc: 56.2%\n",
      "[BATCH 145/149] Loss_D: 0.8009 Loss_G: 0.8263 acc: 59.4%\n",
      "[BATCH 146/149] Loss_D: 0.7707 Loss_G: 0.8109 acc: 70.3%\n",
      "[BATCH 147/149] Loss_D: 0.7985 Loss_G: 0.8057 acc: 59.4%\n",
      "[BATCH 148/149] Loss_D: 0.7734 Loss_G: 0.8048 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7814 Loss_G: 0.8085 acc: 65.6%\n",
      "-----THE [39/50] epoch end-----\n",
      "-----THE [40/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7940 Loss_G: 0.8053 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.7848 Loss_G: 0.8052 acc: 59.4%\n",
      "[BATCH 3/149] Loss_D: 0.8112 Loss_G: 0.8141 acc: 65.6%\n",
      "[BATCH 4/149] Loss_D: 0.8197 Loss_G: 0.8253 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.8818 Loss_G: 0.8450 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.8233 Loss_G: 0.8537 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.8364 Loss_G: 0.8466 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.8439 Loss_G: 0.8540 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.7902 Loss_G: 0.8365 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.8122 Loss_G: 0.8236 acc: 68.8%\n",
      "[BATCH 11/149] Loss_D: 0.7612 Loss_G: 0.8165 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7766 Loss_G: 0.8121 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.7809 Loss_G: 0.8149 acc: 65.6%\n",
      "[BATCH 14/149] Loss_D: 0.8222 Loss_G: 0.8277 acc: 70.3%\n",
      "[BATCH 15/149] Loss_D: 0.7968 Loss_G: 0.8326 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.8045 Loss_G: 0.8194 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.8120 Loss_G: 0.8142 acc: 54.7%\n",
      "[BATCH 18/149] Loss_D: 0.8123 Loss_G: 0.8188 acc: 68.8%\n",
      "[BATCH 19/149] Loss_D: 0.7959 Loss_G: 0.8292 acc: 57.8%\n",
      "[BATCH 20/149] Loss_D: 0.8320 Loss_G: 0.8465 acc: 53.1%\n",
      "[BATCH 21/149] Loss_D: 0.8073 Loss_G: 0.8276 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.7965 Loss_G: 0.8164 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.8364 Loss_G: 0.8182 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7967 Loss_G: 0.8270 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.7913 Loss_G: 0.8356 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.7993 Loss_G: 0.8302 acc: 50.0%\n",
      "[BATCH 27/149] Loss_D: 0.8063 Loss_G: 0.8329 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.8252 Loss_G: 0.8327 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.7905 Loss_G: 0.8261 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.8463 Loss_G: 0.8298 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.7870 Loss_G: 0.8272 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.7930 Loss_G: 0.8289 acc: 54.7%\n",
      "[BATCH 33/149] Loss_D: 0.8353 Loss_G: 0.8334 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.8326 Loss_G: 0.8443 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7962 Loss_G: 0.8262 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.8410 Loss_G: 0.8283 acc: 59.4%\n",
      "[BATCH 37/149] Loss_D: 0.8469 Loss_G: 0.8460 acc: 48.4%\n",
      "[BATCH 38/149] Loss_D: 0.7957 Loss_G: 0.8492 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.8006 Loss_G: 0.8346 acc: 67.2%\n",
      "[EPOCH 5850] TEST ACC is : 72.9%\n",
      "[BATCH 40/149] Loss_D: 0.7905 Loss_G: 0.8244 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.8367 Loss_G: 0.8330 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7705 Loss_G: 0.8351 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.7981 Loss_G: 0.8276 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.8167 Loss_G: 0.8188 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7996 Loss_G: 0.8158 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.8312 Loss_G: 0.8251 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.8315 Loss_G: 0.8344 acc: 56.2%\n",
      "[BATCH 48/149] Loss_D: 0.7604 Loss_G: 0.8287 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.7728 Loss_G: 0.8118 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.8066 Loss_G: 0.8148 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7632 Loss_G: 0.8186 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7973 Loss_G: 0.8174 acc: 62.5%\n",
      "[BATCH 53/149] Loss_D: 0.8626 Loss_G: 0.8298 acc: 71.9%\n",
      "[BATCH 54/149] Loss_D: 0.8173 Loss_G: 0.8387 acc: 57.8%\n",
      "[BATCH 55/149] Loss_D: 0.7997 Loss_G: 0.8290 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.8164 Loss_G: 0.8232 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.8097 Loss_G: 0.8205 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.8011 Loss_G: 0.8239 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7797 Loss_G: 0.8198 acc: 60.9%\n",
      "[BATCH 60/149] Loss_D: 0.8088 Loss_G: 0.8189 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.8144 Loss_G: 0.8214 acc: 54.7%\n",
      "[BATCH 62/149] Loss_D: 0.8096 Loss_G: 0.8247 acc: 46.9%\n",
      "[BATCH 63/149] Loss_D: 0.7926 Loss_G: 0.8146 acc: 56.2%\n",
      "[BATCH 64/149] Loss_D: 0.8099 Loss_G: 0.8121 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.8221 Loss_G: 0.8268 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7932 Loss_G: 0.8300 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7969 Loss_G: 0.8360 acc: 73.4%\n",
      "[BATCH 68/149] Loss_D: 0.8174 Loss_G: 0.8366 acc: 62.5%\n",
      "[BATCH 69/149] Loss_D: 0.7952 Loss_G: 0.8339 acc: 68.8%\n",
      "[BATCH 70/149] Loss_D: 0.8307 Loss_G: 0.8433 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.7857 Loss_G: 0.8324 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7859 Loss_G: 0.8295 acc: 59.4%\n",
      "[BATCH 73/149] Loss_D: 0.7794 Loss_G: 0.8182 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.8105 Loss_G: 0.8212 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.7820 Loss_G: 0.8225 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7793 Loss_G: 0.8080 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.8073 Loss_G: 0.8065 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.8172 Loss_G: 0.8097 acc: 54.7%\n",
      "[BATCH 79/149] Loss_D: 0.7920 Loss_G: 0.8149 acc: 62.5%\n",
      "[BATCH 80/149] Loss_D: 0.7707 Loss_G: 0.8112 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7814 Loss_G: 0.8064 acc: 53.1%\n",
      "[BATCH 82/149] Loss_D: 0.7682 Loss_G: 0.8049 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8317 Loss_G: 0.8192 acc: 79.7%\n",
      "[BATCH 84/149] Loss_D: 0.8551 Loss_G: 0.8341 acc: 56.2%\n",
      "[BATCH 85/149] Loss_D: 0.7635 Loss_G: 0.8161 acc: 59.4%\n",
      "[BATCH 86/149] Loss_D: 0.7999 Loss_G: 0.8025 acc: 62.5%\n",
      "[BATCH 87/149] Loss_D: 0.7879 Loss_G: 0.8094 acc: 57.8%\n",
      "[BATCH 88/149] Loss_D: 0.7941 Loss_G: 0.8104 acc: 70.3%\n",
      "[BATCH 89/149] Loss_D: 0.7895 Loss_G: 0.8190 acc: 64.1%\n",
      "[EPOCH 5900] TEST ACC is : 74.8%\n",
      "[BATCH 90/149] Loss_D: 0.7962 Loss_G: 0.8213 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.8279 Loss_G: 0.8318 acc: 59.4%\n",
      "[BATCH 92/149] Loss_D: 0.8057 Loss_G: 0.8356 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.7686 Loss_G: 0.8220 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7678 Loss_G: 0.8110 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.8275 Loss_G: 0.8179 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.8079 Loss_G: 0.8223 acc: 54.7%\n",
      "[BATCH 97/149] Loss_D: 0.8236 Loss_G: 0.8219 acc: 71.9%\n",
      "[BATCH 98/149] Loss_D: 0.8056 Loss_G: 0.8240 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.8702 Loss_G: 0.8466 acc: 75.0%\n",
      "[BATCH 100/149] Loss_D: 0.7753 Loss_G: 0.8519 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.7624 Loss_G: 0.8343 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.8064 Loss_G: 0.8267 acc: 53.1%\n",
      "[BATCH 103/149] Loss_D: 0.7623 Loss_G: 0.8133 acc: 50.0%\n",
      "[BATCH 104/149] Loss_D: 0.8114 Loss_G: 0.8102 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.8151 Loss_G: 0.8208 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.7739 Loss_G: 0.8153 acc: 68.8%\n",
      "[BATCH 107/149] Loss_D: 0.8525 Loss_G: 0.8239 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7702 Loss_G: 0.8194 acc: 60.9%\n",
      "[BATCH 109/149] Loss_D: 0.8005 Loss_G: 0.8167 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7847 Loss_G: 0.8214 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7965 Loss_G: 0.8151 acc: 57.8%\n",
      "[BATCH 112/149] Loss_D: 0.7962 Loss_G: 0.8112 acc: 53.1%\n",
      "[BATCH 113/149] Loss_D: 0.7468 Loss_G: 0.8019 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7727 Loss_G: 0.8078 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.7897 Loss_G: 0.8147 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.7585 Loss_G: 0.8074 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.8420 Loss_G: 0.8232 acc: 51.6%\n",
      "[BATCH 118/149] Loss_D: 0.7845 Loss_G: 0.8168 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.7490 Loss_G: 0.8094 acc: 59.4%\n",
      "[BATCH 120/149] Loss_D: 0.7908 Loss_G: 0.8028 acc: 73.4%\n",
      "[BATCH 121/149] Loss_D: 0.8140 Loss_G: 0.8213 acc: 56.2%\n",
      "[BATCH 122/149] Loss_D: 0.8154 Loss_G: 0.8350 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.8102 Loss_G: 0.8392 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.8041 Loss_G: 0.8277 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.7652 Loss_G: 0.8143 acc: 60.9%\n",
      "[BATCH 126/149] Loss_D: 0.7958 Loss_G: 0.8056 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.8616 Loss_G: 0.8249 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7673 Loss_G: 0.8107 acc: 68.8%\n",
      "[BATCH 129/149] Loss_D: 0.8445 Loss_G: 0.8179 acc: 68.8%\n",
      "[BATCH 130/149] Loss_D: 0.8374 Loss_G: 0.8230 acc: 45.3%\n",
      "[BATCH 131/149] Loss_D: 0.8437 Loss_G: 0.8362 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.8946 Loss_G: 0.8621 acc: 54.7%\n",
      "[BATCH 133/149] Loss_D: 0.8299 Loss_G: 0.8418 acc: 51.6%\n",
      "[BATCH 134/149] Loss_D: 0.7420 Loss_G: 0.8167 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.7620 Loss_G: 0.8029 acc: 62.5%\n",
      "[BATCH 136/149] Loss_D: 0.7941 Loss_G: 0.8007 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.8121 Loss_G: 0.8142 acc: 68.8%\n",
      "[BATCH 138/149] Loss_D: 0.8492 Loss_G: 0.8258 acc: 70.3%\n",
      "[BATCH 139/149] Loss_D: 0.7880 Loss_G: 0.8246 acc: 67.2%\n",
      "[EPOCH 5950] TEST ACC is : 73.2%\n",
      "[BATCH 140/149] Loss_D: 0.8541 Loss_G: 0.8296 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.8580 Loss_G: 0.8408 acc: 76.6%\n",
      "[BATCH 142/149] Loss_D: 0.7974 Loss_G: 0.8368 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.7973 Loss_G: 0.8288 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.8009 Loss_G: 0.8242 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.8197 Loss_G: 0.8298 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.8101 Loss_G: 0.8298 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.8031 Loss_G: 0.8309 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.8354 Loss_G: 0.8342 acc: 64.1%\n",
      "[BATCH 149/149] Loss_D: 0.7851 Loss_G: 0.8260 acc: 57.8%\n",
      "-----THE [40/50] epoch end-----\n",
      "-----THE [41/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8822 Loss_G: 0.8477 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.8033 Loss_G: 0.8428 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.8548 Loss_G: 0.8462 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.8103 Loss_G: 0.8305 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.8032 Loss_G: 0.8229 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.8630 Loss_G: 0.8328 acc: 57.8%\n",
      "[BATCH 7/149] Loss_D: 0.7955 Loss_G: 0.8279 acc: 75.0%\n",
      "[BATCH 8/149] Loss_D: 0.8120 Loss_G: 0.8200 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.7994 Loss_G: 0.8150 acc: 46.9%\n",
      "[BATCH 10/149] Loss_D: 0.7869 Loss_G: 0.8139 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.8158 Loss_G: 0.8176 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.8199 Loss_G: 0.8266 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.8001 Loss_G: 0.8260 acc: 73.4%\n",
      "[BATCH 14/149] Loss_D: 0.7796 Loss_G: 0.8212 acc: 56.2%\n",
      "[BATCH 15/149] Loss_D: 0.8261 Loss_G: 0.8275 acc: 60.9%\n",
      "[BATCH 16/149] Loss_D: 0.8226 Loss_G: 0.8371 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.8350 Loss_G: 0.8454 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.8182 Loss_G: 0.8390 acc: 64.1%\n",
      "[BATCH 19/149] Loss_D: 0.8109 Loss_G: 0.8294 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.7939 Loss_G: 0.8228 acc: 51.6%\n",
      "[BATCH 21/149] Loss_D: 0.7751 Loss_G: 0.8126 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.7798 Loss_G: 0.8083 acc: 68.8%\n",
      "[BATCH 23/149] Loss_D: 0.8407 Loss_G: 0.8238 acc: 50.0%\n",
      "[BATCH 24/149] Loss_D: 0.7795 Loss_G: 0.8186 acc: 70.3%\n",
      "[BATCH 25/149] Loss_D: 0.8120 Loss_G: 0.8220 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7956 Loss_G: 0.8210 acc: 48.4%\n",
      "[BATCH 27/149] Loss_D: 0.7852 Loss_G: 0.8174 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.8724 Loss_G: 0.8365 acc: 56.2%\n",
      "[BATCH 29/149] Loss_D: 0.7668 Loss_G: 0.8248 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.8346 Loss_G: 0.8237 acc: 70.3%\n",
      "[BATCH 31/149] Loss_D: 0.7831 Loss_G: 0.8237 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.8071 Loss_G: 0.8242 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.7877 Loss_G: 0.8247 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.8191 Loss_G: 0.8245 acc: 56.2%\n",
      "[BATCH 35/149] Loss_D: 0.8375 Loss_G: 0.8381 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.7801 Loss_G: 0.8272 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7620 Loss_G: 0.8086 acc: 53.1%\n",
      "[BATCH 38/149] Loss_D: 0.8133 Loss_G: 0.8067 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.7634 Loss_G: 0.8075 acc: 65.6%\n",
      "[BATCH 40/149] Loss_D: 0.7650 Loss_G: 0.8066 acc: 68.8%\n",
      "[EPOCH 6000] TEST ACC is : 73.8%\n",
      "[BATCH 41/149] Loss_D: 0.8808 Loss_G: 0.8329 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.8003 Loss_G: 0.8394 acc: 75.0%\n",
      "[BATCH 43/149] Loss_D: 0.8034 Loss_G: 0.8421 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.7623 Loss_G: 0.8213 acc: 62.5%\n",
      "[BATCH 45/149] Loss_D: 0.7972 Loss_G: 0.8146 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.7925 Loss_G: 0.8130 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.9195 Loss_G: 0.8390 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.8129 Loss_G: 0.8408 acc: 59.4%\n",
      "[BATCH 49/149] Loss_D: 0.8024 Loss_G: 0.8286 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.7926 Loss_G: 0.8298 acc: 56.2%\n",
      "[BATCH 51/149] Loss_D: 0.8832 Loss_G: 0.8391 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.8287 Loss_G: 0.8476 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.7909 Loss_G: 0.8328 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.8046 Loss_G: 0.8310 acc: 59.4%\n",
      "[BATCH 55/149] Loss_D: 0.7864 Loss_G: 0.8208 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.8136 Loss_G: 0.8209 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.8082 Loss_G: 0.8214 acc: 67.2%\n",
      "[BATCH 58/149] Loss_D: 0.8052 Loss_G: 0.8210 acc: 62.5%\n",
      "[BATCH 59/149] Loss_D: 0.7829 Loss_G: 0.8239 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7907 Loss_G: 0.8175 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.8248 Loss_G: 0.8180 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.7730 Loss_G: 0.8207 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7981 Loss_G: 0.8200 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.8920 Loss_G: 0.8482 acc: 53.1%\n",
      "[BATCH 65/149] Loss_D: 0.7618 Loss_G: 0.8306 acc: 68.8%\n",
      "[BATCH 66/149] Loss_D: 0.8304 Loss_G: 0.8210 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.9064 Loss_G: 0.8455 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7804 Loss_G: 0.8306 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.8171 Loss_G: 0.8234 acc: 46.9%\n",
      "[BATCH 70/149] Loss_D: 0.8220 Loss_G: 0.8263 acc: 62.5%\n",
      "[BATCH 71/149] Loss_D: 0.7962 Loss_G: 0.8231 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.8016 Loss_G: 0.8235 acc: 65.6%\n",
      "[BATCH 73/149] Loss_D: 0.8578 Loss_G: 0.8395 acc: 60.9%\n",
      "[BATCH 74/149] Loss_D: 0.8266 Loss_G: 0.8425 acc: 67.2%\n",
      "[BATCH 75/149] Loss_D: 0.7831 Loss_G: 0.8340 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.8026 Loss_G: 0.8245 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.8169 Loss_G: 0.8197 acc: 48.4%\n",
      "[BATCH 78/149] Loss_D: 0.7977 Loss_G: 0.8189 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.8114 Loss_G: 0.8188 acc: 56.2%\n",
      "[BATCH 80/149] Loss_D: 0.8047 Loss_G: 0.8158 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.7721 Loss_G: 0.8136 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.7698 Loss_G: 0.8043 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7483 Loss_G: 0.8028 acc: 71.9%\n",
      "[BATCH 84/149] Loss_D: 0.7943 Loss_G: 0.8023 acc: 64.1%\n",
      "[BATCH 85/149] Loss_D: 0.7962 Loss_G: 0.8008 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.7687 Loss_G: 0.8034 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.7625 Loss_G: 0.7978 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.8224 Loss_G: 0.8043 acc: 56.2%\n",
      "[BATCH 89/149] Loss_D: 0.7961 Loss_G: 0.8191 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.8408 Loss_G: 0.8355 acc: 65.6%\n",
      "[EPOCH 6050] TEST ACC is : 73.2%\n",
      "[BATCH 91/149] Loss_D: 0.8290 Loss_G: 0.8365 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.7837 Loss_G: 0.8215 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.8625 Loss_G: 0.8439 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.8067 Loss_G: 0.8469 acc: 73.4%\n",
      "[BATCH 95/149] Loss_D: 0.8009 Loss_G: 0.8313 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.8112 Loss_G: 0.8269 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7804 Loss_G: 0.8253 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.8239 Loss_G: 0.8268 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.8083 Loss_G: 0.8320 acc: 71.9%\n",
      "[BATCH 100/149] Loss_D: 0.7617 Loss_G: 0.8193 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.7978 Loss_G: 0.8211 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.8112 Loss_G: 0.8287 acc: 59.4%\n",
      "[BATCH 103/149] Loss_D: 0.8458 Loss_G: 0.8487 acc: 73.4%\n",
      "[BATCH 104/149] Loss_D: 0.8093 Loss_G: 0.8558 acc: 71.9%\n",
      "[BATCH 105/149] Loss_D: 0.7598 Loss_G: 0.8191 acc: 51.6%\n",
      "[BATCH 106/149] Loss_D: 0.7765 Loss_G: 0.8063 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.8039 Loss_G: 0.8099 acc: 53.1%\n",
      "[BATCH 108/149] Loss_D: 0.7677 Loss_G: 0.8057 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.8542 Loss_G: 0.8262 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.7989 Loss_G: 0.8277 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7632 Loss_G: 0.8063 acc: 54.7%\n",
      "[BATCH 112/149] Loss_D: 0.8107 Loss_G: 0.8014 acc: 60.9%\n",
      "[BATCH 113/149] Loss_D: 0.7943 Loss_G: 0.8062 acc: 59.4%\n",
      "[BATCH 114/149] Loss_D: 0.7752 Loss_G: 0.8093 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.7621 Loss_G: 0.8032 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.8426 Loss_G: 0.8165 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.8047 Loss_G: 0.8325 acc: 56.2%\n",
      "[BATCH 118/149] Loss_D: 0.8050 Loss_G: 0.8224 acc: 54.7%\n",
      "[BATCH 119/149] Loss_D: 0.8607 Loss_G: 0.8246 acc: 78.1%\n",
      "[BATCH 120/149] Loss_D: 0.8261 Loss_G: 0.8325 acc: 60.9%\n",
      "[BATCH 121/149] Loss_D: 0.7855 Loss_G: 0.8202 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8340 Loss_G: 0.8279 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.7748 Loss_G: 0.8218 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.8125 Loss_G: 0.8199 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.7686 Loss_G: 0.8116 acc: 57.8%\n",
      "[BATCH 126/149] Loss_D: 0.7778 Loss_G: 0.8084 acc: 65.6%\n",
      "[BATCH 127/149] Loss_D: 0.8483 Loss_G: 0.8214 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.7919 Loss_G: 0.8194 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.8425 Loss_G: 0.8298 acc: 67.2%\n",
      "[BATCH 130/149] Loss_D: 0.7865 Loss_G: 0.8232 acc: 67.2%\n",
      "[BATCH 131/149] Loss_D: 0.7928 Loss_G: 0.8202 acc: 57.8%\n",
      "[BATCH 132/149] Loss_D: 0.8124 Loss_G: 0.8234 acc: 71.9%\n",
      "[BATCH 133/149] Loss_D: 0.7651 Loss_G: 0.8141 acc: 60.9%\n",
      "[BATCH 134/149] Loss_D: 0.7823 Loss_G: 0.8104 acc: 54.7%\n",
      "[BATCH 135/149] Loss_D: 0.7570 Loss_G: 0.8049 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.7908 Loss_G: 0.8088 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.7826 Loss_G: 0.8127 acc: 57.8%\n",
      "[BATCH 138/149] Loss_D: 0.8070 Loss_G: 0.8256 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.8112 Loss_G: 0.8290 acc: 54.7%\n",
      "[BATCH 140/149] Loss_D: 0.7589 Loss_G: 0.8090 acc: 62.5%\n",
      "[EPOCH 6100] TEST ACC is : 73.0%\n",
      "[BATCH 141/149] Loss_D: 0.7542 Loss_G: 0.8028 acc: 57.8%\n",
      "[BATCH 142/149] Loss_D: 0.7886 Loss_G: 0.8037 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.7989 Loss_G: 0.8132 acc: 71.9%\n",
      "[BATCH 144/149] Loss_D: 0.7568 Loss_G: 0.8097 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.7899 Loss_G: 0.8118 acc: 64.1%\n",
      "[BATCH 146/149] Loss_D: 0.7970 Loss_G: 0.8180 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.8026 Loss_G: 0.8309 acc: 53.1%\n",
      "[BATCH 148/149] Loss_D: 0.8020 Loss_G: 0.8223 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.8071 Loss_G: 0.8204 acc: 68.8%\n",
      "-----THE [41/50] epoch end-----\n",
      "-----THE [42/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8092 Loss_G: 0.8308 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.7930 Loss_G: 0.8319 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.8153 Loss_G: 0.8350 acc: 71.9%\n",
      "[BATCH 4/149] Loss_D: 0.7759 Loss_G: 0.8320 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.7801 Loss_G: 0.8258 acc: 76.6%\n",
      "[BATCH 6/149] Loss_D: 0.7791 Loss_G: 0.8164 acc: 70.3%\n",
      "[BATCH 7/149] Loss_D: 0.8301 Loss_G: 0.8295 acc: 65.6%\n",
      "[BATCH 8/149] Loss_D: 0.8146 Loss_G: 0.8371 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.8561 Loss_G: 0.8402 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8505 Loss_G: 0.8531 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.7657 Loss_G: 0.8364 acc: 62.5%\n",
      "[BATCH 12/149] Loss_D: 0.7841 Loss_G: 0.8279 acc: 53.1%\n",
      "[BATCH 13/149] Loss_D: 0.7753 Loss_G: 0.8173 acc: 65.6%\n",
      "[BATCH 14/149] Loss_D: 0.8476 Loss_G: 0.8421 acc: 56.2%\n",
      "[BATCH 15/149] Loss_D: 0.7738 Loss_G: 0.8216 acc: 78.1%\n",
      "[BATCH 16/149] Loss_D: 0.8244 Loss_G: 0.8163 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.8074 Loss_G: 0.8293 acc: 56.2%\n",
      "[BATCH 18/149] Loss_D: 0.7951 Loss_G: 0.8175 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.8750 Loss_G: 0.8309 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.7891 Loss_G: 0.8301 acc: 71.9%\n",
      "[BATCH 21/149] Loss_D: 0.8063 Loss_G: 0.8230 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.8060 Loss_G: 0.8269 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.8384 Loss_G: 0.8338 acc: 60.9%\n",
      "[BATCH 24/149] Loss_D: 0.7653 Loss_G: 0.8156 acc: 57.8%\n",
      "[BATCH 25/149] Loss_D: 0.8584 Loss_G: 0.8271 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7992 Loss_G: 0.8203 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.8144 Loss_G: 0.8234 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.7923 Loss_G: 0.8212 acc: 65.6%\n",
      "[BATCH 29/149] Loss_D: 0.8246 Loss_G: 0.8273 acc: 62.5%\n",
      "[BATCH 30/149] Loss_D: 0.7502 Loss_G: 0.8105 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.8184 Loss_G: 0.8153 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7933 Loss_G: 0.8240 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.8119 Loss_G: 0.8347 acc: 57.8%\n",
      "[BATCH 34/149] Loss_D: 0.8696 Loss_G: 0.8481 acc: 73.4%\n",
      "[BATCH 35/149] Loss_D: 0.8146 Loss_G: 0.8410 acc: 53.1%\n",
      "[BATCH 36/149] Loss_D: 0.8122 Loss_G: 0.8331 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.8076 Loss_G: 0.8325 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7904 Loss_G: 0.8497 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8176 Loss_G: 0.8280 acc: 70.3%\n",
      "[BATCH 40/149] Loss_D: 0.8256 Loss_G: 0.8262 acc: 54.7%\n",
      "[BATCH 41/149] Loss_D: 0.7649 Loss_G: 0.8208 acc: 62.5%\n",
      "[EPOCH 6150] TEST ACC is : 75.4%\n",
      "[BATCH 42/149] Loss_D: 0.7926 Loss_G: 0.8109 acc: 60.9%\n",
      "[BATCH 43/149] Loss_D: 0.8125 Loss_G: 0.8133 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7879 Loss_G: 0.8189 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.8177 Loss_G: 0.8216 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.8442 Loss_G: 0.8317 acc: 76.6%\n",
      "[BATCH 47/149] Loss_D: 0.8868 Loss_G: 0.8476 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7759 Loss_G: 0.8381 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.7977 Loss_G: 0.8239 acc: 64.1%\n",
      "[BATCH 50/149] Loss_D: 0.8420 Loss_G: 0.8286 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.8037 Loss_G: 0.8244 acc: 71.9%\n",
      "[BATCH 52/149] Loss_D: 0.7988 Loss_G: 0.8249 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.8352 Loss_G: 0.8351 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.7896 Loss_G: 0.8218 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.8085 Loss_G: 0.8269 acc: 70.3%\n",
      "[BATCH 56/149] Loss_D: 0.7741 Loss_G: 0.8259 acc: 62.5%\n",
      "[BATCH 57/149] Loss_D: 0.8186 Loss_G: 0.8320 acc: 54.7%\n",
      "[BATCH 58/149] Loss_D: 0.8470 Loss_G: 0.8344 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.8242 Loss_G: 0.8297 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.7844 Loss_G: 0.8229 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.7760 Loss_G: 0.8130 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.8431 Loss_G: 0.8241 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.7928 Loss_G: 0.8292 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.7748 Loss_G: 0.8227 acc: 68.8%\n",
      "[BATCH 65/149] Loss_D: 0.7790 Loss_G: 0.8090 acc: 68.8%\n",
      "[BATCH 66/149] Loss_D: 0.8019 Loss_G: 0.8076 acc: 67.2%\n",
      "[BATCH 67/149] Loss_D: 0.8251 Loss_G: 0.8142 acc: 57.8%\n",
      "[BATCH 68/149] Loss_D: 0.7797 Loss_G: 0.8205 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.7599 Loss_G: 0.8157 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.8245 Loss_G: 0.8221 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.7820 Loss_G: 0.8192 acc: 60.9%\n",
      "[BATCH 72/149] Loss_D: 0.7787 Loss_G: 0.8153 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.8308 Loss_G: 0.8235 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.7934 Loss_G: 0.8309 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.8320 Loss_G: 0.8251 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7939 Loss_G: 0.8206 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.8639 Loss_G: 0.8391 acc: 57.8%\n",
      "[BATCH 78/149] Loss_D: 0.7547 Loss_G: 0.8174 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.8001 Loss_G: 0.8099 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.7929 Loss_G: 0.8107 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7857 Loss_G: 0.8199 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.7825 Loss_G: 0.8183 acc: 71.9%\n",
      "[BATCH 83/149] Loss_D: 0.7839 Loss_G: 0.8327 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.8287 Loss_G: 0.8368 acc: 48.4%\n",
      "[BATCH 85/149] Loss_D: 0.7867 Loss_G: 0.8285 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.7934 Loss_G: 0.8315 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.8597 Loss_G: 0.8367 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.7769 Loss_G: 0.8356 acc: 71.9%\n",
      "[BATCH 89/149] Loss_D: 0.8385 Loss_G: 0.8373 acc: 64.1%\n",
      "[BATCH 90/149] Loss_D: 0.7953 Loss_G: 0.8299 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.7997 Loss_G: 0.8167 acc: 65.6%\n",
      "[EPOCH 6200] TEST ACC is : 72.3%\n",
      "[BATCH 92/149] Loss_D: 0.8143 Loss_G: 0.8222 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.7831 Loss_G: 0.8183 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.7885 Loss_G: 0.8128 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.8493 Loss_G: 0.8300 acc: 75.0%\n",
      "[BATCH 96/149] Loss_D: 0.8183 Loss_G: 0.8556 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7729 Loss_G: 0.8240 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.7981 Loss_G: 0.8260 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.7615 Loss_G: 0.8053 acc: 68.8%\n",
      "[BATCH 100/149] Loss_D: 0.7924 Loss_G: 0.8096 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.8423 Loss_G: 0.8229 acc: 65.6%\n",
      "[BATCH 102/149] Loss_D: 0.8713 Loss_G: 0.8321 acc: 60.9%\n",
      "[BATCH 103/149] Loss_D: 0.7782 Loss_G: 0.8174 acc: 56.2%\n",
      "[BATCH 104/149] Loss_D: 0.8075 Loss_G: 0.8111 acc: 59.4%\n",
      "[BATCH 105/149] Loss_D: 0.7759 Loss_G: 0.8051 acc: 56.2%\n",
      "[BATCH 106/149] Loss_D: 0.7794 Loss_G: 0.7996 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.7634 Loss_G: 0.7999 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8280 Loss_G: 0.8128 acc: 60.9%\n",
      "[BATCH 109/149] Loss_D: 0.7575 Loss_G: 0.8097 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.8396 Loss_G: 0.8209 acc: 67.2%\n",
      "[BATCH 111/149] Loss_D: 0.8584 Loss_G: 0.8390 acc: 67.2%\n",
      "[BATCH 112/149] Loss_D: 0.7926 Loss_G: 0.8320 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.7647 Loss_G: 0.8248 acc: 71.9%\n",
      "[BATCH 114/149] Loss_D: 0.7755 Loss_G: 0.8158 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.8769 Loss_G: 0.8308 acc: 56.2%\n",
      "[BATCH 116/149] Loss_D: 0.7973 Loss_G: 0.8266 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7823 Loss_G: 0.8353 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.7877 Loss_G: 0.8153 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.7866 Loss_G: 0.8182 acc: 70.3%\n",
      "[BATCH 120/149] Loss_D: 0.7898 Loss_G: 0.8264 acc: 70.3%\n",
      "[BATCH 121/149] Loss_D: 0.7887 Loss_G: 0.8187 acc: 65.6%\n",
      "[BATCH 122/149] Loss_D: 0.7803 Loss_G: 0.8131 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.7791 Loss_G: 0.8085 acc: 73.4%\n",
      "[BATCH 124/149] Loss_D: 0.7640 Loss_G: 0.8043 acc: 59.4%\n",
      "[BATCH 125/149] Loss_D: 0.7772 Loss_G: 0.8011 acc: 53.1%\n",
      "[BATCH 126/149] Loss_D: 0.7518 Loss_G: 0.7998 acc: 75.0%\n",
      "[BATCH 127/149] Loss_D: 0.8231 Loss_G: 0.8110 acc: 60.9%\n",
      "[BATCH 128/149] Loss_D: 0.7551 Loss_G: 0.8054 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.8050 Loss_G: 0.8080 acc: 60.9%\n",
      "[BATCH 130/149] Loss_D: 0.8003 Loss_G: 0.8455 acc: 59.4%\n",
      "[BATCH 131/149] Loss_D: 0.8450 Loss_G: 0.8303 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.8034 Loss_G: 0.8405 acc: 57.8%\n",
      "[BATCH 133/149] Loss_D: 0.7891 Loss_G: 0.8279 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.7782 Loss_G: 0.8223 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.7694 Loss_G: 0.8198 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7985 Loss_G: 0.8173 acc: 68.8%\n",
      "[BATCH 137/149] Loss_D: 0.8672 Loss_G: 0.8416 acc: 71.9%\n",
      "[BATCH 138/149] Loss_D: 0.8237 Loss_G: 0.8578 acc: 62.5%\n",
      "[BATCH 139/149] Loss_D: 0.8672 Loss_G: 0.8652 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.7501 Loss_G: 0.8437 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7795 Loss_G: 0.8209 acc: 53.1%\n",
      "[EPOCH 6250] TEST ACC is : 74.6%\n",
      "[BATCH 142/149] Loss_D: 0.7719 Loss_G: 0.8068 acc: 71.9%\n",
      "[BATCH 143/149] Loss_D: 0.7946 Loss_G: 0.8097 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7891 Loss_G: 0.8181 acc: 68.8%\n",
      "[BATCH 145/149] Loss_D: 0.8021 Loss_G: 0.8194 acc: 50.0%\n",
      "[BATCH 146/149] Loss_D: 0.8393 Loss_G: 0.8312 acc: 73.4%\n",
      "[BATCH 147/149] Loss_D: 0.7791 Loss_G: 0.8232 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.8061 Loss_G: 0.8285 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7727 Loss_G: 0.8187 acc: 65.6%\n",
      "-----THE [42/50] epoch end-----\n",
      "-----THE [43/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7553 Loss_G: 0.8165 acc: 64.1%\n",
      "[BATCH 2/149] Loss_D: 0.7888 Loss_G: 0.8137 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.8657 Loss_G: 0.8285 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.7775 Loss_G: 0.8384 acc: 59.4%\n",
      "[BATCH 5/149] Loss_D: 0.8295 Loss_G: 0.8310 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.8238 Loss_G: 0.8282 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.8231 Loss_G: 0.8293 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7557 Loss_G: 0.8197 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.7903 Loss_G: 0.8163 acc: 60.9%\n",
      "[BATCH 10/149] Loss_D: 0.7804 Loss_G: 0.8095 acc: 73.4%\n",
      "[BATCH 11/149] Loss_D: 0.7976 Loss_G: 0.8146 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7998 Loss_G: 0.8217 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7566 Loss_G: 0.8101 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7857 Loss_G: 0.8104 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.7681 Loss_G: 0.8055 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.7963 Loss_G: 0.8098 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.8169 Loss_G: 0.8201 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.8215 Loss_G: 0.8271 acc: 59.4%\n",
      "[BATCH 19/149] Loss_D: 0.8021 Loss_G: 0.8258 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7973 Loss_G: 0.8179 acc: 65.6%\n",
      "[BATCH 21/149] Loss_D: 0.8143 Loss_G: 0.8213 acc: 65.6%\n",
      "[BATCH 22/149] Loss_D: 0.8252 Loss_G: 0.8282 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.8460 Loss_G: 0.8265 acc: 75.0%\n",
      "[BATCH 24/149] Loss_D: 0.7825 Loss_G: 0.8234 acc: 67.2%\n",
      "[BATCH 25/149] Loss_D: 0.8041 Loss_G: 0.8212 acc: 70.3%\n",
      "[BATCH 26/149] Loss_D: 0.7858 Loss_G: 0.8153 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.8064 Loss_G: 0.8171 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.7735 Loss_G: 0.8161 acc: 68.8%\n",
      "[BATCH 29/149] Loss_D: 0.7892 Loss_G: 0.8119 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7738 Loss_G: 0.8113 acc: 73.4%\n",
      "[BATCH 31/149] Loss_D: 0.8385 Loss_G: 0.8239 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.8441 Loss_G: 0.8341 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.8383 Loss_G: 0.8593 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.7921 Loss_G: 0.8285 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.8001 Loss_G: 0.8231 acc: 67.2%\n",
      "[BATCH 36/149] Loss_D: 0.7845 Loss_G: 0.8230 acc: 64.1%\n",
      "[BATCH 37/149] Loss_D: 0.9416 Loss_G: 0.8583 acc: 57.8%\n",
      "[BATCH 38/149] Loss_D: 0.7534 Loss_G: 0.8505 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.8577 Loss_G: 0.8354 acc: 51.6%\n",
      "[BATCH 40/149] Loss_D: 0.8151 Loss_G: 0.8314 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.8116 Loss_G: 0.8200 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.7950 Loss_G: 0.8209 acc: 65.6%\n",
      "[EPOCH 6300] TEST ACC is : 76.0%\n",
      "[BATCH 43/149] Loss_D: 0.7886 Loss_G: 0.8237 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.8718 Loss_G: 0.8411 acc: 62.5%\n",
      "[BATCH 45/149] Loss_D: 0.8193 Loss_G: 0.8440 acc: 70.3%\n",
      "[BATCH 46/149] Loss_D: 0.7828 Loss_G: 0.8236 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.8239 Loss_G: 0.8291 acc: 76.6%\n",
      "[BATCH 48/149] Loss_D: 0.8050 Loss_G: 0.8245 acc: 57.8%\n",
      "[BATCH 49/149] Loss_D: 0.8092 Loss_G: 0.8251 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.8356 Loss_G: 0.8268 acc: 60.9%\n",
      "[BATCH 51/149] Loss_D: 0.7941 Loss_G: 0.8254 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7984 Loss_G: 0.8204 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.8183 Loss_G: 0.8173 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.8506 Loss_G: 0.8296 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.7965 Loss_G: 0.8392 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.8332 Loss_G: 0.8461 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.8033 Loss_G: 0.8312 acc: 73.4%\n",
      "[BATCH 58/149] Loss_D: 0.7524 Loss_G: 0.8165 acc: 68.8%\n",
      "[BATCH 59/149] Loss_D: 0.7890 Loss_G: 0.8075 acc: 64.1%\n",
      "[BATCH 60/149] Loss_D: 0.8034 Loss_G: 0.8072 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.8021 Loss_G: 0.8114 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.7760 Loss_G: 0.8077 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.8032 Loss_G: 0.8091 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.8287 Loss_G: 0.8163 acc: 67.2%\n",
      "[BATCH 65/149] Loss_D: 0.8336 Loss_G: 0.8269 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7805 Loss_G: 0.8163 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.8919 Loss_G: 0.8301 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.8218 Loss_G: 0.8310 acc: 53.1%\n",
      "[BATCH 69/149] Loss_D: 0.8159 Loss_G: 0.8262 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.7864 Loss_G: 0.8246 acc: 56.2%\n",
      "[BATCH 71/149] Loss_D: 0.7375 Loss_G: 0.8129 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.8198 Loss_G: 0.8169 acc: 64.1%\n",
      "[BATCH 73/149] Loss_D: 0.7434 Loss_G: 0.8124 acc: 60.9%\n",
      "[BATCH 74/149] Loss_D: 0.7901 Loss_G: 0.8146 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.7646 Loss_G: 0.8149 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.8067 Loss_G: 0.8226 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.7681 Loss_G: 0.8232 acc: 64.1%\n",
      "[BATCH 78/149] Loss_D: 0.7601 Loss_G: 0.8074 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.7731 Loss_G: 0.8057 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.7884 Loss_G: 0.8071 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7837 Loss_G: 0.8101 acc: 57.8%\n",
      "[BATCH 82/149] Loss_D: 0.8009 Loss_G: 0.8157 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7908 Loss_G: 0.8139 acc: 54.7%\n",
      "[BATCH 84/149] Loss_D: 0.7909 Loss_G: 0.8106 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.8256 Loss_G: 0.8109 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.8581 Loss_G: 0.8346 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.7802 Loss_G: 0.8307 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 0.7812 Loss_G: 0.8147 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.8121 Loss_G: 0.8168 acc: 79.7%\n",
      "[BATCH 90/149] Loss_D: 0.7806 Loss_G: 0.8170 acc: 73.4%\n",
      "[BATCH 91/149] Loss_D: 0.7789 Loss_G: 0.8138 acc: 57.8%\n",
      "[BATCH 92/149] Loss_D: 0.7879 Loss_G: 0.8178 acc: 62.5%\n",
      "[EPOCH 6350] TEST ACC is : 73.6%\n",
      "[BATCH 93/149] Loss_D: 0.8565 Loss_G: 0.8312 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7870 Loss_G: 0.8552 acc: 65.6%\n",
      "[BATCH 95/149] Loss_D: 0.8030 Loss_G: 0.8373 acc: 65.6%\n",
      "[BATCH 96/149] Loss_D: 0.7584 Loss_G: 0.8119 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.8021 Loss_G: 0.8142 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.8104 Loss_G: 0.8195 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.7448 Loss_G: 0.8154 acc: 56.2%\n",
      "[BATCH 100/149] Loss_D: 0.7894 Loss_G: 0.8110 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.7897 Loss_G: 0.8079 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.8062 Loss_G: 0.8140 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.8462 Loss_G: 0.8382 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.7939 Loss_G: 0.8449 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7963 Loss_G: 0.8384 acc: 70.3%\n",
      "[BATCH 106/149] Loss_D: 0.8091 Loss_G: 0.8238 acc: 51.6%\n",
      "[BATCH 107/149] Loss_D: 0.8627 Loss_G: 0.8307 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.8023 Loss_G: 0.8300 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.7988 Loss_G: 0.8211 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.8032 Loss_G: 0.8143 acc: 56.2%\n",
      "[BATCH 111/149] Loss_D: 0.8008 Loss_G: 0.8115 acc: 65.6%\n",
      "[BATCH 112/149] Loss_D: 0.7951 Loss_G: 0.8093 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.7909 Loss_G: 0.8065 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.8160 Loss_G: 0.8116 acc: 57.8%\n",
      "[BATCH 115/149] Loss_D: 0.8224 Loss_G: 0.8200 acc: 75.0%\n",
      "[BATCH 116/149] Loss_D: 0.8055 Loss_G: 0.8197 acc: 53.1%\n",
      "[BATCH 117/149] Loss_D: 0.8230 Loss_G: 0.8260 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.8221 Loss_G: 0.8304 acc: 68.8%\n",
      "[BATCH 119/149] Loss_D: 0.8016 Loss_G: 0.8308 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.8305 Loss_G: 0.8420 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.8050 Loss_G: 0.8352 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.7810 Loss_G: 0.8257 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7932 Loss_G: 0.8174 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.7977 Loss_G: 0.8147 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.8333 Loss_G: 0.8275 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.8731 Loss_G: 0.8499 acc: 68.8%\n",
      "[BATCH 127/149] Loss_D: 0.8015 Loss_G: 0.8421 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7816 Loss_G: 0.8256 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.7793 Loss_G: 0.8143 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.8226 Loss_G: 0.8170 acc: 59.4%\n",
      "[BATCH 131/149] Loss_D: 0.7446 Loss_G: 0.8040 acc: 53.1%\n",
      "[BATCH 132/149] Loss_D: 0.8037 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.8233 Loss_G: 0.8070 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.8041 Loss_G: 0.8204 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.8264 Loss_G: 0.8241 acc: 62.5%\n",
      "[BATCH 136/149] Loss_D: 0.8308 Loss_G: 0.8316 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7670 Loss_G: 0.8272 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.7901 Loss_G: 0.8258 acc: 67.2%\n",
      "[BATCH 139/149] Loss_D: 0.7669 Loss_G: 0.8208 acc: 57.8%\n",
      "[BATCH 140/149] Loss_D: 0.8119 Loss_G: 0.8205 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.7738 Loss_G: 0.8206 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7657 Loss_G: 0.8136 acc: 50.0%\n",
      "[EPOCH 6400] TEST ACC is : 74.6%\n",
      "[BATCH 143/149] Loss_D: 0.7771 Loss_G: 0.8141 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.8029 Loss_G: 0.8239 acc: 56.2%\n",
      "[BATCH 145/149] Loss_D: 0.7573 Loss_G: 0.8157 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.8446 Loss_G: 0.8172 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.8314 Loss_G: 0.8338 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7663 Loss_G: 0.8374 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.8063 Loss_G: 0.8171 acc: 70.3%\n",
      "-----THE [43/50] epoch end-----\n",
      "-----THE [44/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8446 Loss_G: 0.8182 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7822 Loss_G: 0.8209 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.8120 Loss_G: 0.8127 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.7796 Loss_G: 0.8063 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.8145 Loss_G: 0.8105 acc: 57.8%\n",
      "[BATCH 6/149] Loss_D: 0.8038 Loss_G: 0.8149 acc: 68.8%\n",
      "[BATCH 7/149] Loss_D: 0.8129 Loss_G: 0.8193 acc: 56.2%\n",
      "[BATCH 8/149] Loss_D: 0.7749 Loss_G: 0.8186 acc: 59.4%\n",
      "[BATCH 9/149] Loss_D: 0.7922 Loss_G: 0.8117 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.8065 Loss_G: 0.8150 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.7767 Loss_G: 0.8111 acc: 51.6%\n",
      "[BATCH 12/149] Loss_D: 0.8001 Loss_G: 0.8109 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7835 Loss_G: 0.8108 acc: 64.1%\n",
      "[BATCH 14/149] Loss_D: 0.8025 Loss_G: 0.8169 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.7777 Loss_G: 0.8236 acc: 48.4%\n",
      "[BATCH 16/149] Loss_D: 0.7946 Loss_G: 0.8163 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.8211 Loss_G: 0.8299 acc: 75.0%\n",
      "[BATCH 18/149] Loss_D: 0.7803 Loss_G: 0.8165 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.8196 Loss_G: 0.8129 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.7846 Loss_G: 0.8151 acc: 70.3%\n",
      "[BATCH 21/149] Loss_D: 0.7856 Loss_G: 0.8148 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.8223 Loss_G: 0.8178 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.7724 Loss_G: 0.8252 acc: 57.8%\n",
      "[BATCH 24/149] Loss_D: 0.7983 Loss_G: 0.8204 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.8070 Loss_G: 0.8155 acc: 59.4%\n",
      "[BATCH 26/149] Loss_D: 0.7955 Loss_G: 0.8149 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7907 Loss_G: 0.8174 acc: 60.9%\n",
      "[BATCH 28/149] Loss_D: 0.8047 Loss_G: 0.8289 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.7813 Loss_G: 0.8221 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.7972 Loss_G: 0.8176 acc: 54.7%\n",
      "[BATCH 31/149] Loss_D: 0.8202 Loss_G: 0.8211 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.7985 Loss_G: 0.8213 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.7755 Loss_G: 0.8163 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.7800 Loss_G: 0.8058 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.8056 Loss_G: 0.8080 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7644 Loss_G: 0.7974 acc: 57.8%\n",
      "[BATCH 37/149] Loss_D: 0.8042 Loss_G: 0.7998 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.7905 Loss_G: 0.8042 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8156 Loss_G: 0.8116 acc: 59.4%\n",
      "[BATCH 40/149] Loss_D: 0.7701 Loss_G: 0.8070 acc: 75.0%\n",
      "[BATCH 41/149] Loss_D: 0.8092 Loss_G: 0.8065 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.7743 Loss_G: 0.8003 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.7681 Loss_G: 0.7974 acc: 60.9%\n",
      "[EPOCH 6450] TEST ACC is : 74.8%\n",
      "[BATCH 44/149] Loss_D: 0.7974 Loss_G: 0.8001 acc: 70.3%\n",
      "[BATCH 45/149] Loss_D: 0.7730 Loss_G: 0.8044 acc: 54.7%\n",
      "[BATCH 46/149] Loss_D: 0.8029 Loss_G: 0.8097 acc: 64.1%\n",
      "[BATCH 47/149] Loss_D: 0.8054 Loss_G: 0.8168 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7920 Loss_G: 0.8165 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.7982 Loss_G: 0.8117 acc: 64.1%\n",
      "[BATCH 50/149] Loss_D: 0.8223 Loss_G: 0.8240 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.8691 Loss_G: 0.8554 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.8613 Loss_G: 0.8503 acc: 59.4%\n",
      "[BATCH 53/149] Loss_D: 0.7476 Loss_G: 0.8158 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.7748 Loss_G: 0.8112 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7811 Loss_G: 0.8109 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.7868 Loss_G: 0.8112 acc: 57.8%\n",
      "[BATCH 57/149] Loss_D: 0.7957 Loss_G: 0.8183 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7423 Loss_G: 0.8086 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.8468 Loss_G: 0.8206 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.8006 Loss_G: 0.8182 acc: 56.2%\n",
      "[BATCH 61/149] Loss_D: 0.7850 Loss_G: 0.8167 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.7515 Loss_G: 0.8001 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.7838 Loss_G: 0.8000 acc: 56.2%\n",
      "[BATCH 64/149] Loss_D: 0.8238 Loss_G: 0.8150 acc: 64.1%\n",
      "[BATCH 65/149] Loss_D: 0.7925 Loss_G: 0.8307 acc: 67.2%\n",
      "[BATCH 66/149] Loss_D: 0.8013 Loss_G: 0.8191 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.8437 Loss_G: 0.8275 acc: 59.4%\n",
      "[BATCH 68/149] Loss_D: 0.7934 Loss_G: 0.8251 acc: 75.0%\n",
      "[BATCH 69/149] Loss_D: 0.8029 Loss_G: 0.8228 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.8272 Loss_G: 0.8311 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.8065 Loss_G: 0.8435 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.8197 Loss_G: 0.8232 acc: 59.4%\n",
      "[BATCH 73/149] Loss_D: 0.8282 Loss_G: 0.8334 acc: 57.8%\n",
      "[BATCH 74/149] Loss_D: 0.7975 Loss_G: 0.8430 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.8087 Loss_G: 0.8287 acc: 59.4%\n",
      "[BATCH 76/149] Loss_D: 0.8096 Loss_G: 0.8224 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.8055 Loss_G: 0.8321 acc: 57.8%\n",
      "[BATCH 78/149] Loss_D: 0.7966 Loss_G: 0.8218 acc: 48.4%\n",
      "[BATCH 79/149] Loss_D: 0.7908 Loss_G: 0.8153 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.8244 Loss_G: 0.8126 acc: 67.2%\n",
      "[BATCH 81/149] Loss_D: 0.8015 Loss_G: 0.8155 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.8036 Loss_G: 0.8176 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.7927 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.8048 Loss_G: 0.8174 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.8307 Loss_G: 0.8331 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7928 Loss_G: 0.8234 acc: 46.9%\n",
      "[BATCH 87/149] Loss_D: 0.7729 Loss_G: 0.8203 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7694 Loss_G: 0.8121 acc: 62.5%\n",
      "[BATCH 89/149] Loss_D: 0.7820 Loss_G: 0.8139 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7782 Loss_G: 0.8163 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7751 Loss_G: 0.8095 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7953 Loss_G: 0.8059 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.7831 Loss_G: 0.8018 acc: 68.8%\n",
      "[EPOCH 6500] TEST ACC is : 74.4%\n",
      "[BATCH 94/149] Loss_D: 0.7914 Loss_G: 0.8053 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7793 Loss_G: 0.8048 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.8185 Loss_G: 0.8147 acc: 71.9%\n",
      "[BATCH 97/149] Loss_D: 0.8101 Loss_G: 0.8296 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.7940 Loss_G: 0.8232 acc: 70.3%\n",
      "[BATCH 99/149] Loss_D: 0.7722 Loss_G: 0.8082 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.8530 Loss_G: 0.8186 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.7811 Loss_G: 0.8170 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.8325 Loss_G: 0.8217 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.8272 Loss_G: 0.8380 acc: 56.2%\n",
      "[BATCH 104/149] Loss_D: 0.7864 Loss_G: 0.8268 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.8124 Loss_G: 0.8187 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.8501 Loss_G: 0.8283 acc: 70.3%\n",
      "[BATCH 107/149] Loss_D: 0.8084 Loss_G: 0.8367 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.8237 Loss_G: 0.8415 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.8618 Loss_G: 0.8586 acc: 73.4%\n",
      "[BATCH 110/149] Loss_D: 0.8026 Loss_G: 0.8430 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7831 Loss_G: 0.8220 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.7709 Loss_G: 0.8137 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7622 Loss_G: 0.8042 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.8627 Loss_G: 0.8272 acc: 53.1%\n",
      "[BATCH 115/149] Loss_D: 0.7800 Loss_G: 0.8294 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8037 Loss_G: 0.8180 acc: 73.4%\n",
      "[BATCH 117/149] Loss_D: 0.8054 Loss_G: 0.8131 acc: 57.8%\n",
      "[BATCH 118/149] Loss_D: 0.7848 Loss_G: 0.8050 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.7400 Loss_G: 0.8018 acc: 70.3%\n",
      "[BATCH 120/149] Loss_D: 0.8007 Loss_G: 0.7972 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.8111 Loss_G: 0.8072 acc: 51.6%\n",
      "[BATCH 122/149] Loss_D: 0.8546 Loss_G: 0.8412 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.7808 Loss_G: 0.8333 acc: 50.0%\n",
      "[BATCH 124/149] Loss_D: 0.8135 Loss_G: 0.8119 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.8198 Loss_G: 0.8127 acc: 59.4%\n",
      "[BATCH 126/149] Loss_D: 0.8131 Loss_G: 0.8178 acc: 65.6%\n",
      "[BATCH 127/149] Loss_D: 0.8889 Loss_G: 0.8340 acc: 59.4%\n",
      "[BATCH 128/149] Loss_D: 0.7421 Loss_G: 0.8240 acc: 68.8%\n",
      "[BATCH 129/149] Loss_D: 0.7962 Loss_G: 0.8198 acc: 67.2%\n",
      "[BATCH 130/149] Loss_D: 0.8086 Loss_G: 0.8247 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.7990 Loss_G: 0.8336 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.8439 Loss_G: 0.8398 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.8646 Loss_G: 0.8545 acc: 48.4%\n",
      "[BATCH 134/149] Loss_D: 0.7873 Loss_G: 0.8326 acc: 70.3%\n",
      "[BATCH 135/149] Loss_D: 0.8088 Loss_G: 0.8267 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.8419 Loss_G: 0.8294 acc: 57.8%\n",
      "[BATCH 137/149] Loss_D: 0.8026 Loss_G: 0.8327 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7958 Loss_G: 0.8258 acc: 57.8%\n",
      "[BATCH 139/149] Loss_D: 0.8081 Loss_G: 0.8321 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.8023 Loss_G: 0.8224 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.8018 Loss_G: 0.8174 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.7923 Loss_G: 0.8170 acc: 56.2%\n",
      "[BATCH 143/149] Loss_D: 0.8264 Loss_G: 0.8272 acc: 57.8%\n",
      "[EPOCH 6550] TEST ACC is : 73.8%\n",
      "[BATCH 144/149] Loss_D: 0.7920 Loss_G: 0.8249 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.8287 Loss_G: 0.8267 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.8215 Loss_G: 0.8265 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.8150 Loss_G: 0.8245 acc: 59.4%\n",
      "[BATCH 148/149] Loss_D: 0.8305 Loss_G: 0.8290 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7950 Loss_G: 0.8294 acc: 71.9%\n",
      "-----THE [44/50] epoch end-----\n",
      "-----THE [45/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8153 Loss_G: 0.8336 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.8398 Loss_G: 0.8390 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7838 Loss_G: 0.8278 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.8258 Loss_G: 0.8298 acc: 59.4%\n",
      "[BATCH 5/149] Loss_D: 0.8586 Loss_G: 0.8353 acc: 62.5%\n",
      "[BATCH 6/149] Loss_D: 0.7900 Loss_G: 0.8321 acc: 59.4%\n",
      "[BATCH 7/149] Loss_D: 0.7752 Loss_G: 0.8243 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.8169 Loss_G: 0.8433 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.8476 Loss_G: 0.8385 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7637 Loss_G: 0.8179 acc: 75.0%\n",
      "[BATCH 11/149] Loss_D: 0.8308 Loss_G: 0.8215 acc: 67.2%\n",
      "[BATCH 12/149] Loss_D: 0.7965 Loss_G: 0.8191 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.8247 Loss_G: 0.8254 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.7879 Loss_G: 0.8328 acc: 71.9%\n",
      "[BATCH 15/149] Loss_D: 0.8131 Loss_G: 0.8372 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.8100 Loss_G: 0.8366 acc: 65.6%\n",
      "[BATCH 17/149] Loss_D: 0.8257 Loss_G: 0.8280 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.7496 Loss_G: 0.8124 acc: 68.8%\n",
      "[BATCH 19/149] Loss_D: 0.8015 Loss_G: 0.8106 acc: 71.9%\n",
      "[BATCH 20/149] Loss_D: 0.7811 Loss_G: 0.8096 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8033 Loss_G: 0.8098 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.7719 Loss_G: 0.8032 acc: 65.6%\n",
      "[BATCH 23/149] Loss_D: 0.8369 Loss_G: 0.8116 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7831 Loss_G: 0.8191 acc: 67.2%\n",
      "[BATCH 25/149] Loss_D: 0.7916 Loss_G: 0.8016 acc: 62.5%\n",
      "[BATCH 26/149] Loss_D: 0.8329 Loss_G: 0.8032 acc: 75.0%\n",
      "[BATCH 27/149] Loss_D: 0.7828 Loss_G: 0.8115 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7929 Loss_G: 0.8153 acc: 54.7%\n",
      "[BATCH 29/149] Loss_D: 0.7988 Loss_G: 0.8192 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 0.8123 Loss_G: 0.8217 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.8736 Loss_G: 0.8491 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.8284 Loss_G: 0.8596 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.7640 Loss_G: 0.8270 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.7620 Loss_G: 0.8120 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.8092 Loss_G: 0.8078 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.8517 Loss_G: 0.8249 acc: 68.8%\n",
      "[BATCH 37/149] Loss_D: 0.7663 Loss_G: 0.8203 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7890 Loss_G: 0.8190 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.8111 Loss_G: 0.8347 acc: 59.4%\n",
      "[BATCH 40/149] Loss_D: 0.8142 Loss_G: 0.8328 acc: 62.5%\n",
      "[BATCH 41/149] Loss_D: 0.8337 Loss_G: 0.8357 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.8293 Loss_G: 0.8392 acc: 70.3%\n",
      "[BATCH 43/149] Loss_D: 0.8013 Loss_G: 0.8276 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7903 Loss_G: 0.8168 acc: 62.5%\n",
      "[EPOCH 6600] TEST ACC is : 73.8%\n",
      "[BATCH 45/149] Loss_D: 0.7588 Loss_G: 0.8031 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.7804 Loss_G: 0.7979 acc: 71.9%\n",
      "[BATCH 47/149] Loss_D: 0.7797 Loss_G: 0.7973 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.7801 Loss_G: 0.8004 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.8238 Loss_G: 0.8114 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.7934 Loss_G: 0.8145 acc: 73.4%\n",
      "[BATCH 51/149] Loss_D: 0.7700 Loss_G: 0.8103 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.8486 Loss_G: 0.8329 acc: 56.2%\n",
      "[BATCH 53/149] Loss_D: 0.8045 Loss_G: 0.8197 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.8272 Loss_G: 0.8338 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.7907 Loss_G: 0.8186 acc: 70.3%\n",
      "[BATCH 56/149] Loss_D: 0.7992 Loss_G: 0.8122 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.8461 Loss_G: 0.8275 acc: 62.5%\n",
      "[BATCH 58/149] Loss_D: 0.7687 Loss_G: 0.8266 acc: 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.7856 Loss_G: 0.8113 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7956 Loss_G: 0.8122 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.7887 Loss_G: 0.8134 acc: 57.8%\n",
      "[BATCH 62/149] Loss_D: 0.8077 Loss_G: 0.8267 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.7921 Loss_G: 0.8295 acc: 73.4%\n",
      "[BATCH 64/149] Loss_D: 0.8040 Loss_G: 0.8248 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8319 Loss_G: 0.8387 acc: 56.2%\n",
      "[BATCH 66/149] Loss_D: 0.8268 Loss_G: 0.8340 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7932 Loss_G: 0.8249 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7850 Loss_G: 0.8083 acc: 68.8%\n",
      "[BATCH 69/149] Loss_D: 0.8102 Loss_G: 0.8084 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.7652 Loss_G: 0.8066 acc: 57.8%\n",
      "[BATCH 71/149] Loss_D: 0.8186 Loss_G: 0.8091 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7981 Loss_G: 0.8190 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.8068 Loss_G: 0.8277 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.7864 Loss_G: 0.8174 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.8179 Loss_G: 0.8177 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7743 Loss_G: 0.8069 acc: 54.7%\n",
      "[BATCH 77/149] Loss_D: 0.7742 Loss_G: 0.7997 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.7900 Loss_G: 0.8054 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.8501 Loss_G: 0.8190 acc: 79.7%\n",
      "[BATCH 80/149] Loss_D: 0.8206 Loss_G: 0.8276 acc: 56.2%\n",
      "[BATCH 81/149] Loss_D: 0.8284 Loss_G: 0.8302 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.8280 Loss_G: 0.8209 acc: 59.4%\n",
      "[BATCH 83/149] Loss_D: 0.7687 Loss_G: 0.8116 acc: 76.6%\n",
      "[BATCH 84/149] Loss_D: 0.7893 Loss_G: 0.8058 acc: 57.8%\n",
      "[BATCH 85/149] Loss_D: 0.7563 Loss_G: 0.8018 acc: 71.9%\n",
      "[BATCH 86/149] Loss_D: 0.8620 Loss_G: 0.8276 acc: 62.5%\n",
      "[BATCH 87/149] Loss_D: 0.7894 Loss_G: 0.8243 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.8002 Loss_G: 0.8189 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.7600 Loss_G: 0.8085 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.8264 Loss_G: 0.8134 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.8122 Loss_G: 0.8247 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.7647 Loss_G: 0.8210 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.7949 Loss_G: 0.8123 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.8138 Loss_G: 0.8125 acc: 59.4%\n",
      "[EPOCH 6650] TEST ACC is : 76.0%\n",
      "[BATCH 95/149] Loss_D: 0.8049 Loss_G: 0.8101 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.8594 Loss_G: 0.8338 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7626 Loss_G: 0.8260 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.8369 Loss_G: 0.8335 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.8023 Loss_G: 0.8238 acc: 59.4%\n",
      "[BATCH 100/149] Loss_D: 0.7937 Loss_G: 0.8131 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.7919 Loss_G: 0.8066 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7868 Loss_G: 0.8153 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.8552 Loss_G: 0.8442 acc: 67.2%\n",
      "[BATCH 104/149] Loss_D: 0.8338 Loss_G: 0.8692 acc: 54.7%\n",
      "[BATCH 105/149] Loss_D: 0.7957 Loss_G: 0.8282 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.7666 Loss_G: 0.8169 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.7759 Loss_G: 0.8094 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.7967 Loss_G: 0.8100 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.7715 Loss_G: 0.8094 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.7718 Loss_G: 0.8030 acc: 59.4%\n",
      "[BATCH 111/149] Loss_D: 0.8204 Loss_G: 0.8113 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7729 Loss_G: 0.8136 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.7620 Loss_G: 0.8053 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.7456 Loss_G: 0.7980 acc: 57.8%\n",
      "[BATCH 115/149] Loss_D: 0.8462 Loss_G: 0.8073 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.8106 Loss_G: 0.8204 acc: 75.0%\n",
      "[BATCH 117/149] Loss_D: 0.7903 Loss_G: 0.8157 acc: 71.9%\n",
      "[BATCH 118/149] Loss_D: 0.7751 Loss_G: 0.8165 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.8394 Loss_G: 0.8313 acc: 59.4%\n",
      "[BATCH 120/149] Loss_D: 0.8285 Loss_G: 0.8411 acc: 67.2%\n",
      "[BATCH 121/149] Loss_D: 0.8015 Loss_G: 0.8250 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.7687 Loss_G: 0.8099 acc: 64.1%\n",
      "[BATCH 123/149] Loss_D: 0.8540 Loss_G: 0.8175 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7915 Loss_G: 0.8176 acc: 73.4%\n",
      "[BATCH 125/149] Loss_D: 0.7912 Loss_G: 0.8169 acc: 73.4%\n",
      "[BATCH 126/149] Loss_D: 0.8313 Loss_G: 0.8249 acc: 67.2%\n",
      "[BATCH 127/149] Loss_D: 0.7996 Loss_G: 0.8256 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.7990 Loss_G: 0.8208 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.7903 Loss_G: 0.8057 acc: 73.4%\n",
      "[BATCH 130/149] Loss_D: 0.8047 Loss_G: 0.8069 acc: 59.4%\n",
      "[BATCH 131/149] Loss_D: 0.7850 Loss_G: 0.8064 acc: 65.6%\n",
      "[BATCH 132/149] Loss_D: 0.8016 Loss_G: 0.8107 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.9043 Loss_G: 0.8347 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.7645 Loss_G: 0.8167 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.7976 Loss_G: 0.8145 acc: 70.3%\n",
      "[BATCH 136/149] Loss_D: 0.7930 Loss_G: 0.8145 acc: 57.8%\n",
      "[BATCH 137/149] Loss_D: 0.8011 Loss_G: 0.8174 acc: 73.4%\n",
      "[BATCH 138/149] Loss_D: 0.7687 Loss_G: 0.8093 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.8397 Loss_G: 0.8238 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.7754 Loss_G: 0.8283 acc: 68.8%\n",
      "[BATCH 141/149] Loss_D: 0.7758 Loss_G: 0.8199 acc: 53.1%\n",
      "[BATCH 142/149] Loss_D: 0.8676 Loss_G: 0.8271 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.8048 Loss_G: 0.8328 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.7955 Loss_G: 0.8304 acc: 71.9%\n",
      "[EPOCH 6700] TEST ACC is : 72.9%\n",
      "[BATCH 145/149] Loss_D: 0.7927 Loss_G: 0.8217 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.7630 Loss_G: 0.8110 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.8057 Loss_G: 0.8160 acc: 68.8%\n",
      "[BATCH 148/149] Loss_D: 0.7632 Loss_G: 0.8072 acc: 71.9%\n",
      "[BATCH 149/149] Loss_D: 0.7562 Loss_G: 0.8020 acc: 62.5%\n",
      "-----THE [45/50] epoch end-----\n",
      "-----THE [46/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7670 Loss_G: 0.8000 acc: 76.6%\n",
      "[BATCH 2/149] Loss_D: 0.8361 Loss_G: 0.8060 acc: 73.4%\n",
      "[BATCH 3/149] Loss_D: 0.7932 Loss_G: 0.8128 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.8144 Loss_G: 0.8200 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7769 Loss_G: 0.8269 acc: 75.0%\n",
      "[BATCH 6/149] Loss_D: 0.8110 Loss_G: 0.8138 acc: 70.3%\n",
      "[BATCH 7/149] Loss_D: 0.7752 Loss_G: 0.8198 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7660 Loss_G: 0.8051 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7879 Loss_G: 0.8063 acc: 60.9%\n",
      "[BATCH 10/149] Loss_D: 0.7647 Loss_G: 0.8018 acc: 59.4%\n",
      "[BATCH 11/149] Loss_D: 0.8101 Loss_G: 0.8158 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.8122 Loss_G: 0.8229 acc: 62.5%\n",
      "[BATCH 13/149] Loss_D: 0.8128 Loss_G: 0.8280 acc: 73.4%\n",
      "[BATCH 14/149] Loss_D: 0.7943 Loss_G: 0.8165 acc: 70.3%\n",
      "[BATCH 15/149] Loss_D: 0.7846 Loss_G: 0.8183 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.7981 Loss_G: 0.8180 acc: 73.4%\n",
      "[BATCH 17/149] Loss_D: 0.8199 Loss_G: 0.8196 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.8199 Loss_G: 0.8223 acc: 54.7%\n",
      "[BATCH 19/149] Loss_D: 0.8261 Loss_G: 0.8220 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.7844 Loss_G: 0.8173 acc: 57.8%\n",
      "[BATCH 21/149] Loss_D: 0.8147 Loss_G: 0.8153 acc: 65.6%\n",
      "[BATCH 22/149] Loss_D: 0.8050 Loss_G: 0.8121 acc: 60.9%\n",
      "[BATCH 23/149] Loss_D: 0.7992 Loss_G: 0.8146 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.7535 Loss_G: 0.8055 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.8344 Loss_G: 0.8149 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.8001 Loss_G: 0.8175 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7570 Loss_G: 0.8051 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.7906 Loss_G: 0.8053 acc: 79.7%\n",
      "[BATCH 29/149] Loss_D: 0.8854 Loss_G: 0.8358 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.8346 Loss_G: 0.8620 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7723 Loss_G: 0.8262 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.7995 Loss_G: 0.8155 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.7775 Loss_G: 0.8110 acc: 67.2%\n",
      "[BATCH 34/149] Loss_D: 0.8255 Loss_G: 0.8171 acc: 57.8%\n",
      "[BATCH 35/149] Loss_D: 0.8031 Loss_G: 0.8276 acc: 67.2%\n",
      "[BATCH 36/149] Loss_D: 0.8089 Loss_G: 0.8252 acc: 68.8%\n",
      "[BATCH 37/149] Loss_D: 0.7813 Loss_G: 0.8189 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7486 Loss_G: 0.8178 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7963 Loss_G: 0.8197 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.8025 Loss_G: 0.8274 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.7954 Loss_G: 0.8274 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7772 Loss_G: 0.8159 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.7953 Loss_G: 0.8132 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7885 Loss_G: 0.8090 acc: 62.5%\n",
      "[BATCH 45/149] Loss_D: 0.8197 Loss_G: 0.8162 acc: 78.1%\n",
      "[EPOCH 6750] TEST ACC is : 74.4%\n",
      "[BATCH 46/149] Loss_D: 0.8150 Loss_G: 0.8178 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.8223 Loss_G: 0.8168 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.8279 Loss_G: 0.8212 acc: 56.2%\n",
      "[BATCH 49/149] Loss_D: 0.8067 Loss_G: 0.8123 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.8310 Loss_G: 0.8226 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7943 Loss_G: 0.8324 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.8204 Loss_G: 0.8276 acc: 54.7%\n",
      "[BATCH 53/149] Loss_D: 0.7978 Loss_G: 0.8164 acc: 71.9%\n",
      "[BATCH 54/149] Loss_D: 0.8106 Loss_G: 0.8138 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.8062 Loss_G: 0.8123 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.8063 Loss_G: 0.8192 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.8133 Loss_G: 0.8217 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.8034 Loss_G: 0.8235 acc: 75.0%\n",
      "[BATCH 59/149] Loss_D: 0.7699 Loss_G: 0.8129 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.7944 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7556 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 62/149] Loss_D: 0.7709 Loss_G: 0.8025 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.7860 Loss_G: 0.8039 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.7582 Loss_G: 0.7955 acc: 59.4%\n",
      "[BATCH 65/149] Loss_D: 0.8466 Loss_G: 0.8085 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.7567 Loss_G: 0.8050 acc: 68.8%\n",
      "[BATCH 67/149] Loss_D: 0.7695 Loss_G: 0.7997 acc: 79.7%\n",
      "[BATCH 68/149] Loss_D: 0.8008 Loss_G: 0.8050 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.8530 Loss_G: 0.8249 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.7779 Loss_G: 0.8338 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.8534 Loss_G: 0.8326 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8116 Loss_G: 0.8344 acc: 54.7%\n",
      "[BATCH 73/149] Loss_D: 0.7715 Loss_G: 0.8271 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7998 Loss_G: 0.8166 acc: 68.8%\n",
      "[BATCH 75/149] Loss_D: 0.8204 Loss_G: 0.8161 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.7949 Loss_G: 0.8213 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.8196 Loss_G: 0.8282 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.8232 Loss_G: 0.8231 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.8300 Loss_G: 0.8327 acc: 65.6%\n",
      "[BATCH 80/149] Loss_D: 0.8173 Loss_G: 0.8379 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.8122 Loss_G: 0.8258 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.8179 Loss_G: 0.8251 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7756 Loss_G: 0.8165 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.8458 Loss_G: 0.8221 acc: 54.7%\n",
      "[BATCH 85/149] Loss_D: 0.7842 Loss_G: 0.8160 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.8009 Loss_G: 0.8151 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.7843 Loss_G: 0.8059 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.8201 Loss_G: 0.8136 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.7560 Loss_G: 0.8093 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7749 Loss_G: 0.8090 acc: 62.5%\n",
      "[BATCH 91/149] Loss_D: 0.8581 Loss_G: 0.8268 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.7933 Loss_G: 0.8371 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.8494 Loss_G: 0.8442 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.8628 Loss_G: 0.8482 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.7702 Loss_G: 0.8268 acc: 65.6%\n",
      "[EPOCH 6800] TEST ACC is : 72.5%\n",
      "[BATCH 96/149] Loss_D: 0.8256 Loss_G: 0.8248 acc: 67.2%\n",
      "[BATCH 97/149] Loss_D: 0.7914 Loss_G: 0.8203 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7983 Loss_G: 0.8179 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.7604 Loss_G: 0.8131 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.8159 Loss_G: 0.8159 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.8336 Loss_G: 0.8244 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7698 Loss_G: 0.8199 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.8551 Loss_G: 0.8298 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7704 Loss_G: 0.8179 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.8227 Loss_G: 0.8133 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.8151 Loss_G: 0.8202 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.7956 Loss_G: 0.8216 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.8168 Loss_G: 0.8190 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.8002 Loss_G: 0.8214 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.7954 Loss_G: 0.8211 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.7978 Loss_G: 0.8184 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.8320 Loss_G: 0.8252 acc: 59.4%\n",
      "[BATCH 113/149] Loss_D: 0.7559 Loss_G: 0.8073 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.8004 Loss_G: 0.8012 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.8337 Loss_G: 0.8080 acc: 62.5%\n",
      "[BATCH 116/149] Loss_D: 0.7517 Loss_G: 0.8050 acc: 71.9%\n",
      "[BATCH 117/149] Loss_D: 0.7586 Loss_G: 0.7977 acc: 70.3%\n",
      "[BATCH 118/149] Loss_D: 0.7721 Loss_G: 0.8094 acc: 71.9%\n",
      "[BATCH 119/149] Loss_D: 0.7838 Loss_G: 0.8099 acc: 60.9%\n",
      "[BATCH 120/149] Loss_D: 0.7578 Loss_G: 0.8065 acc: 60.9%\n",
      "[BATCH 121/149] Loss_D: 0.8030 Loss_G: 0.8143 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.8023 Loss_G: 0.8386 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.8586 Loss_G: 0.8620 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.8097 Loss_G: 0.8470 acc: 51.6%\n",
      "[BATCH 125/149] Loss_D: 0.7875 Loss_G: 0.8297 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.8215 Loss_G: 0.8229 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7822 Loss_G: 0.8149 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.8738 Loss_G: 0.8448 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.7586 Loss_G: 0.8289 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.8351 Loss_G: 0.8215 acc: 48.4%\n",
      "[BATCH 131/149] Loss_D: 0.8214 Loss_G: 0.8280 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.8468 Loss_G: 0.8401 acc: 57.8%\n",
      "[BATCH 133/149] Loss_D: 0.7771 Loss_G: 0.8288 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7772 Loss_G: 0.8261 acc: 62.5%\n",
      "[BATCH 135/149] Loss_D: 0.7383 Loss_G: 0.8066 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7550 Loss_G: 0.8001 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.8392 Loss_G: 0.8205 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.7825 Loss_G: 0.8217 acc: 76.6%\n",
      "[BATCH 139/149] Loss_D: 0.8188 Loss_G: 0.8414 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.8257 Loss_G: 0.8373 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.7966 Loss_G: 0.8126 acc: 71.9%\n",
      "[BATCH 142/149] Loss_D: 0.7882 Loss_G: 0.8121 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.7550 Loss_G: 0.8044 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.7848 Loss_G: 0.8073 acc: 62.5%\n",
      "[BATCH 145/149] Loss_D: 0.7564 Loss_G: 0.7995 acc: 70.3%\n",
      "[EPOCH 6850] TEST ACC is : 74.0%\n",
      "[BATCH 146/149] Loss_D: 0.7825 Loss_G: 0.8023 acc: 59.4%\n",
      "[BATCH 147/149] Loss_D: 0.8535 Loss_G: 0.8219 acc: 54.7%\n",
      "[BATCH 148/149] Loss_D: 0.7984 Loss_G: 0.8212 acc: 59.4%\n",
      "[BATCH 149/149] Loss_D: 0.8139 Loss_G: 0.8238 acc: 67.2%\n",
      "-----THE [46/50] epoch end-----\n",
      "-----THE [47/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8353 Loss_G: 0.8285 acc: 64.1%\n",
      "[BATCH 2/149] Loss_D: 0.8700 Loss_G: 0.8486 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.8048 Loss_G: 0.8313 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.7819 Loss_G: 0.8133 acc: 59.4%\n",
      "[BATCH 5/149] Loss_D: 0.8208 Loss_G: 0.8261 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7895 Loss_G: 0.8306 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.7654 Loss_G: 0.8265 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.8528 Loss_G: 0.8305 acc: 57.8%\n",
      "[BATCH 9/149] Loss_D: 0.7792 Loss_G: 0.8275 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.8314 Loss_G: 0.8244 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.8305 Loss_G: 0.8358 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.8010 Loss_G: 0.8270 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.8021 Loss_G: 0.8253 acc: 60.9%\n",
      "[BATCH 14/149] Loss_D: 0.8849 Loss_G: 0.8502 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.8259 Loss_G: 0.8552 acc: 59.4%\n",
      "[BATCH 16/149] Loss_D: 0.8078 Loss_G: 0.8390 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.8172 Loss_G: 0.8327 acc: 70.3%\n",
      "[BATCH 18/149] Loss_D: 0.7946 Loss_G: 0.8190 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.7732 Loss_G: 0.8132 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.8406 Loss_G: 0.8211 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.7754 Loss_G: 0.8160 acc: 56.2%\n",
      "[BATCH 22/149] Loss_D: 0.7506 Loss_G: 0.8034 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7526 Loss_G: 0.7991 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.8592 Loss_G: 0.8087 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.8122 Loss_G: 0.8276 acc: 75.0%\n",
      "[BATCH 26/149] Loss_D: 0.8248 Loss_G: 0.8287 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7734 Loss_G: 0.8184 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.8589 Loss_G: 0.8247 acc: 65.6%\n",
      "[BATCH 29/149] Loss_D: 0.8028 Loss_G: 0.8216 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.8138 Loss_G: 0.8245 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.8247 Loss_G: 0.8320 acc: 71.9%\n",
      "[BATCH 32/149] Loss_D: 0.7627 Loss_G: 0.8218 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7813 Loss_G: 0.8214 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8896 Loss_G: 0.8350 acc: 57.8%\n",
      "[BATCH 35/149] Loss_D: 0.8323 Loss_G: 0.8383 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.7946 Loss_G: 0.8273 acc: 64.1%\n",
      "[BATCH 37/149] Loss_D: 0.7854 Loss_G: 0.8216 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7672 Loss_G: 0.8143 acc: 75.0%\n",
      "[BATCH 39/149] Loss_D: 0.8001 Loss_G: 0.8076 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.8121 Loss_G: 0.8251 acc: 56.2%\n",
      "[BATCH 41/149] Loss_D: 0.8094 Loss_G: 0.8249 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.7711 Loss_G: 0.8184 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.8093 Loss_G: 0.8101 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7836 Loss_G: 0.8091 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7985 Loss_G: 0.8253 acc: 75.0%\n",
      "[BATCH 46/149] Loss_D: 0.7716 Loss_G: 0.8182 acc: 70.3%\n",
      "[EPOCH 6900] TEST ACC is : 73.0%\n",
      "[BATCH 47/149] Loss_D: 0.8095 Loss_G: 0.8172 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7790 Loss_G: 0.8094 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.8515 Loss_G: 0.8245 acc: 56.2%\n",
      "[BATCH 50/149] Loss_D: 0.7851 Loss_G: 0.8309 acc: 54.7%\n",
      "[BATCH 51/149] Loss_D: 0.8102 Loss_G: 0.8302 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.7694 Loss_G: 0.8086 acc: 75.0%\n",
      "[BATCH 53/149] Loss_D: 0.7410 Loss_G: 0.7979 acc: 54.7%\n",
      "[BATCH 54/149] Loss_D: 0.8272 Loss_G: 0.8026 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.8663 Loss_G: 0.8205 acc: 59.4%\n",
      "[BATCH 56/149] Loss_D: 0.8000 Loss_G: 0.8440 acc: 56.2%\n",
      "[BATCH 57/149] Loss_D: 0.8287 Loss_G: 0.8181 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.7945 Loss_G: 0.8126 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.7959 Loss_G: 0.8180 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7961 Loss_G: 0.8215 acc: 71.9%\n",
      "[BATCH 61/149] Loss_D: 0.8216 Loss_G: 0.8349 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.7745 Loss_G: 0.8275 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7790 Loss_G: 0.8216 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7565 Loss_G: 0.8133 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.8041 Loss_G: 0.8242 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.7882 Loss_G: 0.8190 acc: 56.2%\n",
      "[BATCH 67/149] Loss_D: 0.8196 Loss_G: 0.8297 acc: 56.2%\n",
      "[BATCH 68/149] Loss_D: 0.8181 Loss_G: 0.8334 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.8255 Loss_G: 0.8395 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.7872 Loss_G: 0.8362 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.8147 Loss_G: 0.8186 acc: 67.2%\n",
      "[BATCH 72/149] Loss_D: 0.7631 Loss_G: 0.8098 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.7746 Loss_G: 0.8090 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.7800 Loss_G: 0.8064 acc: 54.7%\n",
      "[BATCH 75/149] Loss_D: 0.8144 Loss_G: 0.8168 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7967 Loss_G: 0.8225 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.8274 Loss_G: 0.8335 acc: 64.1%\n",
      "[BATCH 78/149] Loss_D: 0.8183 Loss_G: 0.8297 acc: 53.1%\n",
      "[BATCH 79/149] Loss_D: 0.7532 Loss_G: 0.8260 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.8033 Loss_G: 0.8131 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.8356 Loss_G: 0.8144 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.8196 Loss_G: 0.8177 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7934 Loss_G: 0.8128 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.7787 Loss_G: 0.8087 acc: 68.8%\n",
      "[BATCH 85/149] Loss_D: 0.7751 Loss_G: 0.8088 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.8316 Loss_G: 0.8153 acc: 71.9%\n",
      "[BATCH 87/149] Loss_D: 0.8064 Loss_G: 0.8261 acc: 57.8%\n",
      "[BATCH 88/149] Loss_D: 0.7973 Loss_G: 0.8184 acc: 56.2%\n",
      "[BATCH 89/149] Loss_D: 0.7870 Loss_G: 0.8142 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7662 Loss_G: 0.8106 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.7994 Loss_G: 0.8186 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7742 Loss_G: 0.8119 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.7978 Loss_G: 0.8170 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.7859 Loss_G: 0.8115 acc: 59.4%\n",
      "[BATCH 95/149] Loss_D: 0.7673 Loss_G: 0.8119 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.7825 Loss_G: 0.8113 acc: 60.9%\n",
      "[EPOCH 6950] TEST ACC is : 71.9%\n",
      "[BATCH 97/149] Loss_D: 0.8356 Loss_G: 0.8186 acc: 62.5%\n",
      "[BATCH 98/149] Loss_D: 0.8017 Loss_G: 0.8233 acc: 65.6%\n",
      "[BATCH 99/149] Loss_D: 0.7722 Loss_G: 0.8061 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.7582 Loss_G: 0.8063 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.7972 Loss_G: 0.8079 acc: 75.0%\n",
      "[BATCH 102/149] Loss_D: 0.7804 Loss_G: 0.8130 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.7565 Loss_G: 0.8061 acc: 59.4%\n",
      "[BATCH 104/149] Loss_D: 0.7929 Loss_G: 0.8067 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.7849 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.8137 Loss_G: 0.8183 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.8169 Loss_G: 0.8181 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.8139 Loss_G: 0.8255 acc: 73.4%\n",
      "[BATCH 109/149] Loss_D: 0.7512 Loss_G: 0.8132 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.7966 Loss_G: 0.8060 acc: 67.2%\n",
      "[BATCH 111/149] Loss_D: 0.8152 Loss_G: 0.8156 acc: 67.2%\n",
      "[BATCH 112/149] Loss_D: 0.7472 Loss_G: 0.8089 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.8140 Loss_G: 0.8214 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.7978 Loss_G: 0.8140 acc: 81.2%\n",
      "[BATCH 115/149] Loss_D: 0.8065 Loss_G: 0.8112 acc: 59.4%\n",
      "[BATCH 116/149] Loss_D: 0.7917 Loss_G: 0.8113 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7834 Loss_G: 0.8087 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.8673 Loss_G: 0.8367 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.8051 Loss_G: 0.8457 acc: 73.4%\n",
      "[BATCH 120/149] Loss_D: 0.8123 Loss_G: 0.8311 acc: 71.9%\n",
      "[BATCH 121/149] Loss_D: 0.8007 Loss_G: 0.8280 acc: 65.6%\n",
      "[BATCH 122/149] Loss_D: 0.7446 Loss_G: 0.8165 acc: 75.0%\n",
      "[BATCH 123/149] Loss_D: 0.7973 Loss_G: 0.8157 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.7954 Loss_G: 0.8229 acc: 73.4%\n",
      "[BATCH 125/149] Loss_D: 0.7842 Loss_G: 0.8191 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.8094 Loss_G: 0.8196 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.8064 Loss_G: 0.8217 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.8045 Loss_G: 0.8234 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.8062 Loss_G: 0.8240 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7854 Loss_G: 0.8138 acc: 60.9%\n",
      "[BATCH 131/149] Loss_D: 0.7568 Loss_G: 0.8091 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.8217 Loss_G: 0.8126 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.7871 Loss_G: 0.8157 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.8309 Loss_G: 0.8230 acc: 54.7%\n",
      "[BATCH 135/149] Loss_D: 0.7545 Loss_G: 0.8069 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.8196 Loss_G: 0.8101 acc: 56.2%\n",
      "[BATCH 137/149] Loss_D: 0.8162 Loss_G: 0.8175 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.8658 Loss_G: 0.8370 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.8508 Loss_G: 0.8423 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.8000 Loss_G: 0.8345 acc: 75.0%\n",
      "[BATCH 141/149] Loss_D: 0.7542 Loss_G: 0.8206 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.7966 Loss_G: 0.8188 acc: 81.2%\n",
      "[BATCH 143/149] Loss_D: 0.7811 Loss_G: 0.8222 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.7718 Loss_G: 0.8131 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.7872 Loss_G: 0.8145 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7773 Loss_G: 0.8168 acc: 59.4%\n",
      "[EPOCH 7000] TEST ACC is : 74.0%\n",
      "[BATCH 147/149] Loss_D: 0.8182 Loss_G: 0.8271 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.7931 Loss_G: 0.8169 acc: 76.6%\n",
      "[BATCH 149/149] Loss_D: 0.8052 Loss_G: 0.8230 acc: 59.4%\n",
      "-----THE [47/50] epoch end-----\n",
      "-----THE [48/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8050 Loss_G: 0.8151 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.8252 Loss_G: 0.8178 acc: 56.2%\n",
      "[BATCH 3/149] Loss_D: 0.7466 Loss_G: 0.8072 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.8155 Loss_G: 0.8127 acc: 67.2%\n",
      "[BATCH 5/149] Loss_D: 0.8203 Loss_G: 0.8165 acc: 59.4%\n",
      "[BATCH 6/149] Loss_D: 0.8076 Loss_G: 0.8163 acc: 70.3%\n",
      "[BATCH 7/149] Loss_D: 0.7778 Loss_G: 0.8205 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.7895 Loss_G: 0.8133 acc: 59.4%\n",
      "[BATCH 9/149] Loss_D: 0.7646 Loss_G: 0.8058 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.8460 Loss_G: 0.8088 acc: 70.3%\n",
      "[BATCH 11/149] Loss_D: 0.8158 Loss_G: 0.8233 acc: 71.9%\n",
      "[BATCH 12/149] Loss_D: 0.7873 Loss_G: 0.8116 acc: 62.5%\n",
      "[BATCH 13/149] Loss_D: 0.7508 Loss_G: 0.8047 acc: 76.6%\n",
      "[BATCH 14/149] Loss_D: 0.8098 Loss_G: 0.8086 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.8451 Loss_G: 0.8256 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.8427 Loss_G: 0.8496 acc: 65.6%\n",
      "[BATCH 17/149] Loss_D: 0.7964 Loss_G: 0.8473 acc: 79.7%\n",
      "[BATCH 18/149] Loss_D: 0.7816 Loss_G: 0.8236 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.7559 Loss_G: 0.8089 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.8016 Loss_G: 0.8056 acc: 59.4%\n",
      "[BATCH 21/149] Loss_D: 0.7954 Loss_G: 0.8103 acc: 57.8%\n",
      "[BATCH 22/149] Loss_D: 0.8089 Loss_G: 0.8123 acc: 76.6%\n",
      "[BATCH 23/149] Loss_D: 0.7738 Loss_G: 0.8092 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.8789 Loss_G: 0.8266 acc: 67.2%\n",
      "[BATCH 25/149] Loss_D: 0.7872 Loss_G: 0.8323 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7834 Loss_G: 0.8218 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.8216 Loss_G: 0.8147 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.8217 Loss_G: 0.8234 acc: 56.2%\n",
      "[BATCH 29/149] Loss_D: 0.8109 Loss_G: 0.8219 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7602 Loss_G: 0.8168 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.8237 Loss_G: 0.8197 acc: 51.6%\n",
      "[BATCH 32/149] Loss_D: 0.7985 Loss_G: 0.8197 acc: 60.9%\n",
      "[BATCH 33/149] Loss_D: 0.7584 Loss_G: 0.8088 acc: 57.8%\n",
      "[BATCH 34/149] Loss_D: 0.8398 Loss_G: 0.8137 acc: 62.5%\n",
      "[BATCH 35/149] Loss_D: 0.7885 Loss_G: 0.8090 acc: 71.9%\n",
      "[BATCH 36/149] Loss_D: 0.7787 Loss_G: 0.8055 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7923 Loss_G: 0.8021 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7990 Loss_G: 0.8078 acc: 70.3%\n",
      "[BATCH 39/149] Loss_D: 0.7914 Loss_G: 0.8131 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.8518 Loss_G: 0.8306 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.7690 Loss_G: 0.8271 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7777 Loss_G: 0.8151 acc: 50.0%\n",
      "[BATCH 43/149] Loss_D: 0.8203 Loss_G: 0.8062 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.7975 Loss_G: 0.8058 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.7687 Loss_G: 0.8039 acc: 71.9%\n",
      "[BATCH 46/149] Loss_D: 0.7695 Loss_G: 0.7933 acc: 60.9%\n",
      "[BATCH 47/149] Loss_D: 0.8440 Loss_G: 0.8123 acc: 65.6%\n",
      "[EPOCH 7050] TEST ACC is : 75.8%\n",
      "[BATCH 48/149] Loss_D: 0.8331 Loss_G: 0.8261 acc: 51.6%\n",
      "[BATCH 49/149] Loss_D: 0.7483 Loss_G: 0.8102 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7715 Loss_G: 0.7980 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.8099 Loss_G: 0.8070 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.7601 Loss_G: 0.8145 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.8507 Loss_G: 0.8246 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.8013 Loss_G: 0.8256 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.7891 Loss_G: 0.8221 acc: 75.0%\n",
      "[BATCH 56/149] Loss_D: 0.7972 Loss_G: 0.8207 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.8023 Loss_G: 0.8251 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.7727 Loss_G: 0.8183 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.8120 Loss_G: 0.8268 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.8324 Loss_G: 0.8327 acc: 75.0%\n",
      "[BATCH 61/149] Loss_D: 0.7714 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 62/149] Loss_D: 0.7826 Loss_G: 0.8137 acc: 73.4%\n",
      "[BATCH 63/149] Loss_D: 0.7331 Loss_G: 0.8046 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.8018 Loss_G: 0.8026 acc: 67.2%\n",
      "[BATCH 65/149] Loss_D: 0.8162 Loss_G: 0.8098 acc: 71.9%\n",
      "[BATCH 66/149] Loss_D: 0.7776 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.8088 Loss_G: 0.8173 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.8140 Loss_G: 0.8185 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.8566 Loss_G: 0.8337 acc: 67.2%\n",
      "[BATCH 70/149] Loss_D: 0.8255 Loss_G: 0.8222 acc: 57.8%\n",
      "[BATCH 71/149] Loss_D: 0.8075 Loss_G: 0.8129 acc: 56.2%\n",
      "[BATCH 72/149] Loss_D: 0.7661 Loss_G: 0.8070 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7973 Loss_G: 0.8097 acc: 65.6%\n",
      "[BATCH 74/149] Loss_D: 0.8468 Loss_G: 0.8250 acc: 79.7%\n",
      "[BATCH 75/149] Loss_D: 0.7781 Loss_G: 0.8212 acc: 62.5%\n",
      "[BATCH 76/149] Loss_D: 0.8085 Loss_G: 0.8216 acc: 70.3%\n",
      "[BATCH 77/149] Loss_D: 0.7679 Loss_G: 0.8119 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7897 Loss_G: 0.8180 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.8037 Loss_G: 0.8165 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.8131 Loss_G: 0.8277 acc: 56.2%\n",
      "[BATCH 81/149] Loss_D: 0.7969 Loss_G: 0.8317 acc: 67.2%\n",
      "[BATCH 82/149] Loss_D: 0.7843 Loss_G: 0.8156 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.8303 Loss_G: 0.8192 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.8036 Loss_G: 0.8225 acc: 59.4%\n",
      "[BATCH 85/149] Loss_D: 0.7841 Loss_G: 0.8150 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.7928 Loss_G: 0.8107 acc: 73.4%\n",
      "[BATCH 87/149] Loss_D: 0.8310 Loss_G: 0.8177 acc: 65.6%\n",
      "[BATCH 88/149] Loss_D: 0.7782 Loss_G: 0.8179 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.7986 Loss_G: 0.8072 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7843 Loss_G: 0.8054 acc: 60.9%\n",
      "[BATCH 91/149] Loss_D: 0.8216 Loss_G: 0.8148 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.8569 Loss_G: 0.8331 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.8468 Loss_G: 0.8367 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.7615 Loss_G: 0.8158 acc: 75.0%\n",
      "[BATCH 95/149] Loss_D: 0.7885 Loss_G: 0.8124 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.8037 Loss_G: 0.8235 acc: 57.8%\n",
      "[BATCH 97/149] Loss_D: 0.8658 Loss_G: 0.8393 acc: 57.8%\n",
      "[EPOCH 7100] TEST ACC is : 76.2%\n",
      "[BATCH 98/149] Loss_D: 0.7926 Loss_G: 0.8348 acc: 65.6%\n",
      "[BATCH 99/149] Loss_D: 0.7953 Loss_G: 0.8213 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.8333 Loss_G: 0.8214 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.8209 Loss_G: 0.8253 acc: 71.9%\n",
      "[BATCH 102/149] Loss_D: 0.7759 Loss_G: 0.8087 acc: 56.2%\n",
      "[BATCH 103/149] Loss_D: 0.7741 Loss_G: 0.8068 acc: 56.2%\n",
      "[BATCH 104/149] Loss_D: 0.8260 Loss_G: 0.8219 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.8150 Loss_G: 0.8286 acc: 62.5%\n",
      "[BATCH 106/149] Loss_D: 0.8598 Loss_G: 0.8662 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.7709 Loss_G: 0.8282 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8019 Loss_G: 0.8118 acc: 60.9%\n",
      "[BATCH 109/149] Loss_D: 0.8250 Loss_G: 0.8224 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.7795 Loss_G: 0.8167 acc: 73.4%\n",
      "[BATCH 111/149] Loss_D: 0.8365 Loss_G: 0.8251 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7807 Loss_G: 0.8177 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7910 Loss_G: 0.8122 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.8140 Loss_G: 0.8077 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.8206 Loss_G: 0.8089 acc: 59.4%\n",
      "[BATCH 116/149] Loss_D: 0.7625 Loss_G: 0.8072 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7502 Loss_G: 0.8002 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7880 Loss_G: 0.7964 acc: 73.4%\n",
      "[BATCH 119/149] Loss_D: 0.7941 Loss_G: 0.8069 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.7543 Loss_G: 0.8063 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.7628 Loss_G: 0.8003 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.7607 Loss_G: 0.7995 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.7501 Loss_G: 0.7933 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.8093 Loss_G: 0.8085 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.8132 Loss_G: 0.8291 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.7740 Loss_G: 0.8305 acc: 60.9%\n",
      "[BATCH 127/149] Loss_D: 0.8156 Loss_G: 0.8461 acc: 68.8%\n",
      "[BATCH 128/149] Loss_D: 0.7919 Loss_G: 0.8303 acc: 54.7%\n",
      "[BATCH 129/149] Loss_D: 0.7759 Loss_G: 0.8208 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7443 Loss_G: 0.8042 acc: 60.9%\n",
      "[BATCH 131/149] Loss_D: 0.8079 Loss_G: 0.8042 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.7882 Loss_G: 0.8079 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.7872 Loss_G: 0.8047 acc: 54.7%\n",
      "[BATCH 134/149] Loss_D: 0.7765 Loss_G: 0.8061 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.8068 Loss_G: 0.8109 acc: 73.4%\n",
      "[BATCH 136/149] Loss_D: 0.8153 Loss_G: 0.8189 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.7908 Loss_G: 0.8197 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.7998 Loss_G: 0.8202 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.7867 Loss_G: 0.8216 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.8147 Loss_G: 0.8306 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.8302 Loss_G: 0.8369 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.8044 Loss_G: 0.8337 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.7899 Loss_G: 0.8162 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.7989 Loss_G: 0.8134 acc: 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.8488 Loss_G: 0.8258 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7940 Loss_G: 0.8256 acc: 59.4%\n",
      "[BATCH 147/149] Loss_D: 0.8313 Loss_G: 0.8209 acc: 68.8%\n",
      "[EPOCH 7150] TEST ACC is : 73.8%\n",
      "[BATCH 148/149] Loss_D: 0.7823 Loss_G: 0.8180 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.8270 Loss_G: 0.8289 acc: 76.6%\n",
      "-----THE [48/50] epoch end-----\n",
      "-----THE [49/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7772 Loss_G: 0.8226 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7913 Loss_G: 0.8154 acc: 59.4%\n",
      "[BATCH 3/149] Loss_D: 0.7980 Loss_G: 0.8134 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.8762 Loss_G: 0.8244 acc: 54.7%\n",
      "[BATCH 5/149] Loss_D: 0.7937 Loss_G: 0.8249 acc: 56.2%\n",
      "[BATCH 6/149] Loss_D: 0.8226 Loss_G: 0.8288 acc: 62.5%\n",
      "[BATCH 7/149] Loss_D: 0.7782 Loss_G: 0.8072 acc: 57.8%\n",
      "[BATCH 8/149] Loss_D: 0.7588 Loss_G: 0.8003 acc: 75.0%\n",
      "[BATCH 9/149] Loss_D: 0.7961 Loss_G: 0.8062 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.8412 Loss_G: 0.8219 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.7935 Loss_G: 0.8258 acc: 59.4%\n",
      "[BATCH 12/149] Loss_D: 0.7843 Loss_G: 0.8133 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.8136 Loss_G: 0.8195 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.8289 Loss_G: 0.8339 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.8221 Loss_G: 0.8414 acc: 67.2%\n",
      "[BATCH 16/149] Loss_D: 0.7390 Loss_G: 0.8261 acc: 68.8%\n",
      "[BATCH 17/149] Loss_D: 0.7920 Loss_G: 0.8177 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.7944 Loss_G: 0.8096 acc: 54.7%\n",
      "[BATCH 19/149] Loss_D: 0.7621 Loss_G: 0.8097 acc: 59.4%\n",
      "[BATCH 20/149] Loss_D: 0.8269 Loss_G: 0.8192 acc: 59.4%\n",
      "[BATCH 21/149] Loss_D: 0.8680 Loss_G: 0.8532 acc: 71.9%\n",
      "[BATCH 22/149] Loss_D: 0.7860 Loss_G: 0.8277 acc: 60.9%\n",
      "[BATCH 23/149] Loss_D: 0.7853 Loss_G: 0.8113 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.7986 Loss_G: 0.8081 acc: 59.4%\n",
      "[BATCH 25/149] Loss_D: 0.7973 Loss_G: 0.8167 acc: 73.4%\n",
      "[BATCH 26/149] Loss_D: 0.8044 Loss_G: 0.8229 acc: 59.4%\n",
      "[BATCH 27/149] Loss_D: 0.7616 Loss_G: 0.8117 acc: 75.0%\n",
      "[BATCH 28/149] Loss_D: 0.8040 Loss_G: 0.8149 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.8371 Loss_G: 0.8266 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.7998 Loss_G: 0.8257 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7907 Loss_G: 0.8131 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.7516 Loss_G: 0.8041 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.8565 Loss_G: 0.8128 acc: 67.2%\n",
      "[BATCH 34/149] Loss_D: 0.8135 Loss_G: 0.8188 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7573 Loss_G: 0.8130 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.8274 Loss_G: 0.8231 acc: 67.2%\n",
      "[BATCH 37/149] Loss_D: 0.8665 Loss_G: 0.8300 acc: 68.8%\n",
      "[BATCH 38/149] Loss_D: 0.7916 Loss_G: 0.8200 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.8425 Loss_G: 0.8185 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.8093 Loss_G: 0.8250 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.8067 Loss_G: 0.8235 acc: 59.4%\n",
      "[BATCH 42/149] Loss_D: 0.8040 Loss_G: 0.8216 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7839 Loss_G: 0.8207 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.7760 Loss_G: 0.8160 acc: 73.4%\n",
      "[BATCH 45/149] Loss_D: 0.8068 Loss_G: 0.8220 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.7565 Loss_G: 0.8136 acc: 53.1%\n",
      "[BATCH 47/149] Loss_D: 0.7820 Loss_G: 0.8105 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.8018 Loss_G: 0.8083 acc: 59.4%\n",
      "[EPOCH 7200] TEST ACC is : 73.8%\n",
      "[BATCH 49/149] Loss_D: 0.8335 Loss_G: 0.8159 acc: 60.9%\n",
      "[BATCH 50/149] Loss_D: 0.7864 Loss_G: 0.8199 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7762 Loss_G: 0.8084 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.8129 Loss_G: 0.8089 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.8263 Loss_G: 0.8175 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.8077 Loss_G: 0.8335 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.8086 Loss_G: 0.8206 acc: 59.4%\n",
      "[BATCH 56/149] Loss_D: 0.7919 Loss_G: 0.8076 acc: 75.0%\n",
      "[BATCH 57/149] Loss_D: 0.8235 Loss_G: 0.8185 acc: 75.0%\n",
      "[BATCH 58/149] Loss_D: 0.8359 Loss_G: 0.8308 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7935 Loss_G: 0.8200 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7735 Loss_G: 0.8154 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.7715 Loss_G: 0.8037 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.8204 Loss_G: 0.8147 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.8272 Loss_G: 0.8213 acc: 62.5%\n",
      "[BATCH 64/149] Loss_D: 0.7627 Loss_G: 0.8093 acc: 67.2%\n",
      "[BATCH 65/149] Loss_D: 0.7478 Loss_G: 0.7926 acc: 53.1%\n",
      "[BATCH 66/149] Loss_D: 0.7657 Loss_G: 0.7922 acc: 73.4%\n",
      "[BATCH 67/149] Loss_D: 0.7809 Loss_G: 0.7928 acc: 62.5%\n",
      "[BATCH 68/149] Loss_D: 0.7791 Loss_G: 0.7932 acc: 57.8%\n",
      "[BATCH 69/149] Loss_D: 0.8040 Loss_G: 0.8021 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.7621 Loss_G: 0.7956 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.8240 Loss_G: 0.8013 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.7805 Loss_G: 0.8039 acc: 54.7%\n",
      "[BATCH 73/149] Loss_D: 0.8320 Loss_G: 0.8111 acc: 65.6%\n",
      "[BATCH 74/149] Loss_D: 0.7924 Loss_G: 0.8157 acc: 54.7%\n",
      "[BATCH 75/149] Loss_D: 0.8224 Loss_G: 0.8216 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.7835 Loss_G: 0.8128 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.8130 Loss_G: 0.8175 acc: 57.8%\n",
      "[BATCH 78/149] Loss_D: 0.8308 Loss_G: 0.8375 acc: 71.9%\n",
      "[BATCH 79/149] Loss_D: 0.8045 Loss_G: 0.8278 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.7603 Loss_G: 0.8136 acc: 68.8%\n",
      "[BATCH 81/149] Loss_D: 0.7723 Loss_G: 0.8083 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.7732 Loss_G: 0.8059 acc: 70.3%\n",
      "[BATCH 83/149] Loss_D: 0.7634 Loss_G: 0.8055 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.8407 Loss_G: 0.8214 acc: 68.8%\n",
      "[BATCH 85/149] Loss_D: 0.8037 Loss_G: 0.8348 acc: 64.1%\n",
      "[BATCH 86/149] Loss_D: 0.8167 Loss_G: 0.8276 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.8374 Loss_G: 0.8411 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7657 Loss_G: 0.8222 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.7900 Loss_G: 0.8184 acc: 73.4%\n",
      "[BATCH 90/149] Loss_D: 0.7585 Loss_G: 0.8057 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.7839 Loss_G: 0.8075 acc: 62.5%\n",
      "[BATCH 92/149] Loss_D: 0.7609 Loss_G: 0.8023 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.8977 Loss_G: 0.8210 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.8226 Loss_G: 0.8247 acc: 65.6%\n",
      "[BATCH 95/149] Loss_D: 0.7691 Loss_G: 0.8123 acc: 73.4%\n",
      "[BATCH 96/149] Loss_D: 0.7871 Loss_G: 0.8076 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7759 Loss_G: 0.8030 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7942 Loss_G: 0.8092 acc: 67.2%\n",
      "[EPOCH 7250] TEST ACC is : 73.6%\n",
      "[BATCH 99/149] Loss_D: 0.7849 Loss_G: 0.8182 acc: 56.2%\n",
      "[BATCH 100/149] Loss_D: 0.7771 Loss_G: 0.8083 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.7776 Loss_G: 0.8058 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.7954 Loss_G: 0.8002 acc: 50.0%\n",
      "[BATCH 103/149] Loss_D: 0.7959 Loss_G: 0.8020 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8643 Loss_G: 0.8315 acc: 54.7%\n",
      "[BATCH 105/149] Loss_D: 0.8294 Loss_G: 0.8463 acc: 65.6%\n",
      "[BATCH 106/149] Loss_D: 0.7985 Loss_G: 0.8316 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7740 Loss_G: 0.8171 acc: 71.9%\n",
      "[BATCH 108/149] Loss_D: 0.7801 Loss_G: 0.8144 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.8070 Loss_G: 0.8243 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.8191 Loss_G: 0.8229 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.7762 Loss_G: 0.8193 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.8229 Loss_G: 0.8156 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7838 Loss_G: 0.8149 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7948 Loss_G: 0.8173 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.7956 Loss_G: 0.8149 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.8028 Loss_G: 0.8113 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.8139 Loss_G: 0.8178 acc: 70.3%\n",
      "[BATCH 118/149] Loss_D: 0.7972 Loss_G: 0.8123 acc: 68.8%\n",
      "[BATCH 119/149] Loss_D: 0.7539 Loss_G: 0.8048 acc: 79.7%\n",
      "[BATCH 120/149] Loss_D: 0.7683 Loss_G: 0.7994 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.8475 Loss_G: 0.8151 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8135 Loss_G: 0.8177 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7818 Loss_G: 0.8176 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7950 Loss_G: 0.8118 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.8077 Loss_G: 0.8112 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.8226 Loss_G: 0.8215 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7989 Loss_G: 0.8272 acc: 75.0%\n",
      "[BATCH 128/149] Loss_D: 0.7557 Loss_G: 0.8075 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.7917 Loss_G: 0.8010 acc: 53.1%\n",
      "[BATCH 130/149] Loss_D: 0.8270 Loss_G: 0.8136 acc: 71.9%\n",
      "[BATCH 131/149] Loss_D: 0.8415 Loss_G: 0.8201 acc: 68.8%\n",
      "[BATCH 132/149] Loss_D: 0.8056 Loss_G: 0.8168 acc: 76.6%\n",
      "[BATCH 133/149] Loss_D: 0.7898 Loss_G: 0.8060 acc: 71.9%\n",
      "[BATCH 134/149] Loss_D: 0.7922 Loss_G: 0.8056 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7772 Loss_G: 0.8050 acc: 68.8%\n",
      "[BATCH 136/149] Loss_D: 0.7904 Loss_G: 0.8075 acc: 70.3%\n",
      "[BATCH 137/149] Loss_D: 0.7813 Loss_G: 0.8088 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.8044 Loss_G: 0.8209 acc: 76.6%\n",
      "[BATCH 139/149] Loss_D: 0.8130 Loss_G: 0.8332 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.8279 Loss_G: 0.8247 acc: 76.6%\n",
      "[BATCH 141/149] Loss_D: 0.7490 Loss_G: 0.8157 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.7622 Loss_G: 0.8059 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.7671 Loss_G: 0.7944 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.8169 Loss_G: 0.8050 acc: 68.8%\n",
      "[BATCH 145/149] Loss_D: 0.8440 Loss_G: 0.8205 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.8566 Loss_G: 0.8334 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.7895 Loss_G: 0.8313 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.8314 Loss_G: 0.8386 acc: 67.2%\n",
      "[EPOCH 7300] TEST ACC is : 73.4%\n",
      "[BATCH 149/149] Loss_D: 0.7755 Loss_G: 0.8207 acc: 65.6%\n",
      "-----THE [49/50] epoch end-----\n",
      "-----THE [50/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8456 Loss_G: 0.8308 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.8362 Loss_G: 0.8435 acc: 59.4%\n",
      "[BATCH 3/149] Loss_D: 0.8030 Loss_G: 0.8376 acc: 59.4%\n",
      "[BATCH 4/149] Loss_D: 0.8414 Loss_G: 0.8380 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.8297 Loss_G: 0.8273 acc: 71.9%\n",
      "[BATCH 6/149] Loss_D: 0.7822 Loss_G: 0.8196 acc: 65.6%\n",
      "[BATCH 7/149] Loss_D: 0.8013 Loss_G: 0.8243 acc: 75.0%\n",
      "[BATCH 8/149] Loss_D: 0.8059 Loss_G: 0.8177 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.7386 Loss_G: 0.8038 acc: 53.1%\n",
      "[BATCH 10/149] Loss_D: 0.7909 Loss_G: 0.8064 acc: 57.8%\n",
      "[BATCH 11/149] Loss_D: 0.7840 Loss_G: 0.8043 acc: 48.4%\n",
      "[BATCH 12/149] Loss_D: 0.7854 Loss_G: 0.8067 acc: 57.8%\n",
      "[BATCH 13/149] Loss_D: 0.8027 Loss_G: 0.8071 acc: 54.7%\n",
      "[BATCH 14/149] Loss_D: 0.7870 Loss_G: 0.8060 acc: 59.4%\n",
      "[BATCH 15/149] Loss_D: 0.7680 Loss_G: 0.8049 acc: 60.9%\n",
      "[BATCH 16/149] Loss_D: 0.7520 Loss_G: 0.7950 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.8018 Loss_G: 0.8055 acc: 76.6%\n",
      "[BATCH 18/149] Loss_D: 0.7817 Loss_G: 0.8076 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.7742 Loss_G: 0.8044 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.7992 Loss_G: 0.8051 acc: 76.6%\n",
      "[BATCH 21/149] Loss_D: 0.8467 Loss_G: 0.8294 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7968 Loss_G: 0.8240 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.8119 Loss_G: 0.8301 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.8008 Loss_G: 0.8276 acc: 71.9%\n",
      "[BATCH 25/149] Loss_D: 0.7752 Loss_G: 0.8246 acc: 62.5%\n",
      "[BATCH 26/149] Loss_D: 0.8555 Loss_G: 0.8286 acc: 60.9%\n",
      "[BATCH 27/149] Loss_D: 0.7870 Loss_G: 0.8223 acc: 60.9%\n",
      "[BATCH 28/149] Loss_D: 0.7873 Loss_G: 0.8172 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.8142 Loss_G: 0.8213 acc: 62.5%\n",
      "[BATCH 30/149] Loss_D: 0.7907 Loss_G: 0.8263 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.7582 Loss_G: 0.8122 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.8800 Loss_G: 0.8278 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.8451 Loss_G: 0.8460 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.8081 Loss_G: 0.8294 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.8371 Loss_G: 0.8333 acc: 53.1%\n",
      "[BATCH 36/149] Loss_D: 0.8328 Loss_G: 0.8327 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7799 Loss_G: 0.8159 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.7811 Loss_G: 0.8108 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.7874 Loss_G: 0.8068 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.8194 Loss_G: 0.8106 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.8121 Loss_G: 0.8207 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.8089 Loss_G: 0.8256 acc: 70.3%\n",
      "[BATCH 43/149] Loss_D: 0.7823 Loss_G: 0.8146 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.8013 Loss_G: 0.8105 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.8213 Loss_G: 0.8164 acc: 75.0%\n",
      "[BATCH 46/149] Loss_D: 0.7976 Loss_G: 0.8253 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.7794 Loss_G: 0.8091 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.7910 Loss_G: 0.8025 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.8015 Loss_G: 0.8054 acc: 70.3%\n",
      "[EPOCH 7350] TEST ACC is : 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7614 Loss_G: 0.8105 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7992 Loss_G: 0.8062 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7694 Loss_G: 0.8013 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.8383 Loss_G: 0.8113 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.7989 Loss_G: 0.8163 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.8416 Loss_G: 0.8319 acc: 57.8%\n",
      "[BATCH 56/149] Loss_D: 0.8033 Loss_G: 0.8250 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7772 Loss_G: 0.8256 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.7676 Loss_G: 0.8102 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.8022 Loss_G: 0.8151 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7920 Loss_G: 0.8156 acc: 54.7%\n",
      "[BATCH 61/149] Loss_D: 0.8183 Loss_G: 0.8214 acc: 60.9%\n",
      "[BATCH 62/149] Loss_D: 0.8351 Loss_G: 0.8186 acc: 54.7%\n",
      "[BATCH 63/149] Loss_D: 0.7597 Loss_G: 0.8026 acc: 67.2%\n",
      "[BATCH 64/149] Loss_D: 0.7783 Loss_G: 0.7959 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8227 Loss_G: 0.8106 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.7927 Loss_G: 0.8142 acc: 57.8%\n",
      "[BATCH 67/149] Loss_D: 0.7992 Loss_G: 0.8090 acc: 70.3%\n",
      "[BATCH 68/149] Loss_D: 0.7863 Loss_G: 0.8101 acc: 68.8%\n",
      "[BATCH 69/149] Loss_D: 0.8041 Loss_G: 0.8174 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.8014 Loss_G: 0.8178 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7808 Loss_G: 0.8184 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.7972 Loss_G: 0.8159 acc: 73.4%\n",
      "[BATCH 73/149] Loss_D: 0.7685 Loss_G: 0.8147 acc: 73.4%\n",
      "[BATCH 74/149] Loss_D: 0.8088 Loss_G: 0.8190 acc: 57.8%\n",
      "[BATCH 75/149] Loss_D: 0.7979 Loss_G: 0.8199 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.7823 Loss_G: 0.8116 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.8348 Loss_G: 0.8199 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.7822 Loss_G: 0.8155 acc: 56.2%\n",
      "[BATCH 79/149] Loss_D: 0.7437 Loss_G: 0.8058 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7514 Loss_G: 0.8008 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.7968 Loss_G: 0.8040 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.7537 Loss_G: 0.8055 acc: 70.3%\n",
      "[BATCH 83/149] Loss_D: 0.7699 Loss_G: 0.7962 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.7751 Loss_G: 0.7929 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.7756 Loss_G: 0.7871 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.8357 Loss_G: 0.8122 acc: 57.8%\n",
      "[BATCH 87/149] Loss_D: 0.8304 Loss_G: 0.8353 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7508 Loss_G: 0.8143 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.8709 Loss_G: 0.8383 acc: 57.8%\n",
      "[BATCH 90/149] Loss_D: 0.7758 Loss_G: 0.8235 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.8385 Loss_G: 0.8228 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.8119 Loss_G: 0.8326 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.7853 Loss_G: 0.8177 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.8647 Loss_G: 0.8275 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.7609 Loss_G: 0.8255 acc: 75.0%\n",
      "[BATCH 96/149] Loss_D: 0.8125 Loss_G: 0.8314 acc: 56.2%\n",
      "[BATCH 97/149] Loss_D: 0.7419 Loss_G: 0.8156 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.8447 Loss_G: 0.8176 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.7994 Loss_G: 0.8266 acc: 64.1%\n",
      "[EPOCH 7400] TEST ACC is : 76.2%\n",
      "[BATCH 100/149] Loss_D: 0.7849 Loss_G: 0.8276 acc: 76.6%\n",
      "[BATCH 101/149] Loss_D: 0.7914 Loss_G: 0.8215 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7728 Loss_G: 0.8272 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7551 Loss_G: 0.8077 acc: 59.4%\n",
      "[BATCH 104/149] Loss_D: 0.7809 Loss_G: 0.8051 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7862 Loss_G: 0.8039 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7771 Loss_G: 0.8067 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.8154 Loss_G: 0.8032 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.7527 Loss_G: 0.7971 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.7764 Loss_G: 0.7984 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.7587 Loss_G: 0.7987 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7861 Loss_G: 0.8050 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8235 Loss_G: 0.8187 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.8254 Loss_G: 0.8250 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.7686 Loss_G: 0.8129 acc: 57.8%\n",
      "[BATCH 115/149] Loss_D: 0.7923 Loss_G: 0.8069 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.8444 Loss_G: 0.8226 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7754 Loss_G: 0.8206 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.7423 Loss_G: 0.8108 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.8250 Loss_G: 0.8188 acc: 75.0%\n",
      "[BATCH 120/149] Loss_D: 0.7914 Loss_G: 0.8119 acc: 56.2%\n",
      "[BATCH 121/149] Loss_D: 0.8270 Loss_G: 0.8174 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8035 Loss_G: 0.8225 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.8164 Loss_G: 0.8251 acc: 75.0%\n",
      "[BATCH 124/149] Loss_D: 0.8011 Loss_G: 0.8332 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.8126 Loss_G: 0.8262 acc: 59.4%\n",
      "[BATCH 126/149] Loss_D: 0.8206 Loss_G: 0.8301 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.8213 Loss_G: 0.8319 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.8695 Loss_G: 0.8392 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.8283 Loss_G: 0.8456 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.8179 Loss_G: 0.8374 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.8126 Loss_G: 0.8264 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.8254 Loss_G: 0.8243 acc: 59.4%\n",
      "[BATCH 133/149] Loss_D: 0.8137 Loss_G: 0.8235 acc: 71.9%\n",
      "[BATCH 134/149] Loss_D: 0.8049 Loss_G: 0.8323 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.8193 Loss_G: 0.8292 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.7743 Loss_G: 0.8207 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.7612 Loss_G: 0.8178 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7602 Loss_G: 0.7956 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.8058 Loss_G: 0.7986 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.7868 Loss_G: 0.8080 acc: 65.6%\n",
      "[BATCH 141/149] Loss_D: 0.7657 Loss_G: 0.8010 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.8703 Loss_G: 0.8200 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.7955 Loss_G: 0.8190 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7568 Loss_G: 0.8099 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.8079 Loss_G: 0.8061 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.7710 Loss_G: 0.7987 acc: 53.1%\n",
      "[BATCH 147/149] Loss_D: 0.8480 Loss_G: 0.8107 acc: 60.9%\n",
      "[BATCH 148/149] Loss_D: 0.7854 Loss_G: 0.8218 acc: 65.6%\n",
      "[BATCH 149/149] Loss_D: 0.7683 Loss_G: 0.8064 acc: 64.1%\n",
      "[EPOCH 7450] TEST ACC is : 76.0%\n",
      "-----THE [50/50] epoch end-----\n",
      "The 2 * 50 epochs train starts:\n",
      "-----THE [1/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8390 Loss_G: 0.8155 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.7829 Loss_G: 0.8161 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.8005 Loss_G: 0.8244 acc: 75.0%\n",
      "[BATCH 4/149] Loss_D: 0.8119 Loss_G: 0.8269 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.8062 Loss_G: 0.8178 acc: 78.1%\n",
      "[BATCH 6/149] Loss_D: 0.7792 Loss_G: 0.8144 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7969 Loss_G: 0.8107 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.8267 Loss_G: 0.8287 acc: 68.8%\n",
      "[BATCH 9/149] Loss_D: 0.7969 Loss_G: 0.8281 acc: 73.4%\n",
      "[BATCH 10/149] Loss_D: 0.8047 Loss_G: 0.8203 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.7810 Loss_G: 0.8051 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.8118 Loss_G: 0.8080 acc: 62.5%\n",
      "[BATCH 13/149] Loss_D: 0.7900 Loss_G: 0.8016 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.8452 Loss_G: 0.8104 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.8338 Loss_G: 0.8238 acc: 57.8%\n",
      "[BATCH 16/149] Loss_D: 0.7427 Loss_G: 0.8041 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.7918 Loss_G: 0.7999 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.8222 Loss_G: 0.8147 acc: 78.1%\n",
      "[BATCH 19/149] Loss_D: 0.7876 Loss_G: 0.8141 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.7536 Loss_G: 0.8072 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.8177 Loss_G: 0.8142 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.8117 Loss_G: 0.8220 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.7566 Loss_G: 0.8072 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7901 Loss_G: 0.8065 acc: 70.3%\n",
      "[BATCH 25/149] Loss_D: 0.7869 Loss_G: 0.8089 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7731 Loss_G: 0.8098 acc: 53.1%\n",
      "[BATCH 27/149] Loss_D: 0.7583 Loss_G: 0.8046 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.7840 Loss_G: 0.8087 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7688 Loss_G: 0.8019 acc: 57.8%\n",
      "[BATCH 30/149] Loss_D: 0.8009 Loss_G: 0.8094 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.8065 Loss_G: 0.8201 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.8472 Loss_G: 0.8349 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.8412 Loss_G: 0.8398 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8408 Loss_G: 0.8351 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7923 Loss_G: 0.8276 acc: 73.4%\n",
      "[BATCH 36/149] Loss_D: 0.7653 Loss_G: 0.8056 acc: 57.8%\n",
      "[BATCH 37/149] Loss_D: 0.7485 Loss_G: 0.7940 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.8238 Loss_G: 0.8026 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.8178 Loss_G: 0.8228 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.7824 Loss_G: 0.8206 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.8307 Loss_G: 0.8243 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7340 Loss_G: 0.8058 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.8009 Loss_G: 0.8000 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.8266 Loss_G: 0.8085 acc: 78.1%\n",
      "[BATCH 45/149] Loss_D: 0.7571 Loss_G: 0.8072 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8163 Loss_G: 0.8209 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7742 Loss_G: 0.8098 acc: 67.2%\n",
      "[BATCH 48/149] Loss_D: 0.8017 Loss_G: 0.8087 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.7526 Loss_G: 0.8035 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7775 Loss_G: 0.8007 acc: 56.2%\n",
      "[EPOCH 50] TEST ACC is : 75.0%\n",
      "[BATCH 51/149] Loss_D: 0.8290 Loss_G: 0.8092 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7872 Loss_G: 0.8136 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.8365 Loss_G: 0.8238 acc: 71.9%\n",
      "[BATCH 54/149] Loss_D: 0.7364 Loss_G: 0.8100 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.7522 Loss_G: 0.8001 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.8205 Loss_G: 0.8019 acc: 51.6%\n",
      "[BATCH 57/149] Loss_D: 0.7956 Loss_G: 0.8115 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.8061 Loss_G: 0.8220 acc: 73.4%\n",
      "[BATCH 59/149] Loss_D: 0.7813 Loss_G: 0.8164 acc: 73.4%\n",
      "[BATCH 60/149] Loss_D: 0.7814 Loss_G: 0.8209 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.8218 Loss_G: 0.8252 acc: 64.1%\n",
      "[BATCH 62/149] Loss_D: 0.7991 Loss_G: 0.8309 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.7917 Loss_G: 0.8311 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7698 Loss_G: 0.8221 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.7737 Loss_G: 0.8093 acc: 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.7988 Loss_G: 0.8163 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.7795 Loss_G: 0.8143 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7917 Loss_G: 0.8166 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.8551 Loss_G: 0.8288 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.7719 Loss_G: 0.8197 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.7897 Loss_G: 0.8127 acc: 59.4%\n",
      "[BATCH 72/149] Loss_D: 0.7864 Loss_G: 0.8217 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.7915 Loss_G: 0.8230 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7927 Loss_G: 0.8183 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.8218 Loss_G: 0.8319 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7868 Loss_G: 0.8170 acc: 64.1%\n",
      "[BATCH 77/149] Loss_D: 0.8235 Loss_G: 0.8196 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.8417 Loss_G: 0.8268 acc: 54.7%\n",
      "[BATCH 79/149] Loss_D: 0.7677 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7868 Loss_G: 0.8117 acc: 57.8%\n",
      "[BATCH 81/149] Loss_D: 0.8122 Loss_G: 0.8161 acc: 62.5%\n",
      "[BATCH 82/149] Loss_D: 0.7871 Loss_G: 0.8179 acc: 70.3%\n",
      "[BATCH 83/149] Loss_D: 0.8173 Loss_G: 0.8199 acc: 53.1%\n",
      "[BATCH 84/149] Loss_D: 0.7882 Loss_G: 0.8109 acc: 57.8%\n",
      "[BATCH 85/149] Loss_D: 0.7788 Loss_G: 0.8054 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.8233 Loss_G: 0.8185 acc: 62.5%\n",
      "[BATCH 87/149] Loss_D: 0.7570 Loss_G: 0.8183 acc: 73.4%\n",
      "[BATCH 88/149] Loss_D: 0.8040 Loss_G: 0.8228 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.7993 Loss_G: 0.8340 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.7915 Loss_G: 0.8261 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.8299 Loss_G: 0.8267 acc: 73.4%\n",
      "[BATCH 92/149] Loss_D: 0.7791 Loss_G: 0.8157 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.7565 Loss_G: 0.8014 acc: 65.6%\n",
      "[BATCH 94/149] Loss_D: 0.7729 Loss_G: 0.7945 acc: 75.0%\n",
      "[BATCH 95/149] Loss_D: 0.7890 Loss_G: 0.7994 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.8420 Loss_G: 0.8102 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.8063 Loss_G: 0.8104 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.7687 Loss_G: 0.8109 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.8115 Loss_G: 0.8159 acc: 71.9%\n",
      "[BATCH 100/149] Loss_D: 0.8305 Loss_G: 0.8280 acc: 57.8%\n",
      "[EPOCH 100] TEST ACC is : 74.2%\n",
      "[BATCH 101/149] Loss_D: 0.7971 Loss_G: 0.8220 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.8788 Loss_G: 0.8370 acc: 62.5%\n",
      "[BATCH 103/149] Loss_D: 0.7834 Loss_G: 0.8295 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.7989 Loss_G: 0.8310 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.7793 Loss_G: 0.8132 acc: 70.3%\n",
      "[BATCH 106/149] Loss_D: 0.7815 Loss_G: 0.8122 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.7900 Loss_G: 0.8091 acc: 59.4%\n",
      "[BATCH 108/149] Loss_D: 0.7718 Loss_G: 0.8059 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.8173 Loss_G: 0.8063 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7489 Loss_G: 0.7990 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7830 Loss_G: 0.8008 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7849 Loss_G: 0.8054 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.8295 Loss_G: 0.8273 acc: 68.8%\n",
      "[BATCH 114/149] Loss_D: 0.8361 Loss_G: 0.8542 acc: 70.3%\n",
      "[BATCH 115/149] Loss_D: 0.7966 Loss_G: 0.8337 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8077 Loss_G: 0.8346 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7853 Loss_G: 0.8164 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.8132 Loss_G: 0.8176 acc: 59.4%\n",
      "[BATCH 119/149] Loss_D: 0.8130 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.8213 Loss_G: 0.8196 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.7813 Loss_G: 0.8109 acc: 70.3%\n",
      "[BATCH 122/149] Loss_D: 0.8569 Loss_G: 0.8236 acc: 57.8%\n",
      "[BATCH 123/149] Loss_D: 0.8148 Loss_G: 0.8331 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.8103 Loss_G: 0.8319 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.7980 Loss_G: 0.8229 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.8122 Loss_G: 0.8197 acc: 64.1%\n",
      "[BATCH 127/149] Loss_D: 0.7760 Loss_G: 0.8186 acc: 68.8%\n",
      "[BATCH 128/149] Loss_D: 0.7776 Loss_G: 0.8109 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.8601 Loss_G: 0.8325 acc: 59.4%\n",
      "[BATCH 130/149] Loss_D: 0.8079 Loss_G: 0.8201 acc: 60.9%\n",
      "[BATCH 131/149] Loss_D: 0.7786 Loss_G: 0.8036 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.8012 Loss_G: 0.8050 acc: 53.1%\n",
      "[BATCH 133/149] Loss_D: 0.8247 Loss_G: 0.8116 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.8417 Loss_G: 0.8276 acc: 62.5%\n",
      "[BATCH 135/149] Loss_D: 0.7724 Loss_G: 0.8158 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.8010 Loss_G: 0.8075 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7969 Loss_G: 0.8027 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.8178 Loss_G: 0.8097 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.7943 Loss_G: 0.8105 acc: 75.0%\n",
      "[BATCH 140/149] Loss_D: 0.7482 Loss_G: 0.8006 acc: 54.7%\n",
      "[BATCH 141/149] Loss_D: 0.7686 Loss_G: 0.7962 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.7863 Loss_G: 0.8004 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.7601 Loss_G: 0.7993 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.8498 Loss_G: 0.8141 acc: 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.8118 Loss_G: 0.8165 acc: 53.1%\n",
      "[BATCH 146/149] Loss_D: 0.7851 Loss_G: 0.8139 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.8427 Loss_G: 0.8200 acc: 68.8%\n",
      "[BATCH 148/149] Loss_D: 0.7658 Loss_G: 0.8076 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.8205 Loss_G: 0.8113 acc: 73.4%\n",
      "-----THE [1/50] epoch end-----\n",
      "-----THE [2/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8042 Loss_G: 0.8120 acc: 60.9%\n",
      "[EPOCH 150] TEST ACC is : 72.9%\n",
      "[BATCH 2/149] Loss_D: 0.7522 Loss_G: 0.8038 acc: 59.4%\n",
      "[BATCH 3/149] Loss_D: 0.7864 Loss_G: 0.8066 acc: 53.1%\n",
      "[BATCH 4/149] Loss_D: 0.8354 Loss_G: 0.8236 acc: 56.2%\n",
      "[BATCH 5/149] Loss_D: 0.8060 Loss_G: 0.8253 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.7753 Loss_G: 0.8250 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.8269 Loss_G: 0.8264 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.8032 Loss_G: 0.8293 acc: 56.2%\n",
      "[BATCH 9/149] Loss_D: 0.8118 Loss_G: 0.8210 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8150 Loss_G: 0.8198 acc: 54.7%\n",
      "[BATCH 11/149] Loss_D: 0.7880 Loss_G: 0.8089 acc: 68.8%\n",
      "[BATCH 12/149] Loss_D: 0.7553 Loss_G: 0.8005 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7603 Loss_G: 0.7928 acc: 67.2%\n",
      "[BATCH 14/149] Loss_D: 0.7896 Loss_G: 0.8022 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7720 Loss_G: 0.7999 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.7790 Loss_G: 0.8033 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.8041 Loss_G: 0.8077 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.7692 Loss_G: 0.8153 acc: 75.0%\n",
      "[BATCH 19/149] Loss_D: 0.8185 Loss_G: 0.8165 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7883 Loss_G: 0.8097 acc: 70.3%\n",
      "[BATCH 21/149] Loss_D: 0.8010 Loss_G: 0.8087 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.7782 Loss_G: 0.8146 acc: 73.4%\n",
      "[BATCH 23/149] Loss_D: 0.7955 Loss_G: 0.8104 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7776 Loss_G: 0.8086 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.8199 Loss_G: 0.8286 acc: 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.7898 Loss_G: 0.8286 acc: 57.8%\n",
      "[BATCH 27/149] Loss_D: 0.7974 Loss_G: 0.8168 acc: 73.4%\n",
      "[BATCH 28/149] Loss_D: 0.8279 Loss_G: 0.8247 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.7701 Loss_G: 0.8099 acc: 67.2%\n",
      "[BATCH 30/149] Loss_D: 0.9083 Loss_G: 0.8407 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.7852 Loss_G: 0.8190 acc: 76.6%\n",
      "[BATCH 32/149] Loss_D: 0.8023 Loss_G: 0.8145 acc: 56.2%\n",
      "[BATCH 33/149] Loss_D: 0.8180 Loss_G: 0.8096 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7845 Loss_G: 0.8173 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7848 Loss_G: 0.8127 acc: 67.2%\n",
      "[BATCH 36/149] Loss_D: 0.7828 Loss_G: 0.8084 acc: 59.4%\n",
      "[BATCH 37/149] Loss_D: 0.7664 Loss_G: 0.8134 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.7714 Loss_G: 0.8075 acc: 62.5%\n",
      "[BATCH 39/149] Loss_D: 0.8489 Loss_G: 0.8284 acc: 70.3%\n",
      "[BATCH 40/149] Loss_D: 0.7923 Loss_G: 0.8218 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.8297 Loss_G: 0.8315 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7936 Loss_G: 0.8306 acc: 65.6%\n",
      "[BATCH 43/149] Loss_D: 0.7898 Loss_G: 0.8247 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.7815 Loss_G: 0.8244 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.8017 Loss_G: 0.8197 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7993 Loss_G: 0.8230 acc: 64.1%\n",
      "[BATCH 47/149] Loss_D: 0.7888 Loss_G: 0.8213 acc: 73.4%\n",
      "[BATCH 48/149] Loss_D: 0.8165 Loss_G: 0.8164 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7865 Loss_G: 0.8149 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.7548 Loss_G: 0.8101 acc: 57.8%\n",
      "[BATCH 51/149] Loss_D: 0.7865 Loss_G: 0.8087 acc: 57.8%\n",
      "[EPOCH 200] TEST ACC is : 74.2%\n",
      "[BATCH 52/149] Loss_D: 0.8166 Loss_G: 0.8159 acc: 59.4%\n",
      "[BATCH 53/149] Loss_D: 0.8000 Loss_G: 0.8194 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.7850 Loss_G: 0.8187 acc: 56.2%\n",
      "[BATCH 55/149] Loss_D: 0.7801 Loss_G: 0.8128 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7952 Loss_G: 0.8026 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.7470 Loss_G: 0.7988 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.7782 Loss_G: 0.7969 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.7952 Loss_G: 0.8059 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7884 Loss_G: 0.8161 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.8098 Loss_G: 0.8187 acc: 60.9%\n",
      "[BATCH 62/149] Loss_D: 0.7531 Loss_G: 0.8145 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.7630 Loss_G: 0.7988 acc: 62.5%\n",
      "[BATCH 64/149] Loss_D: 0.7860 Loss_G: 0.7978 acc: 64.1%\n",
      "[BATCH 65/149] Loss_D: 0.7536 Loss_G: 0.7973 acc: 53.1%\n",
      "[BATCH 66/149] Loss_D: 0.8105 Loss_G: 0.8033 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7920 Loss_G: 0.8060 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.7907 Loss_G: 0.8039 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.8063 Loss_G: 0.8130 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.7969 Loss_G: 0.8110 acc: 70.3%\n",
      "[BATCH 71/149] Loss_D: 0.8732 Loss_G: 0.8399 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.8090 Loss_G: 0.8376 acc: 71.9%\n",
      "[BATCH 73/149] Loss_D: 0.7750 Loss_G: 0.8238 acc: 65.6%\n",
      "[BATCH 74/149] Loss_D: 0.7819 Loss_G: 0.8198 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.7549 Loss_G: 0.8148 acc: 57.8%\n",
      "[BATCH 76/149] Loss_D: 0.8025 Loss_G: 0.8304 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7990 Loss_G: 0.8298 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.7807 Loss_G: 0.8293 acc: 70.3%\n",
      "[BATCH 79/149] Loss_D: 0.7757 Loss_G: 0.8157 acc: 62.5%\n",
      "[BATCH 80/149] Loss_D: 0.7891 Loss_G: 0.8139 acc: 73.4%\n",
      "[BATCH 81/149] Loss_D: 0.8235 Loss_G: 0.8192 acc: 57.8%\n",
      "[BATCH 82/149] Loss_D: 0.7633 Loss_G: 0.8213 acc: 73.4%\n",
      "[BATCH 83/149] Loss_D: 0.8098 Loss_G: 0.8235 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.7903 Loss_G: 0.8372 acc: 67.2%\n",
      "[BATCH 85/149] Loss_D: 0.7892 Loss_G: 0.8169 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.8511 Loss_G: 0.8259 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.8035 Loss_G: 0.8183 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.7670 Loss_G: 0.8105 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.8113 Loss_G: 0.8117 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.8946 Loss_G: 0.8424 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.8243 Loss_G: 0.8425 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.8037 Loss_G: 0.8127 acc: 64.1%\n",
      "[BATCH 93/149] Loss_D: 0.7731 Loss_G: 0.8039 acc: 59.4%\n",
      "[BATCH 94/149] Loss_D: 0.7896 Loss_G: 0.8036 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.8247 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.8054 Loss_G: 0.8132 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.8200 Loss_G: 0.8174 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.8312 Loss_G: 0.8272 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.8027 Loss_G: 0.8235 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.8158 Loss_G: 0.8356 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.8020 Loss_G: 0.8383 acc: 65.6%\n",
      "[EPOCH 250] TEST ACC is : 76.4%\n",
      "[BATCH 102/149] Loss_D: 0.8136 Loss_G: 0.8545 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.8058 Loss_G: 0.8476 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.7986 Loss_G: 0.8337 acc: 68.8%\n",
      "[BATCH 105/149] Loss_D: 0.7942 Loss_G: 0.8249 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.8088 Loss_G: 0.8229 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.8244 Loss_G: 0.8201 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8407 Loss_G: 0.8256 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.8553 Loss_G: 0.8299 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.8019 Loss_G: 0.8211 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.8571 Loss_G: 0.8304 acc: 60.9%\n",
      "[BATCH 112/149] Loss_D: 0.7983 Loss_G: 0.8344 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7788 Loss_G: 0.8278 acc: 68.8%\n",
      "[BATCH 114/149] Loss_D: 0.8150 Loss_G: 0.8212 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.8127 Loss_G: 0.8130 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.7837 Loss_G: 0.8093 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7946 Loss_G: 0.8072 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.8051 Loss_G: 0.8113 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.8246 Loss_G: 0.8203 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.8218 Loss_G: 0.8314 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.8006 Loss_G: 0.8292 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.7970 Loss_G: 0.8235 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.7936 Loss_G: 0.8165 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.7748 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.8197 Loss_G: 0.8181 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7661 Loss_G: 0.8076 acc: 71.9%\n",
      "[BATCH 127/149] Loss_D: 0.8030 Loss_G: 0.8085 acc: 75.0%\n",
      "[BATCH 128/149] Loss_D: 0.7559 Loss_G: 0.8010 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 0.7643 Loss_G: 0.7943 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7517 Loss_G: 0.7923 acc: 67.2%\n",
      "[BATCH 131/149] Loss_D: 0.8294 Loss_G: 0.8094 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.7716 Loss_G: 0.8028 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.7410 Loss_G: 0.7935 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.8383 Loss_G: 0.8043 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7585 Loss_G: 0.8070 acc: 68.8%\n",
      "[BATCH 136/149] Loss_D: 0.7628 Loss_G: 0.7946 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.8547 Loss_G: 0.8154 acc: 56.2%\n",
      "[BATCH 138/149] Loss_D: 0.7981 Loss_G: 0.8343 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.7798 Loss_G: 0.8191 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.8462 Loss_G: 0.8174 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.7914 Loss_G: 0.8201 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7867 Loss_G: 0.8149 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.7969 Loss_G: 0.8222 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7946 Loss_G: 0.8194 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.8064 Loss_G: 0.8185 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7889 Loss_G: 0.8238 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7691 Loss_G: 0.8110 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7473 Loss_G: 0.8019 acc: 60.9%\n",
      "[BATCH 149/149] Loss_D: 0.8030 Loss_G: 0.8105 acc: 64.1%\n",
      "-----THE [2/50] epoch end-----\n",
      "-----THE [3/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7620 Loss_G: 0.8219 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.8274 Loss_G: 0.8138 acc: 67.2%\n",
      "[EPOCH 300] TEST ACC is : 76.0%\n",
      "[BATCH 3/149] Loss_D: 0.8100 Loss_G: 0.8241 acc: 59.4%\n",
      "[BATCH 4/149] Loss_D: 0.7960 Loss_G: 0.8257 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7570 Loss_G: 0.8131 acc: 56.2%\n",
      "[BATCH 6/149] Loss_D: 0.8204 Loss_G: 0.8226 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.8111 Loss_G: 0.8247 acc: 56.2%\n",
      "[BATCH 8/149] Loss_D: 0.7855 Loss_G: 0.8238 acc: 75.0%\n",
      "[BATCH 9/149] Loss_D: 0.7579 Loss_G: 0.8104 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7844 Loss_G: 0.8128 acc: 57.8%\n",
      "[BATCH 11/149] Loss_D: 0.8079 Loss_G: 0.8330 acc: 67.2%\n",
      "[BATCH 12/149] Loss_D: 0.7852 Loss_G: 0.8431 acc: 73.4%\n",
      "[BATCH 13/149] Loss_D: 0.8432 Loss_G: 0.8276 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.8125 Loss_G: 0.8365 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.7473 Loss_G: 0.8155 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.8038 Loss_G: 0.8069 acc: 65.6%\n",
      "[BATCH 17/149] Loss_D: 0.7662 Loss_G: 0.8001 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.8541 Loss_G: 0.8168 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7880 Loss_G: 0.8201 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.7611 Loss_G: 0.8104 acc: 51.6%\n",
      "[BATCH 21/149] Loss_D: 0.8069 Loss_G: 0.8098 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.7797 Loss_G: 0.8059 acc: 57.8%\n",
      "[BATCH 23/149] Loss_D: 0.8011 Loss_G: 0.8003 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.7934 Loss_G: 0.7975 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.7960 Loss_G: 0.8043 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7864 Loss_G: 0.8073 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.8073 Loss_G: 0.8076 acc: 73.4%\n",
      "[BATCH 28/149] Loss_D: 0.8563 Loss_G: 0.8269 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.8255 Loss_G: 0.8303 acc: 62.5%\n",
      "[BATCH 30/149] Loss_D: 0.7758 Loss_G: 0.8144 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.8037 Loss_G: 0.8198 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.8083 Loss_G: 0.8284 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.8258 Loss_G: 0.8278 acc: 73.4%\n",
      "[BATCH 34/149] Loss_D: 0.8333 Loss_G: 0.8346 acc: 60.9%\n",
      "[BATCH 35/149] Loss_D: 0.7551 Loss_G: 0.8126 acc: 57.8%\n",
      "[BATCH 36/149] Loss_D: 0.7886 Loss_G: 0.8045 acc: 59.4%\n",
      "[BATCH 37/149] Loss_D: 0.7717 Loss_G: 0.8017 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.8408 Loss_G: 0.8169 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.8788 Loss_G: 0.8397 acc: 60.9%\n",
      "[BATCH 40/149] Loss_D: 0.7792 Loss_G: 0.8282 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.8349 Loss_G: 0.8094 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.8173 Loss_G: 0.8180 acc: 56.2%\n",
      "[BATCH 43/149] Loss_D: 0.8050 Loss_G: 0.8193 acc: 57.8%\n",
      "[BATCH 44/149] Loss_D: 0.7717 Loss_G: 0.8116 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.8304 Loss_G: 0.8314 acc: 75.0%\n",
      "[BATCH 46/149] Loss_D: 0.7860 Loss_G: 0.8242 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.8209 Loss_G: 0.8175 acc: 71.9%\n",
      "[BATCH 48/149] Loss_D: 0.7911 Loss_G: 0.8161 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7835 Loss_G: 0.8043 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7860 Loss_G: 0.8090 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.7753 Loss_G: 0.8153 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7866 Loss_G: 0.8229 acc: 68.8%\n",
      "[EPOCH 350] TEST ACC is : 75.0%\n",
      "[BATCH 53/149] Loss_D: 0.7963 Loss_G: 0.8156 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.7687 Loss_G: 0.8077 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.7979 Loss_G: 0.8004 acc: 59.4%\n",
      "[BATCH 56/149] Loss_D: 0.8311 Loss_G: 0.8085 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.8098 Loss_G: 0.8111 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.7745 Loss_G: 0.8043 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.8069 Loss_G: 0.8068 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.8219 Loss_G: 0.8143 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.7914 Loss_G: 0.8102 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.8158 Loss_G: 0.8045 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.7520 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.8231 Loss_G: 0.7943 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.8260 Loss_G: 0.8092 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.7945 Loss_G: 0.8165 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.8194 Loss_G: 0.8273 acc: 70.3%\n",
      "[BATCH 68/149] Loss_D: 0.7820 Loss_G: 0.8271 acc: 57.8%\n",
      "[BATCH 69/149] Loss_D: 0.7725 Loss_G: 0.8122 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.8230 Loss_G: 0.8262 acc: 75.0%\n",
      "[BATCH 71/149] Loss_D: 0.8009 Loss_G: 0.8184 acc: 79.7%\n",
      "[BATCH 72/149] Loss_D: 0.7723 Loss_G: 0.8144 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7785 Loss_G: 0.8160 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.8199 Loss_G: 0.8246 acc: 54.7%\n",
      "[BATCH 75/149] Loss_D: 0.8112 Loss_G: 0.8267 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.7801 Loss_G: 0.8237 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.7957 Loss_G: 0.8155 acc: 76.6%\n",
      "[BATCH 78/149] Loss_D: 0.8343 Loss_G: 0.8249 acc: 70.3%\n",
      "[BATCH 79/149] Loss_D: 0.7798 Loss_G: 0.8200 acc: 79.7%\n",
      "[BATCH 80/149] Loss_D: 0.7919 Loss_G: 0.8145 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7828 Loss_G: 0.8101 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.7971 Loss_G: 0.8094 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7852 Loss_G: 0.8098 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.7614 Loss_G: 0.7990 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.8186 Loss_G: 0.8086 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.8105 Loss_G: 0.8195 acc: 73.4%\n",
      "[BATCH 87/149] Loss_D: 0.7483 Loss_G: 0.8068 acc: 76.6%\n",
      "[BATCH 88/149] Loss_D: 0.8040 Loss_G: 0.7974 acc: 54.7%\n",
      "[BATCH 89/149] Loss_D: 0.8034 Loss_G: 0.8014 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.8234 Loss_G: 0.8088 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.7591 Loss_G: 0.8002 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.7950 Loss_G: 0.8024 acc: 64.1%\n",
      "[BATCH 93/149] Loss_D: 0.7703 Loss_G: 0.8010 acc: 53.1%\n",
      "[BATCH 94/149] Loss_D: 0.7967 Loss_G: 0.7923 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7926 Loss_G: 0.8066 acc: 54.7%\n",
      "[BATCH 96/149] Loss_D: 0.7909 Loss_G: 0.8104 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7974 Loss_G: 0.8101 acc: 57.8%\n",
      "[BATCH 98/149] Loss_D: 0.8366 Loss_G: 0.8168 acc: 65.6%\n",
      "[BATCH 99/149] Loss_D: 0.7385 Loss_G: 0.8023 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.8009 Loss_G: 0.7996 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.7789 Loss_G: 0.7996 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.7922 Loss_G: 0.8176 acc: 54.7%\n",
      "[EPOCH 400] TEST ACC is : 72.9%\n",
      "[BATCH 103/149] Loss_D: 0.8092 Loss_G: 0.8131 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.7808 Loss_G: 0.8063 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.8302 Loss_G: 0.8162 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7478 Loss_G: 0.8178 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7518 Loss_G: 0.8158 acc: 75.0%\n",
      "[BATCH 108/149] Loss_D: 0.8074 Loss_G: 0.8255 acc: 60.9%\n",
      "[BATCH 109/149] Loss_D: 0.7997 Loss_G: 0.8241 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.7745 Loss_G: 0.8215 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.7806 Loss_G: 0.8081 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7861 Loss_G: 0.8122 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.8290 Loss_G: 0.8268 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7973 Loss_G: 0.8309 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.7965 Loss_G: 0.8282 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7936 Loss_G: 0.8269 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7491 Loss_G: 0.8146 acc: 67.2%\n",
      "[BATCH 118/149] Loss_D: 0.8080 Loss_G: 0.8168 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.8326 Loss_G: 0.8323 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7454 Loss_G: 0.8197 acc: 54.7%\n",
      "[BATCH 121/149] Loss_D: 0.7930 Loss_G: 0.8064 acc: 59.4%\n",
      "[BATCH 122/149] Loss_D: 0.8166 Loss_G: 0.8135 acc: 62.5%\n",
      "[BATCH 123/149] Loss_D: 0.7382 Loss_G: 0.8006 acc: 51.6%\n",
      "[BATCH 124/149] Loss_D: 0.8443 Loss_G: 0.8072 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.8296 Loss_G: 0.8119 acc: 70.3%\n",
      "[BATCH 126/149] Loss_D: 0.8284 Loss_G: 0.8223 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.7873 Loss_G: 0.8180 acc: 68.8%\n",
      "[BATCH 128/149] Loss_D: 0.7765 Loss_G: 0.8072 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.8277 Loss_G: 0.8180 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7759 Loss_G: 0.8141 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.7819 Loss_G: 0.8137 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.8147 Loss_G: 0.8145 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.8634 Loss_G: 0.8248 acc: 62.5%\n",
      "[BATCH 134/149] Loss_D: 0.8010 Loss_G: 0.8273 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.7678 Loss_G: 0.8155 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.7937 Loss_G: 0.8114 acc: 68.8%\n",
      "[BATCH 137/149] Loss_D: 0.8135 Loss_G: 0.8174 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7906 Loss_G: 0.8168 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7986 Loss_G: 0.8214 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7662 Loss_G: 0.8092 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7972 Loss_G: 0.8132 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.7972 Loss_G: 0.8141 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.8089 Loss_G: 0.8236 acc: 59.4%\n",
      "[BATCH 144/149] Loss_D: 0.7844 Loss_G: 0.8137 acc: 68.8%\n",
      "[BATCH 145/149] Loss_D: 0.8633 Loss_G: 0.8517 acc: 64.1%\n",
      "[BATCH 146/149] Loss_D: 0.7568 Loss_G: 0.8518 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.8148 Loss_G: 0.8467 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.8036 Loss_G: 0.8335 acc: 78.1%\n",
      "[BATCH 149/149] Loss_D: 0.7628 Loss_G: 0.8297 acc: 64.1%\n",
      "-----THE [3/50] epoch end-----\n",
      "-----THE [4/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7874 Loss_G: 0.8234 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.8308 Loss_G: 0.8399 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7622 Loss_G: 0.8305 acc: 70.3%\n",
      "[EPOCH 450] TEST ACC is : 73.6%\n",
      "[BATCH 4/149] Loss_D: 0.8085 Loss_G: 0.8205 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.7658 Loss_G: 0.8206 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.7599 Loss_G: 0.8126 acc: 64.1%\n",
      "[BATCH 7/149] Loss_D: 0.8345 Loss_G: 0.8200 acc: 57.8%\n",
      "[BATCH 8/149] Loss_D: 0.7995 Loss_G: 0.8352 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.7996 Loss_G: 0.8224 acc: 56.2%\n",
      "[BATCH 10/149] Loss_D: 0.7533 Loss_G: 0.8066 acc: 68.8%\n",
      "[BATCH 11/149] Loss_D: 0.7695 Loss_G: 0.7985 acc: 70.3%\n",
      "[BATCH 12/149] Loss_D: 0.8151 Loss_G: 0.8056 acc: 71.9%\n",
      "[BATCH 13/149] Loss_D: 0.8427 Loss_G: 0.8202 acc: 64.1%\n",
      "[BATCH 14/149] Loss_D: 0.8183 Loss_G: 0.8285 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7914 Loss_G: 0.8107 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.8486 Loss_G: 0.8263 acc: 59.4%\n",
      "[BATCH 17/149] Loss_D: 0.8254 Loss_G: 0.8288 acc: 51.6%\n",
      "[BATCH 18/149] Loss_D: 0.7673 Loss_G: 0.8105 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.8084 Loss_G: 0.8068 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7784 Loss_G: 0.8049 acc: 67.2%\n",
      "[BATCH 21/149] Loss_D: 0.8226 Loss_G: 0.8157 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.8013 Loss_G: 0.8174 acc: 75.0%\n",
      "[BATCH 23/149] Loss_D: 0.8120 Loss_G: 0.8230 acc: 68.8%\n",
      "[BATCH 24/149] Loss_D: 0.7711 Loss_G: 0.8227 acc: 59.4%\n",
      "[BATCH 25/149] Loss_D: 0.8208 Loss_G: 0.8299 acc: 57.8%\n",
      "[BATCH 26/149] Loss_D: 0.8063 Loss_G: 0.8169 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7886 Loss_G: 0.8277 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.8319 Loss_G: 0.8244 acc: 57.8%\n",
      "[BATCH 29/149] Loss_D: 0.8160 Loss_G: 0.8179 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.7731 Loss_G: 0.8062 acc: 70.3%\n",
      "[BATCH 31/149] Loss_D: 0.7836 Loss_G: 0.8053 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.7787 Loss_G: 0.8004 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.7987 Loss_G: 0.8019 acc: 60.9%\n",
      "[BATCH 34/149] Loss_D: 0.8016 Loss_G: 0.8084 acc: 51.6%\n",
      "[BATCH 35/149] Loss_D: 0.8221 Loss_G: 0.8163 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7783 Loss_G: 0.8067 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.7541 Loss_G: 0.8010 acc: 60.9%\n",
      "[BATCH 38/149] Loss_D: 0.8473 Loss_G: 0.8195 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.7852 Loss_G: 0.8270 acc: 68.8%\n",
      "[BATCH 40/149] Loss_D: 0.7890 Loss_G: 0.8246 acc: 62.5%\n",
      "[BATCH 41/149] Loss_D: 0.8398 Loss_G: 0.8297 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.8197 Loss_G: 0.8265 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.8001 Loss_G: 0.8205 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.8009 Loss_G: 0.8175 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.8050 Loss_G: 0.8092 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.8133 Loss_G: 0.8145 acc: 73.4%\n",
      "[BATCH 47/149] Loss_D: 0.7885 Loss_G: 0.8147 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.7791 Loss_G: 0.8112 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.7980 Loss_G: 0.8214 acc: 65.6%\n",
      "[BATCH 50/149] Loss_D: 0.8339 Loss_G: 0.8278 acc: 71.9%\n",
      "[BATCH 51/149] Loss_D: 0.8220 Loss_G: 0.8360 acc: 76.6%\n",
      "[BATCH 52/149] Loss_D: 0.7868 Loss_G: 0.8191 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.7561 Loss_G: 0.8031 acc: 70.3%\n",
      "[EPOCH 500] TEST ACC is : 74.2%\n",
      "[BATCH 54/149] Loss_D: 0.7589 Loss_G: 0.7947 acc: 71.9%\n",
      "[BATCH 55/149] Loss_D: 0.7562 Loss_G: 0.7911 acc: 57.8%\n",
      "[BATCH 56/149] Loss_D: 0.7834 Loss_G: 0.7924 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.7473 Loss_G: 0.7907 acc: 70.3%\n",
      "[BATCH 58/149] Loss_D: 0.7899 Loss_G: 0.7976 acc: 70.3%\n",
      "[BATCH 59/149] Loss_D: 0.8120 Loss_G: 0.8114 acc: 71.9%\n",
      "[BATCH 60/149] Loss_D: 0.7958 Loss_G: 0.8076 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.8078 Loss_G: 0.8140 acc: 56.2%\n",
      "[BATCH 62/149] Loss_D: 0.7996 Loss_G: 0.8098 acc: 57.8%\n",
      "[BATCH 63/149] Loss_D: 0.8028 Loss_G: 0.8138 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.8322 Loss_G: 0.8401 acc: 59.4%\n",
      "[BATCH 65/149] Loss_D: 0.8037 Loss_G: 0.8350 acc: 67.2%\n",
      "[BATCH 66/149] Loss_D: 0.7972 Loss_G: 0.8194 acc: 73.4%\n",
      "[BATCH 67/149] Loss_D: 0.8062 Loss_G: 0.8161 acc: 73.4%\n",
      "[BATCH 68/149] Loss_D: 0.8383 Loss_G: 0.8303 acc: 56.2%\n",
      "[BATCH 69/149] Loss_D: 0.8184 Loss_G: 0.8311 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.8292 Loss_G: 0.8327 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.8020 Loss_G: 0.8144 acc: 56.2%\n",
      "[BATCH 72/149] Loss_D: 0.7804 Loss_G: 0.8085 acc: 76.6%\n",
      "[BATCH 73/149] Loss_D: 0.7973 Loss_G: 0.8047 acc: 53.1%\n",
      "[BATCH 74/149] Loss_D: 0.7948 Loss_G: 0.8076 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.7720 Loss_G: 0.8066 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7506 Loss_G: 0.7945 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.8046 Loss_G: 0.7948 acc: 57.8%\n",
      "[BATCH 78/149] Loss_D: 0.7968 Loss_G: 0.8079 acc: 59.4%\n",
      "[BATCH 79/149] Loss_D: 0.8203 Loss_G: 0.8158 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7849 Loss_G: 0.8083 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.8439 Loss_G: 0.8129 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.7849 Loss_G: 0.8096 acc: 75.0%\n",
      "[BATCH 83/149] Loss_D: 0.7855 Loss_G: 0.8104 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.8404 Loss_G: 0.8172 acc: 57.8%\n",
      "[BATCH 85/149] Loss_D: 0.8529 Loss_G: 0.8298 acc: 71.9%\n",
      "[BATCH 86/149] Loss_D: 0.8007 Loss_G: 0.8189 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.7647 Loss_G: 0.8033 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.7816 Loss_G: 0.7983 acc: 62.5%\n",
      "[BATCH 89/149] Loss_D: 0.7885 Loss_G: 0.8039 acc: 54.7%\n",
      "[BATCH 90/149] Loss_D: 0.8064 Loss_G: 0.8148 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.7843 Loss_G: 0.8076 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7784 Loss_G: 0.8047 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.8092 Loss_G: 0.8047 acc: 51.6%\n",
      "[BATCH 94/149] Loss_D: 0.8139 Loss_G: 0.8125 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7750 Loss_G: 0.8099 acc: 53.1%\n",
      "[BATCH 96/149] Loss_D: 0.8366 Loss_G: 0.8134 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7858 Loss_G: 0.8179 acc: 73.4%\n",
      "[BATCH 98/149] Loss_D: 0.7992 Loss_G: 0.8167 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.7858 Loss_G: 0.8181 acc: 68.8%\n",
      "[BATCH 100/149] Loss_D: 0.7593 Loss_G: 0.8111 acc: 67.2%\n",
      "[BATCH 101/149] Loss_D: 0.7534 Loss_G: 0.8044 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.7617 Loss_G: 0.8084 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.8072 Loss_G: 0.8067 acc: 64.1%\n",
      "[EPOCH 550] TEST ACC is : 74.6%\n",
      "[BATCH 104/149] Loss_D: 0.7911 Loss_G: 0.8272 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.7451 Loss_G: 0.8074 acc: 62.5%\n",
      "[BATCH 106/149] Loss_D: 0.7661 Loss_G: 0.7930 acc: 73.4%\n",
      "[BATCH 107/149] Loss_D: 0.7794 Loss_G: 0.7973 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.8060 Loss_G: 0.8050 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.8335 Loss_G: 0.8239 acc: 59.4%\n",
      "[BATCH 110/149] Loss_D: 0.7757 Loss_G: 0.8177 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.7734 Loss_G: 0.8034 acc: 60.9%\n",
      "[BATCH 112/149] Loss_D: 0.7979 Loss_G: 0.8089 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.7699 Loss_G: 0.8022 acc: 68.8%\n",
      "[BATCH 114/149] Loss_D: 0.8000 Loss_G: 0.8156 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.8575 Loss_G: 0.8335 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.7832 Loss_G: 0.8296 acc: 71.9%\n",
      "[BATCH 117/149] Loss_D: 0.7364 Loss_G: 0.8104 acc: 78.1%\n",
      "[BATCH 118/149] Loss_D: 0.7889 Loss_G: 0.8067 acc: 59.4%\n",
      "[BATCH 119/149] Loss_D: 0.7841 Loss_G: 0.8044 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.8239 Loss_G: 0.8133 acc: 65.6%\n",
      "[BATCH 121/149] Loss_D: 0.7834 Loss_G: 0.8139 acc: 56.2%\n",
      "[BATCH 122/149] Loss_D: 0.7804 Loss_G: 0.8154 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.8114 Loss_G: 0.8279 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.7929 Loss_G: 0.8151 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.8532 Loss_G: 0.8200 acc: 56.2%\n",
      "[BATCH 126/149] Loss_D: 0.8020 Loss_G: 0.8299 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.7786 Loss_G: 0.8211 acc: 56.2%\n",
      "[BATCH 128/149] Loss_D: 0.8131 Loss_G: 0.8227 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.7758 Loss_G: 0.8207 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.7947 Loss_G: 0.8099 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.7787 Loss_G: 0.8057 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7926 Loss_G: 0.8095 acc: 57.8%\n",
      "[BATCH 133/149] Loss_D: 0.7740 Loss_G: 0.8112 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7880 Loss_G: 0.8198 acc: 54.7%\n",
      "[BATCH 135/149] Loss_D: 0.7886 Loss_G: 0.8093 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7890 Loss_G: 0.8028 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.8537 Loss_G: 0.8159 acc: 70.3%\n",
      "[BATCH 138/149] Loss_D: 0.7843 Loss_G: 0.8095 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.8477 Loss_G: 0.8103 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.7466 Loss_G: 0.7941 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7796 Loss_G: 0.7914 acc: 65.6%\n",
      "[BATCH 142/149] Loss_D: 0.7589 Loss_G: 0.7928 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.8293 Loss_G: 0.8132 acc: 57.8%\n",
      "[BATCH 144/149] Loss_D: 0.7833 Loss_G: 0.8104 acc: 79.7%\n",
      "[BATCH 145/149] Loss_D: 0.8028 Loss_G: 0.8144 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7932 Loss_G: 0.8185 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7665 Loss_G: 0.8089 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7812 Loss_G: 0.7994 acc: 54.7%\n",
      "[BATCH 149/149] Loss_D: 0.7669 Loss_G: 0.7969 acc: 67.2%\n",
      "-----THE [4/50] epoch end-----\n",
      "-----THE [5/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7792 Loss_G: 0.7977 acc: 64.1%\n",
      "[BATCH 2/149] Loss_D: 0.7943 Loss_G: 0.8183 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.8186 Loss_G: 0.8298 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.7858 Loss_G: 0.8216 acc: 68.8%\n",
      "[EPOCH 600] TEST ACC is : 75.8%\n",
      "[BATCH 5/149] Loss_D: 0.7792 Loss_G: 0.8170 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7883 Loss_G: 0.8151 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.7724 Loss_G: 0.8295 acc: 75.0%\n",
      "[BATCH 8/149] Loss_D: 0.8521 Loss_G: 0.8430 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.7829 Loss_G: 0.8275 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.7619 Loss_G: 0.8138 acc: 71.9%\n",
      "[BATCH 11/149] Loss_D: 0.7691 Loss_G: 0.8084 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.8177 Loss_G: 0.8241 acc: 60.9%\n",
      "[BATCH 13/149] Loss_D: 0.8281 Loss_G: 0.8326 acc: 60.9%\n",
      "[BATCH 14/149] Loss_D: 0.8167 Loss_G: 0.8261 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.8193 Loss_G: 0.8346 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.7770 Loss_G: 0.8216 acc: 70.3%\n",
      "[BATCH 17/149] Loss_D: 0.7951 Loss_G: 0.8133 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.7686 Loss_G: 0.8153 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7665 Loss_G: 0.8033 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7649 Loss_G: 0.8106 acc: 75.0%\n",
      "[BATCH 21/149] Loss_D: 0.7876 Loss_G: 0.8095 acc: 56.2%\n",
      "[BATCH 22/149] Loss_D: 0.7934 Loss_G: 0.8070 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.8449 Loss_G: 0.8076 acc: 68.8%\n",
      "[BATCH 24/149] Loss_D: 0.8009 Loss_G: 0.8041 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.7561 Loss_G: 0.8020 acc: 70.3%\n",
      "[BATCH 26/149] Loss_D: 0.8204 Loss_G: 0.8095 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.7753 Loss_G: 0.8087 acc: 71.9%\n",
      "[BATCH 28/149] Loss_D: 0.7801 Loss_G: 0.8064 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.8128 Loss_G: 0.8186 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7719 Loss_G: 0.8172 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.7636 Loss_G: 0.8119 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7750 Loss_G: 0.8316 acc: 57.8%\n",
      "[BATCH 33/149] Loss_D: 0.8460 Loss_G: 0.8536 acc: 56.2%\n",
      "[BATCH 34/149] Loss_D: 0.7606 Loss_G: 0.8209 acc: 60.9%\n",
      "[BATCH 35/149] Loss_D: 0.7838 Loss_G: 0.8055 acc: 67.2%\n",
      "[BATCH 36/149] Loss_D: 0.8127 Loss_G: 0.8067 acc: 67.2%\n",
      "[BATCH 37/149] Loss_D: 0.8193 Loss_G: 0.8161 acc: 75.0%\n",
      "[BATCH 38/149] Loss_D: 0.7727 Loss_G: 0.8127 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.7998 Loss_G: 0.8138 acc: 54.7%\n",
      "[BATCH 40/149] Loss_D: 0.8358 Loss_G: 0.8220 acc: 62.5%\n",
      "[BATCH 41/149] Loss_D: 0.7681 Loss_G: 0.8132 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.7799 Loss_G: 0.8051 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7824 Loss_G: 0.8054 acc: 65.6%\n",
      "[BATCH 44/149] Loss_D: 0.7694 Loss_G: 0.8013 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7483 Loss_G: 0.7918 acc: 67.2%\n",
      "[BATCH 46/149] Loss_D: 0.7645 Loss_G: 0.7882 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7945 Loss_G: 0.7895 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.7920 Loss_G: 0.8113 acc: 54.7%\n",
      "[BATCH 49/149] Loss_D: 0.8154 Loss_G: 0.8116 acc: 70.3%\n",
      "[BATCH 50/149] Loss_D: 0.7443 Loss_G: 0.8080 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.8249 Loss_G: 0.8034 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.8078 Loss_G: 0.8181 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.7670 Loss_G: 0.8014 acc: 75.0%\n",
      "[BATCH 54/149] Loss_D: 0.7852 Loss_G: 0.8091 acc: 70.3%\n",
      "[EPOCH 650] TEST ACC is : 75.2%\n",
      "[BATCH 55/149] Loss_D: 0.7658 Loss_G: 0.8071 acc: 73.4%\n",
      "[BATCH 56/149] Loss_D: 0.7947 Loss_G: 0.8152 acc: 56.2%\n",
      "[BATCH 57/149] Loss_D: 0.7994 Loss_G: 0.8251 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.8617 Loss_G: 0.8440 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.7848 Loss_G: 0.8258 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.8221 Loss_G: 0.8324 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.7824 Loss_G: 0.8084 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.8180 Loss_G: 0.8173 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.8404 Loss_G: 0.8299 acc: 79.7%\n",
      "[BATCH 64/149] Loss_D: 0.7533 Loss_G: 0.8107 acc: 54.7%\n",
      "[BATCH 65/149] Loss_D: 0.8113 Loss_G: 0.8063 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.8220 Loss_G: 0.8149 acc: 78.1%\n",
      "[BATCH 67/149] Loss_D: 0.8128 Loss_G: 0.8221 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.8025 Loss_G: 0.8234 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.8270 Loss_G: 0.8388 acc: 59.4%\n",
      "[BATCH 70/149] Loss_D: 0.7749 Loss_G: 0.8301 acc: 62.5%\n",
      "[BATCH 71/149] Loss_D: 0.8016 Loss_G: 0.8098 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.7700 Loss_G: 0.8112 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.8132 Loss_G: 0.8173 acc: 68.8%\n",
      "[BATCH 74/149] Loss_D: 0.8322 Loss_G: 0.8276 acc: 54.7%\n",
      "[BATCH 75/149] Loss_D: 0.8020 Loss_G: 0.8208 acc: 62.5%\n",
      "[BATCH 76/149] Loss_D: 0.8027 Loss_G: 0.8177 acc: 67.2%\n",
      "[BATCH 77/149] Loss_D: 0.7612 Loss_G: 0.8040 acc: 76.6%\n",
      "[BATCH 78/149] Loss_D: 0.7820 Loss_G: 0.8032 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.8010 Loss_G: 0.8060 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.7780 Loss_G: 0.8176 acc: 56.2%\n",
      "[BATCH 81/149] Loss_D: 0.7981 Loss_G: 0.8260 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.7413 Loss_G: 0.8150 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.8326 Loss_G: 0.7991 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.7634 Loss_G: 0.7992 acc: 67.2%\n",
      "[BATCH 85/149] Loss_D: 0.7915 Loss_G: 0.8010 acc: 70.3%\n",
      "[BATCH 86/149] Loss_D: 0.8140 Loss_G: 0.8096 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.7943 Loss_G: 0.8113 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.7939 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.7660 Loss_G: 0.8132 acc: 70.3%\n",
      "[BATCH 90/149] Loss_D: 0.8046 Loss_G: 0.8153 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.7789 Loss_G: 0.8198 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.8317 Loss_G: 0.8256 acc: 64.1%\n",
      "[BATCH 93/149] Loss_D: 0.8061 Loss_G: 0.8236 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.8408 Loss_G: 0.8328 acc: 56.2%\n",
      "[BATCH 95/149] Loss_D: 0.8410 Loss_G: 0.8379 acc: 65.6%\n",
      "[BATCH 96/149] Loss_D: 0.8591 Loss_G: 0.8428 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7823 Loss_G: 0.8306 acc: 71.9%\n",
      "[BATCH 98/149] Loss_D: 0.8275 Loss_G: 0.8185 acc: 68.8%\n",
      "[BATCH 99/149] Loss_D: 0.7959 Loss_G: 0.8149 acc: 59.4%\n",
      "[BATCH 100/149] Loss_D: 0.8445 Loss_G: 0.8242 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.8101 Loss_G: 0.8155 acc: 73.4%\n",
      "[BATCH 102/149] Loss_D: 0.8196 Loss_G: 0.8137 acc: 71.9%\n",
      "[BATCH 103/149] Loss_D: 0.7492 Loss_G: 0.8106 acc: 70.3%\n",
      "[BATCH 104/149] Loss_D: 0.8011 Loss_G: 0.8066 acc: 67.2%\n",
      "[EPOCH 700] TEST ACC is : 75.8%\n",
      "[BATCH 105/149] Loss_D: 0.7857 Loss_G: 0.8060 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.7700 Loss_G: 0.8008 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.8113 Loss_G: 0.8039 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.7548 Loss_G: 0.8011 acc: 60.9%\n",
      "[BATCH 109/149] Loss_D: 0.8141 Loss_G: 0.8009 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.8162 Loss_G: 0.8076 acc: 70.3%\n",
      "[BATCH 111/149] Loss_D: 0.7649 Loss_G: 0.7933 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.7296 Loss_G: 0.7872 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.8463 Loss_G: 0.8082 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.8178 Loss_G: 0.8242 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.8186 Loss_G: 0.8545 acc: 68.8%\n",
      "[BATCH 116/149] Loss_D: 0.7707 Loss_G: 0.8122 acc: 68.8%\n",
      "[BATCH 117/149] Loss_D: 0.7993 Loss_G: 0.8070 acc: 57.8%\n",
      "[BATCH 118/149] Loss_D: 0.7859 Loss_G: 0.8045 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.8391 Loss_G: 0.8116 acc: 70.3%\n",
      "[BATCH 120/149] Loss_D: 0.8163 Loss_G: 0.8240 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.7898 Loss_G: 0.8202 acc: 65.6%\n",
      "[BATCH 122/149] Loss_D: 0.8069 Loss_G: 0.8209 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.7818 Loss_G: 0.8251 acc: 65.6%\n",
      "[BATCH 124/149] Loss_D: 0.8441 Loss_G: 0.8339 acc: 56.2%\n",
      "[BATCH 125/149] Loss_D: 0.7819 Loss_G: 0.8252 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8373 Loss_G: 0.8324 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.8044 Loss_G: 0.8302 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7715 Loss_G: 0.8225 acc: 65.6%\n",
      "[BATCH 129/149] Loss_D: 0.8155 Loss_G: 0.8184 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.8578 Loss_G: 0.8369 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.7981 Loss_G: 0.8289 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.7841 Loss_G: 0.8167 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.7809 Loss_G: 0.8056 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.7895 Loss_G: 0.8085 acc: 78.1%\n",
      "[BATCH 135/149] Loss_D: 0.7747 Loss_G: 0.8031 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.7843 Loss_G: 0.7950 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8269 Loss_G: 0.8058 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.8047 Loss_G: 0.8173 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7893 Loss_G: 0.8110 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.7773 Loss_G: 0.8078 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7788 Loss_G: 0.8061 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7469 Loss_G: 0.7973 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.8398 Loss_G: 0.8078 acc: 56.2%\n",
      "[BATCH 144/149] Loss_D: 0.8035 Loss_G: 0.8163 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.8090 Loss_G: 0.8173 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.7858 Loss_G: 0.8086 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7849 Loss_G: 0.8128 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.7671 Loss_G: 0.8120 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7791 Loss_G: 0.8096 acc: 67.2%\n",
      "-----THE [5/50] epoch end-----\n",
      "-----THE [6/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8218 Loss_G: 0.8139 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.7617 Loss_G: 0.8062 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.7649 Loss_G: 0.8004 acc: 73.4%\n",
      "[BATCH 4/149] Loss_D: 0.8314 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 5/149] Loss_D: 0.7985 Loss_G: 0.8221 acc: 60.9%\n",
      "[EPOCH 750] TEST ACC is : 75.0%\n",
      "[BATCH 6/149] Loss_D: 0.7705 Loss_G: 0.8161 acc: 73.4%\n",
      "[BATCH 7/149] Loss_D: 0.7791 Loss_G: 0.8104 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7508 Loss_G: 0.8050 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.8094 Loss_G: 0.8222 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.7624 Loss_G: 0.8268 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.8114 Loss_G: 0.8292 acc: 57.8%\n",
      "[BATCH 12/149] Loss_D: 0.8073 Loss_G: 0.8273 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.7954 Loss_G: 0.8207 acc: 65.6%\n",
      "[BATCH 14/149] Loss_D: 0.8197 Loss_G: 0.8249 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.7926 Loss_G: 0.8275 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7970 Loss_G: 0.8222 acc: 57.8%\n",
      "[BATCH 17/149] Loss_D: 0.7831 Loss_G: 0.8124 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.7637 Loss_G: 0.7997 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.7903 Loss_G: 0.8035 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7799 Loss_G: 0.8063 acc: 57.8%\n",
      "[BATCH 21/149] Loss_D: 0.7577 Loss_G: 0.8007 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.7596 Loss_G: 0.7972 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.7860 Loss_G: 0.8032 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.7840 Loss_G: 0.8088 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.7708 Loss_G: 0.8047 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7980 Loss_G: 0.8102 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7560 Loss_G: 0.8042 acc: 57.8%\n",
      "[BATCH 28/149] Loss_D: 0.8181 Loss_G: 0.8131 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7809 Loss_G: 0.8140 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.8136 Loss_G: 0.8141 acc: 75.0%\n",
      "[BATCH 31/149] Loss_D: 0.7774 Loss_G: 0.8097 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.8143 Loss_G: 0.8141 acc: 60.9%\n",
      "[BATCH 33/149] Loss_D: 0.7837 Loss_G: 0.8074 acc: 59.4%\n",
      "[BATCH 34/149] Loss_D: 0.8152 Loss_G: 0.8127 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7818 Loss_G: 0.8110 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.7882 Loss_G: 0.8083 acc: 73.4%\n",
      "[BATCH 37/149] Loss_D: 0.8002 Loss_G: 0.8100 acc: 57.8%\n",
      "[BATCH 38/149] Loss_D: 0.7979 Loss_G: 0.8045 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.8213 Loss_G: 0.8198 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7652 Loss_G: 0.8128 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.7840 Loss_G: 0.7984 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.7505 Loss_G: 0.7917 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.8061 Loss_G: 0.8017 acc: 78.1%\n",
      "[BATCH 44/149] Loss_D: 0.7757 Loss_G: 0.8041 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.8192 Loss_G: 0.8093 acc: 70.3%\n",
      "[BATCH 46/149] Loss_D: 0.8110 Loss_G: 0.8168 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.8515 Loss_G: 0.8180 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.7740 Loss_G: 0.8076 acc: 60.9%\n",
      "[BATCH 49/149] Loss_D: 0.7763 Loss_G: 0.7992 acc: 60.9%\n",
      "[BATCH 50/149] Loss_D: 0.7717 Loss_G: 0.7992 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.8241 Loss_G: 0.8080 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.7970 Loss_G: 0.8150 acc: 75.0%\n",
      "[BATCH 53/149] Loss_D: 0.7689 Loss_G: 0.8043 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.7725 Loss_G: 0.7976 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.8093 Loss_G: 0.8064 acc: 68.8%\n",
      "[EPOCH 800] TEST ACC is : 75.8%\n",
      "[BATCH 56/149] Loss_D: 0.7691 Loss_G: 0.8119 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.8523 Loss_G: 0.8252 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.8202 Loss_G: 0.8310 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.7721 Loss_G: 0.8194 acc: 54.7%\n",
      "[BATCH 60/149] Loss_D: 0.8153 Loss_G: 0.8131 acc: 70.3%\n",
      "[BATCH 61/149] Loss_D: 0.7688 Loss_G: 0.8136 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.8173 Loss_G: 0.8219 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.7717 Loss_G: 0.8065 acc: 75.0%\n",
      "[BATCH 64/149] Loss_D: 0.8132 Loss_G: 0.8076 acc: 68.8%\n",
      "[BATCH 65/149] Loss_D: 0.7748 Loss_G: 0.8052 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.8380 Loss_G: 0.8204 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.7549 Loss_G: 0.8065 acc: 64.1%\n",
      "[BATCH 68/149] Loss_D: 0.7807 Loss_G: 0.8041 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.8421 Loss_G: 0.8119 acc: 56.2%\n",
      "[BATCH 70/149] Loss_D: 0.8065 Loss_G: 0.8155 acc: 59.4%\n",
      "[BATCH 71/149] Loss_D: 0.7930 Loss_G: 0.8167 acc: 67.2%\n",
      "[BATCH 72/149] Loss_D: 0.8168 Loss_G: 0.8186 acc: 65.6%\n",
      "[BATCH 73/149] Loss_D: 0.8123 Loss_G: 0.8239 acc: 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.7693 Loss_G: 0.8269 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.8167 Loss_G: 0.8321 acc: 71.9%\n",
      "[BATCH 76/149] Loss_D: 0.7723 Loss_G: 0.8219 acc: 73.4%\n",
      "[BATCH 77/149] Loss_D: 0.7862 Loss_G: 0.8118 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.7899 Loss_G: 0.8098 acc: 53.1%\n",
      "[BATCH 79/149] Loss_D: 0.7966 Loss_G: 0.8082 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.8094 Loss_G: 0.8203 acc: 70.3%\n",
      "[BATCH 81/149] Loss_D: 0.7742 Loss_G: 0.8071 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.7755 Loss_G: 0.8014 acc: 73.4%\n",
      "[BATCH 83/149] Loss_D: 0.7959 Loss_G: 0.8073 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.7656 Loss_G: 0.7997 acc: 59.4%\n",
      "[BATCH 85/149] Loss_D: 0.7792 Loss_G: 0.7997 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.7823 Loss_G: 0.8003 acc: 81.2%\n",
      "[BATCH 87/149] Loss_D: 0.7934 Loss_G: 0.8003 acc: 70.3%\n",
      "[BATCH 88/149] Loss_D: 0.8438 Loss_G: 0.8164 acc: 62.5%\n",
      "[BATCH 89/149] Loss_D: 0.8288 Loss_G: 0.8241 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.7949 Loss_G: 0.8157 acc: 51.6%\n",
      "[BATCH 91/149] Loss_D: 0.7962 Loss_G: 0.8093 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.8019 Loss_G: 0.8059 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.7620 Loss_G: 0.7993 acc: 54.7%\n",
      "[BATCH 94/149] Loss_D: 0.8169 Loss_G: 0.8108 acc: 56.2%\n",
      "[BATCH 95/149] Loss_D: 0.8003 Loss_G: 0.8168 acc: 75.0%\n",
      "[BATCH 96/149] Loss_D: 0.7962 Loss_G: 0.8157 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.7981 Loss_G: 0.8149 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.7933 Loss_G: 0.8164 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7900 Loss_G: 0.8203 acc: 57.8%\n",
      "[BATCH 100/149] Loss_D: 0.7726 Loss_G: 0.8155 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.7720 Loss_G: 0.8136 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.8293 Loss_G: 0.8249 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.8314 Loss_G: 0.8322 acc: 65.6%\n",
      "[BATCH 104/149] Loss_D: 0.8157 Loss_G: 0.8325 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7822 Loss_G: 0.8141 acc: 71.9%\n",
      "[EPOCH 850] TEST ACC is : 75.6%\n",
      "[BATCH 106/149] Loss_D: 0.8010 Loss_G: 0.8041 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7968 Loss_G: 0.8088 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7827 Loss_G: 0.8103 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.7891 Loss_G: 0.8074 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.8130 Loss_G: 0.8108 acc: 76.6%\n",
      "[BATCH 111/149] Loss_D: 0.8092 Loss_G: 0.8175 acc: 57.8%\n",
      "[BATCH 112/149] Loss_D: 0.7773 Loss_G: 0.8139 acc: 50.0%\n",
      "[BATCH 113/149] Loss_D: 0.8055 Loss_G: 0.8046 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.8052 Loss_G: 0.8032 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.7824 Loss_G: 0.8053 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7858 Loss_G: 0.8016 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7959 Loss_G: 0.8011 acc: 51.6%\n",
      "[BATCH 118/149] Loss_D: 0.8254 Loss_G: 0.8073 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.7750 Loss_G: 0.8047 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.7662 Loss_G: 0.7988 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.7878 Loss_G: 0.7992 acc: 65.6%\n",
      "[BATCH 122/149] Loss_D: 0.8054 Loss_G: 0.8100 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.8233 Loss_G: 0.8274 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.8067 Loss_G: 0.8320 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.7901 Loss_G: 0.8280 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7555 Loss_G: 0.8137 acc: 67.2%\n",
      "[BATCH 127/149] Loss_D: 0.8011 Loss_G: 0.8085 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.8389 Loss_G: 0.8240 acc: 75.0%\n",
      "[BATCH 129/149] Loss_D: 0.7826 Loss_G: 0.8268 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.7785 Loss_G: 0.8165 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.7901 Loss_G: 0.8134 acc: 65.6%\n",
      "[BATCH 132/149] Loss_D: 0.8079 Loss_G: 0.8204 acc: 54.7%\n",
      "[BATCH 133/149] Loss_D: 0.8315 Loss_G: 0.8240 acc: 54.7%\n",
      "[BATCH 134/149] Loss_D: 0.7645 Loss_G: 0.8169 acc: 67.2%\n",
      "[BATCH 135/149] Loss_D: 0.8216 Loss_G: 0.8265 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.7628 Loss_G: 0.8246 acc: 65.6%\n",
      "[BATCH 137/149] Loss_D: 0.7829 Loss_G: 0.8195 acc: 68.8%\n",
      "[BATCH 138/149] Loss_D: 0.8074 Loss_G: 0.8179 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.8222 Loss_G: 0.8213 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.8182 Loss_G: 0.8272 acc: 53.1%\n",
      "[BATCH 141/149] Loss_D: 0.8343 Loss_G: 0.8322 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.7976 Loss_G: 0.8232 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.8014 Loss_G: 0.8103 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.7703 Loss_G: 0.8136 acc: 62.5%\n",
      "[BATCH 145/149] Loss_D: 0.8515 Loss_G: 0.8294 acc: 68.8%\n",
      "[BATCH 146/149] Loss_D: 0.8570 Loss_G: 0.8514 acc: 68.8%\n",
      "[BATCH 147/149] Loss_D: 0.8067 Loss_G: 0.8324 acc: 71.9%\n",
      "[BATCH 148/149] Loss_D: 0.7557 Loss_G: 0.8049 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.8244 Loss_G: 0.8038 acc: 53.1%\n",
      "-----THE [6/50] epoch end-----\n",
      "-----THE [7/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7704 Loss_G: 0.8034 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.7861 Loss_G: 0.8034 acc: 76.6%\n",
      "[BATCH 3/149] Loss_D: 0.7730 Loss_G: 0.8075 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.7735 Loss_G: 0.8032 acc: 54.7%\n",
      "[BATCH 5/149] Loss_D: 0.7892 Loss_G: 0.7981 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.8091 Loss_G: 0.8049 acc: 59.4%\n",
      "[EPOCH 900] TEST ACC is : 74.6%\n",
      "[BATCH 7/149] Loss_D: 0.7662 Loss_G: 0.8116 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.8225 Loss_G: 0.8128 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.8567 Loss_G: 0.8445 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7867 Loss_G: 0.8202 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.7707 Loss_G: 0.8051 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7844 Loss_G: 0.8024 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7481 Loss_G: 0.8030 acc: 70.3%\n",
      "[BATCH 14/149] Loss_D: 0.8109 Loss_G: 0.8117 acc: 56.2%\n",
      "[BATCH 15/149] Loss_D: 0.7917 Loss_G: 0.8230 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7399 Loss_G: 0.8042 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.7912 Loss_G: 0.8085 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.7952 Loss_G: 0.8156 acc: 73.4%\n",
      "[BATCH 19/149] Loss_D: 0.8254 Loss_G: 0.8225 acc: 57.8%\n",
      "[BATCH 20/149] Loss_D: 0.7625 Loss_G: 0.8079 acc: 50.0%\n",
      "[BATCH 21/149] Loss_D: 0.7719 Loss_G: 0.8072 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.8346 Loss_G: 0.8131 acc: 60.9%\n",
      "[BATCH 23/149] Loss_D: 0.7785 Loss_G: 0.8156 acc: 65.6%\n",
      "[BATCH 24/149] Loss_D: 0.8464 Loss_G: 0.8283 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.7657 Loss_G: 0.8051 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.7939 Loss_G: 0.8020 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.8046 Loss_G: 0.7973 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.7723 Loss_G: 0.7935 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.7946 Loss_G: 0.8045 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7634 Loss_G: 0.8051 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.8450 Loss_G: 0.8207 acc: 57.8%\n",
      "[BATCH 32/149] Loss_D: 0.7715 Loss_G: 0.8192 acc: 57.8%\n",
      "[BATCH 33/149] Loss_D: 0.8153 Loss_G: 0.8192 acc: 73.4%\n",
      "[BATCH 34/149] Loss_D: 0.7769 Loss_G: 0.8108 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.8529 Loss_G: 0.8169 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.7883 Loss_G: 0.8189 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.7594 Loss_G: 0.8112 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.8293 Loss_G: 0.8174 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.8344 Loss_G: 0.8379 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7778 Loss_G: 0.8524 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.8007 Loss_G: 0.8223 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.8077 Loss_G: 0.8179 acc: 73.4%\n",
      "[BATCH 43/149] Loss_D: 0.7873 Loss_G: 0.8127 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.7649 Loss_G: 0.8027 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.8129 Loss_G: 0.8073 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8078 Loss_G: 0.8065 acc: 57.8%\n",
      "[BATCH 47/149] Loss_D: 0.8183 Loss_G: 0.8155 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.8052 Loss_G: 0.8150 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.7780 Loss_G: 0.8092 acc: 65.6%\n",
      "[BATCH 50/149] Loss_D: 0.8035 Loss_G: 0.8109 acc: 57.8%\n",
      "[BATCH 51/149] Loss_D: 0.8156 Loss_G: 0.8166 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7572 Loss_G: 0.8030 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.8425 Loss_G: 0.8183 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.7861 Loss_G: 0.8127 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.7715 Loss_G: 0.7996 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.8187 Loss_G: 0.8094 acc: 67.2%\n",
      "[EPOCH 950] TEST ACC is : 76.0%\n",
      "[BATCH 57/149] Loss_D: 0.7635 Loss_G: 0.8097 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.8066 Loss_G: 0.8082 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.8128 Loss_G: 0.8097 acc: 57.8%\n",
      "[BATCH 60/149] Loss_D: 0.7705 Loss_G: 0.7994 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7729 Loss_G: 0.7932 acc: 73.4%\n",
      "[BATCH 62/149] Loss_D: 0.7600 Loss_G: 0.7866 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.7831 Loss_G: 0.7982 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.8178 Loss_G: 0.8186 acc: 75.0%\n",
      "[BATCH 65/149] Loss_D: 0.8215 Loss_G: 0.8198 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.8026 Loss_G: 0.8205 acc: 57.8%\n",
      "[BATCH 67/149] Loss_D: 0.8027 Loss_G: 0.8162 acc: 57.8%\n",
      "[BATCH 68/149] Loss_D: 0.7586 Loss_G: 0.8075 acc: 68.8%\n",
      "[BATCH 69/149] Loss_D: 0.7630 Loss_G: 0.7973 acc: 78.1%\n",
      "[BATCH 70/149] Loss_D: 0.8124 Loss_G: 0.8059 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.7642 Loss_G: 0.8061 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7550 Loss_G: 0.8020 acc: 65.6%\n",
      "[BATCH 73/149] Loss_D: 0.8163 Loss_G: 0.8153 acc: 70.3%\n",
      "[BATCH 74/149] Loss_D: 0.7893 Loss_G: 0.8093 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.7956 Loss_G: 0.8076 acc: 76.6%\n",
      "[BATCH 76/149] Loss_D: 0.8399 Loss_G: 0.8108 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.8015 Loss_G: 0.8129 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.7832 Loss_G: 0.8106 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.8032 Loss_G: 0.8157 acc: 71.9%\n",
      "[BATCH 80/149] Loss_D: 0.7668 Loss_G: 0.8156 acc: 67.2%\n",
      "[BATCH 81/149] Loss_D: 0.7774 Loss_G: 0.8100 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.7926 Loss_G: 0.8103 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.8178 Loss_G: 0.8165 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.8069 Loss_G: 0.8092 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.8209 Loss_G: 0.8226 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7354 Loss_G: 0.7991 acc: 64.1%\n",
      "[BATCH 87/149] Loss_D: 0.7758 Loss_G: 0.7908 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 0.7747 Loss_G: 0.8017 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.8026 Loss_G: 0.8064 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.8022 Loss_G: 0.8142 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.8143 Loss_G: 0.8173 acc: 62.5%\n",
      "[BATCH 92/149] Loss_D: 0.7818 Loss_G: 0.8122 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.8429 Loss_G: 0.8176 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.7993 Loss_G: 0.8176 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7789 Loss_G: 0.8086 acc: 56.2%\n",
      "[BATCH 96/149] Loss_D: 0.7890 Loss_G: 0.8089 acc: 71.9%\n",
      "[BATCH 97/149] Loss_D: 0.7746 Loss_G: 0.8146 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.8112 Loss_G: 0.8045 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.7956 Loss_G: 0.8086 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.7858 Loss_G: 0.8098 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7624 Loss_G: 0.8084 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7753 Loss_G: 0.8084 acc: 57.8%\n",
      "[BATCH 103/149] Loss_D: 0.8025 Loss_G: 0.8128 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8148 Loss_G: 0.8281 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7663 Loss_G: 0.8174 acc: 65.6%\n",
      "[BATCH 106/149] Loss_D: 0.7672 Loss_G: 0.8044 acc: 62.5%\n",
      "[EPOCH 1000] TEST ACC is : 74.8%\n",
      "[BATCH 107/149] Loss_D: 0.8107 Loss_G: 0.8124 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8090 Loss_G: 0.8193 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.8095 Loss_G: 0.8223 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.8102 Loss_G: 0.8249 acc: 73.4%\n",
      "[BATCH 111/149] Loss_D: 0.8304 Loss_G: 0.8238 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.7901 Loss_G: 0.8232 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.7745 Loss_G: 0.8083 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.7757 Loss_G: 0.8057 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.8092 Loss_G: 0.8240 acc: 70.3%\n",
      "[BATCH 116/149] Loss_D: 0.8008 Loss_G: 0.8357 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7921 Loss_G: 0.8220 acc: 67.2%\n",
      "[BATCH 118/149] Loss_D: 0.7671 Loss_G: 0.8190 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.7606 Loss_G: 0.8005 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.7860 Loss_G: 0.7951 acc: 60.9%\n",
      "[BATCH 121/149] Loss_D: 0.7856 Loss_G: 0.8120 acc: 60.9%\n",
      "[BATCH 122/149] Loss_D: 0.7743 Loss_G: 0.8063 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.7989 Loss_G: 0.8105 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.8041 Loss_G: 0.8180 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7986 Loss_G: 0.8213 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.8104 Loss_G: 0.8260 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.8342 Loss_G: 0.8254 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.7727 Loss_G: 0.8164 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.8241 Loss_G: 0.8200 acc: 59.4%\n",
      "[BATCH 130/149] Loss_D: 0.8474 Loss_G: 0.8312 acc: 68.8%\n",
      "[BATCH 131/149] Loss_D: 0.8049 Loss_G: 0.8223 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.8299 Loss_G: 0.8241 acc: 64.1%\n",
      "[BATCH 133/149] Loss_D: 0.8263 Loss_G: 0.8276 acc: 76.6%\n",
      "[BATCH 134/149] Loss_D: 0.7752 Loss_G: 0.8135 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.7482 Loss_G: 0.7976 acc: 70.3%\n",
      "[BATCH 136/149] Loss_D: 0.8044 Loss_G: 0.8042 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7749 Loss_G: 0.8098 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.7892 Loss_G: 0.8076 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.7780 Loss_G: 0.8045 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.8124 Loss_G: 0.8129 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7521 Loss_G: 0.8014 acc: 79.7%\n",
      "[BATCH 142/149] Loss_D: 0.7781 Loss_G: 0.8000 acc: 59.4%\n",
      "[BATCH 143/149] Loss_D: 0.8129 Loss_G: 0.8083 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.7830 Loss_G: 0.8163 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.8224 Loss_G: 0.8209 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.8203 Loss_G: 0.8251 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.7699 Loss_G: 0.8126 acc: 76.6%\n",
      "[BATCH 148/149] Loss_D: 0.8987 Loss_G: 0.8415 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.8463 Loss_G: 0.8397 acc: 57.8%\n",
      "-----THE [7/50] epoch end-----\n",
      "-----THE [8/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8178 Loss_G: 0.8355 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.7963 Loss_G: 0.8224 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7930 Loss_G: 0.8199 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.8286 Loss_G: 0.8290 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.8259 Loss_G: 0.8331 acc: 57.8%\n",
      "[BATCH 6/149] Loss_D: 0.8006 Loss_G: 0.8302 acc: 70.3%\n",
      "[BATCH 7/149] Loss_D: 0.7882 Loss_G: 0.8148 acc: 54.7%\n",
      "[EPOCH 1050] TEST ACC is : 74.0%\n",
      "[BATCH 8/149] Loss_D: 0.7844 Loss_G: 0.8119 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.7796 Loss_G: 0.8074 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8194 Loss_G: 0.8204 acc: 53.1%\n",
      "[BATCH 11/149] Loss_D: 0.8216 Loss_G: 0.8204 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7978 Loss_G: 0.8023 acc: 75.0%\n",
      "[BATCH 13/149] Loss_D: 0.8004 Loss_G: 0.8040 acc: 54.7%\n",
      "[BATCH 14/149] Loss_D: 0.7755 Loss_G: 0.8037 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.7911 Loss_G: 0.7994 acc: 56.2%\n",
      "[BATCH 16/149] Loss_D: 0.8051 Loss_G: 0.8048 acc: 57.8%\n",
      "[BATCH 17/149] Loss_D: 0.7505 Loss_G: 0.8021 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.8072 Loss_G: 0.8049 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.7939 Loss_G: 0.8066 acc: 59.4%\n",
      "[BATCH 20/149] Loss_D: 0.8137 Loss_G: 0.8083 acc: 64.1%\n",
      "[BATCH 21/149] Loss_D: 0.8423 Loss_G: 0.8249 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7548 Loss_G: 0.8210 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.8098 Loss_G: 0.8136 acc: 60.9%\n",
      "[BATCH 24/149] Loss_D: 0.7435 Loss_G: 0.8018 acc: 56.2%\n",
      "[BATCH 25/149] Loss_D: 0.7870 Loss_G: 0.8044 acc: 56.2%\n",
      "[BATCH 26/149] Loss_D: 0.7809 Loss_G: 0.8068 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.7755 Loss_G: 0.8042 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.8010 Loss_G: 0.8114 acc: 67.2%\n",
      "[BATCH 29/149] Loss_D: 0.8015 Loss_G: 0.8182 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.8500 Loss_G: 0.8349 acc: 71.9%\n",
      "[BATCH 31/149] Loss_D: 0.7827 Loss_G: 0.8159 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.8014 Loss_G: 0.8070 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.7967 Loss_G: 0.8027 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.8298 Loss_G: 0.8151 acc: 70.3%\n",
      "[BATCH 35/149] Loss_D: 0.7742 Loss_G: 0.8160 acc: 67.2%\n",
      "[BATCH 36/149] Loss_D: 0.8117 Loss_G: 0.8094 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.8136 Loss_G: 0.8087 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7556 Loss_G: 0.8024 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.7639 Loss_G: 0.8004 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7753 Loss_G: 0.7961 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.8216 Loss_G: 0.8098 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7813 Loss_G: 0.8158 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.8285 Loss_G: 0.8211 acc: 46.9%\n",
      "[BATCH 44/149] Loss_D: 0.8163 Loss_G: 0.8343 acc: 71.9%\n",
      "[BATCH 45/149] Loss_D: 0.7651 Loss_G: 0.8205 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.8130 Loss_G: 0.8300 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.8387 Loss_G: 0.8357 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.7811 Loss_G: 0.8296 acc: 73.4%\n",
      "[BATCH 49/149] Loss_D: 0.7920 Loss_G: 0.8283 acc: 51.6%\n",
      "[BATCH 50/149] Loss_D: 0.7903 Loss_G: 0.8255 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7647 Loss_G: 0.8101 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.7837 Loss_G: 0.8115 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7951 Loss_G: 0.8171 acc: 57.8%\n",
      "[BATCH 54/149] Loss_D: 0.8162 Loss_G: 0.8170 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.8187 Loss_G: 0.8181 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.8106 Loss_G: 0.8148 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7719 Loss_G: 0.8011 acc: 64.1%\n",
      "[EPOCH 1100] TEST ACC is : 76.0%\n",
      "[BATCH 58/149] Loss_D: 0.7716 Loss_G: 0.7960 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.8103 Loss_G: 0.8005 acc: 73.4%\n",
      "[BATCH 60/149] Loss_D: 0.7954 Loss_G: 0.7995 acc: 68.8%\n",
      "[BATCH 61/149] Loss_D: 0.7609 Loss_G: 0.7945 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.8014 Loss_G: 0.7891 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7958 Loss_G: 0.8025 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.8280 Loss_G: 0.8223 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.7700 Loss_G: 0.8256 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7670 Loss_G: 0.8129 acc: 73.4%\n",
      "[BATCH 67/149] Loss_D: 0.7743 Loss_G: 0.8117 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7786 Loss_G: 0.8172 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.7602 Loss_G: 0.8070 acc: 56.2%\n",
      "[BATCH 70/149] Loss_D: 0.8055 Loss_G: 0.8149 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.7649 Loss_G: 0.8077 acc: 56.2%\n",
      "[BATCH 72/149] Loss_D: 0.7592 Loss_G: 0.7962 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.8320 Loss_G: 0.8177 acc: 70.3%\n",
      "[BATCH 74/149] Loss_D: 0.7865 Loss_G: 0.8175 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.7900 Loss_G: 0.8223 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.8278 Loss_G: 0.8580 acc: 71.9%\n",
      "[BATCH 77/149] Loss_D: 0.7921 Loss_G: 0.8256 acc: 65.6%\n",
      "[BATCH 78/149] Loss_D: 0.8139 Loss_G: 0.8195 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.8117 Loss_G: 0.8133 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.8016 Loss_G: 0.8111 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7875 Loss_G: 0.8045 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.8229 Loss_G: 0.8106 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7836 Loss_G: 0.8076 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.7986 Loss_G: 0.8132 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.8190 Loss_G: 0.8181 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.7843 Loss_G: 0.8112 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.7974 Loss_G: 0.8068 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.7997 Loss_G: 0.8031 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.8149 Loss_G: 0.8131 acc: 73.4%\n",
      "[BATCH 90/149] Loss_D: 0.8213 Loss_G: 0.8206 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.8388 Loss_G: 0.8258 acc: 76.6%\n",
      "[BATCH 92/149] Loss_D: 0.7860 Loss_G: 0.8255 acc: 64.1%\n",
      "[BATCH 93/149] Loss_D: 0.8587 Loss_G: 0.8247 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.7703 Loss_G: 0.8162 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7717 Loss_G: 0.8006 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.8115 Loss_G: 0.8062 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7441 Loss_G: 0.7995 acc: 73.4%\n",
      "[BATCH 98/149] Loss_D: 0.8032 Loss_G: 0.8042 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.8034 Loss_G: 0.8037 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.7739 Loss_G: 0.8035 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.7797 Loss_G: 0.7990 acc: 71.9%\n",
      "[BATCH 102/149] Loss_D: 0.8232 Loss_G: 0.8083 acc: 59.4%\n",
      "[BATCH 103/149] Loss_D: 0.7828 Loss_G: 0.8194 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.8018 Loss_G: 0.8143 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.8125 Loss_G: 0.8180 acc: 71.9%\n",
      "[BATCH 106/149] Loss_D: 0.7507 Loss_G: 0.7927 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.7864 Loss_G: 0.7979 acc: 68.8%\n",
      "[EPOCH 1150] TEST ACC is : 74.8%\n",
      "[BATCH 108/149] Loss_D: 0.7746 Loss_G: 0.8030 acc: 73.4%\n",
      "[BATCH 109/149] Loss_D: 0.7861 Loss_G: 0.8131 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.8054 Loss_G: 0.8264 acc: 59.4%\n",
      "[BATCH 111/149] Loss_D: 0.8521 Loss_G: 0.8300 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.7660 Loss_G: 0.8225 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7794 Loss_G: 0.8103 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.8432 Loss_G: 0.8217 acc: 70.3%\n",
      "[BATCH 115/149] Loss_D: 0.7990 Loss_G: 0.8236 acc: 62.5%\n",
      "[BATCH 116/149] Loss_D: 0.8029 Loss_G: 0.8163 acc: 73.4%\n",
      "[BATCH 117/149] Loss_D: 0.7968 Loss_G: 0.8103 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7575 Loss_G: 0.8016 acc: 73.4%\n",
      "[BATCH 119/149] Loss_D: 0.7800 Loss_G: 0.7958 acc: 75.0%\n",
      "[BATCH 120/149] Loss_D: 0.8218 Loss_G: 0.8077 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.7938 Loss_G: 0.8046 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.7568 Loss_G: 0.7969 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.7741 Loss_G: 0.7941 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.7444 Loss_G: 0.7877 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.7951 Loss_G: 0.8009 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7779 Loss_G: 0.8069 acc: 65.6%\n",
      "[BATCH 127/149] Loss_D: 0.7699 Loss_G: 0.8089 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.7546 Loss_G: 0.7995 acc: 68.8%\n",
      "[BATCH 129/149] Loss_D: 0.8425 Loss_G: 0.8140 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.7830 Loss_G: 0.8128 acc: 60.9%\n",
      "[BATCH 131/149] Loss_D: 0.7478 Loss_G: 0.8114 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.7744 Loss_G: 0.8116 acc: 59.4%\n",
      "[BATCH 133/149] Loss_D: 0.7970 Loss_G: 0.8117 acc: 76.6%\n",
      "[BATCH 134/149] Loss_D: 0.7762 Loss_G: 0.8076 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7919 Loss_G: 0.8042 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.8127 Loss_G: 0.8065 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.7700 Loss_G: 0.8046 acc: 73.4%\n",
      "[BATCH 138/149] Loss_D: 0.8302 Loss_G: 0.8273 acc: 70.3%\n",
      "[BATCH 139/149] Loss_D: 0.7985 Loss_G: 0.8325 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.7695 Loss_G: 0.8116 acc: 54.7%\n",
      "[BATCH 141/149] Loss_D: 0.8142 Loss_G: 0.8251 acc: 71.9%\n",
      "[BATCH 142/149] Loss_D: 0.7957 Loss_G: 0.8187 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.7782 Loss_G: 0.8171 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7925 Loss_G: 0.8116 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.7964 Loss_G: 0.8041 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.8459 Loss_G: 0.8287 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.7869 Loss_G: 0.8277 acc: 75.0%\n",
      "[BATCH 148/149] Loss_D: 0.8106 Loss_G: 0.8170 acc: 68.8%\n",
      "[BATCH 149/149] Loss_D: 0.7764 Loss_G: 0.8105 acc: 64.1%\n",
      "-----THE [8/50] epoch end-----\n",
      "-----THE [9/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7969 Loss_G: 0.8096 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7822 Loss_G: 0.8161 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7801 Loss_G: 0.8127 acc: 59.4%\n",
      "[BATCH 4/149] Loss_D: 0.8225 Loss_G: 0.8214 acc: 62.5%\n",
      "[BATCH 5/149] Loss_D: 0.8124 Loss_G: 0.8259 acc: 75.0%\n",
      "[BATCH 6/149] Loss_D: 0.7738 Loss_G: 0.8098 acc: 70.3%\n",
      "[BATCH 7/149] Loss_D: 0.7951 Loss_G: 0.7940 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.7866 Loss_G: 0.7957 acc: 67.2%\n",
      "[EPOCH 1200] TEST ACC is : 75.8%\n",
      "[BATCH 9/149] Loss_D: 0.7838 Loss_G: 0.7996 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.8073 Loss_G: 0.8013 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.8021 Loss_G: 0.8037 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7998 Loss_G: 0.8062 acc: 62.5%\n",
      "[BATCH 13/149] Loss_D: 0.7850 Loss_G: 0.7990 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7424 Loss_G: 0.7918 acc: 54.7%\n",
      "[BATCH 15/149] Loss_D: 0.7940 Loss_G: 0.7974 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.7517 Loss_G: 0.7945 acc: 57.8%\n",
      "[BATCH 17/149] Loss_D: 0.7666 Loss_G: 0.7934 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.7929 Loss_G: 0.7997 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.8019 Loss_G: 0.8048 acc: 56.2%\n",
      "[BATCH 20/149] Loss_D: 0.7558 Loss_G: 0.7932 acc: 62.5%\n",
      "[BATCH 21/149] Loss_D: 0.7685 Loss_G: 0.7913 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.8343 Loss_G: 0.8163 acc: 53.1%\n",
      "[BATCH 23/149] Loss_D: 0.8062 Loss_G: 0.8140 acc: 65.6%\n",
      "[BATCH 24/149] Loss_D: 0.8099 Loss_G: 0.8073 acc: 70.3%\n",
      "[BATCH 25/149] Loss_D: 0.8140 Loss_G: 0.8037 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.8180 Loss_G: 0.8197 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.8073 Loss_G: 0.8159 acc: 57.8%\n",
      "[BATCH 28/149] Loss_D: 0.7650 Loss_G: 0.8050 acc: 67.2%\n",
      "[BATCH 29/149] Loss_D: 0.8250 Loss_G: 0.8198 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.7812 Loss_G: 0.8146 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.8385 Loss_G: 0.8169 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.8265 Loss_G: 0.8245 acc: 71.9%\n",
      "[BATCH 33/149] Loss_D: 0.7674 Loss_G: 0.8050 acc: 75.0%\n",
      "[BATCH 34/149] Loss_D: 0.7874 Loss_G: 0.8065 acc: 60.9%\n",
      "[BATCH 35/149] Loss_D: 0.7757 Loss_G: 0.8028 acc: 70.3%\n",
      "[BATCH 36/149] Loss_D: 0.7949 Loss_G: 0.8038 acc: 64.1%\n",
      "[BATCH 37/149] Loss_D: 0.8426 Loss_G: 0.8182 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.7884 Loss_G: 0.8142 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.7684 Loss_G: 0.8108 acc: 68.8%\n",
      "[BATCH 40/149] Loss_D: 0.8473 Loss_G: 0.8243 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.7519 Loss_G: 0.8178 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7861 Loss_G: 0.8089 acc: 57.8%\n",
      "[BATCH 43/149] Loss_D: 0.8213 Loss_G: 0.8223 acc: 76.6%\n",
      "[BATCH 44/149] Loss_D: 0.7767 Loss_G: 0.8265 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7937 Loss_G: 0.8087 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.8276 Loss_G: 0.8258 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.7657 Loss_G: 0.8123 acc: 67.2%\n",
      "[BATCH 48/149] Loss_D: 0.8094 Loss_G: 0.8167 acc: 71.9%\n",
      "[BATCH 49/149] Loss_D: 0.7537 Loss_G: 0.8029 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.7456 Loss_G: 0.7941 acc: 71.9%\n",
      "[BATCH 51/149] Loss_D: 0.7871 Loss_G: 0.7957 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.8214 Loss_G: 0.8014 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.7699 Loss_G: 0.8070 acc: 53.1%\n",
      "[BATCH 54/149] Loss_D: 0.8288 Loss_G: 0.8080 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.7814 Loss_G: 0.8066 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.8300 Loss_G: 0.8171 acc: 57.8%\n",
      "[BATCH 57/149] Loss_D: 0.7795 Loss_G: 0.8169 acc: 70.3%\n",
      "[BATCH 58/149] Loss_D: 0.8115 Loss_G: 0.8158 acc: 70.3%\n",
      "[EPOCH 1250] TEST ACC is : 75.8%\n",
      "[BATCH 59/149] Loss_D: 0.7709 Loss_G: 0.8112 acc: 76.6%\n",
      "[BATCH 60/149] Loss_D: 0.7876 Loss_G: 0.8124 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7878 Loss_G: 0.8119 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7907 Loss_G: 0.8210 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.7700 Loss_G: 0.8116 acc: 62.5%\n",
      "[BATCH 64/149] Loss_D: 0.8003 Loss_G: 0.8212 acc: 68.8%\n",
      "[BATCH 65/149] Loss_D: 0.7966 Loss_G: 0.8207 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.7675 Loss_G: 0.8117 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7731 Loss_G: 0.8048 acc: 51.6%\n",
      "[BATCH 68/149] Loss_D: 0.7614 Loss_G: 0.7965 acc: 59.4%\n",
      "[BATCH 69/149] Loss_D: 0.8559 Loss_G: 0.8116 acc: 68.8%\n",
      "[BATCH 70/149] Loss_D: 0.7711 Loss_G: 0.8102 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.7843 Loss_G: 0.8026 acc: 71.9%\n",
      "[BATCH 72/149] Loss_D: 0.7568 Loss_G: 0.7967 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.8125 Loss_G: 0.8097 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.8008 Loss_G: 0.8089 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.8246 Loss_G: 0.8129 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.7923 Loss_G: 0.8119 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.8142 Loss_G: 0.8168 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.8056 Loss_G: 0.8152 acc: 67.2%\n",
      "[BATCH 79/149] Loss_D: 0.7965 Loss_G: 0.8081 acc: 62.5%\n",
      "[BATCH 80/149] Loss_D: 0.7659 Loss_G: 0.7993 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.8397 Loss_G: 0.8110 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.7826 Loss_G: 0.8089 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7623 Loss_G: 0.7981 acc: 67.2%\n",
      "[BATCH 84/149] Loss_D: 0.7619 Loss_G: 0.7989 acc: 59.4%\n",
      "[BATCH 85/149] Loss_D: 0.7858 Loss_G: 0.7976 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.7710 Loss_G: 0.8040 acc: 75.0%\n",
      "[BATCH 87/149] Loss_D: 0.8262 Loss_G: 0.8226 acc: 56.2%\n",
      "[BATCH 88/149] Loss_D: 0.7951 Loss_G: 0.8114 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.8006 Loss_G: 0.8079 acc: 64.1%\n",
      "[BATCH 90/149] Loss_D: 0.8219 Loss_G: 0.8094 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.7857 Loss_G: 0.8158 acc: 71.9%\n",
      "[BATCH 92/149] Loss_D: 0.7584 Loss_G: 0.8031 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.8143 Loss_G: 0.8075 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.8590 Loss_G: 0.8194 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.8479 Loss_G: 0.8233 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.8462 Loss_G: 0.8242 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7975 Loss_G: 0.8088 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.7824 Loss_G: 0.7974 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7920 Loss_G: 0.8037 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.7655 Loss_G: 0.8023 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.7508 Loss_G: 0.8046 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.8006 Loss_G: 0.7997 acc: 59.4%\n",
      "[BATCH 103/149] Loss_D: 0.7843 Loss_G: 0.8035 acc: 70.3%\n",
      "[BATCH 104/149] Loss_D: 0.7765 Loss_G: 0.8003 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.8404 Loss_G: 0.8240 acc: 62.5%\n",
      "[BATCH 106/149] Loss_D: 0.7621 Loss_G: 0.8084 acc: 73.4%\n",
      "[BATCH 107/149] Loss_D: 0.8003 Loss_G: 0.8103 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7761 Loss_G: 0.8028 acc: 67.2%\n",
      "[EPOCH 1300] TEST ACC is : 75.8%\n",
      "[BATCH 109/149] Loss_D: 0.7898 Loss_G: 0.8054 acc: 59.4%\n",
      "[BATCH 110/149] Loss_D: 0.8111 Loss_G: 0.8147 acc: 70.3%\n",
      "[BATCH 111/149] Loss_D: 0.7918 Loss_G: 0.8166 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.8151 Loss_G: 0.8145 acc: 59.4%\n",
      "[BATCH 113/149] Loss_D: 0.7781 Loss_G: 0.8154 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.8151 Loss_G: 0.8198 acc: 76.6%\n",
      "[BATCH 115/149] Loss_D: 0.8558 Loss_G: 0.8256 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.8136 Loss_G: 0.8180 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.8045 Loss_G: 0.8181 acc: 73.4%\n",
      "[BATCH 118/149] Loss_D: 0.7721 Loss_G: 0.8077 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.7920 Loss_G: 0.8047 acc: 59.4%\n",
      "[BATCH 120/149] Loss_D: 0.7972 Loss_G: 0.8110 acc: 67.2%\n",
      "[BATCH 121/149] Loss_D: 0.7851 Loss_G: 0.8088 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.7541 Loss_G: 0.8017 acc: 62.5%\n",
      "[BATCH 123/149] Loss_D: 0.7745 Loss_G: 0.8004 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.7550 Loss_G: 0.7873 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.8057 Loss_G: 0.7986 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8125 Loss_G: 0.8178 acc: 57.8%\n",
      "[BATCH 127/149] Loss_D: 0.7899 Loss_G: 0.8300 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7560 Loss_G: 0.8116 acc: 65.6%\n",
      "[BATCH 129/149] Loss_D: 0.7717 Loss_G: 0.8140 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7550 Loss_G: 0.8100 acc: 62.5%\n",
      "[BATCH 131/149] Loss_D: 0.7815 Loss_G: 0.8066 acc: 68.8%\n",
      "[BATCH 132/149] Loss_D: 0.8577 Loss_G: 0.8389 acc: 70.3%\n",
      "[BATCH 133/149] Loss_D: 0.7922 Loss_G: 0.8368 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.7970 Loss_G: 0.8238 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.7783 Loss_G: 0.8052 acc: 70.3%\n",
      "[BATCH 136/149] Loss_D: 0.8114 Loss_G: 0.8077 acc: 65.6%\n",
      "[BATCH 137/149] Loss_D: 0.7624 Loss_G: 0.8035 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.8302 Loss_G: 0.8105 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.7738 Loss_G: 0.8187 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.8161 Loss_G: 0.8188 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.8413 Loss_G: 0.8312 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.8435 Loss_G: 0.8436 acc: 57.8%\n",
      "[BATCH 143/149] Loss_D: 0.7966 Loss_G: 0.8338 acc: 59.4%\n",
      "[BATCH 144/149] Loss_D: 0.8124 Loss_G: 0.8193 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.7580 Loss_G: 0.8116 acc: 73.4%\n",
      "[BATCH 146/149] Loss_D: 0.7772 Loss_G: 0.8137 acc: 76.6%\n",
      "[BATCH 147/149] Loss_D: 0.8308 Loss_G: 0.8277 acc: 67.2%\n",
      "[BATCH 148/149] Loss_D: 0.8088 Loss_G: 0.8318 acc: 57.8%\n",
      "[BATCH 149/149] Loss_D: 0.7493 Loss_G: 0.8104 acc: 68.8%\n",
      "-----THE [9/50] epoch end-----\n",
      "-----THE [10/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8033 Loss_G: 0.7992 acc: 59.4%\n",
      "[BATCH 2/149] Loss_D: 0.8206 Loss_G: 0.8066 acc: 53.1%\n",
      "[BATCH 3/149] Loss_D: 0.8436 Loss_G: 0.8278 acc: 57.8%\n",
      "[BATCH 4/149] Loss_D: 0.7609 Loss_G: 0.8085 acc: 71.9%\n",
      "[BATCH 5/149] Loss_D: 0.7790 Loss_G: 0.8076 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7756 Loss_G: 0.8027 acc: 70.3%\n",
      "[BATCH 7/149] Loss_D: 0.7899 Loss_G: 0.8008 acc: 54.7%\n",
      "[BATCH 8/149] Loss_D: 0.7263 Loss_G: 0.7906 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.8197 Loss_G: 0.7953 acc: 71.9%\n",
      "[EPOCH 1350] TEST ACC is : 74.2%\n",
      "[BATCH 10/149] Loss_D: 0.7951 Loss_G: 0.8063 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.8131 Loss_G: 0.8174 acc: 57.8%\n",
      "[BATCH 12/149] Loss_D: 0.7828 Loss_G: 0.8141 acc: 73.4%\n",
      "[BATCH 13/149] Loss_D: 0.7787 Loss_G: 0.8047 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.8126 Loss_G: 0.8177 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.8047 Loss_G: 0.8223 acc: 67.2%\n",
      "[BATCH 16/149] Loss_D: 0.7367 Loss_G: 0.8051 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7669 Loss_G: 0.8003 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.8198 Loss_G: 0.8147 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.7716 Loss_G: 0.8114 acc: 53.1%\n",
      "[BATCH 20/149] Loss_D: 0.7793 Loss_G: 0.8054 acc: 67.2%\n",
      "[BATCH 21/149] Loss_D: 0.7750 Loss_G: 0.7996 acc: 56.2%\n",
      "[BATCH 22/149] Loss_D: 0.8092 Loss_G: 0.8060 acc: 78.1%\n",
      "[BATCH 23/149] Loss_D: 0.7836 Loss_G: 0.8101 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.8200 Loss_G: 0.8164 acc: 70.3%\n",
      "[BATCH 25/149] Loss_D: 0.8172 Loss_G: 0.8211 acc: 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.7565 Loss_G: 0.8135 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7848 Loss_G: 0.8106 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.7569 Loss_G: 0.8023 acc: 64.1%\n",
      "[BATCH 29/149] Loss_D: 0.7883 Loss_G: 0.8030 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.7676 Loss_G: 0.8044 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.8171 Loss_G: 0.8149 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.8035 Loss_G: 0.8293 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.7859 Loss_G: 0.8214 acc: 70.3%\n",
      "[BATCH 34/149] Loss_D: 0.7648 Loss_G: 0.8036 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7674 Loss_G: 0.7928 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.8568 Loss_G: 0.8128 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7723 Loss_G: 0.8057 acc: 68.8%\n",
      "[BATCH 38/149] Loss_D: 0.8175 Loss_G: 0.8102 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.8006 Loss_G: 0.8140 acc: 56.2%\n",
      "[BATCH 40/149] Loss_D: 0.8108 Loss_G: 0.8182 acc: 76.6%\n",
      "[BATCH 41/149] Loss_D: 0.8550 Loss_G: 0.8436 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.7911 Loss_G: 0.8346 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.8158 Loss_G: 0.8409 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7878 Loss_G: 0.8115 acc: 54.7%\n",
      "[BATCH 45/149] Loss_D: 0.7569 Loss_G: 0.8016 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.8802 Loss_G: 0.8244 acc: 64.1%\n",
      "[BATCH 47/149] Loss_D: 0.7837 Loss_G: 0.8201 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.8144 Loss_G: 0.8128 acc: 76.6%\n",
      "[BATCH 49/149] Loss_D: 0.7638 Loss_G: 0.8028 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.7748 Loss_G: 0.8022 acc: 65.6%\n",
      "[BATCH 51/149] Loss_D: 0.8100 Loss_G: 0.8097 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.8073 Loss_G: 0.8112 acc: 51.6%\n",
      "[BATCH 53/149] Loss_D: 0.8056 Loss_G: 0.8077 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.8496 Loss_G: 0.8236 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.7953 Loss_G: 0.8349 acc: 70.3%\n",
      "[BATCH 56/149] Loss_D: 0.7632 Loss_G: 0.8256 acc: 57.8%\n",
      "[BATCH 57/149] Loss_D: 0.7726 Loss_G: 0.8177 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.7827 Loss_G: 0.8077 acc: 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.7749 Loss_G: 0.8076 acc: 75.0%\n",
      "[EPOCH 1400] TEST ACC is : 76.2%\n",
      "[BATCH 60/149] Loss_D: 0.7805 Loss_G: 0.8022 acc: 57.8%\n",
      "[BATCH 61/149] Loss_D: 0.8403 Loss_G: 0.8105 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.7918 Loss_G: 0.8158 acc: 71.9%\n",
      "[BATCH 63/149] Loss_D: 0.8371 Loss_G: 0.8237 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.7905 Loss_G: 0.8187 acc: 59.4%\n",
      "[BATCH 65/149] Loss_D: 0.7591 Loss_G: 0.8079 acc: 56.2%\n",
      "[BATCH 66/149] Loss_D: 0.8262 Loss_G: 0.8197 acc: 53.1%\n",
      "[BATCH 67/149] Loss_D: 0.8213 Loss_G: 0.8283 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7945 Loss_G: 0.8106 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.7628 Loss_G: 0.7970 acc: 67.2%\n",
      "[BATCH 70/149] Loss_D: 0.8594 Loss_G: 0.8125 acc: 79.7%\n",
      "[BATCH 71/149] Loss_D: 0.7796 Loss_G: 0.8086 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8495 Loss_G: 0.8235 acc: 64.1%\n",
      "[BATCH 73/149] Loss_D: 0.7900 Loss_G: 0.8136 acc: 65.6%\n",
      "[BATCH 74/149] Loss_D: 0.7874 Loss_G: 0.8024 acc: 71.9%\n",
      "[BATCH 75/149] Loss_D: 0.8292 Loss_G: 0.8142 acc: 71.9%\n",
      "[BATCH 76/149] Loss_D: 0.7877 Loss_G: 0.8134 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.7888 Loss_G: 0.8030 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.8832 Loss_G: 0.8426 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.8197 Loss_G: 0.8310 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.7897 Loss_G: 0.8158 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7916 Loss_G: 0.8103 acc: 62.5%\n",
      "[BATCH 82/149] Loss_D: 0.8084 Loss_G: 0.8224 acc: 71.9%\n",
      "[BATCH 83/149] Loss_D: 0.7977 Loss_G: 0.8203 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.7595 Loss_G: 0.8127 acc: 67.2%\n",
      "[BATCH 85/149] Loss_D: 0.7531 Loss_G: 0.8003 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.7803 Loss_G: 0.8029 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.7675 Loss_G: 0.7967 acc: 70.3%\n",
      "[BATCH 88/149] Loss_D: 0.8024 Loss_G: 0.8110 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.7832 Loss_G: 0.8105 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7528 Loss_G: 0.8048 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.8693 Loss_G: 0.8298 acc: 71.9%\n",
      "[BATCH 92/149] Loss_D: 0.8272 Loss_G: 0.8349 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.8165 Loss_G: 0.8410 acc: 71.9%\n",
      "[BATCH 94/149] Loss_D: 0.7899 Loss_G: 0.8338 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.7929 Loss_G: 0.8293 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.7855 Loss_G: 0.8154 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.7762 Loss_G: 0.8097 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.7617 Loss_G: 0.8023 acc: 71.9%\n",
      "[BATCH 99/149] Loss_D: 0.7690 Loss_G: 0.7977 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.7562 Loss_G: 0.7955 acc: 54.7%\n",
      "[BATCH 101/149] Loss_D: 0.7753 Loss_G: 0.7958 acc: 73.4%\n",
      "[BATCH 102/149] Loss_D: 0.7552 Loss_G: 0.8004 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.7964 Loss_G: 0.8088 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.7669 Loss_G: 0.8012 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.7901 Loss_G: 0.8002 acc: 65.6%\n",
      "[BATCH 106/149] Loss_D: 0.8090 Loss_G: 0.8070 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.7859 Loss_G: 0.8045 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7986 Loss_G: 0.8101 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.8030 Loss_G: 0.8173 acc: 68.8%\n",
      "[EPOCH 1450] TEST ACC is : 74.2%\n",
      "[BATCH 110/149] Loss_D: 0.7805 Loss_G: 0.8210 acc: 76.6%\n",
      "[BATCH 111/149] Loss_D: 0.7844 Loss_G: 0.8054 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.7679 Loss_G: 0.8004 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.7802 Loss_G: 0.8013 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.8049 Loss_G: 0.8001 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.7721 Loss_G: 0.8048 acc: 54.7%\n",
      "[BATCH 116/149] Loss_D: 0.8462 Loss_G: 0.8290 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.8383 Loss_G: 0.8341 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7690 Loss_G: 0.8167 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.7941 Loss_G: 0.8105 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.7823 Loss_G: 0.8109 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.7859 Loss_G: 0.8161 acc: 71.9%\n",
      "[BATCH 122/149] Loss_D: 0.7948 Loss_G: 0.8169 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.7657 Loss_G: 0.8080 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7768 Loss_G: 0.8028 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.8200 Loss_G: 0.8114 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.7660 Loss_G: 0.8116 acc: 68.8%\n",
      "[BATCH 127/149] Loss_D: 0.7572 Loss_G: 0.8010 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7924 Loss_G: 0.8041 acc: 57.8%\n",
      "[BATCH 129/149] Loss_D: 0.7782 Loss_G: 0.8030 acc: 78.1%\n",
      "[BATCH 130/149] Loss_D: 0.8249 Loss_G: 0.8107 acc: 67.2%\n",
      "[BATCH 131/149] Loss_D: 0.8065 Loss_G: 0.8225 acc: 76.6%\n",
      "[BATCH 132/149] Loss_D: 0.8060 Loss_G: 0.8162 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.8018 Loss_G: 0.8215 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.7891 Loss_G: 0.8113 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7556 Loss_G: 0.8077 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.8051 Loss_G: 0.8014 acc: 65.6%\n",
      "[BATCH 137/149] Loss_D: 0.7749 Loss_G: 0.7999 acc: 51.6%\n",
      "[BATCH 138/149] Loss_D: 0.8418 Loss_G: 0.8209 acc: 67.2%\n",
      "[BATCH 139/149] Loss_D: 0.7458 Loss_G: 0.8044 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7770 Loss_G: 0.7959 acc: 68.8%\n",
      "[BATCH 141/149] Loss_D: 0.7540 Loss_G: 0.7899 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.8068 Loss_G: 0.7962 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.7935 Loss_G: 0.8014 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.7889 Loss_G: 0.7988 acc: 73.4%\n",
      "[BATCH 145/149] Loss_D: 0.7914 Loss_G: 0.8022 acc: 71.9%\n",
      "[BATCH 146/149] Loss_D: 0.8013 Loss_G: 0.8051 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.8310 Loss_G: 0.8249 acc: 60.9%\n",
      "[BATCH 148/149] Loss_D: 0.8043 Loss_G: 0.8288 acc: 56.2%\n",
      "[BATCH 149/149] Loss_D: 0.7977 Loss_G: 0.8104 acc: 59.4%\n",
      "-----THE [10/50] epoch end-----\n",
      "-----THE [11/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7767 Loss_G: 0.8047 acc: 71.9%\n",
      "[BATCH 2/149] Loss_D: 0.7777 Loss_G: 0.8035 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.7612 Loss_G: 0.8033 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.8024 Loss_G: 0.8015 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7919 Loss_G: 0.8083 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.8734 Loss_G: 0.8398 acc: 51.6%\n",
      "[BATCH 7/149] Loss_D: 0.7686 Loss_G: 0.8187 acc: 65.6%\n",
      "[BATCH 8/149] Loss_D: 0.8125 Loss_G: 0.8104 acc: 54.7%\n",
      "[BATCH 9/149] Loss_D: 0.7845 Loss_G: 0.7997 acc: 73.4%\n",
      "[BATCH 10/149] Loss_D: 0.7397 Loss_G: 0.7943 acc: 62.5%\n",
      "[EPOCH 1500] TEST ACC is : 76.4%\n",
      "[BATCH 11/149] Loss_D: 0.7736 Loss_G: 0.7932 acc: 73.4%\n",
      "[BATCH 12/149] Loss_D: 0.8340 Loss_G: 0.8080 acc: 60.9%\n",
      "[BATCH 13/149] Loss_D: 0.7602 Loss_G: 0.8097 acc: 67.2%\n",
      "[BATCH 14/149] Loss_D: 0.7945 Loss_G: 0.8052 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.8118 Loss_G: 0.8061 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.7908 Loss_G: 0.8097 acc: 68.8%\n",
      "[BATCH 17/149] Loss_D: 0.8377 Loss_G: 0.8162 acc: 70.3%\n",
      "[BATCH 18/149] Loss_D: 0.7998 Loss_G: 0.8214 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.8025 Loss_G: 0.8299 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7452 Loss_G: 0.8088 acc: 70.3%\n",
      "[BATCH 21/149] Loss_D: 0.8047 Loss_G: 0.8139 acc: 65.6%\n",
      "[BATCH 22/149] Loss_D: 0.7972 Loss_G: 0.8227 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7971 Loss_G: 0.8232 acc: 68.8%\n",
      "[BATCH 24/149] Loss_D: 0.7513 Loss_G: 0.8055 acc: 68.8%\n",
      "[BATCH 25/149] Loss_D: 0.8680 Loss_G: 0.8205 acc: 73.4%\n",
      "[BATCH 26/149] Loss_D: 0.7781 Loss_G: 0.8128 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.8123 Loss_G: 0.8033 acc: 57.8%\n",
      "[BATCH 28/149] Loss_D: 0.7665 Loss_G: 0.8022 acc: 64.1%\n",
      "[BATCH 29/149] Loss_D: 0.7589 Loss_G: 0.8081 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.8274 Loss_G: 0.8081 acc: 73.4%\n",
      "[BATCH 31/149] Loss_D: 0.8203 Loss_G: 0.8130 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7662 Loss_G: 0.8035 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.7961 Loss_G: 0.8021 acc: 73.4%\n",
      "[BATCH 34/149] Loss_D: 0.7921 Loss_G: 0.8021 acc: 59.4%\n",
      "[BATCH 35/149] Loss_D: 0.7820 Loss_G: 0.8141 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.7558 Loss_G: 0.7872 acc: 78.1%\n",
      "[BATCH 37/149] Loss_D: 0.7783 Loss_G: 0.7926 acc: 57.8%\n",
      "[BATCH 38/149] Loss_D: 0.7786 Loss_G: 0.8021 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8425 Loss_G: 0.8193 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.7619 Loss_G: 0.8152 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7979 Loss_G: 0.8128 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.8003 Loss_G: 0.8142 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7956 Loss_G: 0.8206 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.7872 Loss_G: 0.8184 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7721 Loss_G: 0.8072 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8491 Loss_G: 0.8158 acc: 51.6%\n",
      "[BATCH 47/149] Loss_D: 0.8517 Loss_G: 0.8457 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7721 Loss_G: 0.8296 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.8122 Loss_G: 0.8156 acc: 70.3%\n",
      "[BATCH 50/149] Loss_D: 0.7887 Loss_G: 0.8095 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.8003 Loss_G: 0.8112 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.7789 Loss_G: 0.8107 acc: 60.9%\n",
      "[BATCH 53/149] Loss_D: 0.8070 Loss_G: 0.8156 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.8008 Loss_G: 0.8159 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.7801 Loss_G: 0.8137 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.8109 Loss_G: 0.8244 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7522 Loss_G: 0.8086 acc: 62.5%\n",
      "[BATCH 58/149] Loss_D: 0.7704 Loss_G: 0.8002 acc: 62.5%\n",
      "[BATCH 59/149] Loss_D: 0.8141 Loss_G: 0.8109 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.7764 Loss_G: 0.8010 acc: 78.1%\n",
      "[EPOCH 1550] TEST ACC is : 74.6%\n",
      "[BATCH 61/149] Loss_D: 0.8470 Loss_G: 0.8128 acc: 54.7%\n",
      "[BATCH 62/149] Loss_D: 0.7573 Loss_G: 0.8107 acc: 75.0%\n",
      "[BATCH 63/149] Loss_D: 0.8625 Loss_G: 0.8219 acc: 73.4%\n",
      "[BATCH 64/149] Loss_D: 0.7842 Loss_G: 0.8128 acc: 75.0%\n",
      "[BATCH 65/149] Loss_D: 0.8126 Loss_G: 0.8100 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.7914 Loss_G: 0.8069 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.7908 Loss_G: 0.8087 acc: 57.8%\n",
      "[BATCH 68/149] Loss_D: 0.8314 Loss_G: 0.8116 acc: 56.2%\n",
      "[BATCH 69/149] Loss_D: 0.7997 Loss_G: 0.8085 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.7750 Loss_G: 0.8056 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.8286 Loss_G: 0.8124 acc: 59.4%\n",
      "[BATCH 72/149] Loss_D: 0.7723 Loss_G: 0.8124 acc: 71.9%\n",
      "[BATCH 73/149] Loss_D: 0.7504 Loss_G: 0.7943 acc: 68.8%\n",
      "[BATCH 74/149] Loss_D: 0.7492 Loss_G: 0.7897 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.7808 Loss_G: 0.7927 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7610 Loss_G: 0.7981 acc: 70.3%\n",
      "[BATCH 77/149] Loss_D: 0.7866 Loss_G: 0.7993 acc: 73.4%\n",
      "[BATCH 78/149] Loss_D: 0.7709 Loss_G: 0.7968 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.8187 Loss_G: 0.8090 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7858 Loss_G: 0.8137 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.7739 Loss_G: 0.8084 acc: 53.1%\n",
      "[BATCH 82/149] Loss_D: 0.7993 Loss_G: 0.8055 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.7723 Loss_G: 0.8006 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 0.7848 Loss_G: 0.8075 acc: 73.4%\n",
      "[BATCH 85/149] Loss_D: 0.7856 Loss_G: 0.8111 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.7826 Loss_G: 0.8122 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.8038 Loss_G: 0.7991 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7805 Loss_G: 0.7987 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.8071 Loss_G: 0.8029 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.8292 Loss_G: 0.8113 acc: 73.4%\n",
      "[BATCH 91/149] Loss_D: 0.7633 Loss_G: 0.8008 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7540 Loss_G: 0.7899 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.7688 Loss_G: 0.7918 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.8085 Loss_G: 0.8006 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.7955 Loss_G: 0.8071 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.8258 Loss_G: 0.8194 acc: 53.1%\n",
      "[BATCH 97/149] Loss_D: 0.8121 Loss_G: 0.8239 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.8342 Loss_G: 0.8253 acc: 70.3%\n",
      "[BATCH 99/149] Loss_D: 0.8109 Loss_G: 0.8384 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.7696 Loss_G: 0.8201 acc: 67.2%\n",
      "[BATCH 101/149] Loss_D: 0.7790 Loss_G: 0.8181 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.7865 Loss_G: 0.8143 acc: 75.0%\n",
      "[BATCH 103/149] Loss_D: 0.7947 Loss_G: 0.8295 acc: 65.6%\n",
      "[BATCH 104/149] Loss_D: 0.8038 Loss_G: 0.8275 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7943 Loss_G: 0.8159 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7876 Loss_G: 0.8134 acc: 57.8%\n",
      "[BATCH 107/149] Loss_D: 0.7636 Loss_G: 0.8015 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8263 Loss_G: 0.8092 acc: 62.5%\n",
      "[BATCH 109/149] Loss_D: 0.8014 Loss_G: 0.8175 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7722 Loss_G: 0.8127 acc: 64.1%\n",
      "[EPOCH 1600] TEST ACC is : 74.8%\n",
      "[BATCH 111/149] Loss_D: 0.7999 Loss_G: 0.8160 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.7763 Loss_G: 0.8011 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.7715 Loss_G: 0.8035 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.7630 Loss_G: 0.7932 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.7489 Loss_G: 0.7864 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7681 Loss_G: 0.7825 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7769 Loss_G: 0.7909 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.7693 Loss_G: 0.7877 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.8005 Loss_G: 0.8114 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.8089 Loss_G: 0.8196 acc: 70.3%\n",
      "[BATCH 121/149] Loss_D: 0.8219 Loss_G: 0.8104 acc: 75.0%\n",
      "[BATCH 122/149] Loss_D: 0.7673 Loss_G: 0.8076 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.8150 Loss_G: 0.8137 acc: 70.3%\n",
      "[BATCH 124/149] Loss_D: 0.8272 Loss_G: 0.8174 acc: 57.8%\n",
      "[BATCH 125/149] Loss_D: 0.8476 Loss_G: 0.8344 acc: 53.1%\n",
      "[BATCH 126/149] Loss_D: 0.8313 Loss_G: 0.8434 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.8057 Loss_G: 0.8334 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7922 Loss_G: 0.8171 acc: 48.4%\n",
      "[BATCH 129/149] Loss_D: 0.8204 Loss_G: 0.8099 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.7826 Loss_G: 0.8001 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7729 Loss_G: 0.7970 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.7852 Loss_G: 0.8017 acc: 53.1%\n",
      "[BATCH 133/149] Loss_D: 0.7550 Loss_G: 0.7921 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7882 Loss_G: 0.7904 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.7866 Loss_G: 0.7968 acc: 70.3%\n",
      "[BATCH 136/149] Loss_D: 0.8058 Loss_G: 0.8029 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8519 Loss_G: 0.8269 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.7671 Loss_G: 0.8257 acc: 56.2%\n",
      "[BATCH 139/149] Loss_D: 0.8614 Loss_G: 0.8255 acc: 71.9%\n",
      "[BATCH 140/149] Loss_D: 0.7932 Loss_G: 0.8315 acc: 56.2%\n",
      "[BATCH 141/149] Loss_D: 0.8252 Loss_G: 0.8405 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.7534 Loss_G: 0.8153 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.7762 Loss_G: 0.8078 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.8071 Loss_G: 0.8182 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.7564 Loss_G: 0.8117 acc: 75.0%\n",
      "[BATCH 146/149] Loss_D: 0.7851 Loss_G: 0.8062 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.8208 Loss_G: 0.8092 acc: 56.2%\n",
      "[BATCH 148/149] Loss_D: 0.7899 Loss_G: 0.8053 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.8047 Loss_G: 0.8047 acc: 62.5%\n",
      "-----THE [11/50] epoch end-----\n",
      "-----THE [12/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7695 Loss_G: 0.8097 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.7686 Loss_G: 0.8012 acc: 54.7%\n",
      "[BATCH 3/149] Loss_D: 0.7412 Loss_G: 0.7939 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.8099 Loss_G: 0.7996 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.7685 Loss_G: 0.8044 acc: 56.2%\n",
      "[BATCH 6/149] Loss_D: 0.8291 Loss_G: 0.8300 acc: 62.5%\n",
      "[BATCH 7/149] Loss_D: 0.8071 Loss_G: 0.8379 acc: 59.4%\n",
      "[BATCH 8/149] Loss_D: 0.8211 Loss_G: 0.8177 acc: 68.8%\n",
      "[BATCH 9/149] Loss_D: 0.7561 Loss_G: 0.8078 acc: 60.9%\n",
      "[BATCH 10/149] Loss_D: 0.7689 Loss_G: 0.8008 acc: 68.8%\n",
      "[BATCH 11/149] Loss_D: 0.7910 Loss_G: 0.8037 acc: 75.0%\n",
      "[EPOCH 1650] TEST ACC is : 73.6%\n",
      "[BATCH 12/149] Loss_D: 0.7902 Loss_G: 0.8117 acc: 51.6%\n",
      "[BATCH 13/149] Loss_D: 0.8330 Loss_G: 0.8153 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.8684 Loss_G: 0.8188 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.7645 Loss_G: 0.8075 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.8194 Loss_G: 0.8047 acc: 46.9%\n",
      "[BATCH 17/149] Loss_D: 0.7501 Loss_G: 0.7963 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.8366 Loss_G: 0.8125 acc: 75.0%\n",
      "[BATCH 19/149] Loss_D: 0.7661 Loss_G: 0.8079 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7960 Loss_G: 0.8055 acc: 53.1%\n",
      "[BATCH 21/149] Loss_D: 0.7694 Loss_G: 0.8024 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.7823 Loss_G: 0.8077 acc: 54.7%\n",
      "[BATCH 23/149] Loss_D: 0.8308 Loss_G: 0.8197 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.8022 Loss_G: 0.8304 acc: 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.7878 Loss_G: 0.8190 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.7680 Loss_G: 0.8081 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.8000 Loss_G: 0.8126 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7654 Loss_G: 0.8175 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7874 Loss_G: 0.8210 acc: 67.2%\n",
      "[BATCH 30/149] Loss_D: 0.7867 Loss_G: 0.8264 acc: 57.8%\n",
      "[BATCH 31/149] Loss_D: 0.8740 Loss_G: 0.8371 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.7937 Loss_G: 0.8297 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.8119 Loss_G: 0.8380 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.7864 Loss_G: 0.8228 acc: 76.6%\n",
      "[BATCH 35/149] Loss_D: 0.8119 Loss_G: 0.8170 acc: 70.3%\n",
      "[BATCH 36/149] Loss_D: 0.7980 Loss_G: 0.8289 acc: 64.1%\n",
      "[BATCH 37/149] Loss_D: 0.7931 Loss_G: 0.8207 acc: 60.9%\n",
      "[BATCH 38/149] Loss_D: 0.7608 Loss_G: 0.8126 acc: 70.3%\n",
      "[BATCH 39/149] Loss_D: 0.7673 Loss_G: 0.8086 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.8071 Loss_G: 0.8116 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.8256 Loss_G: 0.8170 acc: 56.2%\n",
      "[BATCH 42/149] Loss_D: 0.7929 Loss_G: 0.8196 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.8245 Loss_G: 0.8273 acc: 73.4%\n",
      "[BATCH 44/149] Loss_D: 0.7537 Loss_G: 0.8111 acc: 75.0%\n",
      "[BATCH 45/149] Loss_D: 0.7583 Loss_G: 0.7957 acc: 56.2%\n",
      "[BATCH 46/149] Loss_D: 0.7720 Loss_G: 0.7918 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.8040 Loss_G: 0.8069 acc: 75.0%\n",
      "[BATCH 48/149] Loss_D: 0.7522 Loss_G: 0.8068 acc: 60.9%\n",
      "[BATCH 49/149] Loss_D: 0.7996 Loss_G: 0.8129 acc: 70.3%\n",
      "[BATCH 50/149] Loss_D: 0.7672 Loss_G: 0.8029 acc: 82.8%\n",
      "[BATCH 51/149] Loss_D: 0.7778 Loss_G: 0.8014 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7826 Loss_G: 0.7953 acc: 67.2%\n",
      "[BATCH 53/149] Loss_D: 0.8108 Loss_G: 0.7986 acc: 60.9%\n",
      "[BATCH 54/149] Loss_D: 0.7912 Loss_G: 0.8106 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.8129 Loss_G: 0.8078 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.7937 Loss_G: 0.8168 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.7544 Loss_G: 0.7996 acc: 50.0%\n",
      "[BATCH 58/149] Loss_D: 0.7583 Loss_G: 0.7941 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.8024 Loss_G: 0.8054 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.7487 Loss_G: 0.7938 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.7690 Loss_G: 0.7974 acc: 71.9%\n",
      "[EPOCH 1700] TEST ACC is : 76.2%\n",
      "[BATCH 62/149] Loss_D: 0.7825 Loss_G: 0.7890 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7893 Loss_G: 0.7973 acc: 53.1%\n",
      "[BATCH 64/149] Loss_D: 0.7940 Loss_G: 0.8029 acc: 70.3%\n",
      "[BATCH 65/149] Loss_D: 0.7949 Loss_G: 0.8155 acc: 71.9%\n",
      "[BATCH 66/149] Loss_D: 0.7794 Loss_G: 0.8182 acc: 71.9%\n",
      "[BATCH 67/149] Loss_D: 0.7764 Loss_G: 0.8124 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7713 Loss_G: 0.8079 acc: 57.8%\n",
      "[BATCH 69/149] Loss_D: 0.7806 Loss_G: 0.8036 acc: 68.8%\n",
      "[BATCH 70/149] Loss_D: 0.7566 Loss_G: 0.8006 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7922 Loss_G: 0.8044 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7850 Loss_G: 0.8089 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.8089 Loss_G: 0.8133 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.8137 Loss_G: 0.8126 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.8697 Loss_G: 0.8366 acc: 73.4%\n",
      "[BATCH 76/149] Loss_D: 0.8283 Loss_G: 0.8351 acc: 67.2%\n",
      "[BATCH 77/149] Loss_D: 0.7885 Loss_G: 0.8266 acc: 64.1%\n",
      "[BATCH 78/149] Loss_D: 0.8118 Loss_G: 0.8201 acc: 70.3%\n",
      "[BATCH 79/149] Loss_D: 0.7843 Loss_G: 0.8020 acc: 57.8%\n",
      "[BATCH 80/149] Loss_D: 0.7686 Loss_G: 0.7995 acc: 57.8%\n",
      "[BATCH 81/149] Loss_D: 0.7684 Loss_G: 0.7962 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.7869 Loss_G: 0.8007 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.8210 Loss_G: 0.8097 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.7927 Loss_G: 0.8167 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.7824 Loss_G: 0.8145 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.8171 Loss_G: 0.8341 acc: 53.1%\n",
      "[BATCH 87/149] Loss_D: 0.8385 Loss_G: 0.8403 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7969 Loss_G: 0.8254 acc: 62.5%\n",
      "[BATCH 89/149] Loss_D: 0.7640 Loss_G: 0.8107 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7845 Loss_G: 0.8149 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.7684 Loss_G: 0.8034 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7699 Loss_G: 0.8046 acc: 57.8%\n",
      "[BATCH 93/149] Loss_D: 0.7743 Loss_G: 0.7947 acc: 65.6%\n",
      "[BATCH 94/149] Loss_D: 0.7757 Loss_G: 0.8012 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7708 Loss_G: 0.7954 acc: 54.7%\n",
      "[BATCH 96/149] Loss_D: 0.8135 Loss_G: 0.8075 acc: 70.3%\n",
      "[BATCH 97/149] Loss_D: 0.7785 Loss_G: 0.8003 acc: 62.5%\n",
      "[BATCH 98/149] Loss_D: 0.7819 Loss_G: 0.7933 acc: 70.3%\n",
      "[BATCH 99/149] Loss_D: 0.7592 Loss_G: 0.7866 acc: 71.9%\n",
      "[BATCH 100/149] Loss_D: 0.8208 Loss_G: 0.7893 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.8588 Loss_G: 0.8176 acc: 51.6%\n",
      "[BATCH 102/149] Loss_D: 0.7560 Loss_G: 0.8044 acc: 60.9%\n",
      "[BATCH 103/149] Loss_D: 0.7815 Loss_G: 0.7999 acc: 65.6%\n",
      "[BATCH 104/149] Loss_D: 0.7758 Loss_G: 0.7960 acc: 68.8%\n",
      "[BATCH 105/149] Loss_D: 0.7636 Loss_G: 0.7956 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.8052 Loss_G: 0.8004 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.7489 Loss_G: 0.7965 acc: 71.9%\n",
      "[BATCH 108/149] Loss_D: 0.7941 Loss_G: 0.8051 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.8274 Loss_G: 0.8099 acc: 70.3%\n",
      "[BATCH 110/149] Loss_D: 0.8070 Loss_G: 0.8149 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.8254 Loss_G: 0.8294 acc: 70.3%\n",
      "[EPOCH 1750] TEST ACC is : 74.2%\n",
      "[BATCH 112/149] Loss_D: 0.8033 Loss_G: 0.8314 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.7617 Loss_G: 0.8173 acc: 73.4%\n",
      "[BATCH 114/149] Loss_D: 0.8673 Loss_G: 0.8292 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.8476 Loss_G: 0.8425 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8214 Loss_G: 0.8369 acc: 76.6%\n",
      "[BATCH 117/149] Loss_D: 0.7671 Loss_G: 0.8192 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.8076 Loss_G: 0.8110 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.8080 Loss_G: 0.8151 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7790 Loss_G: 0.8013 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.7783 Loss_G: 0.8011 acc: 70.3%\n",
      "[BATCH 122/149] Loss_D: 0.8139 Loss_G: 0.8161 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.8370 Loss_G: 0.8329 acc: 65.6%\n",
      "[BATCH 124/149] Loss_D: 0.8154 Loss_G: 0.8296 acc: 53.1%\n",
      "[BATCH 125/149] Loss_D: 0.8020 Loss_G: 0.8420 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7529 Loss_G: 0.8208 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.8023 Loss_G: 0.8195 acc: 59.4%\n",
      "[BATCH 128/149] Loss_D: 0.7736 Loss_G: 0.8171 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7846 Loss_G: 0.8117 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.7496 Loss_G: 0.8069 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.8004 Loss_G: 0.8021 acc: 54.7%\n",
      "[BATCH 132/149] Loss_D: 0.8253 Loss_G: 0.8170 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.8185 Loss_G: 0.8180 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7935 Loss_G: 0.8140 acc: 70.3%\n",
      "[BATCH 135/149] Loss_D: 0.7737 Loss_G: 0.8031 acc: 68.8%\n",
      "[BATCH 136/149] Loss_D: 0.8266 Loss_G: 0.8166 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8036 Loss_G: 0.8194 acc: 53.1%\n",
      "[BATCH 138/149] Loss_D: 0.8050 Loss_G: 0.8226 acc: 64.1%\n",
      "[BATCH 139/149] Loss_D: 0.7867 Loss_G: 0.8072 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.8050 Loss_G: 0.8106 acc: 53.1%\n",
      "[BATCH 141/149] Loss_D: 0.7785 Loss_G: 0.8057 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.8214 Loss_G: 0.8040 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.8105 Loss_G: 0.8165 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.8219 Loss_G: 0.8188 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.7729 Loss_G: 0.8109 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.7978 Loss_G: 0.8079 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7642 Loss_G: 0.7955 acc: 56.2%\n",
      "[BATCH 148/149] Loss_D: 0.7827 Loss_G: 0.7957 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7582 Loss_G: 0.7938 acc: 75.0%\n",
      "-----THE [12/50] epoch end-----\n",
      "-----THE [13/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7970 Loss_G: 0.7994 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7933 Loss_G: 0.8046 acc: 75.0%\n",
      "[BATCH 3/149] Loss_D: 0.7761 Loss_G: 0.8013 acc: 56.2%\n",
      "[BATCH 4/149] Loss_D: 0.8125 Loss_G: 0.8149 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.7732 Loss_G: 0.7998 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.8226 Loss_G: 0.8006 acc: 62.5%\n",
      "[BATCH 7/149] Loss_D: 0.8213 Loss_G: 0.8085 acc: 67.2%\n",
      "[BATCH 8/149] Loss_D: 0.7523 Loss_G: 0.7992 acc: 56.2%\n",
      "[BATCH 9/149] Loss_D: 0.8036 Loss_G: 0.7961 acc: 60.9%\n",
      "[BATCH 10/149] Loss_D: 0.8056 Loss_G: 0.8140 acc: 56.2%\n",
      "[BATCH 11/149] Loss_D: 0.7850 Loss_G: 0.8018 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.7929 Loss_G: 0.7996 acc: 56.2%\n",
      "[EPOCH 1800] TEST ACC is : 74.6%\n",
      "[BATCH 13/149] Loss_D: 0.7853 Loss_G: 0.8106 acc: 57.8%\n",
      "[BATCH 14/149] Loss_D: 0.8060 Loss_G: 0.8302 acc: 68.8%\n",
      "[BATCH 15/149] Loss_D: 0.7655 Loss_G: 0.8216 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7916 Loss_G: 0.8233 acc: 73.4%\n",
      "[BATCH 17/149] Loss_D: 0.7499 Loss_G: 0.8113 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.7620 Loss_G: 0.8001 acc: 73.4%\n",
      "[BATCH 19/149] Loss_D: 0.8063 Loss_G: 0.8043 acc: 71.9%\n",
      "[BATCH 20/149] Loss_D: 0.7636 Loss_G: 0.8027 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.7953 Loss_G: 0.8010 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7624 Loss_G: 0.8025 acc: 65.6%\n",
      "[BATCH 23/149] Loss_D: 0.7912 Loss_G: 0.8004 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.8180 Loss_G: 0.8089 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.7897 Loss_G: 0.8077 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.7935 Loss_G: 0.8100 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.7883 Loss_G: 0.8107 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.7440 Loss_G: 0.7982 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.8060 Loss_G: 0.8024 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.8781 Loss_G: 0.8268 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.7982 Loss_G: 0.8180 acc: 60.9%\n",
      "[BATCH 32/149] Loss_D: 0.8036 Loss_G: 0.8107 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.8586 Loss_G: 0.8220 acc: 51.6%\n",
      "[BATCH 34/149] Loss_D: 0.7696 Loss_G: 0.8231 acc: 71.9%\n",
      "[BATCH 35/149] Loss_D: 0.7537 Loss_G: 0.8033 acc: 68.8%\n",
      "[BATCH 36/149] Loss_D: 0.7977 Loss_G: 0.7951 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.7840 Loss_G: 0.7977 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.8365 Loss_G: 0.8214 acc: 51.6%\n",
      "[BATCH 39/149] Loss_D: 0.7971 Loss_G: 0.8122 acc: 65.6%\n",
      "[BATCH 40/149] Loss_D: 0.7637 Loss_G: 0.7998 acc: 57.8%\n",
      "[BATCH 41/149] Loss_D: 0.7539 Loss_G: 0.7911 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7565 Loss_G: 0.7901 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.8460 Loss_G: 0.8053 acc: 56.2%\n",
      "[BATCH 44/149] Loss_D: 0.8325 Loss_G: 0.8207 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.8200 Loss_G: 0.8254 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7929 Loss_G: 0.8222 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.7689 Loss_G: 0.8118 acc: 71.9%\n",
      "[BATCH 48/149] Loss_D: 0.8016 Loss_G: 0.8071 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.7901 Loss_G: 0.8062 acc: 70.3%\n",
      "[BATCH 50/149] Loss_D: 0.7659 Loss_G: 0.8020 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.8277 Loss_G: 0.8074 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.7574 Loss_G: 0.8008 acc: 76.6%\n",
      "[BATCH 53/149] Loss_D: 0.7708 Loss_G: 0.7988 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.7910 Loss_G: 0.8061 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.8434 Loss_G: 0.8214 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7918 Loss_G: 0.8092 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.8103 Loss_G: 0.8126 acc: 67.2%\n",
      "[BATCH 58/149] Loss_D: 0.7666 Loss_G: 0.7971 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.8323 Loss_G: 0.8054 acc: 60.9%\n",
      "[BATCH 60/149] Loss_D: 0.8116 Loss_G: 0.8129 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.7887 Loss_G: 0.8127 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.7765 Loss_G: 0.8035 acc: 60.9%\n",
      "[EPOCH 1850] TEST ACC is : 74.0%\n",
      "[BATCH 63/149] Loss_D: 0.8283 Loss_G: 0.8115 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.8355 Loss_G: 0.8203 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7909 Loss_G: 0.8275 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.8239 Loss_G: 0.8378 acc: 67.2%\n",
      "[BATCH 67/149] Loss_D: 0.7679 Loss_G: 0.8273 acc: 62.5%\n",
      "[BATCH 68/149] Loss_D: 0.8056 Loss_G: 0.8258 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.8463 Loss_G: 0.8397 acc: 54.7%\n",
      "[BATCH 70/149] Loss_D: 0.7799 Loss_G: 0.8240 acc: 81.2%\n",
      "[BATCH 71/149] Loss_D: 0.7726 Loss_G: 0.8120 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.7821 Loss_G: 0.8021 acc: 70.3%\n",
      "[BATCH 73/149] Loss_D: 0.8118 Loss_G: 0.8014 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.8220 Loss_G: 0.8151 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.8126 Loss_G: 0.8261 acc: 71.9%\n",
      "[BATCH 76/149] Loss_D: 0.7678 Loss_G: 0.8026 acc: 62.5%\n",
      "[BATCH 77/149] Loss_D: 0.7937 Loss_G: 0.8124 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7644 Loss_G: 0.7934 acc: 59.4%\n",
      "[BATCH 79/149] Loss_D: 0.7559 Loss_G: 0.7827 acc: 76.6%\n",
      "[BATCH 80/149] Loss_D: 0.7647 Loss_G: 0.7853 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7953 Loss_G: 0.7926 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.7767 Loss_G: 0.7989 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7567 Loss_G: 0.7971 acc: 57.8%\n",
      "[BATCH 84/149] Loss_D: 0.8032 Loss_G: 0.7976 acc: 64.1%\n",
      "[BATCH 85/149] Loss_D: 0.7819 Loss_G: 0.8086 acc: 54.7%\n",
      "[BATCH 86/149] Loss_D: 0.8995 Loss_G: 0.8307 acc: 56.2%\n",
      "[BATCH 87/149] Loss_D: 0.8014 Loss_G: 0.8332 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7646 Loss_G: 0.8194 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.8376 Loss_G: 0.8244 acc: 75.0%\n",
      "[BATCH 90/149] Loss_D: 0.7751 Loss_G: 0.8210 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.7687 Loss_G: 0.8110 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7866 Loss_G: 0.8102 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.7990 Loss_G: 0.8171 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.7796 Loss_G: 0.8135 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7754 Loss_G: 0.7924 acc: 54.7%\n",
      "[BATCH 96/149] Loss_D: 0.7911 Loss_G: 0.7882 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.7708 Loss_G: 0.7914 acc: 57.8%\n",
      "[BATCH 98/149] Loss_D: 0.8115 Loss_G: 0.8100 acc: 78.1%\n",
      "[BATCH 99/149] Loss_D: 0.7986 Loss_G: 0.8210 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.8031 Loss_G: 0.8277 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.8516 Loss_G: 0.8367 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.8035 Loss_G: 0.8321 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7978 Loss_G: 0.8144 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.7969 Loss_G: 0.8115 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.7940 Loss_G: 0.8132 acc: 70.3%\n",
      "[BATCH 106/149] Loss_D: 0.7785 Loss_G: 0.8019 acc: 70.3%\n",
      "[BATCH 107/149] Loss_D: 0.8059 Loss_G: 0.8098 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7652 Loss_G: 0.8131 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.7866 Loss_G: 0.8102 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.8259 Loss_G: 0.8196 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.8223 Loss_G: 0.8293 acc: 65.6%\n",
      "[BATCH 112/149] Loss_D: 0.7717 Loss_G: 0.8170 acc: 68.8%\n",
      "[EPOCH 1900] TEST ACC is : 76.0%\n",
      "[BATCH 113/149] Loss_D: 0.7572 Loss_G: 0.8057 acc: 78.1%\n",
      "[BATCH 114/149] Loss_D: 0.7787 Loss_G: 0.8076 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.7738 Loss_G: 0.8078 acc: 59.4%\n",
      "[BATCH 116/149] Loss_D: 0.7991 Loss_G: 0.8092 acc: 71.9%\n",
      "[BATCH 117/149] Loss_D: 0.8118 Loss_G: 0.8185 acc: 60.9%\n",
      "[BATCH 118/149] Loss_D: 0.7759 Loss_G: 0.8080 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.7975 Loss_G: 0.8020 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.8038 Loss_G: 0.8085 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.7769 Loss_G: 0.8085 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.8203 Loss_G: 0.8133 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.8274 Loss_G: 0.8237 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.7681 Loss_G: 0.8341 acc: 73.4%\n",
      "[BATCH 125/149] Loss_D: 0.8123 Loss_G: 0.8268 acc: 59.4%\n",
      "[BATCH 126/149] Loss_D: 0.8195 Loss_G: 0.8181 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.8065 Loss_G: 0.8124 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.7634 Loss_G: 0.8103 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.7584 Loss_G: 0.7960 acc: 67.2%\n",
      "[BATCH 130/149] Loss_D: 0.7727 Loss_G: 0.7869 acc: 62.5%\n",
      "[BATCH 131/149] Loss_D: 0.7335 Loss_G: 0.7827 acc: 71.9%\n",
      "[BATCH 132/149] Loss_D: 0.8306 Loss_G: 0.8017 acc: 54.7%\n",
      "[BATCH 133/149] Loss_D: 0.8190 Loss_G: 0.8226 acc: 57.8%\n",
      "[BATCH 134/149] Loss_D: 0.7661 Loss_G: 0.7964 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.8150 Loss_G: 0.7988 acc: 67.2%\n",
      "[BATCH 136/149] Loss_D: 0.8097 Loss_G: 0.8054 acc: 54.7%\n",
      "[BATCH 137/149] Loss_D: 0.7873 Loss_G: 0.8054 acc: 71.9%\n",
      "[BATCH 138/149] Loss_D: 0.7739 Loss_G: 0.8062 acc: 67.2%\n",
      "[BATCH 139/149] Loss_D: 0.7581 Loss_G: 0.7966 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.8154 Loss_G: 0.8094 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7568 Loss_G: 0.8105 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7546 Loss_G: 0.7990 acc: 65.6%\n",
      "[BATCH 143/149] Loss_D: 0.7677 Loss_G: 0.7880 acc: 71.9%\n",
      "[BATCH 144/149] Loss_D: 0.7667 Loss_G: 0.7973 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.8048 Loss_G: 0.8099 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.7666 Loss_G: 0.8303 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.7881 Loss_G: 0.8141 acc: 59.4%\n",
      "[BATCH 148/149] Loss_D: 0.7682 Loss_G: 0.8072 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7837 Loss_G: 0.8007 acc: 68.8%\n",
      "-----THE [13/50] epoch end-----\n",
      "-----THE [14/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8043 Loss_G: 0.8035 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.7944 Loss_G: 0.8071 acc: 62.5%\n",
      "[BATCH 3/149] Loss_D: 0.7868 Loss_G: 0.8039 acc: 65.6%\n",
      "[BATCH 4/149] Loss_D: 0.8151 Loss_G: 0.8119 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7977 Loss_G: 0.8167 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.7742 Loss_G: 0.8121 acc: 75.0%\n",
      "[BATCH 7/149] Loss_D: 0.7743 Loss_G: 0.8039 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.7868 Loss_G: 0.7988 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.7548 Loss_G: 0.7897 acc: 76.6%\n",
      "[BATCH 10/149] Loss_D: 0.7710 Loss_G: 0.7893 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.7994 Loss_G: 0.7973 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.8708 Loss_G: 0.8391 acc: 56.2%\n",
      "[BATCH 13/149] Loss_D: 0.7851 Loss_G: 0.8095 acc: 65.6%\n",
      "[EPOCH 1950] TEST ACC is : 74.8%\n",
      "[BATCH 14/149] Loss_D: 0.8030 Loss_G: 0.8071 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.7961 Loss_G: 0.8062 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.7783 Loss_G: 0.8050 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.7963 Loss_G: 0.8030 acc: 70.3%\n",
      "[BATCH 18/149] Loss_D: 0.8579 Loss_G: 0.8137 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.7970 Loss_G: 0.8099 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.7720 Loss_G: 0.8059 acc: 75.0%\n",
      "[BATCH 21/149] Loss_D: 0.7723 Loss_G: 0.8054 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.8309 Loss_G: 0.8077 acc: 59.4%\n",
      "[BATCH 23/149] Loss_D: 0.7764 Loss_G: 0.8109 acc: 60.9%\n",
      "[BATCH 24/149] Loss_D: 0.7669 Loss_G: 0.8009 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.7713 Loss_G: 0.7998 acc: 59.4%\n",
      "[BATCH 26/149] Loss_D: 0.7966 Loss_G: 0.8097 acc: 73.4%\n",
      "[BATCH 27/149] Loss_D: 0.7771 Loss_G: 0.8059 acc: 57.8%\n",
      "[BATCH 28/149] Loss_D: 0.7802 Loss_G: 0.8014 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.7755 Loss_G: 0.7982 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.8320 Loss_G: 0.8179 acc: 75.0%\n",
      "[BATCH 31/149] Loss_D: 0.7999 Loss_G: 0.8250 acc: 57.8%\n",
      "[BATCH 32/149] Loss_D: 0.8054 Loss_G: 0.8259 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.8374 Loss_G: 0.8209 acc: 56.2%\n",
      "[BATCH 34/149] Loss_D: 0.7732 Loss_G: 0.8071 acc: 56.2%\n",
      "[BATCH 35/149] Loss_D: 0.7849 Loss_G: 0.8005 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.7586 Loss_G: 0.8030 acc: 68.8%\n",
      "[BATCH 37/149] Loss_D: 0.8121 Loss_G: 0.8085 acc: 73.4%\n",
      "[BATCH 38/149] Loss_D: 0.8016 Loss_G: 0.8194 acc: 67.2%\n",
      "[BATCH 39/149] Loss_D: 0.7975 Loss_G: 0.8224 acc: 68.8%\n",
      "[BATCH 40/149] Loss_D: 0.7978 Loss_G: 0.8243 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.7703 Loss_G: 0.8197 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7965 Loss_G: 0.8195 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.8463 Loss_G: 0.8356 acc: 65.6%\n",
      "[BATCH 44/149] Loss_D: 0.8507 Loss_G: 0.8462 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7475 Loss_G: 0.8114 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.7821 Loss_G: 0.7991 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.7413 Loss_G: 0.7983 acc: 71.9%\n",
      "[BATCH 48/149] Loss_D: 0.7933 Loss_G: 0.8113 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.7986 Loss_G: 0.8163 acc: 64.1%\n",
      "[BATCH 50/149] Loss_D: 0.7676 Loss_G: 0.8053 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7781 Loss_G: 0.8088 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.7881 Loss_G: 0.8122 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7569 Loss_G: 0.8016 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.8300 Loss_G: 0.8136 acc: 59.4%\n",
      "[BATCH 55/149] Loss_D: 0.7872 Loss_G: 0.8111 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.7536 Loss_G: 0.8028 acc: 73.4%\n",
      "[BATCH 57/149] Loss_D: 0.7413 Loss_G: 0.7891 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.7985 Loss_G: 0.7958 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.7969 Loss_G: 0.8099 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7923 Loss_G: 0.8129 acc: 73.4%\n",
      "[BATCH 61/149] Loss_D: 0.8078 Loss_G: 0.8149 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.7682 Loss_G: 0.7999 acc: 57.8%\n",
      "[BATCH 63/149] Loss_D: 0.8512 Loss_G: 0.8259 acc: 50.0%\n",
      "[EPOCH 2000] TEST ACC is : 74.4%\n",
      "[BATCH 64/149] Loss_D: 0.8211 Loss_G: 0.8221 acc: 67.2%\n",
      "[BATCH 65/149] Loss_D: 0.8025 Loss_G: 0.8099 acc: 68.8%\n",
      "[BATCH 66/149] Loss_D: 0.7599 Loss_G: 0.7959 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7661 Loss_G: 0.7943 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.7732 Loss_G: 0.7968 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.7962 Loss_G: 0.8103 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.7926 Loss_G: 0.8243 acc: 75.0%\n",
      "[BATCH 71/149] Loss_D: 0.7749 Loss_G: 0.8275 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.7542 Loss_G: 0.8096 acc: 70.3%\n",
      "[BATCH 73/149] Loss_D: 0.8208 Loss_G: 0.8218 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.7650 Loss_G: 0.8340 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.8144 Loss_G: 0.8261 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.7933 Loss_G: 0.8179 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.7803 Loss_G: 0.8026 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7882 Loss_G: 0.8017 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.8119 Loss_G: 0.8086 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.8501 Loss_G: 0.8170 acc: 70.3%\n",
      "[BATCH 81/149] Loss_D: 0.8340 Loss_G: 0.8307 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.7875 Loss_G: 0.8175 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.7922 Loss_G: 0.8175 acc: 73.4%\n",
      "[BATCH 84/149] Loss_D: 0.8061 Loss_G: 0.8209 acc: 64.1%\n",
      "[BATCH 85/149] Loss_D: 0.8115 Loss_G: 0.8303 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7564 Loss_G: 0.8109 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.8323 Loss_G: 0.8259 acc: 73.4%\n",
      "[BATCH 88/149] Loss_D: 0.7775 Loss_G: 0.8288 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.7657 Loss_G: 0.8016 acc: 59.4%\n",
      "[BATCH 90/149] Loss_D: 0.7749 Loss_G: 0.7962 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.8780 Loss_G: 0.8158 acc: 56.2%\n",
      "[BATCH 92/149] Loss_D: 0.8415 Loss_G: 0.8247 acc: 53.1%\n",
      "[BATCH 93/149] Loss_D: 0.7643 Loss_G: 0.8139 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.7451 Loss_G: 0.7982 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7744 Loss_G: 0.7926 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.7983 Loss_G: 0.7928 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.8008 Loss_G: 0.8007 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.8147 Loss_G: 0.8089 acc: 71.9%\n",
      "[BATCH 99/149] Loss_D: 0.7815 Loss_G: 0.8085 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.7967 Loss_G: 0.8104 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.7926 Loss_G: 0.8194 acc: 73.4%\n",
      "[BATCH 102/149] Loss_D: 0.7475 Loss_G: 0.8168 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.7602 Loss_G: 0.8095 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.8413 Loss_G: 0.8270 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.8228 Loss_G: 0.8170 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7657 Loss_G: 0.8087 acc: 54.7%\n",
      "[BATCH 107/149] Loss_D: 0.8213 Loss_G: 0.8065 acc: 73.4%\n",
      "[BATCH 108/149] Loss_D: 0.8287 Loss_G: 0.8113 acc: 62.5%\n",
      "[BATCH 109/149] Loss_D: 0.7926 Loss_G: 0.8153 acc: 75.0%\n",
      "[BATCH 110/149] Loss_D: 0.8051 Loss_G: 0.8194 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7619 Loss_G: 0.8063 acc: 48.4%\n",
      "[BATCH 112/149] Loss_D: 0.7787 Loss_G: 0.8025 acc: 75.0%\n",
      "[BATCH 113/149] Loss_D: 0.8021 Loss_G: 0.8072 acc: 60.9%\n",
      "[EPOCH 2050] TEST ACC is : 75.0%\n",
      "[BATCH 114/149] Loss_D: 0.8030 Loss_G: 0.8239 acc: 60.9%\n",
      "[BATCH 115/149] Loss_D: 0.8463 Loss_G: 0.8322 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.8142 Loss_G: 0.8207 acc: 62.5%\n",
      "[BATCH 117/149] Loss_D: 0.7505 Loss_G: 0.8106 acc: 71.9%\n",
      "[BATCH 118/149] Loss_D: 0.7496 Loss_G: 0.8008 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.8787 Loss_G: 0.8157 acc: 70.3%\n",
      "[BATCH 120/149] Loss_D: 0.8001 Loss_G: 0.8202 acc: 71.9%\n",
      "[BATCH 121/149] Loss_D: 0.7997 Loss_G: 0.8106 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.7877 Loss_G: 0.7988 acc: 51.6%\n",
      "[BATCH 123/149] Loss_D: 0.7653 Loss_G: 0.7926 acc: 75.0%\n",
      "[BATCH 124/149] Loss_D: 0.8163 Loss_G: 0.8036 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.7763 Loss_G: 0.8100 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.8289 Loss_G: 0.8259 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7889 Loss_G: 0.8208 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.7686 Loss_G: 0.8058 acc: 57.8%\n",
      "[BATCH 129/149] Loss_D: 0.7934 Loss_G: 0.8028 acc: 62.5%\n",
      "[BATCH 130/149] Loss_D: 0.7510 Loss_G: 0.7954 acc: 54.7%\n",
      "[BATCH 131/149] Loss_D: 0.7830 Loss_G: 0.7937 acc: 57.8%\n",
      "[BATCH 132/149] Loss_D: 0.7565 Loss_G: 0.7923 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.7912 Loss_G: 0.7968 acc: 53.1%\n",
      "[BATCH 134/149] Loss_D: 0.7689 Loss_G: 0.7975 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.7550 Loss_G: 0.7913 acc: 46.9%\n",
      "[BATCH 136/149] Loss_D: 0.7925 Loss_G: 0.7986 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.7977 Loss_G: 0.8042 acc: 54.7%\n",
      "[BATCH 138/149] Loss_D: 0.7882 Loss_G: 0.8115 acc: 64.1%\n",
      "[BATCH 139/149] Loss_D: 0.7995 Loss_G: 0.8124 acc: 70.3%\n",
      "[BATCH 140/149] Loss_D: 0.8172 Loss_G: 0.8209 acc: 65.6%\n",
      "[BATCH 141/149] Loss_D: 0.7896 Loss_G: 0.8176 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.7603 Loss_G: 0.8035 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.7678 Loss_G: 0.8059 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7733 Loss_G: 0.8007 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.7639 Loss_G: 0.7997 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7906 Loss_G: 0.8059 acc: 76.6%\n",
      "[BATCH 147/149] Loss_D: 0.7577 Loss_G: 0.8005 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.7979 Loss_G: 0.8027 acc: 71.9%\n",
      "[BATCH 149/149] Loss_D: 0.7924 Loss_G: 0.8062 acc: 60.9%\n",
      "-----THE [14/50] epoch end-----\n",
      "-----THE [15/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7891 Loss_G: 0.8103 acc: 56.2%\n",
      "[BATCH 2/149] Loss_D: 0.7839 Loss_G: 0.8036 acc: 59.4%\n",
      "[BATCH 3/149] Loss_D: 0.8070 Loss_G: 0.8106 acc: 50.0%\n",
      "[BATCH 4/149] Loss_D: 0.8061 Loss_G: 0.8189 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.7657 Loss_G: 0.8060 acc: 78.1%\n",
      "[BATCH 6/149] Loss_D: 0.7585 Loss_G: 0.7961 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.8268 Loss_G: 0.8085 acc: 57.8%\n",
      "[BATCH 8/149] Loss_D: 0.7626 Loss_G: 0.8060 acc: 62.5%\n",
      "[BATCH 9/149] Loss_D: 0.7799 Loss_G: 0.8036 acc: 54.7%\n",
      "[BATCH 10/149] Loss_D: 0.7944 Loss_G: 0.8046 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.8154 Loss_G: 0.8111 acc: 57.8%\n",
      "[BATCH 12/149] Loss_D: 0.7928 Loss_G: 0.8100 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7999 Loss_G: 0.8060 acc: 73.4%\n",
      "[BATCH 14/149] Loss_D: 0.8310 Loss_G: 0.8191 acc: 73.4%\n",
      "[EPOCH 2100] TEST ACC is : 74.2%\n",
      "[BATCH 15/149] Loss_D: 0.7697 Loss_G: 0.8190 acc: 73.4%\n",
      "[BATCH 16/149] Loss_D: 0.7934 Loss_G: 0.8235 acc: 65.6%\n",
      "[BATCH 17/149] Loss_D: 0.7936 Loss_G: 0.8094 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.9089 Loss_G: 0.8656 acc: 71.9%\n",
      "[BATCH 19/149] Loss_D: 0.8144 Loss_G: 0.8437 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.7500 Loss_G: 0.8185 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8298 Loss_G: 0.8102 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.8012 Loss_G: 0.8188 acc: 73.4%\n",
      "[BATCH 23/149] Loss_D: 0.7644 Loss_G: 0.8133 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7724 Loss_G: 0.8091 acc: 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.7837 Loss_G: 0.8069 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.8116 Loss_G: 0.8226 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.7864 Loss_G: 0.8126 acc: 76.6%\n",
      "[BATCH 28/149] Loss_D: 0.7911 Loss_G: 0.8076 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.7837 Loss_G: 0.8178 acc: 62.5%\n",
      "[BATCH 30/149] Loss_D: 0.7295 Loss_G: 0.8006 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7803 Loss_G: 0.7973 acc: 71.9%\n",
      "[BATCH 32/149] Loss_D: 0.7670 Loss_G: 0.7992 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.8385 Loss_G: 0.8148 acc: 53.1%\n",
      "[BATCH 34/149] Loss_D: 0.7791 Loss_G: 0.8174 acc: 75.0%\n",
      "[BATCH 35/149] Loss_D: 0.7858 Loss_G: 0.8093 acc: 57.8%\n",
      "[BATCH 36/149] Loss_D: 0.8283 Loss_G: 0.8142 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.7737 Loss_G: 0.8114 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7863 Loss_G: 0.8057 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.8157 Loss_G: 0.8166 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.7777 Loss_G: 0.8073 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.7692 Loss_G: 0.8056 acc: 71.9%\n",
      "[BATCH 42/149] Loss_D: 0.8378 Loss_G: 0.8176 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.7597 Loss_G: 0.8195 acc: 56.2%\n",
      "[BATCH 44/149] Loss_D: 0.7609 Loss_G: 0.7991 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7968 Loss_G: 0.7965 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.7937 Loss_G: 0.8034 acc: 59.4%\n",
      "[BATCH 47/149] Loss_D: 0.7761 Loss_G: 0.8054 acc: 76.6%\n",
      "[BATCH 48/149] Loss_D: 0.7731 Loss_G: 0.8038 acc: 71.9%\n",
      "[BATCH 49/149] Loss_D: 0.8306 Loss_G: 0.8170 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7867 Loss_G: 0.8065 acc: 65.6%\n",
      "[BATCH 51/149] Loss_D: 0.7567 Loss_G: 0.8004 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7787 Loss_G: 0.7979 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.7994 Loss_G: 0.8115 acc: 54.7%\n",
      "[BATCH 54/149] Loss_D: 0.7875 Loss_G: 0.8070 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.7623 Loss_G: 0.8122 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7873 Loss_G: 0.7992 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7622 Loss_G: 0.7966 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.7815 Loss_G: 0.8001 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.8168 Loss_G: 0.8119 acc: 64.1%\n",
      "[BATCH 60/149] Loss_D: 0.7604 Loss_G: 0.8076 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.8113 Loss_G: 0.8184 acc: 60.9%\n",
      "[BATCH 62/149] Loss_D: 0.7745 Loss_G: 0.8088 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.7853 Loss_G: 0.8114 acc: 56.2%\n",
      "[BATCH 64/149] Loss_D: 0.7917 Loss_G: 0.8175 acc: 65.6%\n",
      "[EPOCH 2150] TEST ACC is : 74.6%\n",
      "[BATCH 65/149] Loss_D: 0.7682 Loss_G: 0.8131 acc: 68.8%\n",
      "[BATCH 66/149] Loss_D: 0.7992 Loss_G: 0.8085 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.7933 Loss_G: 0.8117 acc: 64.1%\n",
      "[BATCH 68/149] Loss_D: 0.7638 Loss_G: 0.8034 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.8368 Loss_G: 0.8180 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.7993 Loss_G: 0.8232 acc: 70.3%\n",
      "[BATCH 71/149] Loss_D: 0.7607 Loss_G: 0.8108 acc: 75.0%\n",
      "[BATCH 72/149] Loss_D: 0.7887 Loss_G: 0.8040 acc: 70.3%\n",
      "[BATCH 73/149] Loss_D: 0.7749 Loss_G: 0.8009 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.8043 Loss_G: 0.8207 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.7680 Loss_G: 0.8022 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.8029 Loss_G: 0.8000 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.8008 Loss_G: 0.7950 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7544 Loss_G: 0.7915 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.8097 Loss_G: 0.8033 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.7774 Loss_G: 0.8017 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.7242 Loss_G: 0.7915 acc: 57.8%\n",
      "[BATCH 82/149] Loss_D: 0.8024 Loss_G: 0.7930 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.7719 Loss_G: 0.8006 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.8062 Loss_G: 0.8044 acc: 59.4%\n",
      "[BATCH 85/149] Loss_D: 0.7725 Loss_G: 0.7976 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.8023 Loss_G: 0.8019 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.8378 Loss_G: 0.8073 acc: 67.2%\n",
      "[BATCH 88/149] Loss_D: 0.8008 Loss_G: 0.8102 acc: 75.0%\n",
      "[BATCH 89/149] Loss_D: 0.8398 Loss_G: 0.8117 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.7469 Loss_G: 0.8063 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.7895 Loss_G: 0.8015 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7848 Loss_G: 0.8073 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.7989 Loss_G: 0.8161 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.8096 Loss_G: 0.8183 acc: 57.8%\n",
      "[BATCH 95/149] Loss_D: 0.7996 Loss_G: 0.8185 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.7601 Loss_G: 0.8072 acc: 67.2%\n",
      "[BATCH 97/149] Loss_D: 0.8094 Loss_G: 0.8115 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.7910 Loss_G: 0.8156 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7902 Loss_G: 0.8219 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.7887 Loss_G: 0.8234 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.8077 Loss_G: 0.8053 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.8347 Loss_G: 0.8171 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.7788 Loss_G: 0.8093 acc: 70.3%\n",
      "[BATCH 104/149] Loss_D: 0.8275 Loss_G: 0.8135 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7672 Loss_G: 0.8099 acc: 62.5%\n",
      "[BATCH 106/149] Loss_D: 0.7677 Loss_G: 0.7973 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.7782 Loss_G: 0.8016 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7857 Loss_G: 0.7994 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.8090 Loss_G: 0.8124 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7782 Loss_G: 0.8236 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.7886 Loss_G: 0.8246 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.8059 Loss_G: 0.8279 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.7831 Loss_G: 0.8116 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7836 Loss_G: 0.8085 acc: 67.2%\n",
      "[EPOCH 2200] TEST ACC is : 75.8%\n",
      "[BATCH 115/149] Loss_D: 0.8236 Loss_G: 0.8085 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7802 Loss_G: 0.8042 acc: 68.8%\n",
      "[BATCH 117/149] Loss_D: 0.7567 Loss_G: 0.7994 acc: 67.2%\n",
      "[BATCH 118/149] Loss_D: 0.8119 Loss_G: 0.8036 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.8417 Loss_G: 0.8171 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.8089 Loss_G: 0.8239 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.7728 Loss_G: 0.8149 acc: 76.6%\n",
      "[BATCH 122/149] Loss_D: 0.7968 Loss_G: 0.8081 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.8281 Loss_G: 0.8141 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.7953 Loss_G: 0.8256 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.7773 Loss_G: 0.8140 acc: 50.0%\n",
      "[BATCH 126/149] Loss_D: 0.8225 Loss_G: 0.8254 acc: 75.0%\n",
      "[BATCH 127/149] Loss_D: 0.8046 Loss_G: 0.8211 acc: 54.7%\n",
      "[BATCH 128/149] Loss_D: 0.7540 Loss_G: 0.8158 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.7800 Loss_G: 0.8070 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7841 Loss_G: 0.8037 acc: 65.6%\n",
      "[BATCH 131/149] Loss_D: 0.8188 Loss_G: 0.8092 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.7604 Loss_G: 0.8059 acc: 70.3%\n",
      "[BATCH 133/149] Loss_D: 0.7874 Loss_G: 0.8027 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.7782 Loss_G: 0.8004 acc: 70.3%\n",
      "[BATCH 135/149] Loss_D: 0.7866 Loss_G: 0.8078 acc: 70.3%\n",
      "[BATCH 136/149] Loss_D: 0.7791 Loss_G: 0.7973 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.7682 Loss_G: 0.7973 acc: 70.3%\n",
      "[BATCH 138/149] Loss_D: 0.7919 Loss_G: 0.7961 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.7692 Loss_G: 0.7906 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7861 Loss_G: 0.7880 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7462 Loss_G: 0.7863 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8259 Loss_G: 0.7965 acc: 56.2%\n",
      "[BATCH 143/149] Loss_D: 0.7608 Loss_G: 0.8043 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.8238 Loss_G: 0.8115 acc: 62.5%\n",
      "[BATCH 145/149] Loss_D: 0.8416 Loss_G: 0.8297 acc: 70.3%\n",
      "[BATCH 146/149] Loss_D: 0.7908 Loss_G: 0.8153 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.8282 Loss_G: 0.8144 acc: 71.9%\n",
      "[BATCH 148/149] Loss_D: 0.8530 Loss_G: 0.8142 acc: 73.4%\n",
      "[BATCH 149/149] Loss_D: 0.7918 Loss_G: 0.8304 acc: 78.1%\n",
      "-----THE [15/50] epoch end-----\n",
      "-----THE [16/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7870 Loss_G: 0.8303 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.7951 Loss_G: 0.8186 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7509 Loss_G: 0.8075 acc: 70.3%\n",
      "[BATCH 4/149] Loss_D: 0.8218 Loss_G: 0.8092 acc: 56.2%\n",
      "[BATCH 5/149] Loss_D: 0.7857 Loss_G: 0.8130 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7859 Loss_G: 0.8167 acc: 62.5%\n",
      "[BATCH 7/149] Loss_D: 0.8011 Loss_G: 0.8103 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.7651 Loss_G: 0.8027 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.7924 Loss_G: 0.7990 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7951 Loss_G: 0.8114 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.7610 Loss_G: 0.8027 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.8505 Loss_G: 0.8247 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.7729 Loss_G: 0.8210 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.7878 Loss_G: 0.8097 acc: 54.7%\n",
      "[BATCH 15/149] Loss_D: 0.8142 Loss_G: 0.8174 acc: 64.1%\n",
      "[EPOCH 2250] TEST ACC is : 76.6%\n",
      "[BATCH 16/149] Loss_D: 0.8176 Loss_G: 0.8246 acc: 70.3%\n",
      "[BATCH 17/149] Loss_D: 0.7823 Loss_G: 0.8195 acc: 73.4%\n",
      "[BATCH 18/149] Loss_D: 0.7564 Loss_G: 0.8124 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.8193 Loss_G: 0.8089 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.8142 Loss_G: 0.8163 acc: 76.6%\n",
      "[BATCH 21/149] Loss_D: 0.7643 Loss_G: 0.8159 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.7973 Loss_G: 0.8042 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.8091 Loss_G: 0.8056 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.7786 Loss_G: 0.8066 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.7565 Loss_G: 0.8015 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.7782 Loss_G: 0.7977 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.8094 Loss_G: 0.8126 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.7983 Loss_G: 0.8236 acc: 54.7%\n",
      "[BATCH 29/149] Loss_D: 0.7605 Loss_G: 0.8031 acc: 76.6%\n",
      "[BATCH 30/149] Loss_D: 0.8062 Loss_G: 0.7998 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.7709 Loss_G: 0.8056 acc: 71.9%\n",
      "[BATCH 32/149] Loss_D: 0.7610 Loss_G: 0.8009 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.7894 Loss_G: 0.8019 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8384 Loss_G: 0.8122 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7932 Loss_G: 0.8088 acc: 56.2%\n",
      "[BATCH 36/149] Loss_D: 0.7770 Loss_G: 0.8192 acc: 57.8%\n",
      "[BATCH 37/149] Loss_D: 0.7746 Loss_G: 0.7996 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7552 Loss_G: 0.7925 acc: 73.4%\n",
      "[BATCH 39/149] Loss_D: 0.7875 Loss_G: 0.7886 acc: 75.0%\n",
      "[BATCH 40/149] Loss_D: 0.7632 Loss_G: 0.7909 acc: 71.9%\n",
      "[BATCH 41/149] Loss_D: 0.7604 Loss_G: 0.7932 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.7357 Loss_G: 0.7839 acc: 60.9%\n",
      "[BATCH 43/149] Loss_D: 0.8741 Loss_G: 0.8251 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.7983 Loss_G: 0.8314 acc: 62.5%\n",
      "[BATCH 45/149] Loss_D: 0.8242 Loss_G: 0.8369 acc: 76.6%\n",
      "[BATCH 46/149] Loss_D: 0.8164 Loss_G: 0.8284 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.7668 Loss_G: 0.8255 acc: 57.8%\n",
      "[BATCH 48/149] Loss_D: 0.7965 Loss_G: 0.8177 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7787 Loss_G: 0.8089 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.8244 Loss_G: 0.8163 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.8169 Loss_G: 0.8202 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.7496 Loss_G: 0.8048 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.8070 Loss_G: 0.8031 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.8184 Loss_G: 0.8202 acc: 57.8%\n",
      "[BATCH 55/149] Loss_D: 0.7601 Loss_G: 0.8097 acc: 57.8%\n",
      "[BATCH 56/149] Loss_D: 0.7357 Loss_G: 0.7965 acc: 75.0%\n",
      "[BATCH 57/149] Loss_D: 0.8157 Loss_G: 0.8063 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.7489 Loss_G: 0.7973 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.8149 Loss_G: 0.8007 acc: 71.9%\n",
      "[BATCH 60/149] Loss_D: 0.7713 Loss_G: 0.7986 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7529 Loss_G: 0.7908 acc: 60.9%\n",
      "[BATCH 62/149] Loss_D: 0.7817 Loss_G: 0.7915 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.8270 Loss_G: 0.8001 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.8069 Loss_G: 0.8124 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7726 Loss_G: 0.8021 acc: 64.1%\n",
      "[EPOCH 2300] TEST ACC is : 76.2%\n",
      "[BATCH 66/149] Loss_D: 0.7904 Loss_G: 0.8047 acc: 76.6%\n",
      "[BATCH 67/149] Loss_D: 0.7748 Loss_G: 0.8045 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.7605 Loss_G: 0.8096 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.7827 Loss_G: 0.8066 acc: 57.8%\n",
      "[BATCH 70/149] Loss_D: 0.7839 Loss_G: 0.8101 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.8254 Loss_G: 0.8082 acc: 71.9%\n",
      "[BATCH 72/149] Loss_D: 0.8040 Loss_G: 0.8153 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7920 Loss_G: 0.8203 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7873 Loss_G: 0.8193 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.8352 Loss_G: 0.8196 acc: 62.5%\n",
      "[BATCH 76/149] Loss_D: 0.8029 Loss_G: 0.8204 acc: 51.6%\n",
      "[BATCH 77/149] Loss_D: 0.8249 Loss_G: 0.8174 acc: 59.4%\n",
      "[BATCH 78/149] Loss_D: 0.7608 Loss_G: 0.8129 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.8025 Loss_G: 0.8121 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7543 Loss_G: 0.8062 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.8290 Loss_G: 0.8190 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.7547 Loss_G: 0.8066 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7937 Loss_G: 0.8038 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.7816 Loss_G: 0.8013 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.8273 Loss_G: 0.8133 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7718 Loss_G: 0.8035 acc: 71.9%\n",
      "[BATCH 87/149] Loss_D: 0.7684 Loss_G: 0.7997 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7938 Loss_G: 0.8034 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.7997 Loss_G: 0.7966 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.8522 Loss_G: 0.8152 acc: 54.7%\n",
      "[BATCH 91/149] Loss_D: 0.7542 Loss_G: 0.8109 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.7898 Loss_G: 0.8034 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.7811 Loss_G: 0.8078 acc: 53.1%\n",
      "[BATCH 94/149] Loss_D: 0.7847 Loss_G: 0.8071 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.8380 Loss_G: 0.8083 acc: 73.4%\n",
      "[BATCH 96/149] Loss_D: 0.7972 Loss_G: 0.8123 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.8543 Loss_G: 0.8191 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.7346 Loss_G: 0.8048 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.8038 Loss_G: 0.8033 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.7919 Loss_G: 0.8136 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.8063 Loss_G: 0.8056 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.8724 Loss_G: 0.8346 acc: 62.5%\n",
      "[BATCH 103/149] Loss_D: 0.7890 Loss_G: 0.8195 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.8452 Loss_G: 0.8223 acc: 71.9%\n",
      "[BATCH 105/149] Loss_D: 0.7540 Loss_G: 0.8085 acc: 65.6%\n",
      "[BATCH 106/149] Loss_D: 0.7553 Loss_G: 0.8000 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7860 Loss_G: 0.7976 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7561 Loss_G: 0.7948 acc: 60.9%\n",
      "[BATCH 109/149] Loss_D: 0.7819 Loss_G: 0.8031 acc: 70.3%\n",
      "[BATCH 110/149] Loss_D: 0.8504 Loss_G: 0.8241 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.7662 Loss_G: 0.8250 acc: 48.4%\n",
      "[BATCH 112/149] Loss_D: 0.7733 Loss_G: 0.8210 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.8110 Loss_G: 0.8138 acc: 51.6%\n",
      "[BATCH 114/149] Loss_D: 0.7990 Loss_G: 0.8376 acc: 71.9%\n",
      "[BATCH 115/149] Loss_D: 0.7749 Loss_G: 0.8172 acc: 60.9%\n",
      "[EPOCH 2350] TEST ACC is : 75.2%\n",
      "[BATCH 116/149] Loss_D: 0.8093 Loss_G: 0.8067 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7702 Loss_G: 0.8073 acc: 67.2%\n",
      "[BATCH 118/149] Loss_D: 0.7965 Loss_G: 0.8090 acc: 51.6%\n",
      "[BATCH 119/149] Loss_D: 0.7950 Loss_G: 0.8165 acc: 59.4%\n",
      "[BATCH 120/149] Loss_D: 0.7466 Loss_G: 0.8052 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.7737 Loss_G: 0.7962 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.7530 Loss_G: 0.7928 acc: 62.5%\n",
      "[BATCH 123/149] Loss_D: 0.8238 Loss_G: 0.8138 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.8026 Loss_G: 0.8093 acc: 53.1%\n",
      "[BATCH 125/149] Loss_D: 0.8001 Loss_G: 0.8002 acc: 60.9%\n",
      "[BATCH 126/149] Loss_D: 0.7641 Loss_G: 0.7979 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7858 Loss_G: 0.7931 acc: 60.9%\n",
      "[BATCH 128/149] Loss_D: 0.8192 Loss_G: 0.8155 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.7271 Loss_G: 0.7930 acc: 73.4%\n",
      "[BATCH 130/149] Loss_D: 0.8265 Loss_G: 0.7991 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.7838 Loss_G: 0.7977 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.7736 Loss_G: 0.7981 acc: 75.0%\n",
      "[BATCH 133/149] Loss_D: 0.7978 Loss_G: 0.8054 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.7839 Loss_G: 0.8165 acc: 57.8%\n",
      "[BATCH 135/149] Loss_D: 0.8272 Loss_G: 0.8193 acc: 62.5%\n",
      "[BATCH 136/149] Loss_D: 0.7668 Loss_G: 0.8140 acc: 57.8%\n",
      "[BATCH 137/149] Loss_D: 0.8201 Loss_G: 0.8120 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.8390 Loss_G: 0.8186 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.8095 Loss_G: 0.8168 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.7915 Loss_G: 0.8180 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.8067 Loss_G: 0.8144 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7701 Loss_G: 0.8042 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7681 Loss_G: 0.7954 acc: 76.6%\n",
      "[BATCH 144/149] Loss_D: 0.7446 Loss_G: 0.7885 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.7522 Loss_G: 0.7838 acc: 64.1%\n",
      "[BATCH 146/149] Loss_D: 0.7976 Loss_G: 0.7928 acc: 68.8%\n",
      "[BATCH 147/149] Loss_D: 0.8529 Loss_G: 0.8210 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.8153 Loss_G: 0.8421 acc: 65.6%\n",
      "[BATCH 149/149] Loss_D: 0.7875 Loss_G: 0.8238 acc: 75.0%\n",
      "-----THE [16/50] epoch end-----\n",
      "-----THE [17/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8403 Loss_G: 0.8197 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7627 Loss_G: 0.8102 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.7621 Loss_G: 0.8021 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.8159 Loss_G: 0.8102 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.7960 Loss_G: 0.8135 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.7609 Loss_G: 0.8033 acc: 68.8%\n",
      "[BATCH 7/149] Loss_D: 0.7809 Loss_G: 0.8045 acc: 67.2%\n",
      "[BATCH 8/149] Loss_D: 0.8064 Loss_G: 0.8102 acc: 59.4%\n",
      "[BATCH 9/149] Loss_D: 0.7750 Loss_G: 0.8029 acc: 75.0%\n",
      "[BATCH 10/149] Loss_D: 0.7925 Loss_G: 0.8040 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.8047 Loss_G: 0.8083 acc: 54.7%\n",
      "[BATCH 12/149] Loss_D: 0.8173 Loss_G: 0.8089 acc: 56.2%\n",
      "[BATCH 13/149] Loss_D: 0.8132 Loss_G: 0.8164 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7691 Loss_G: 0.8114 acc: 68.8%\n",
      "[BATCH 15/149] Loss_D: 0.7893 Loss_G: 0.8099 acc: 57.8%\n",
      "[BATCH 16/149] Loss_D: 0.7759 Loss_G: 0.7964 acc: 64.1%\n",
      "[EPOCH 2400] TEST ACC is : 74.4%\n",
      "[BATCH 17/149] Loss_D: 0.8376 Loss_G: 0.8195 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.8263 Loss_G: 0.8340 acc: 71.9%\n",
      "[BATCH 19/149] Loss_D: 0.8051 Loss_G: 0.8209 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7582 Loss_G: 0.8119 acc: 56.2%\n",
      "[BATCH 21/149] Loss_D: 0.7517 Loss_G: 0.8083 acc: 57.8%\n",
      "[BATCH 22/149] Loss_D: 0.8202 Loss_G: 0.8172 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.7479 Loss_G: 0.8139 acc: 70.3%\n",
      "[BATCH 24/149] Loss_D: 0.7798 Loss_G: 0.7998 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.7933 Loss_G: 0.8025 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.8214 Loss_G: 0.8209 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7661 Loss_G: 0.8047 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.7824 Loss_G: 0.7940 acc: 68.8%\n",
      "[BATCH 29/149] Loss_D: 0.7674 Loss_G: 0.7883 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.7572 Loss_G: 0.7902 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.8093 Loss_G: 0.8112 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.7594 Loss_G: 0.8006 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7958 Loss_G: 0.8067 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.8038 Loss_G: 0.8006 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.8011 Loss_G: 0.8138 acc: 56.2%\n",
      "[BATCH 36/149] Loss_D: 0.7537 Loss_G: 0.8095 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.7889 Loss_G: 0.8067 acc: 54.7%\n",
      "[BATCH 38/149] Loss_D: 0.8115 Loss_G: 0.8115 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8075 Loss_G: 0.8205 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.7709 Loss_G: 0.8039 acc: 59.4%\n",
      "[BATCH 41/149] Loss_D: 0.7515 Loss_G: 0.7983 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7665 Loss_G: 0.7933 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.8025 Loss_G: 0.7974 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.8060 Loss_G: 0.8104 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.8320 Loss_G: 0.8267 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.7982 Loss_G: 0.8360 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7643 Loss_G: 0.8077 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.8110 Loss_G: 0.8141 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.7734 Loss_G: 0.8107 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.7526 Loss_G: 0.7990 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.8448 Loss_G: 0.8350 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.7861 Loss_G: 0.8377 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.7956 Loss_G: 0.8142 acc: 60.9%\n",
      "[BATCH 54/149] Loss_D: 0.7826 Loss_G: 0.8054 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.8091 Loss_G: 0.7958 acc: 71.9%\n",
      "[BATCH 56/149] Loss_D: 0.7541 Loss_G: 0.7980 acc: 62.5%\n",
      "[BATCH 57/149] Loss_D: 0.7673 Loss_G: 0.7929 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.8254 Loss_G: 0.8066 acc: 73.4%\n",
      "[BATCH 59/149] Loss_D: 0.7727 Loss_G: 0.8075 acc: 54.7%\n",
      "[BATCH 60/149] Loss_D: 0.7975 Loss_G: 0.8051 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.8309 Loss_G: 0.8128 acc: 64.1%\n",
      "[BATCH 62/149] Loss_D: 0.7897 Loss_G: 0.8121 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7704 Loss_G: 0.8000 acc: 56.2%\n",
      "[BATCH 64/149] Loss_D: 0.7599 Loss_G: 0.7946 acc: 56.2%\n",
      "[BATCH 65/149] Loss_D: 0.7953 Loss_G: 0.7959 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.7854 Loss_G: 0.7921 acc: 67.2%\n",
      "[EPOCH 2450] TEST ACC is : 77.5%\n",
      "[BATCH 67/149] Loss_D: 0.8048 Loss_G: 0.7982 acc: 70.3%\n",
      "[BATCH 68/149] Loss_D: 0.7882 Loss_G: 0.7943 acc: 79.7%\n",
      "[BATCH 69/149] Loss_D: 0.8071 Loss_G: 0.8100 acc: 67.2%\n",
      "[BATCH 70/149] Loss_D: 0.7672 Loss_G: 0.8109 acc: 62.5%\n",
      "[BATCH 71/149] Loss_D: 0.7844 Loss_G: 0.8146 acc: 50.0%\n",
      "[BATCH 72/149] Loss_D: 0.7921 Loss_G: 0.8171 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.7672 Loss_G: 0.8159 acc: 68.8%\n",
      "[BATCH 74/149] Loss_D: 0.7828 Loss_G: 0.8137 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.7583 Loss_G: 0.7914 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7619 Loss_G: 0.7845 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7983 Loss_G: 0.7907 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.7809 Loss_G: 0.7935 acc: 59.4%\n",
      "[BATCH 79/149] Loss_D: 0.7925 Loss_G: 0.7891 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.8230 Loss_G: 0.8025 acc: 73.4%\n",
      "[BATCH 81/149] Loss_D: 0.7661 Loss_G: 0.8013 acc: 76.6%\n",
      "[BATCH 82/149] Loss_D: 0.7816 Loss_G: 0.7971 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.8065 Loss_G: 0.8100 acc: 54.7%\n",
      "[BATCH 84/149] Loss_D: 0.7868 Loss_G: 0.8105 acc: 68.8%\n",
      "[BATCH 85/149] Loss_D: 0.7975 Loss_G: 0.8088 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.8634 Loss_G: 0.8304 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.8334 Loss_G: 0.8377 acc: 67.2%\n",
      "[BATCH 88/149] Loss_D: 0.7660 Loss_G: 0.8227 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7675 Loss_G: 0.8101 acc: 57.8%\n",
      "[BATCH 90/149] Loss_D: 0.7775 Loss_G: 0.8046 acc: 62.5%\n",
      "[BATCH 91/149] Loss_D: 0.7925 Loss_G: 0.8091 acc: 71.9%\n",
      "[BATCH 92/149] Loss_D: 0.7757 Loss_G: 0.8147 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.8150 Loss_G: 0.8186 acc: 65.6%\n",
      "[BATCH 94/149] Loss_D: 0.8097 Loss_G: 0.8163 acc: 75.0%\n",
      "[BATCH 95/149] Loss_D: 0.7982 Loss_G: 0.8130 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.8566 Loss_G: 0.8265 acc: 54.7%\n",
      "[BATCH 97/149] Loss_D: 0.7991 Loss_G: 0.8121 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7580 Loss_G: 0.8076 acc: 71.9%\n",
      "[BATCH 99/149] Loss_D: 0.8488 Loss_G: 0.8194 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.8257 Loss_G: 0.8263 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.7765 Loss_G: 0.8052 acc: 56.2%\n",
      "[BATCH 102/149] Loss_D: 0.8007 Loss_G: 0.8001 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.7801 Loss_G: 0.8020 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.8129 Loss_G: 0.8165 acc: 54.7%\n",
      "[BATCH 105/149] Loss_D: 0.7437 Loss_G: 0.8017 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.8332 Loss_G: 0.8046 acc: 79.7%\n",
      "[BATCH 107/149] Loss_D: 0.7657 Loss_G: 0.8029 acc: 71.9%\n",
      "[BATCH 108/149] Loss_D: 0.8405 Loss_G: 0.8158 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.7702 Loss_G: 0.8176 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.7555 Loss_G: 0.8020 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.8013 Loss_G: 0.8008 acc: 57.8%\n",
      "[BATCH 112/149] Loss_D: 0.8310 Loss_G: 0.8088 acc: 75.0%\n",
      "[BATCH 113/149] Loss_D: 0.8189 Loss_G: 0.8146 acc: 68.8%\n",
      "[BATCH 114/149] Loss_D: 0.8028 Loss_G: 0.8088 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.7747 Loss_G: 0.7992 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.7928 Loss_G: 0.7989 acc: 64.1%\n",
      "[EPOCH 2500] TEST ACC is : 75.0%\n",
      "[BATCH 117/149] Loss_D: 0.8099 Loss_G: 0.8093 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.7843 Loss_G: 0.8133 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.8047 Loss_G: 0.8118 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7740 Loss_G: 0.8119 acc: 75.0%\n",
      "[BATCH 121/149] Loss_D: 0.7678 Loss_G: 0.8071 acc: 76.6%\n",
      "[BATCH 122/149] Loss_D: 0.8292 Loss_G: 0.8146 acc: 57.8%\n",
      "[BATCH 123/149] Loss_D: 0.8334 Loss_G: 0.8247 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7903 Loss_G: 0.8220 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.8008 Loss_G: 0.8101 acc: 54.7%\n",
      "[BATCH 126/149] Loss_D: 0.8014 Loss_G: 0.8076 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7960 Loss_G: 0.8103 acc: 60.9%\n",
      "[BATCH 128/149] Loss_D: 0.7580 Loss_G: 0.8046 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.7753 Loss_G: 0.8013 acc: 62.5%\n",
      "[BATCH 130/149] Loss_D: 0.7719 Loss_G: 0.7970 acc: 59.4%\n",
      "[BATCH 131/149] Loss_D: 0.7852 Loss_G: 0.8031 acc: 68.8%\n",
      "[BATCH 132/149] Loss_D: 0.7423 Loss_G: 0.7943 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.7763 Loss_G: 0.7945 acc: 75.0%\n",
      "[BATCH 134/149] Loss_D: 0.7662 Loss_G: 0.8011 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.7463 Loss_G: 0.8009 acc: 54.7%\n",
      "[BATCH 136/149] Loss_D: 0.7699 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 137/149] Loss_D: 0.7944 Loss_G: 0.8007 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7725 Loss_G: 0.8050 acc: 70.3%\n",
      "[BATCH 139/149] Loss_D: 0.8023 Loss_G: 0.8140 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.8011 Loss_G: 0.8374 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7845 Loss_G: 0.8306 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.7619 Loss_G: 0.7985 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.8003 Loss_G: 0.8089 acc: 57.8%\n",
      "[BATCH 144/149] Loss_D: 0.8170 Loss_G: 0.8302 acc: 70.3%\n",
      "[BATCH 145/149] Loss_D: 0.8299 Loss_G: 0.8257 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7753 Loss_G: 0.8151 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7899 Loss_G: 0.8085 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7649 Loss_G: 0.8059 acc: 54.7%\n",
      "[BATCH 149/149] Loss_D: 0.8044 Loss_G: 0.8087 acc: 64.1%\n",
      "-----THE [17/50] epoch end-----\n",
      "-----THE [18/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8097 Loss_G: 0.8196 acc: 64.1%\n",
      "[BATCH 2/149] Loss_D: 0.7677 Loss_G: 0.8031 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.8322 Loss_G: 0.8130 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.8018 Loss_G: 0.8134 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7631 Loss_G: 0.8117 acc: 75.0%\n",
      "[BATCH 6/149] Loss_D: 0.7745 Loss_G: 0.7969 acc: 57.8%\n",
      "[BATCH 7/149] Loss_D: 0.8100 Loss_G: 0.8132 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.7891 Loss_G: 0.8202 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.7650 Loss_G: 0.8103 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7581 Loss_G: 0.8059 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.8528 Loss_G: 0.8205 acc: 68.8%\n",
      "[BATCH 12/149] Loss_D: 0.7958 Loss_G: 0.8230 acc: 64.1%\n",
      "[BATCH 13/149] Loss_D: 0.8017 Loss_G: 0.8197 acc: 73.4%\n",
      "[BATCH 14/149] Loss_D: 0.7738 Loss_G: 0.8115 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7685 Loss_G: 0.8082 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7677 Loss_G: 0.8009 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.8308 Loss_G: 0.8133 acc: 62.5%\n",
      "[EPOCH 2550] TEST ACC is : 75.6%\n",
      "[BATCH 18/149] Loss_D: 0.7972 Loss_G: 0.8215 acc: 64.1%\n",
      "[BATCH 19/149] Loss_D: 0.7874 Loss_G: 0.8247 acc: 59.4%\n",
      "[BATCH 20/149] Loss_D: 0.8257 Loss_G: 0.8198 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8694 Loss_G: 0.8386 acc: 53.1%\n",
      "[BATCH 22/149] Loss_D: 0.7593 Loss_G: 0.8132 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.7706 Loss_G: 0.8046 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7767 Loss_G: 0.8021 acc: 76.6%\n",
      "[BATCH 25/149] Loss_D: 0.7972 Loss_G: 0.8017 acc: 57.8%\n",
      "[BATCH 26/149] Loss_D: 0.7945 Loss_G: 0.8017 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7674 Loss_G: 0.7924 acc: 71.9%\n",
      "[BATCH 28/149] Loss_D: 0.7779 Loss_G: 0.7864 acc: 64.1%\n",
      "[BATCH 29/149] Loss_D: 0.8001 Loss_G: 0.8012 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.7990 Loss_G: 0.8042 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.7836 Loss_G: 0.8038 acc: 65.6%\n",
      "[BATCH 32/149] Loss_D: 0.7771 Loss_G: 0.7958 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.7805 Loss_G: 0.7957 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.7527 Loss_G: 0.7953 acc: 62.5%\n",
      "[BATCH 35/149] Loss_D: 0.7641 Loss_G: 0.7931 acc: 56.2%\n",
      "[BATCH 36/149] Loss_D: 0.8472 Loss_G: 0.8056 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7959 Loss_G: 0.8152 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7717 Loss_G: 0.8046 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.7745 Loss_G: 0.7969 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.7750 Loss_G: 0.8039 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.7782 Loss_G: 0.8023 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.8364 Loss_G: 0.8163 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7628 Loss_G: 0.8096 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7964 Loss_G: 0.8132 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.7549 Loss_G: 0.8084 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.7817 Loss_G: 0.8033 acc: 59.4%\n",
      "[BATCH 47/149] Loss_D: 0.8011 Loss_G: 0.8140 acc: 60.9%\n",
      "[BATCH 48/149] Loss_D: 0.7720 Loss_G: 0.8170 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7727 Loss_G: 0.8041 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.8190 Loss_G: 0.8150 acc: 60.9%\n",
      "[BATCH 51/149] Loss_D: 0.7617 Loss_G: 0.8029 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.7841 Loss_G: 0.8031 acc: 60.9%\n",
      "[BATCH 53/149] Loss_D: 0.7706 Loss_G: 0.8166 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.7817 Loss_G: 0.8071 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.8152 Loss_G: 0.8081 acc: 51.6%\n",
      "[BATCH 56/149] Loss_D: 0.7306 Loss_G: 0.7921 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7482 Loss_G: 0.7878 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.8139 Loss_G: 0.8010 acc: 62.5%\n",
      "[BATCH 59/149] Loss_D: 0.7668 Loss_G: 0.8068 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.8052 Loss_G: 0.8074 acc: 71.9%\n",
      "[BATCH 61/149] Loss_D: 0.8402 Loss_G: 0.8315 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.7759 Loss_G: 0.8134 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.8179 Loss_G: 0.8344 acc: 64.1%\n",
      "[BATCH 64/149] Loss_D: 0.7702 Loss_G: 0.8286 acc: 70.3%\n",
      "[BATCH 65/149] Loss_D: 0.7726 Loss_G: 0.8054 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.7711 Loss_G: 0.8059 acc: 73.4%\n",
      "[BATCH 67/149] Loss_D: 0.7613 Loss_G: 0.8096 acc: 65.6%\n",
      "[EPOCH 2600] TEST ACC is : 75.8%\n",
      "[BATCH 68/149] Loss_D: 0.7997 Loss_G: 0.8139 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.7355 Loss_G: 0.8055 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.7975 Loss_G: 0.8041 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.7943 Loss_G: 0.8138 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8478 Loss_G: 0.8477 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7642 Loss_G: 0.8292 acc: 57.8%\n",
      "[BATCH 74/149] Loss_D: 0.7567 Loss_G: 0.8106 acc: 68.8%\n",
      "[BATCH 75/149] Loss_D: 0.7820 Loss_G: 0.8066 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.8186 Loss_G: 0.8227 acc: 64.1%\n",
      "[BATCH 77/149] Loss_D: 0.8445 Loss_G: 0.8461 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.8147 Loss_G: 0.8507 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.7967 Loss_G: 0.8431 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.7998 Loss_G: 0.8206 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.7941 Loss_G: 0.8042 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.7791 Loss_G: 0.8036 acc: 73.4%\n",
      "[BATCH 83/149] Loss_D: 0.7889 Loss_G: 0.8025 acc: 75.0%\n",
      "[BATCH 84/149] Loss_D: 0.8014 Loss_G: 0.8098 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.7618 Loss_G: 0.8019 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7625 Loss_G: 0.7960 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.8547 Loss_G: 0.8181 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.7542 Loss_G: 0.8133 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.7486 Loss_G: 0.7880 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7949 Loss_G: 0.7960 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.8108 Loss_G: 0.8094 acc: 62.5%\n",
      "[BATCH 92/149] Loss_D: 0.8306 Loss_G: 0.8171 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.7985 Loss_G: 0.8261 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.8628 Loss_G: 0.8454 acc: 65.6%\n",
      "[BATCH 95/149] Loss_D: 0.8021 Loss_G: 0.8457 acc: 65.6%\n",
      "[BATCH 96/149] Loss_D: 0.7926 Loss_G: 0.8284 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7308 Loss_G: 0.7966 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.7569 Loss_G: 0.7835 acc: 68.8%\n",
      "[BATCH 99/149] Loss_D: 0.7603 Loss_G: 0.7842 acc: 56.2%\n",
      "[BATCH 100/149] Loss_D: 0.7771 Loss_G: 0.7881 acc: 56.2%\n",
      "[BATCH 101/149] Loss_D: 0.8322 Loss_G: 0.8033 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.8157 Loss_G: 0.8164 acc: 62.5%\n",
      "[BATCH 103/149] Loss_D: 0.8022 Loss_G: 0.8128 acc: 71.9%\n",
      "[BATCH 104/149] Loss_D: 0.7828 Loss_G: 0.8097 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.8582 Loss_G: 0.8363 acc: 65.6%\n",
      "[BATCH 106/149] Loss_D: 0.8149 Loss_G: 0.8517 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7956 Loss_G: 0.8269 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7788 Loss_G: 0.8103 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.7813 Loss_G: 0.8058 acc: 54.7%\n",
      "[BATCH 110/149] Loss_D: 0.7811 Loss_G: 0.8185 acc: 53.1%\n",
      "[BATCH 111/149] Loss_D: 0.8136 Loss_G: 0.8186 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.8264 Loss_G: 0.8185 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.8289 Loss_G: 0.8211 acc: 68.8%\n",
      "[BATCH 114/149] Loss_D: 0.7678 Loss_G: 0.8113 acc: 56.2%\n",
      "[BATCH 115/149] Loss_D: 0.8060 Loss_G: 0.8042 acc: 70.3%\n",
      "[BATCH 116/149] Loss_D: 0.8274 Loss_G: 0.8192 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7770 Loss_G: 0.8096 acc: 57.8%\n",
      "[EPOCH 2650] TEST ACC is : 76.2%\n",
      "[BATCH 118/149] Loss_D: 0.7626 Loss_G: 0.8029 acc: 76.6%\n",
      "[BATCH 119/149] Loss_D: 0.8234 Loss_G: 0.8043 acc: 59.4%\n",
      "[BATCH 120/149] Loss_D: 0.7882 Loss_G: 0.8151 acc: 57.8%\n",
      "[BATCH 121/149] Loss_D: 0.8189 Loss_G: 0.8125 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.7731 Loss_G: 0.8022 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.7573 Loss_G: 0.8007 acc: 73.4%\n",
      "[BATCH 124/149] Loss_D: 0.8046 Loss_G: 0.8054 acc: 51.6%\n",
      "[BATCH 125/149] Loss_D: 0.8216 Loss_G: 0.8247 acc: 73.4%\n",
      "[BATCH 126/149] Loss_D: 0.8003 Loss_G: 0.8259 acc: 68.8%\n",
      "[BATCH 127/149] Loss_D: 0.7900 Loss_G: 0.8063 acc: 60.9%\n",
      "[BATCH 128/149] Loss_D: 0.7846 Loss_G: 0.8045 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.8157 Loss_G: 0.8010 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7612 Loss_G: 0.8110 acc: 60.9%\n",
      "[BATCH 131/149] Loss_D: 0.7863 Loss_G: 0.8017 acc: 71.9%\n",
      "[BATCH 132/149] Loss_D: 0.7809 Loss_G: 0.8008 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.8127 Loss_G: 0.8004 acc: 57.8%\n",
      "[BATCH 134/149] Loss_D: 0.8007 Loss_G: 0.8142 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.7678 Loss_G: 0.8115 acc: 75.0%\n",
      "[BATCH 136/149] Loss_D: 0.7882 Loss_G: 0.8079 acc: 73.4%\n",
      "[BATCH 137/149] Loss_D: 0.7632 Loss_G: 0.8115 acc: 73.4%\n",
      "[BATCH 138/149] Loss_D: 0.7599 Loss_G: 0.8010 acc: 62.5%\n",
      "[BATCH 139/149] Loss_D: 0.8008 Loss_G: 0.8037 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7758 Loss_G: 0.8042 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7664 Loss_G: 0.7969 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.7473 Loss_G: 0.7895 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.7829 Loss_G: 0.7879 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.7892 Loss_G: 0.8037 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.7960 Loss_G: 0.8100 acc: 71.9%\n",
      "[BATCH 146/149] Loss_D: 0.7535 Loss_G: 0.8116 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.8314 Loss_G: 0.8188 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.8193 Loss_G: 0.8230 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.8032 Loss_G: 0.8120 acc: 64.1%\n",
      "-----THE [18/50] epoch end-----\n",
      "-----THE [19/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7891 Loss_G: 0.8086 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.8047 Loss_G: 0.8164 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7558 Loss_G: 0.8037 acc: 70.3%\n",
      "[BATCH 4/149] Loss_D: 0.8294 Loss_G: 0.8047 acc: 62.5%\n",
      "[BATCH 5/149] Loss_D: 0.7486 Loss_G: 0.8025 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.8065 Loss_G: 0.7985 acc: 65.6%\n",
      "[BATCH 7/149] Loss_D: 0.7752 Loss_G: 0.8026 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7918 Loss_G: 0.8164 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.8436 Loss_G: 0.8154 acc: 70.3%\n",
      "[BATCH 10/149] Loss_D: 0.8039 Loss_G: 0.8119 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.8120 Loss_G: 0.8084 acc: 67.2%\n",
      "[BATCH 12/149] Loss_D: 0.8128 Loss_G: 0.8132 acc: 68.8%\n",
      "[BATCH 13/149] Loss_D: 0.7563 Loss_G: 0.8029 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.7764 Loss_G: 0.8011 acc: 70.3%\n",
      "[BATCH 15/149] Loss_D: 0.8503 Loss_G: 0.8181 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7918 Loss_G: 0.8158 acc: 73.4%\n",
      "[BATCH 17/149] Loss_D: 0.7799 Loss_G: 0.8077 acc: 57.8%\n",
      "[BATCH 18/149] Loss_D: 0.7801 Loss_G: 0.8028 acc: 62.5%\n",
      "[EPOCH 2700] TEST ACC is : 74.2%\n",
      "[BATCH 19/149] Loss_D: 0.7872 Loss_G: 0.8045 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7811 Loss_G: 0.8101 acc: 53.1%\n",
      "[BATCH 21/149] Loss_D: 0.7936 Loss_G: 0.8052 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7437 Loss_G: 0.7984 acc: 60.9%\n",
      "[BATCH 23/149] Loss_D: 0.7380 Loss_G: 0.7905 acc: 57.8%\n",
      "[BATCH 24/149] Loss_D: 0.7335 Loss_G: 0.7823 acc: 59.4%\n",
      "[BATCH 25/149] Loss_D: 0.7802 Loss_G: 0.7956 acc: 59.4%\n",
      "[BATCH 26/149] Loss_D: 0.7852 Loss_G: 0.8065 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.7899 Loss_G: 0.8249 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7681 Loss_G: 0.8320 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.8342 Loss_G: 0.8322 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.8107 Loss_G: 0.8098 acc: 60.9%\n",
      "[BATCH 31/149] Loss_D: 0.8207 Loss_G: 0.8091 acc: 57.8%\n",
      "[BATCH 32/149] Loss_D: 0.7757 Loss_G: 0.8150 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.7434 Loss_G: 0.8021 acc: 78.1%\n",
      "[BATCH 34/149] Loss_D: 0.8244 Loss_G: 0.8137 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7637 Loss_G: 0.8085 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.8408 Loss_G: 0.8251 acc: 59.4%\n",
      "[BATCH 37/149] Loss_D: 0.7959 Loss_G: 0.8324 acc: 68.8%\n",
      "[BATCH 38/149] Loss_D: 0.8094 Loss_G: 0.8345 acc: 70.3%\n",
      "[BATCH 39/149] Loss_D: 0.7470 Loss_G: 0.8135 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.8513 Loss_G: 0.8182 acc: 73.4%\n",
      "[BATCH 41/149] Loss_D: 0.7904 Loss_G: 0.8145 acc: 54.7%\n",
      "[BATCH 42/149] Loss_D: 0.8379 Loss_G: 0.8222 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.8481 Loss_G: 0.8365 acc: 56.2%\n",
      "[BATCH 44/149] Loss_D: 0.7741 Loss_G: 0.8180 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7979 Loss_G: 0.8126 acc: 67.2%\n",
      "[BATCH 46/149] Loss_D: 0.8016 Loss_G: 0.8077 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.7827 Loss_G: 0.8020 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.7702 Loss_G: 0.7927 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.8181 Loss_G: 0.8011 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.7666 Loss_G: 0.8042 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.7978 Loss_G: 0.8072 acc: 71.9%\n",
      "[BATCH 52/149] Loss_D: 0.7839 Loss_G: 0.8031 acc: 54.7%\n",
      "[BATCH 53/149] Loss_D: 0.8072 Loss_G: 0.8072 acc: 60.9%\n",
      "[BATCH 54/149] Loss_D: 0.8164 Loss_G: 0.8182 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.8154 Loss_G: 0.8231 acc: 68.8%\n",
      "[BATCH 56/149] Loss_D: 0.7900 Loss_G: 0.8134 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.8249 Loss_G: 0.8261 acc: 62.5%\n",
      "[BATCH 58/149] Loss_D: 0.7942 Loss_G: 0.8219 acc: 75.0%\n",
      "[BATCH 59/149] Loss_D: 0.8205 Loss_G: 0.8158 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7534 Loss_G: 0.8020 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7706 Loss_G: 0.8054 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.7731 Loss_G: 0.7979 acc: 62.5%\n",
      "[BATCH 63/149] Loss_D: 0.8217 Loss_G: 0.8128 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.8279 Loss_G: 0.8240 acc: 59.4%\n",
      "[BATCH 65/149] Loss_D: 0.7741 Loss_G: 0.8221 acc: 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.8608 Loss_G: 0.8420 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.8059 Loss_G: 0.8501 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.7580 Loss_G: 0.8196 acc: 64.1%\n",
      "[EPOCH 2750] TEST ACC is : 76.0%\n",
      "[BATCH 69/149] Loss_D: 0.8012 Loss_G: 0.8070 acc: 56.2%\n",
      "[BATCH 70/149] Loss_D: 0.8291 Loss_G: 0.8174 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.7872 Loss_G: 0.8192 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.8114 Loss_G: 0.8213 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7652 Loss_G: 0.8069 acc: 81.2%\n",
      "[BATCH 74/149] Loss_D: 0.8057 Loss_G: 0.7987 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.7690 Loss_G: 0.7997 acc: 75.0%\n",
      "[BATCH 76/149] Loss_D: 0.7892 Loss_G: 0.7992 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.8152 Loss_G: 0.8076 acc: 65.6%\n",
      "[BATCH 78/149] Loss_D: 0.8188 Loss_G: 0.8210 acc: 56.2%\n",
      "[BATCH 79/149] Loss_D: 0.8113 Loss_G: 0.8285 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7908 Loss_G: 0.8151 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7980 Loss_G: 0.8076 acc: 62.5%\n",
      "[BATCH 82/149] Loss_D: 0.8091 Loss_G: 0.8259 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7790 Loss_G: 0.8113 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.7910 Loss_G: 0.7990 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.7299 Loss_G: 0.7907 acc: 51.6%\n",
      "[BATCH 86/149] Loss_D: 0.7449 Loss_G: 0.7864 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.8360 Loss_G: 0.8097 acc: 70.3%\n",
      "[BATCH 88/149] Loss_D: 0.7583 Loss_G: 0.8141 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.7508 Loss_G: 0.8029 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7963 Loss_G: 0.8101 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.7377 Loss_G: 0.7962 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7738 Loss_G: 0.7993 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.7655 Loss_G: 0.8040 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.7854 Loss_G: 0.8024 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7770 Loss_G: 0.8091 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.8227 Loss_G: 0.8276 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7664 Loss_G: 0.8300 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.7838 Loss_G: 0.8182 acc: 65.6%\n",
      "[BATCH 99/149] Loss_D: 0.7615 Loss_G: 0.8090 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.7972 Loss_G: 0.8110 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.7541 Loss_G: 0.8060 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.8030 Loss_G: 0.8020 acc: 71.9%\n",
      "[BATCH 103/149] Loss_D: 0.8312 Loss_G: 0.8024 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.7778 Loss_G: 0.8110 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.7616 Loss_G: 0.8092 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.7728 Loss_G: 0.8104 acc: 75.0%\n",
      "[BATCH 107/149] Loss_D: 0.8177 Loss_G: 0.8166 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.8010 Loss_G: 0.8278 acc: 73.4%\n",
      "[BATCH 109/149] Loss_D: 0.8002 Loss_G: 0.8171 acc: 70.3%\n",
      "[BATCH 110/149] Loss_D: 0.7821 Loss_G: 0.8116 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.7556 Loss_G: 0.7945 acc: 67.2%\n",
      "[BATCH 112/149] Loss_D: 0.7372 Loss_G: 0.7919 acc: 75.0%\n",
      "[BATCH 113/149] Loss_D: 0.7860 Loss_G: 0.7897 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7810 Loss_G: 0.7951 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.8172 Loss_G: 0.8040 acc: 81.2%\n",
      "[BATCH 116/149] Loss_D: 0.7759 Loss_G: 0.8037 acc: 48.4%\n",
      "[BATCH 117/149] Loss_D: 0.7790 Loss_G: 0.8058 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.7747 Loss_G: 0.7903 acc: 71.9%\n",
      "[EPOCH 2800] TEST ACC is : 76.8%\n",
      "[BATCH 119/149] Loss_D: 0.7798 Loss_G: 0.7888 acc: 56.2%\n",
      "[BATCH 120/149] Loss_D: 0.7702 Loss_G: 0.7923 acc: 65.6%\n",
      "[BATCH 121/149] Loss_D: 0.7640 Loss_G: 0.7965 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.8207 Loss_G: 0.8094 acc: 56.2%\n",
      "[BATCH 123/149] Loss_D: 0.7776 Loss_G: 0.8107 acc: 65.6%\n",
      "[BATCH 124/149] Loss_D: 0.7599 Loss_G: 0.8080 acc: 54.7%\n",
      "[BATCH 125/149] Loss_D: 0.8266 Loss_G: 0.8177 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.8176 Loss_G: 0.8343 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.8266 Loss_G: 0.8416 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.7953 Loss_G: 0.8218 acc: 50.0%\n",
      "[BATCH 129/149] Loss_D: 0.7395 Loss_G: 0.7961 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.7887 Loss_G: 0.7988 acc: 62.5%\n",
      "[BATCH 131/149] Loss_D: 0.7758 Loss_G: 0.7994 acc: 68.8%\n",
      "[BATCH 132/149] Loss_D: 0.7623 Loss_G: 0.7980 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.7829 Loss_G: 0.8059 acc: 60.9%\n",
      "[BATCH 134/149] Loss_D: 0.7755 Loss_G: 0.7969 acc: 75.0%\n",
      "[BATCH 135/149] Loss_D: 0.7879 Loss_G: 0.8052 acc: 76.6%\n",
      "[BATCH 136/149] Loss_D: 0.8095 Loss_G: 0.8172 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.8199 Loss_G: 0.8200 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.7807 Loss_G: 0.8087 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.7867 Loss_G: 0.7976 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.7702 Loss_G: 0.7989 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7835 Loss_G: 0.8076 acc: 70.3%\n",
      "[BATCH 142/149] Loss_D: 0.7656 Loss_G: 0.7997 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.7832 Loss_G: 0.7973 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7678 Loss_G: 0.7910 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.7778 Loss_G: 0.7950 acc: 75.0%\n",
      "[BATCH 146/149] Loss_D: 0.8311 Loss_G: 0.8052 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7548 Loss_G: 0.7972 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.7884 Loss_G: 0.7969 acc: 65.6%\n",
      "[BATCH 149/149] Loss_D: 0.8136 Loss_G: 0.7986 acc: 67.2%\n",
      "-----THE [19/50] epoch end-----\n",
      "-----THE [20/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7717 Loss_G: 0.7970 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7805 Loss_G: 0.7998 acc: 51.6%\n",
      "[BATCH 3/149] Loss_D: 0.7828 Loss_G: 0.7979 acc: 71.9%\n",
      "[BATCH 4/149] Loss_D: 0.8005 Loss_G: 0.8049 acc: 64.1%\n",
      "[BATCH 5/149] Loss_D: 0.7506 Loss_G: 0.7955 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.8286 Loss_G: 0.8096 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.8360 Loss_G: 0.8306 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.7502 Loss_G: 0.8119 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.8250 Loss_G: 0.8152 acc: 76.6%\n",
      "[BATCH 10/149] Loss_D: 0.7542 Loss_G: 0.8126 acc: 75.0%\n",
      "[BATCH 11/149] Loss_D: 0.7847 Loss_G: 0.8148 acc: 68.8%\n",
      "[BATCH 12/149] Loss_D: 0.7656 Loss_G: 0.8125 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7584 Loss_G: 0.8091 acc: 70.3%\n",
      "[BATCH 14/149] Loss_D: 0.7592 Loss_G: 0.8026 acc: 70.3%\n",
      "[BATCH 15/149] Loss_D: 0.7574 Loss_G: 0.7957 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.7710 Loss_G: 0.7916 acc: 59.4%\n",
      "[BATCH 17/149] Loss_D: 0.8095 Loss_G: 0.7969 acc: 71.9%\n",
      "[BATCH 18/149] Loss_D: 0.7569 Loss_G: 0.7933 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.7642 Loss_G: 0.7970 acc: 57.8%\n",
      "[EPOCH 2850] TEST ACC is : 74.2%\n",
      "[BATCH 20/149] Loss_D: 0.7558 Loss_G: 0.7934 acc: 73.4%\n",
      "[BATCH 21/149] Loss_D: 0.8489 Loss_G: 0.8069 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.8172 Loss_G: 0.8068 acc: 68.8%\n",
      "[BATCH 23/149] Loss_D: 0.8541 Loss_G: 0.8231 acc: 60.9%\n",
      "[BATCH 24/149] Loss_D: 0.7598 Loss_G: 0.8094 acc: 68.8%\n",
      "[BATCH 25/149] Loss_D: 0.7807 Loss_G: 0.8088 acc: 75.0%\n",
      "[BATCH 26/149] Loss_D: 0.7646 Loss_G: 0.8033 acc: 70.3%\n",
      "[BATCH 27/149] Loss_D: 0.7417 Loss_G: 0.7933 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.7724 Loss_G: 0.7911 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7802 Loss_G: 0.8033 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7982 Loss_G: 0.8040 acc: 75.0%\n",
      "[BATCH 31/149] Loss_D: 0.7550 Loss_G: 0.7982 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7620 Loss_G: 0.7882 acc: 75.0%\n",
      "[BATCH 33/149] Loss_D: 0.7764 Loss_G: 0.7860 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8085 Loss_G: 0.7953 acc: 59.4%\n",
      "[BATCH 35/149] Loss_D: 0.7917 Loss_G: 0.8003 acc: 70.3%\n",
      "[BATCH 36/149] Loss_D: 0.7784 Loss_G: 0.8028 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.8145 Loss_G: 0.8075 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7808 Loss_G: 0.8068 acc: 70.3%\n",
      "[BATCH 39/149] Loss_D: 0.8500 Loss_G: 0.8346 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.7410 Loss_G: 0.8089 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.8259 Loss_G: 0.8072 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.7757 Loss_G: 0.7960 acc: 56.2%\n",
      "[BATCH 43/149] Loss_D: 0.8615 Loss_G: 0.8274 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.7643 Loss_G: 0.8282 acc: 56.2%\n",
      "[BATCH 45/149] Loss_D: 0.8217 Loss_G: 0.8312 acc: 71.9%\n",
      "[BATCH 46/149] Loss_D: 0.7920 Loss_G: 0.8389 acc: 71.9%\n",
      "[BATCH 47/149] Loss_D: 0.7714 Loss_G: 0.8121 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.7771 Loss_G: 0.8055 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.8081 Loss_G: 0.8086 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.8327 Loss_G: 0.8253 acc: 75.0%\n",
      "[BATCH 51/149] Loss_D: 0.8064 Loss_G: 0.8148 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.8016 Loss_G: 0.8090 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.7635 Loss_G: 0.8002 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.7708 Loss_G: 0.8011 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.8021 Loss_G: 0.8068 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.7883 Loss_G: 0.8192 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.7726 Loss_G: 0.8096 acc: 57.8%\n",
      "[BATCH 58/149] Loss_D: 0.8097 Loss_G: 0.8108 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.7463 Loss_G: 0.8092 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7488 Loss_G: 0.8057 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.7498 Loss_G: 0.7938 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.8049 Loss_G: 0.8086 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.8039 Loss_G: 0.8129 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.8018 Loss_G: 0.8192 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.8001 Loss_G: 0.8237 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7708 Loss_G: 0.8071 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.8094 Loss_G: 0.7995 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7495 Loss_G: 0.7981 acc: 62.5%\n",
      "[BATCH 69/149] Loss_D: 0.7428 Loss_G: 0.7896 acc: 76.6%\n",
      "[EPOCH 2900] TEST ACC is : 75.0%\n",
      "[BATCH 70/149] Loss_D: 0.8612 Loss_G: 0.8143 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.8063 Loss_G: 0.8233 acc: 59.4%\n",
      "[BATCH 72/149] Loss_D: 0.8365 Loss_G: 0.8314 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7880 Loss_G: 0.8223 acc: 70.3%\n",
      "[BATCH 74/149] Loss_D: 0.7869 Loss_G: 0.8162 acc: 68.8%\n",
      "[BATCH 75/149] Loss_D: 0.8193 Loss_G: 0.8179 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7465 Loss_G: 0.7968 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.7568 Loss_G: 0.7961 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7961 Loss_G: 0.7993 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.7808 Loss_G: 0.7972 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.7816 Loss_G: 0.8013 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.8030 Loss_G: 0.8143 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.7651 Loss_G: 0.8000 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7808 Loss_G: 0.8068 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.7557 Loss_G: 0.8064 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.7878 Loss_G: 0.8184 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.7698 Loss_G: 0.8191 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.8384 Loss_G: 0.8173 acc: 78.1%\n",
      "[BATCH 88/149] Loss_D: 0.8141 Loss_G: 0.8221 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7965 Loss_G: 0.8216 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.8316 Loss_G: 0.8236 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.8029 Loss_G: 0.8199 acc: 68.8%\n",
      "[BATCH 92/149] Loss_D: 0.7639 Loss_G: 0.8040 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.7802 Loss_G: 0.7918 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.7719 Loss_G: 0.7918 acc: 57.8%\n",
      "[BATCH 95/149] Loss_D: 0.7550 Loss_G: 0.8026 acc: 70.3%\n",
      "[BATCH 96/149] Loss_D: 0.8353 Loss_G: 0.8107 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.8584 Loss_G: 0.8476 acc: 48.4%\n",
      "[BATCH 98/149] Loss_D: 0.7674 Loss_G: 0.8459 acc: 70.3%\n",
      "[BATCH 99/149] Loss_D: 0.8364 Loss_G: 0.8309 acc: 73.4%\n",
      "[BATCH 100/149] Loss_D: 0.8275 Loss_G: 0.8178 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.8035 Loss_G: 0.8084 acc: 65.6%\n",
      "[BATCH 102/149] Loss_D: 0.8065 Loss_G: 0.8129 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.7885 Loss_G: 0.8189 acc: 67.2%\n",
      "[BATCH 104/149] Loss_D: 0.7789 Loss_G: 0.8096 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7996 Loss_G: 0.8024 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7800 Loss_G: 0.8161 acc: 78.1%\n",
      "[BATCH 107/149] Loss_D: 0.7953 Loss_G: 0.8102 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.8059 Loss_G: 0.8095 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.7813 Loss_G: 0.8052 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.7682 Loss_G: 0.7997 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.7572 Loss_G: 0.7978 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.8591 Loss_G: 0.8222 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.7953 Loss_G: 0.8263 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.8234 Loss_G: 0.8293 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.8109 Loss_G: 0.8276 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.7843 Loss_G: 0.8131 acc: 56.2%\n",
      "[BATCH 117/149] Loss_D: 0.7620 Loss_G: 0.8000 acc: 59.4%\n",
      "[BATCH 118/149] Loss_D: 0.7906 Loss_G: 0.7951 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.8058 Loss_G: 0.8064 acc: 57.8%\n",
      "[EPOCH 2950] TEST ACC is : 75.8%\n",
      "[BATCH 120/149] Loss_D: 0.7738 Loss_G: 0.8206 acc: 71.9%\n",
      "[BATCH 121/149] Loss_D: 0.8278 Loss_G: 0.8317 acc: 70.3%\n",
      "[BATCH 122/149] Loss_D: 0.7906 Loss_G: 0.8059 acc: 64.1%\n",
      "[BATCH 123/149] Loss_D: 0.7908 Loss_G: 0.8092 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7850 Loss_G: 0.8123 acc: 59.4%\n",
      "[BATCH 125/149] Loss_D: 0.7557 Loss_G: 0.8025 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.7592 Loss_G: 0.8026 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7941 Loss_G: 0.8042 acc: 56.2%\n",
      "[BATCH 128/149] Loss_D: 0.7620 Loss_G: 0.7962 acc: 56.2%\n",
      "[BATCH 129/149] Loss_D: 0.7813 Loss_G: 0.7894 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.7773 Loss_G: 0.7977 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.7984 Loss_G: 0.8093 acc: 54.7%\n",
      "[BATCH 132/149] Loss_D: 0.7691 Loss_G: 0.8042 acc: 76.6%\n",
      "[BATCH 133/149] Loss_D: 0.7880 Loss_G: 0.8033 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.8134 Loss_G: 0.8025 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.8060 Loss_G: 0.8090 acc: 71.9%\n",
      "[BATCH 136/149] Loss_D: 0.7699 Loss_G: 0.8070 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.8260 Loss_G: 0.8137 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.8161 Loss_G: 0.8216 acc: 64.1%\n",
      "[BATCH 139/149] Loss_D: 0.7550 Loss_G: 0.8002 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.8186 Loss_G: 0.7999 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.7782 Loss_G: 0.7985 acc: 57.8%\n",
      "[BATCH 142/149] Loss_D: 0.7872 Loss_G: 0.8058 acc: 71.9%\n",
      "[BATCH 143/149] Loss_D: 0.8041 Loss_G: 0.8001 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7601 Loss_G: 0.7965 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.7692 Loss_G: 0.7990 acc: 68.8%\n",
      "[BATCH 146/149] Loss_D: 0.7506 Loss_G: 0.7913 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7949 Loss_G: 0.7980 acc: 68.8%\n",
      "[BATCH 148/149] Loss_D: 0.8158 Loss_G: 0.8149 acc: 75.0%\n",
      "[BATCH 149/149] Loss_D: 0.8101 Loss_G: 0.8161 acc: 71.9%\n",
      "-----THE [20/50] epoch end-----\n",
      "-----THE [21/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7827 Loss_G: 0.8260 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7907 Loss_G: 0.8260 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7563 Loss_G: 0.8095 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.8018 Loss_G: 0.8145 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.7647 Loss_G: 0.8067 acc: 68.8%\n",
      "[BATCH 6/149] Loss_D: 0.7947 Loss_G: 0.8135 acc: 64.1%\n",
      "[BATCH 7/149] Loss_D: 0.7881 Loss_G: 0.8112 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.7394 Loss_G: 0.7901 acc: 62.5%\n",
      "[BATCH 9/149] Loss_D: 0.7752 Loss_G: 0.7901 acc: 57.8%\n",
      "[BATCH 10/149] Loss_D: 0.7888 Loss_G: 0.7988 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.7864 Loss_G: 0.8067 acc: 70.3%\n",
      "[BATCH 12/149] Loss_D: 0.8058 Loss_G: 0.8195 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7747 Loss_G: 0.8072 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.7812 Loss_G: 0.8049 acc: 64.1%\n",
      "[BATCH 15/149] Loss_D: 0.8622 Loss_G: 0.8264 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.7800 Loss_G: 0.8262 acc: 53.1%\n",
      "[BATCH 17/149] Loss_D: 0.7600 Loss_G: 0.8035 acc: 73.4%\n",
      "[BATCH 18/149] Loss_D: 0.7894 Loss_G: 0.8034 acc: 68.8%\n",
      "[BATCH 19/149] Loss_D: 0.7615 Loss_G: 0.7968 acc: 59.4%\n",
      "[BATCH 20/149] Loss_D: 0.8003 Loss_G: 0.8038 acc: 65.6%\n",
      "[EPOCH 3000] TEST ACC is : 75.8%\n",
      "[BATCH 21/149] Loss_D: 0.7483 Loss_G: 0.7910 acc: 54.7%\n",
      "[BATCH 22/149] Loss_D: 0.8082 Loss_G: 0.7938 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7806 Loss_G: 0.7953 acc: 75.0%\n",
      "[BATCH 24/149] Loss_D: 0.8402 Loss_G: 0.8037 acc: 73.4%\n",
      "[BATCH 25/149] Loss_D: 0.7973 Loss_G: 0.8155 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.7762 Loss_G: 0.8099 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.8029 Loss_G: 0.8137 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.7414 Loss_G: 0.8053 acc: 76.6%\n",
      "[BATCH 29/149] Loss_D: 0.7692 Loss_G: 0.7984 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.8003 Loss_G: 0.8057 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.8175 Loss_G: 0.8257 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.7584 Loss_G: 0.8108 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.7785 Loss_G: 0.8025 acc: 57.8%\n",
      "[BATCH 34/149] Loss_D: 0.8209 Loss_G: 0.8107 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.8014 Loss_G: 0.8212 acc: 57.8%\n",
      "[BATCH 36/149] Loss_D: 0.8144 Loss_G: 0.8469 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7943 Loss_G: 0.8182 acc: 70.3%\n",
      "[BATCH 38/149] Loss_D: 0.7377 Loss_G: 0.7976 acc: 57.8%\n",
      "[BATCH 39/149] Loss_D: 0.8168 Loss_G: 0.8056 acc: 70.3%\n",
      "[BATCH 40/149] Loss_D: 0.7901 Loss_G: 0.8059 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.7744 Loss_G: 0.8136 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7742 Loss_G: 0.8018 acc: 75.0%\n",
      "[BATCH 43/149] Loss_D: 0.7658 Loss_G: 0.7973 acc: 75.0%\n",
      "[BATCH 44/149] Loss_D: 0.8231 Loss_G: 0.8056 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.8230 Loss_G: 0.8164 acc: 68.8%\n",
      "[BATCH 46/149] Loss_D: 0.7762 Loss_G: 0.8096 acc: 57.8%\n",
      "[BATCH 47/149] Loss_D: 0.7758 Loss_G: 0.8033 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.8239 Loss_G: 0.8110 acc: 79.7%\n",
      "[BATCH 49/149] Loss_D: 0.7630 Loss_G: 0.8071 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.7717 Loss_G: 0.8086 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.8429 Loss_G: 0.8205 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.7484 Loss_G: 0.8067 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.7827 Loss_G: 0.8071 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.7952 Loss_G: 0.8081 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.7688 Loss_G: 0.8055 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7726 Loss_G: 0.7940 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.7771 Loss_G: 0.7929 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.8182 Loss_G: 0.7928 acc: 62.5%\n",
      "[BATCH 59/149] Loss_D: 0.7441 Loss_G: 0.7856 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7884 Loss_G: 0.7899 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.7707 Loss_G: 0.7938 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.7884 Loss_G: 0.8002 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.8331 Loss_G: 0.8088 acc: 67.2%\n",
      "[BATCH 64/149] Loss_D: 0.8613 Loss_G: 0.8312 acc: 67.2%\n",
      "[BATCH 65/149] Loss_D: 0.7660 Loss_G: 0.8108 acc: 57.8%\n",
      "[BATCH 66/149] Loss_D: 0.8461 Loss_G: 0.8167 acc: 59.4%\n",
      "[BATCH 67/149] Loss_D: 0.7563 Loss_G: 0.8062 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.7732 Loss_G: 0.7956 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.7435 Loss_G: 0.7860 acc: 59.4%\n",
      "[BATCH 70/149] Loss_D: 0.7895 Loss_G: 0.7915 acc: 59.4%\n",
      "[EPOCH 3050] TEST ACC is : 76.2%\n",
      "[BATCH 71/149] Loss_D: 0.8042 Loss_G: 0.8047 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7895 Loss_G: 0.8025 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.8517 Loss_G: 0.8206 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.8001 Loss_G: 0.8194 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.7835 Loss_G: 0.8119 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.7618 Loss_G: 0.7959 acc: 51.6%\n",
      "[BATCH 77/149] Loss_D: 0.7550 Loss_G: 0.7967 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7563 Loss_G: 0.7914 acc: 57.8%\n",
      "[BATCH 79/149] Loss_D: 0.7643 Loss_G: 0.7871 acc: 62.5%\n",
      "[BATCH 80/149] Loss_D: 0.7796 Loss_G: 0.7945 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7807 Loss_G: 0.8048 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.7967 Loss_G: 0.8177 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.8438 Loss_G: 0.8276 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.7781 Loss_G: 0.8131 acc: 57.8%\n",
      "[BATCH 85/149] Loss_D: 0.8229 Loss_G: 0.8164 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.8027 Loss_G: 0.8348 acc: 73.4%\n",
      "[BATCH 87/149] Loss_D: 0.8276 Loss_G: 0.8404 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.8309 Loss_G: 0.8244 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.7907 Loss_G: 0.8138 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7572 Loss_G: 0.8055 acc: 62.5%\n",
      "[BATCH 91/149] Loss_D: 0.7389 Loss_G: 0.7946 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.7545 Loss_G: 0.7856 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.7423 Loss_G: 0.7843 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.7407 Loss_G: 0.7786 acc: 73.4%\n",
      "[BATCH 95/149] Loss_D: 0.7817 Loss_G: 0.7869 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.7590 Loss_G: 0.7857 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.7840 Loss_G: 0.7898 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7966 Loss_G: 0.7824 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.8069 Loss_G: 0.7961 acc: 59.4%\n",
      "[BATCH 100/149] Loss_D: 0.8016 Loss_G: 0.8024 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.8282 Loss_G: 0.8209 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.8340 Loss_G: 0.8268 acc: 57.8%\n",
      "[BATCH 103/149] Loss_D: 0.7939 Loss_G: 0.8198 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.8282 Loss_G: 0.8263 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7936 Loss_G: 0.8225 acc: 62.5%\n",
      "[BATCH 106/149] Loss_D: 0.8402 Loss_G: 0.8424 acc: 76.6%\n",
      "[BATCH 107/149] Loss_D: 0.8288 Loss_G: 0.8547 acc: 73.4%\n",
      "[BATCH 108/149] Loss_D: 0.8163 Loss_G: 0.8311 acc: 79.7%\n",
      "[BATCH 109/149] Loss_D: 0.7824 Loss_G: 0.8130 acc: 56.2%\n",
      "[BATCH 110/149] Loss_D: 0.7678 Loss_G: 0.7930 acc: 70.3%\n",
      "[BATCH 111/149] Loss_D: 0.8127 Loss_G: 0.7993 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8027 Loss_G: 0.8149 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.8091 Loss_G: 0.8225 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.7972 Loss_G: 0.8254 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.8070 Loss_G: 0.8255 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.7784 Loss_G: 0.8126 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7967 Loss_G: 0.8136 acc: 67.2%\n",
      "[BATCH 118/149] Loss_D: 0.7678 Loss_G: 0.8052 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.7767 Loss_G: 0.7944 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.7631 Loss_G: 0.7997 acc: 75.0%\n",
      "[EPOCH 3100] TEST ACC is : 74.8%\n",
      "[BATCH 121/149] Loss_D: 0.7891 Loss_G: 0.7946 acc: 64.1%\n",
      "[BATCH 122/149] Loss_D: 0.7735 Loss_G: 0.8029 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.7774 Loss_G: 0.8053 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.8062 Loss_G: 0.8044 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.7730 Loss_G: 0.8012 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.8211 Loss_G: 0.8031 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.8062 Loss_G: 0.8092 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7299 Loss_G: 0.7998 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.7964 Loss_G: 0.7999 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.8591 Loss_G: 0.8239 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.7641 Loss_G: 0.8101 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7858 Loss_G: 0.8104 acc: 54.7%\n",
      "[BATCH 133/149] Loss_D: 0.7503 Loss_G: 0.7934 acc: 73.4%\n",
      "[BATCH 134/149] Loss_D: 0.8252 Loss_G: 0.8047 acc: 76.6%\n",
      "[BATCH 135/149] Loss_D: 0.8309 Loss_G: 0.8222 acc: 75.0%\n",
      "[BATCH 136/149] Loss_D: 0.7637 Loss_G: 0.8105 acc: 54.7%\n",
      "[BATCH 137/149] Loss_D: 0.8314 Loss_G: 0.8127 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.8001 Loss_G: 0.8215 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.7591 Loss_G: 0.8094 acc: 75.0%\n",
      "[BATCH 140/149] Loss_D: 0.7854 Loss_G: 0.8072 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7721 Loss_G: 0.8090 acc: 73.4%\n",
      "[BATCH 142/149] Loss_D: 0.7709 Loss_G: 0.8006 acc: 65.6%\n",
      "[BATCH 143/149] Loss_D: 0.7780 Loss_G: 0.7962 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.8084 Loss_G: 0.8034 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.7678 Loss_G: 0.8016 acc: 59.4%\n",
      "[BATCH 146/149] Loss_D: 0.7699 Loss_G: 0.7940 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.7612 Loss_G: 0.7865 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.7786 Loss_G: 0.7935 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7746 Loss_G: 0.7913 acc: 67.2%\n",
      "-----THE [21/50] epoch end-----\n",
      "-----THE [22/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7925 Loss_G: 0.8016 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.7655 Loss_G: 0.8022 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.8240 Loss_G: 0.8121 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.8165 Loss_G: 0.8190 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.8045 Loss_G: 0.8148 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.7664 Loss_G: 0.8036 acc: 64.1%\n",
      "[BATCH 7/149] Loss_D: 0.8246 Loss_G: 0.8139 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7840 Loss_G: 0.8109 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.7953 Loss_G: 0.8180 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.8166 Loss_G: 0.8218 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.8008 Loss_G: 0.8287 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7972 Loss_G: 0.8158 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7722 Loss_G: 0.8060 acc: 53.1%\n",
      "[BATCH 14/149] Loss_D: 0.8296 Loss_G: 0.8199 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.7637 Loss_G: 0.8042 acc: 57.8%\n",
      "[BATCH 16/149] Loss_D: 0.8021 Loss_G: 0.7973 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.7518 Loss_G: 0.7983 acc: 73.4%\n",
      "[BATCH 18/149] Loss_D: 0.7834 Loss_G: 0.7917 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.8191 Loss_G: 0.8133 acc: 71.9%\n",
      "[BATCH 20/149] Loss_D: 0.7997 Loss_G: 0.8084 acc: 64.1%\n",
      "[BATCH 21/149] Loss_D: 0.8667 Loss_G: 0.8366 acc: 68.8%\n",
      "[EPOCH 3150] TEST ACC is : 75.4%\n",
      "[BATCH 22/149] Loss_D: 0.8412 Loss_G: 0.8453 acc: 71.9%\n",
      "[BATCH 23/149] Loss_D: 0.7740 Loss_G: 0.8206 acc: 65.6%\n",
      "[BATCH 24/149] Loss_D: 0.8210 Loss_G: 0.8228 acc: 57.8%\n",
      "[BATCH 25/149] Loss_D: 0.8201 Loss_G: 0.8137 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.8161 Loss_G: 0.8163 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7813 Loss_G: 0.8054 acc: 75.0%\n",
      "[BATCH 28/149] Loss_D: 0.8087 Loss_G: 0.7975 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.7637 Loss_G: 0.7979 acc: 57.8%\n",
      "[BATCH 30/149] Loss_D: 0.8363 Loss_G: 0.8084 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.8157 Loss_G: 0.8199 acc: 65.6%\n",
      "[BATCH 32/149] Loss_D: 0.7680 Loss_G: 0.8119 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.7801 Loss_G: 0.8013 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.7748 Loss_G: 0.7987 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7874 Loss_G: 0.8026 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.8085 Loss_G: 0.8110 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.8261 Loss_G: 0.8121 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.7796 Loss_G: 0.8108 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.7846 Loss_G: 0.8111 acc: 68.8%\n",
      "[BATCH 40/149] Loss_D: 0.7912 Loss_G: 0.8062 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.7598 Loss_G: 0.7997 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7747 Loss_G: 0.7981 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.8273 Loss_G: 0.8154 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.7637 Loss_G: 0.8076 acc: 59.4%\n",
      "[BATCH 45/149] Loss_D: 0.7990 Loss_G: 0.8092 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.7660 Loss_G: 0.8013 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.8121 Loss_G: 0.8094 acc: 73.4%\n",
      "[BATCH 48/149] Loss_D: 0.7908 Loss_G: 0.8112 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7942 Loss_G: 0.8133 acc: 64.1%\n",
      "[BATCH 50/149] Loss_D: 0.7439 Loss_G: 0.8012 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7745 Loss_G: 0.7961 acc: 60.9%\n",
      "[BATCH 52/149] Loss_D: 0.7308 Loss_G: 0.7896 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.8187 Loss_G: 0.8039 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.7430 Loss_G: 0.8058 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.7607 Loss_G: 0.7957 acc: 71.9%\n",
      "[BATCH 56/149] Loss_D: 0.7990 Loss_G: 0.7976 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.7619 Loss_G: 0.8051 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.8071 Loss_G: 0.8046 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7608 Loss_G: 0.7981 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7988 Loss_G: 0.8034 acc: 73.4%\n",
      "[BATCH 61/149] Loss_D: 0.7892 Loss_G: 0.8156 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7901 Loss_G: 0.8108 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.7744 Loss_G: 0.8141 acc: 75.0%\n",
      "[BATCH 64/149] Loss_D: 0.7554 Loss_G: 0.8081 acc: 68.8%\n",
      "[BATCH 65/149] Loss_D: 0.8018 Loss_G: 0.8184 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7760 Loss_G: 0.8077 acc: 73.4%\n",
      "[BATCH 67/149] Loss_D: 0.8143 Loss_G: 0.8056 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.7917 Loss_G: 0.8049 acc: 62.5%\n",
      "[BATCH 69/149] Loss_D: 0.7761 Loss_G: 0.7945 acc: 76.6%\n",
      "[BATCH 70/149] Loss_D: 0.7581 Loss_G: 0.7844 acc: 54.7%\n",
      "[BATCH 71/149] Loss_D: 0.7745 Loss_G: 0.7886 acc: 75.0%\n",
      "[EPOCH 3200] TEST ACC is : 75.0%\n",
      "[BATCH 72/149] Loss_D: 0.8168 Loss_G: 0.8064 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.8010 Loss_G: 0.8090 acc: 65.6%\n",
      "[BATCH 74/149] Loss_D: 0.8415 Loss_G: 0.8395 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.8289 Loss_G: 0.8282 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.7895 Loss_G: 0.8095 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.7548 Loss_G: 0.7960 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.7846 Loss_G: 0.7996 acc: 70.3%\n",
      "[BATCH 79/149] Loss_D: 0.7808 Loss_G: 0.8072 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7577 Loss_G: 0.8022 acc: 70.3%\n",
      "[BATCH 81/149] Loss_D: 0.7869 Loss_G: 0.8098 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.7594 Loss_G: 0.8054 acc: 65.6%\n",
      "[BATCH 83/149] Loss_D: 0.7891 Loss_G: 0.8010 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.8431 Loss_G: 0.8215 acc: 64.1%\n",
      "[BATCH 85/149] Loss_D: 0.8146 Loss_G: 0.8299 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.7606 Loss_G: 0.8066 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.7986 Loss_G: 0.8024 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7905 Loss_G: 0.8027 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.7910 Loss_G: 0.8028 acc: 65.6%\n",
      "[BATCH 90/149] Loss_D: 0.7465 Loss_G: 0.7947 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.7648 Loss_G: 0.7949 acc: 57.8%\n",
      "[BATCH 92/149] Loss_D: 0.7781 Loss_G: 0.7976 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.8074 Loss_G: 0.8048 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.7842 Loss_G: 0.7993 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.7955 Loss_G: 0.7919 acc: 70.3%\n",
      "[BATCH 96/149] Loss_D: 0.7600 Loss_G: 0.7876 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7944 Loss_G: 0.7897 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.7914 Loss_G: 0.7937 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.7499 Loss_G: 0.7903 acc: 73.4%\n",
      "[BATCH 100/149] Loss_D: 0.8052 Loss_G: 0.7912 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.7819 Loss_G: 0.7889 acc: 70.3%\n",
      "[BATCH 102/149] Loss_D: 0.7767 Loss_G: 0.7881 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.7704 Loss_G: 0.7854 acc: 65.6%\n",
      "[BATCH 104/149] Loss_D: 0.7641 Loss_G: 0.7887 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.7938 Loss_G: 0.8024 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.7587 Loss_G: 0.7942 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.7820 Loss_G: 0.7951 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.7742 Loss_G: 0.8027 acc: 76.6%\n",
      "[BATCH 109/149] Loss_D: 0.7691 Loss_G: 0.7951 acc: 56.2%\n",
      "[BATCH 110/149] Loss_D: 0.7650 Loss_G: 0.7916 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.7515 Loss_G: 0.7885 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.8202 Loss_G: 0.8041 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.7793 Loss_G: 0.8246 acc: 71.9%\n",
      "[BATCH 114/149] Loss_D: 0.7994 Loss_G: 0.8161 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.8235 Loss_G: 0.8170 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7823 Loss_G: 0.8090 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.7840 Loss_G: 0.8091 acc: 75.0%\n",
      "[BATCH 118/149] Loss_D: 0.7701 Loss_G: 0.7998 acc: 68.8%\n",
      "[BATCH 119/149] Loss_D: 0.7765 Loss_G: 0.7959 acc: 59.4%\n",
      "[BATCH 120/149] Loss_D: 0.7339 Loss_G: 0.7843 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.8313 Loss_G: 0.7933 acc: 70.3%\n",
      "[EPOCH 3250] TEST ACC is : 75.6%\n",
      "[BATCH 122/149] Loss_D: 0.7979 Loss_G: 0.8030 acc: 62.5%\n",
      "[BATCH 123/149] Loss_D: 0.7901 Loss_G: 0.8146 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7951 Loss_G: 0.8069 acc: 62.5%\n",
      "[BATCH 125/149] Loss_D: 0.8299 Loss_G: 0.8186 acc: 60.9%\n",
      "[BATCH 126/149] Loss_D: 0.7923 Loss_G: 0.8222 acc: 67.2%\n",
      "[BATCH 127/149] Loss_D: 0.8174 Loss_G: 0.8237 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.7763 Loss_G: 0.8180 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7676 Loss_G: 0.8025 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.7766 Loss_G: 0.7921 acc: 65.6%\n",
      "[BATCH 131/149] Loss_D: 0.7706 Loss_G: 0.7869 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.8305 Loss_G: 0.8062 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.8196 Loss_G: 0.8035 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.7614 Loss_G: 0.7908 acc: 62.5%\n",
      "[BATCH 135/149] Loss_D: 0.8028 Loss_G: 0.7924 acc: 75.0%\n",
      "[BATCH 136/149] Loss_D: 0.7793 Loss_G: 0.7936 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.7746 Loss_G: 0.7891 acc: 76.6%\n",
      "[BATCH 138/149] Loss_D: 0.7566 Loss_G: 0.7906 acc: 60.9%\n",
      "[BATCH 139/149] Loss_D: 0.7949 Loss_G: 0.8018 acc: 54.7%\n",
      "[BATCH 140/149] Loss_D: 0.8034 Loss_G: 0.8259 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7967 Loss_G: 0.8161 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7674 Loss_G: 0.8100 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7280 Loss_G: 0.7897 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.7970 Loss_G: 0.7984 acc: 62.5%\n",
      "[BATCH 145/149] Loss_D: 0.8203 Loss_G: 0.8173 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7728 Loss_G: 0.8109 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.7884 Loss_G: 0.7979 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.8050 Loss_G: 0.8067 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.7894 Loss_G: 0.8070 acc: 57.8%\n",
      "-----THE [22/50] epoch end-----\n",
      "-----THE [23/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7837 Loss_G: 0.8029 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7818 Loss_G: 0.8015 acc: 60.9%\n",
      "[BATCH 3/149] Loss_D: 0.7778 Loss_G: 0.7936 acc: 56.2%\n",
      "[BATCH 4/149] Loss_D: 0.7863 Loss_G: 0.7937 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.8015 Loss_G: 0.8014 acc: 53.1%\n",
      "[BATCH 6/149] Loss_D: 0.7825 Loss_G: 0.8028 acc: 64.1%\n",
      "[BATCH 7/149] Loss_D: 0.8038 Loss_G: 0.7979 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.7518 Loss_G: 0.8017 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.8048 Loss_G: 0.8031 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.7793 Loss_G: 0.7978 acc: 73.4%\n",
      "[BATCH 11/149] Loss_D: 0.8179 Loss_G: 0.8086 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7913 Loss_G: 0.8092 acc: 71.9%\n",
      "[BATCH 13/149] Loss_D: 0.8210 Loss_G: 0.8200 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7670 Loss_G: 0.8109 acc: 68.8%\n",
      "[BATCH 15/149] Loss_D: 0.7675 Loss_G: 0.7983 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.7710 Loss_G: 0.8020 acc: 71.9%\n",
      "[BATCH 17/149] Loss_D: 0.8009 Loss_G: 0.8066 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.7609 Loss_G: 0.7986 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.8214 Loss_G: 0.8040 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7990 Loss_G: 0.8126 acc: 71.9%\n",
      "[BATCH 21/149] Loss_D: 0.8028 Loss_G: 0.8137 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7290 Loss_G: 0.7953 acc: 59.4%\n",
      "[EPOCH 3300] TEST ACC is : 74.4%\n",
      "[BATCH 23/149] Loss_D: 0.7850 Loss_G: 0.7869 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.7799 Loss_G: 0.7908 acc: 67.2%\n",
      "[BATCH 25/149] Loss_D: 0.7990 Loss_G: 0.7954 acc: 70.3%\n",
      "[BATCH 26/149] Loss_D: 0.7670 Loss_G: 0.7950 acc: 71.9%\n",
      "[BATCH 27/149] Loss_D: 0.7693 Loss_G: 0.7906 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.7720 Loss_G: 0.7900 acc: 57.8%\n",
      "[BATCH 29/149] Loss_D: 0.7934 Loss_G: 0.7972 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 0.8244 Loss_G: 0.8099 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.7866 Loss_G: 0.8075 acc: 65.6%\n",
      "[BATCH 32/149] Loss_D: 0.8052 Loss_G: 0.8158 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.7995 Loss_G: 0.8162 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7907 Loss_G: 0.8131 acc: 73.4%\n",
      "[BATCH 35/149] Loss_D: 0.7723 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.8288 Loss_G: 0.8187 acc: 64.1%\n",
      "[BATCH 37/149] Loss_D: 0.8199 Loss_G: 0.8110 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7924 Loss_G: 0.8005 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.8102 Loss_G: 0.8030 acc: 56.2%\n",
      "[BATCH 40/149] Loss_D: 0.7506 Loss_G: 0.7927 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7581 Loss_G: 0.7936 acc: 73.4%\n",
      "[BATCH 42/149] Loss_D: 0.7351 Loss_G: 0.7949 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7401 Loss_G: 0.7882 acc: 76.6%\n",
      "[BATCH 44/149] Loss_D: 0.7376 Loss_G: 0.7809 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7579 Loss_G: 0.7881 acc: 67.2%\n",
      "[BATCH 46/149] Loss_D: 0.7431 Loss_G: 0.7772 acc: 60.9%\n",
      "[BATCH 47/149] Loss_D: 0.7862 Loss_G: 0.7864 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.7673 Loss_G: 0.7973 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.7830 Loss_G: 0.8067 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.7565 Loss_G: 0.8022 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.7880 Loss_G: 0.8032 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.7931 Loss_G: 0.8089 acc: 62.5%\n",
      "[BATCH 53/149] Loss_D: 0.7908 Loss_G: 0.8118 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.7425 Loss_G: 0.8047 acc: 53.1%\n",
      "[BATCH 55/149] Loss_D: 0.8537 Loss_G: 0.8248 acc: 75.0%\n",
      "[BATCH 56/149] Loss_D: 0.8310 Loss_G: 0.8349 acc: 57.8%\n",
      "[BATCH 57/149] Loss_D: 0.7641 Loss_G: 0.8086 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7908 Loss_G: 0.8075 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.7476 Loss_G: 0.7934 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7867 Loss_G: 0.7994 acc: 76.6%\n",
      "[BATCH 61/149] Loss_D: 0.7838 Loss_G: 0.8064 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.7587 Loss_G: 0.8042 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.7918 Loss_G: 0.8014 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.7872 Loss_G: 0.8021 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7782 Loss_G: 0.8004 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.7513 Loss_G: 0.7959 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.8261 Loss_G: 0.8189 acc: 57.8%\n",
      "[BATCH 68/149] Loss_D: 0.7845 Loss_G: 0.8158 acc: 56.2%\n",
      "[BATCH 69/149] Loss_D: 0.7989 Loss_G: 0.8254 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.7979 Loss_G: 0.8499 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7969 Loss_G: 0.8281 acc: 73.4%\n",
      "[BATCH 72/149] Loss_D: 0.8108 Loss_G: 0.8194 acc: 57.8%\n",
      "[EPOCH 3350] TEST ACC is : 73.8%\n",
      "[BATCH 73/149] Loss_D: 0.7801 Loss_G: 0.8178 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7734 Loss_G: 0.8065 acc: 71.9%\n",
      "[BATCH 75/149] Loss_D: 0.7723 Loss_G: 0.8021 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.7318 Loss_G: 0.7944 acc: 78.1%\n",
      "[BATCH 77/149] Loss_D: 0.7951 Loss_G: 0.7995 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.7777 Loss_G: 0.7974 acc: 57.8%\n",
      "[BATCH 79/149] Loss_D: 0.8317 Loss_G: 0.8049 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.7529 Loss_G: 0.8000 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.7895 Loss_G: 0.8006 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.7665 Loss_G: 0.7981 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.7935 Loss_G: 0.8010 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.7956 Loss_G: 0.8086 acc: 57.8%\n",
      "[BATCH 85/149] Loss_D: 0.7746 Loss_G: 0.8111 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.7671 Loss_G: 0.7997 acc: 54.7%\n",
      "[BATCH 87/149] Loss_D: 0.8046 Loss_G: 0.8065 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.7346 Loss_G: 0.8005 acc: 76.6%\n",
      "[BATCH 89/149] Loss_D: 0.7678 Loss_G: 0.7970 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.8469 Loss_G: 0.8204 acc: 73.4%\n",
      "[BATCH 91/149] Loss_D: 0.7850 Loss_G: 0.8217 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.7904 Loss_G: 0.8102 acc: 54.7%\n",
      "[BATCH 93/149] Loss_D: 0.7606 Loss_G: 0.7966 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.7848 Loss_G: 0.8001 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.8996 Loss_G: 0.8583 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.8502 Loss_G: 0.8638 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7979 Loss_G: 0.8300 acc: 62.5%\n",
      "[BATCH 98/149] Loss_D: 0.8163 Loss_G: 0.8184 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7503 Loss_G: 0.8042 acc: 75.0%\n",
      "[BATCH 100/149] Loss_D: 0.8139 Loss_G: 0.8049 acc: 67.2%\n",
      "[BATCH 101/149] Loss_D: 0.8415 Loss_G: 0.8202 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7666 Loss_G: 0.8083 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7931 Loss_G: 0.8134 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.7782 Loss_G: 0.8131 acc: 71.9%\n",
      "[BATCH 105/149] Loss_D: 0.7947 Loss_G: 0.8209 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.8305 Loss_G: 0.8260 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.7907 Loss_G: 0.8187 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.7542 Loss_G: 0.8045 acc: 76.6%\n",
      "[BATCH 109/149] Loss_D: 0.7642 Loss_G: 0.7933 acc: 56.2%\n",
      "[BATCH 110/149] Loss_D: 0.8111 Loss_G: 0.8074 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7813 Loss_G: 0.8060 acc: 56.2%\n",
      "[BATCH 112/149] Loss_D: 0.7512 Loss_G: 0.7943 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.8048 Loss_G: 0.7974 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.7770 Loss_G: 0.8004 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.8126 Loss_G: 0.8091 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.8191 Loss_G: 0.8245 acc: 68.8%\n",
      "[BATCH 117/149] Loss_D: 0.7418 Loss_G: 0.7955 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.8339 Loss_G: 0.8016 acc: 62.5%\n",
      "[BATCH 119/149] Loss_D: 0.7656 Loss_G: 0.8055 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.8378 Loss_G: 0.8303 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.8253 Loss_G: 0.8302 acc: 70.3%\n",
      "[BATCH 122/149] Loss_D: 0.8071 Loss_G: 0.8101 acc: 60.9%\n",
      "[EPOCH 3400] TEST ACC is : 76.4%\n",
      "[BATCH 123/149] Loss_D: 0.8124 Loss_G: 0.8069 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.7812 Loss_G: 0.8088 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7537 Loss_G: 0.8024 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.7719 Loss_G: 0.7948 acc: 64.1%\n",
      "[BATCH 127/149] Loss_D: 0.8112 Loss_G: 0.7962 acc: 68.8%\n",
      "[BATCH 128/149] Loss_D: 0.7996 Loss_G: 0.8003 acc: 57.8%\n",
      "[BATCH 129/149] Loss_D: 0.8192 Loss_G: 0.8028 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.8266 Loss_G: 0.8149 acc: 60.9%\n",
      "[BATCH 131/149] Loss_D: 0.7691 Loss_G: 0.8069 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.8134 Loss_G: 0.8089 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.8045 Loss_G: 0.8072 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7671 Loss_G: 0.8030 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.7738 Loss_G: 0.8017 acc: 56.2%\n",
      "[BATCH 136/149] Loss_D: 0.7875 Loss_G: 0.8022 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.8121 Loss_G: 0.8187 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7988 Loss_G: 0.8287 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.8118 Loss_G: 0.8290 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7581 Loss_G: 0.8103 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.8137 Loss_G: 0.8033 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7828 Loss_G: 0.8035 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.7864 Loss_G: 0.7992 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.8023 Loss_G: 0.8071 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.7687 Loss_G: 0.8000 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.7854 Loss_G: 0.8066 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.7956 Loss_G: 0.8163 acc: 67.2%\n",
      "[BATCH 148/149] Loss_D: 0.7975 Loss_G: 0.8028 acc: 59.4%\n",
      "[BATCH 149/149] Loss_D: 0.8169 Loss_G: 0.8064 acc: 57.8%\n",
      "-----THE [23/50] epoch end-----\n",
      "-----THE [24/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7863 Loss_G: 0.8040 acc: 54.7%\n",
      "[BATCH 2/149] Loss_D: 0.7742 Loss_G: 0.7955 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.7549 Loss_G: 0.7907 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.7609 Loss_G: 0.7871 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.7927 Loss_G: 0.7880 acc: 71.9%\n",
      "[BATCH 6/149] Loss_D: 0.7847 Loss_G: 0.7918 acc: 73.4%\n",
      "[BATCH 7/149] Loss_D: 0.8260 Loss_G: 0.8063 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7854 Loss_G: 0.8060 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.8322 Loss_G: 0.8244 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7916 Loss_G: 0.8240 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.7520 Loss_G: 0.8049 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.7962 Loss_G: 0.8071 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7803 Loss_G: 0.8088 acc: 60.9%\n",
      "[BATCH 14/149] Loss_D: 0.8217 Loss_G: 0.8131 acc: 54.7%\n",
      "[BATCH 15/149] Loss_D: 0.7676 Loss_G: 0.8041 acc: 56.2%\n",
      "[BATCH 16/149] Loss_D: 0.8024 Loss_G: 0.8132 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7618 Loss_G: 0.8047 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.7894 Loss_G: 0.7985 acc: 68.8%\n",
      "[BATCH 19/149] Loss_D: 0.7677 Loss_G: 0.7949 acc: 71.9%\n",
      "[BATCH 20/149] Loss_D: 0.7908 Loss_G: 0.7972 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.8039 Loss_G: 0.7953 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7763 Loss_G: 0.7971 acc: 60.9%\n",
      "[BATCH 23/149] Loss_D: 0.8145 Loss_G: 0.8096 acc: 65.6%\n",
      "[EPOCH 3450] TEST ACC is : 76.0%\n",
      "[BATCH 24/149] Loss_D: 0.7902 Loss_G: 0.8114 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.7870 Loss_G: 0.8168 acc: 53.1%\n",
      "[BATCH 26/149] Loss_D: 0.7717 Loss_G: 0.8094 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.8226 Loss_G: 0.8235 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.8060 Loss_G: 0.8120 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.8067 Loss_G: 0.8113 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.7520 Loss_G: 0.8093 acc: 76.6%\n",
      "[BATCH 31/149] Loss_D: 0.7788 Loss_G: 0.8010 acc: 56.2%\n",
      "[BATCH 32/149] Loss_D: 0.7883 Loss_G: 0.8003 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.7960 Loss_G: 0.8045 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7548 Loss_G: 0.8040 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7703 Loss_G: 0.8179 acc: 67.2%\n",
      "[BATCH 36/149] Loss_D: 0.7692 Loss_G: 0.8083 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.8121 Loss_G: 0.8159 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7708 Loss_G: 0.8041 acc: 62.5%\n",
      "[BATCH 39/149] Loss_D: 0.7786 Loss_G: 0.7902 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.8415 Loss_G: 0.8026 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.7651 Loss_G: 0.8108 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.7651 Loss_G: 0.8084 acc: 73.4%\n",
      "[BATCH 43/149] Loss_D: 0.7778 Loss_G: 0.8056 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7996 Loss_G: 0.8000 acc: 73.4%\n",
      "[BATCH 45/149] Loss_D: 0.8104 Loss_G: 0.8012 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8305 Loss_G: 0.8227 acc: 59.4%\n",
      "[BATCH 47/149] Loss_D: 0.7644 Loss_G: 0.8079 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.7819 Loss_G: 0.8073 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.7718 Loss_G: 0.7910 acc: 65.6%\n",
      "[BATCH 50/149] Loss_D: 0.8119 Loss_G: 0.8070 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7642 Loss_G: 0.8087 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.8314 Loss_G: 0.8250 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.7675 Loss_G: 0.8174 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.7781 Loss_G: 0.8045 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.7624 Loss_G: 0.8000 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7646 Loss_G: 0.8005 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.7442 Loss_G: 0.8008 acc: 73.4%\n",
      "[BATCH 58/149] Loss_D: 0.8642 Loss_G: 0.8127 acc: 68.8%\n",
      "[BATCH 59/149] Loss_D: 0.7936 Loss_G: 0.8094 acc: 53.1%\n",
      "[BATCH 60/149] Loss_D: 0.8282 Loss_G: 0.8086 acc: 73.4%\n",
      "[BATCH 61/149] Loss_D: 0.8244 Loss_G: 0.8210 acc: 56.2%\n",
      "[BATCH 62/149] Loss_D: 0.8357 Loss_G: 0.8236 acc: 78.1%\n",
      "[BATCH 63/149] Loss_D: 0.7695 Loss_G: 0.8175 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.7647 Loss_G: 0.8038 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7670 Loss_G: 0.7915 acc: 81.2%\n",
      "[BATCH 66/149] Loss_D: 0.7926 Loss_G: 0.8004 acc: 75.0%\n",
      "[BATCH 67/149] Loss_D: 0.7574 Loss_G: 0.7947 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7803 Loss_G: 0.8006 acc: 62.5%\n",
      "[BATCH 69/149] Loss_D: 0.7648 Loss_G: 0.7990 acc: 54.7%\n",
      "[BATCH 70/149] Loss_D: 0.8077 Loss_G: 0.8005 acc: 56.2%\n",
      "[BATCH 71/149] Loss_D: 0.7608 Loss_G: 0.8064 acc: 59.4%\n",
      "[BATCH 72/149] Loss_D: 0.7393 Loss_G: 0.8143 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.8371 Loss_G: 0.8532 acc: 71.9%\n",
      "[EPOCH 3500] TEST ACC is : 74.8%\n",
      "[BATCH 74/149] Loss_D: 0.8142 Loss_G: 0.8272 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.7752 Loss_G: 0.8171 acc: 73.4%\n",
      "[BATCH 76/149] Loss_D: 0.8011 Loss_G: 0.8042 acc: 62.5%\n",
      "[BATCH 77/149] Loss_D: 0.7909 Loss_G: 0.8150 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.7728 Loss_G: 0.8040 acc: 70.3%\n",
      "[BATCH 79/149] Loss_D: 0.7705 Loss_G: 0.7955 acc: 60.9%\n",
      "[BATCH 80/149] Loss_D: 0.7698 Loss_G: 0.7937 acc: 73.4%\n",
      "[BATCH 81/149] Loss_D: 0.7791 Loss_G: 0.7925 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.7887 Loss_G: 0.7953 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7752 Loss_G: 0.7909 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.7726 Loss_G: 0.7921 acc: 64.1%\n",
      "[BATCH 85/149] Loss_D: 0.8086 Loss_G: 0.8038 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7791 Loss_G: 0.8055 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.7997 Loss_G: 0.8172 acc: 70.3%\n",
      "[BATCH 88/149] Loss_D: 0.7490 Loss_G: 0.8076 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7906 Loss_G: 0.8065 acc: 54.7%\n",
      "[BATCH 90/149] Loss_D: 0.7617 Loss_G: 0.8056 acc: 62.5%\n",
      "[BATCH 91/149] Loss_D: 0.7903 Loss_G: 0.8017 acc: 53.1%\n",
      "[BATCH 92/149] Loss_D: 0.7812 Loss_G: 0.8042 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.7774 Loss_G: 0.7984 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7850 Loss_G: 0.7956 acc: 59.4%\n",
      "[BATCH 95/149] Loss_D: 0.8049 Loss_G: 0.8106 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.8282 Loss_G: 0.8216 acc: 67.2%\n",
      "[BATCH 97/149] Loss_D: 0.8004 Loss_G: 0.8130 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7502 Loss_G: 0.7996 acc: 71.9%\n",
      "[BATCH 99/149] Loss_D: 0.8459 Loss_G: 0.8146 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.7958 Loss_G: 0.8415 acc: 56.2%\n",
      "[BATCH 101/149] Loss_D: 0.8024 Loss_G: 0.8276 acc: 67.2%\n",
      "[BATCH 102/149] Loss_D: 0.7622 Loss_G: 0.8016 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.7271 Loss_G: 0.7908 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.7774 Loss_G: 0.7903 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.7536 Loss_G: 0.7892 acc: 48.4%\n",
      "[BATCH 106/149] Loss_D: 0.7729 Loss_G: 0.7880 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.7798 Loss_G: 0.7890 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.8141 Loss_G: 0.8040 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.7595 Loss_G: 0.7915 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.7938 Loss_G: 0.7936 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.8267 Loss_G: 0.8074 acc: 73.4%\n",
      "[BATCH 112/149] Loss_D: 0.7544 Loss_G: 0.8074 acc: 56.2%\n",
      "[BATCH 113/149] Loss_D: 0.7743 Loss_G: 0.7913 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7896 Loss_G: 0.7958 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.7530 Loss_G: 0.7913 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.8667 Loss_G: 0.8290 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7983 Loss_G: 0.8359 acc: 59.4%\n",
      "[BATCH 118/149] Loss_D: 0.8096 Loss_G: 0.8227 acc: 57.8%\n",
      "[BATCH 119/149] Loss_D: 0.7920 Loss_G: 0.8186 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7477 Loss_G: 0.8063 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.8016 Loss_G: 0.8113 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.7625 Loss_G: 0.8065 acc: 57.8%\n",
      "[BATCH 123/149] Loss_D: 0.8327 Loss_G: 0.8186 acc: 65.6%\n",
      "[EPOCH 3550] TEST ACC is : 74.2%\n",
      "[BATCH 124/149] Loss_D: 0.7907 Loss_G: 0.8118 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.8292 Loss_G: 0.8165 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.8557 Loss_G: 0.8231 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.8200 Loss_G: 0.8245 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.7527 Loss_G: 0.8122 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.7674 Loss_G: 0.8006 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7712 Loss_G: 0.8038 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7708 Loss_G: 0.7965 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.8303 Loss_G: 0.8173 acc: 76.6%\n",
      "[BATCH 133/149] Loss_D: 0.8277 Loss_G: 0.8265 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7815 Loss_G: 0.8168 acc: 67.2%\n",
      "[BATCH 135/149] Loss_D: 0.7340 Loss_G: 0.8014 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.7855 Loss_G: 0.8048 acc: 76.6%\n",
      "[BATCH 137/149] Loss_D: 0.8174 Loss_G: 0.8162 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.7859 Loss_G: 0.8114 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.7483 Loss_G: 0.8047 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.8182 Loss_G: 0.8120 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7975 Loss_G: 0.8163 acc: 71.9%\n",
      "[BATCH 142/149] Loss_D: 0.7738 Loss_G: 0.8076 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.8225 Loss_G: 0.8086 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.8070 Loss_G: 0.7987 acc: 57.8%\n",
      "[BATCH 145/149] Loss_D: 0.7701 Loss_G: 0.7907 acc: 64.1%\n",
      "[BATCH 146/149] Loss_D: 0.8146 Loss_G: 0.8018 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.7633 Loss_G: 0.7982 acc: 54.7%\n",
      "[BATCH 148/149] Loss_D: 0.7846 Loss_G: 0.7975 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.7530 Loss_G: 0.7859 acc: 67.2%\n",
      "-----THE [24/50] epoch end-----\n",
      "-----THE [25/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7556 Loss_G: 0.7820 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7930 Loss_G: 0.7908 acc: 75.0%\n",
      "[BATCH 3/149] Loss_D: 0.7700 Loss_G: 0.7984 acc: 73.4%\n",
      "[BATCH 4/149] Loss_D: 0.7603 Loss_G: 0.7899 acc: 50.0%\n",
      "[BATCH 5/149] Loss_D: 0.8322 Loss_G: 0.8080 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7780 Loss_G: 0.8070 acc: 65.6%\n",
      "[BATCH 7/149] Loss_D: 0.7886 Loss_G: 0.8140 acc: 54.7%\n",
      "[BATCH 8/149] Loss_D: 0.8113 Loss_G: 0.8179 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.7953 Loss_G: 0.8139 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7774 Loss_G: 0.8100 acc: 70.3%\n",
      "[BATCH 11/149] Loss_D: 0.7994 Loss_G: 0.8078 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7500 Loss_G: 0.7955 acc: 54.7%\n",
      "[BATCH 13/149] Loss_D: 0.8091 Loss_G: 0.8085 acc: 67.2%\n",
      "[BATCH 14/149] Loss_D: 0.7712 Loss_G: 0.8010 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.7712 Loss_G: 0.7986 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.7433 Loss_G: 0.7913 acc: 71.9%\n",
      "[BATCH 17/149] Loss_D: 0.7595 Loss_G: 0.7908 acc: 71.9%\n",
      "[BATCH 18/149] Loss_D: 0.7583 Loss_G: 0.7883 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.7965 Loss_G: 0.7942 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.8142 Loss_G: 0.8107 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.7744 Loss_G: 0.8051 acc: 75.0%\n",
      "[BATCH 22/149] Loss_D: 0.8042 Loss_G: 0.8003 acc: 65.6%\n",
      "[BATCH 23/149] Loss_D: 0.8039 Loss_G: 0.8120 acc: 70.3%\n",
      "[BATCH 24/149] Loss_D: 0.7859 Loss_G: 0.8102 acc: 64.1%\n",
      "[EPOCH 3600] TEST ACC is : 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.8792 Loss_G: 0.8339 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.7848 Loss_G: 0.8279 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.7888 Loss_G: 0.8004 acc: 60.9%\n",
      "[BATCH 28/149] Loss_D: 0.8157 Loss_G: 0.7952 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.7631 Loss_G: 0.7990 acc: 57.8%\n",
      "[BATCH 30/149] Loss_D: 0.7530 Loss_G: 0.7896 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.8558 Loss_G: 0.8246 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.8079 Loss_G: 0.8187 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.8300 Loss_G: 0.8158 acc: 67.2%\n",
      "[BATCH 34/149] Loss_D: 0.8238 Loss_G: 0.8144 acc: 76.6%\n",
      "[BATCH 35/149] Loss_D: 0.7847 Loss_G: 0.8144 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7692 Loss_G: 0.8166 acc: 67.2%\n",
      "[BATCH 37/149] Loss_D: 0.8119 Loss_G: 0.8066 acc: 76.6%\n",
      "[BATCH 38/149] Loss_D: 0.8214 Loss_G: 0.8258 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.7654 Loss_G: 0.8181 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.7621 Loss_G: 0.8032 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7957 Loss_G: 0.8050 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.7928 Loss_G: 0.7931 acc: 59.4%\n",
      "[BATCH 43/149] Loss_D: 0.7813 Loss_G: 0.7948 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.8246 Loss_G: 0.8169 acc: 70.3%\n",
      "[BATCH 45/149] Loss_D: 0.7604 Loss_G: 0.8149 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.7877 Loss_G: 0.8147 acc: 73.4%\n",
      "[BATCH 47/149] Loss_D: 0.7461 Loss_G: 0.8047 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.7932 Loss_G: 0.8139 acc: 60.9%\n",
      "[BATCH 49/149] Loss_D: 0.7870 Loss_G: 0.8179 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7823 Loss_G: 0.8135 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.7680 Loss_G: 0.8152 acc: 50.0%\n",
      "[BATCH 52/149] Loss_D: 0.8111 Loss_G: 0.8171 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.7994 Loss_G: 0.8210 acc: 57.8%\n",
      "[BATCH 54/149] Loss_D: 0.7987 Loss_G: 0.8051 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.7802 Loss_G: 0.7977 acc: 53.1%\n",
      "[BATCH 56/149] Loss_D: 0.8197 Loss_G: 0.8014 acc: 75.0%\n",
      "[BATCH 57/149] Loss_D: 0.7996 Loss_G: 0.8106 acc: 62.5%\n",
      "[BATCH 58/149] Loss_D: 0.8207 Loss_G: 0.8146 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.8088 Loss_G: 0.8369 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.8029 Loss_G: 0.8159 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.7618 Loss_G: 0.8020 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.7678 Loss_G: 0.7917 acc: 71.9%\n",
      "[BATCH 63/149] Loss_D: 0.7632 Loss_G: 0.7852 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7787 Loss_G: 0.7926 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.8072 Loss_G: 0.8081 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.7653 Loss_G: 0.7916 acc: 57.8%\n",
      "[BATCH 67/149] Loss_D: 0.7637 Loss_G: 0.7834 acc: 71.9%\n",
      "[BATCH 68/149] Loss_D: 0.7323 Loss_G: 0.7833 acc: 59.4%\n",
      "[BATCH 69/149] Loss_D: 0.7769 Loss_G: 0.7818 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.7276 Loss_G: 0.7818 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.8050 Loss_G: 0.7963 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7872 Loss_G: 0.7999 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7870 Loss_G: 0.8040 acc: 70.3%\n",
      "[BATCH 74/149] Loss_D: 0.7995 Loss_G: 0.8091 acc: 68.8%\n",
      "[EPOCH 3650] TEST ACC is : 75.0%\n",
      "[BATCH 75/149] Loss_D: 0.8041 Loss_G: 0.8220 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.8149 Loss_G: 0.8268 acc: 73.4%\n",
      "[BATCH 77/149] Loss_D: 0.7822 Loss_G: 0.8297 acc: 65.6%\n",
      "[BATCH 78/149] Loss_D: 0.8021 Loss_G: 0.8175 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.8000 Loss_G: 0.8201 acc: 65.6%\n",
      "[BATCH 80/149] Loss_D: 0.7433 Loss_G: 0.8053 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.8026 Loss_G: 0.8066 acc: 50.0%\n",
      "[BATCH 82/149] Loss_D: 0.7885 Loss_G: 0.8064 acc: 56.2%\n",
      "[BATCH 83/149] Loss_D: 0.8034 Loss_G: 0.8239 acc: 71.9%\n",
      "[BATCH 84/149] Loss_D: 0.7607 Loss_G: 0.8137 acc: 75.0%\n",
      "[BATCH 85/149] Loss_D: 0.7874 Loss_G: 0.8021 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.8582 Loss_G: 0.8215 acc: 70.3%\n",
      "[BATCH 87/149] Loss_D: 0.7941 Loss_G: 0.8172 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7772 Loss_G: 0.8164 acc: 62.5%\n",
      "[BATCH 89/149] Loss_D: 0.7591 Loss_G: 0.7986 acc: 50.0%\n",
      "[BATCH 90/149] Loss_D: 0.7937 Loss_G: 0.7918 acc: 57.8%\n",
      "[BATCH 91/149] Loss_D: 0.7843 Loss_G: 0.8025 acc: 71.9%\n",
      "[BATCH 92/149] Loss_D: 0.8174 Loss_G: 0.8244 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.7675 Loss_G: 0.8064 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.8159 Loss_G: 0.7969 acc: 73.4%\n",
      "[BATCH 95/149] Loss_D: 0.8296 Loss_G: 0.7996 acc: 65.6%\n",
      "[BATCH 96/149] Loss_D: 0.8015 Loss_G: 0.8159 acc: 56.2%\n",
      "[BATCH 97/149] Loss_D: 0.7586 Loss_G: 0.7972 acc: 76.6%\n",
      "[BATCH 98/149] Loss_D: 0.7430 Loss_G: 0.7937 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.7610 Loss_G: 0.7855 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.7752 Loss_G: 0.7913 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.7528 Loss_G: 0.7852 acc: 65.6%\n",
      "[BATCH 102/149] Loss_D: 0.7586 Loss_G: 0.7898 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7644 Loss_G: 0.7917 acc: 81.2%\n",
      "[BATCH 104/149] Loss_D: 0.8214 Loss_G: 0.8178 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.8307 Loss_G: 0.8290 acc: 75.0%\n",
      "[BATCH 106/149] Loss_D: 0.8107 Loss_G: 0.8274 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7474 Loss_G: 0.8115 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.7870 Loss_G: 0.8007 acc: 53.1%\n",
      "[BATCH 109/149] Loss_D: 0.7738 Loss_G: 0.8022 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.7927 Loss_G: 0.8087 acc: 59.4%\n",
      "[BATCH 111/149] Loss_D: 0.8032 Loss_G: 0.8198 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8594 Loss_G: 0.8276 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.8033 Loss_G: 0.8186 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7791 Loss_G: 0.8173 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.7876 Loss_G: 0.8147 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.7739 Loss_G: 0.8092 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7869 Loss_G: 0.8121 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.7836 Loss_G: 0.8022 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.7544 Loss_G: 0.7931 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.8294 Loss_G: 0.8100 acc: 76.6%\n",
      "[BATCH 121/149] Loss_D: 0.8086 Loss_G: 0.8169 acc: 70.3%\n",
      "[BATCH 122/149] Loss_D: 0.7805 Loss_G: 0.8054 acc: 79.7%\n",
      "[BATCH 123/149] Loss_D: 0.7686 Loss_G: 0.7995 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.7970 Loss_G: 0.7991 acc: 64.1%\n",
      "[EPOCH 3700] TEST ACC is : 73.6%\n",
      "[BATCH 125/149] Loss_D: 0.8215 Loss_G: 0.8099 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7728 Loss_G: 0.8074 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7665 Loss_G: 0.7856 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.7737 Loss_G: 0.7842 acc: 65.6%\n",
      "[BATCH 129/149] Loss_D: 0.8178 Loss_G: 0.8055 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.7725 Loss_G: 0.8183 acc: 65.6%\n",
      "[BATCH 131/149] Loss_D: 0.8043 Loss_G: 0.8247 acc: 65.6%\n",
      "[BATCH 132/149] Loss_D: 0.7628 Loss_G: 0.8149 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7652 Loss_G: 0.8050 acc: 64.1%\n",
      "[BATCH 134/149] Loss_D: 0.7844 Loss_G: 0.8116 acc: 56.2%\n",
      "[BATCH 135/149] Loss_D: 0.7929 Loss_G: 0.8166 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7948 Loss_G: 0.8192 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.7657 Loss_G: 0.8110 acc: 57.8%\n",
      "[BATCH 138/149] Loss_D: 0.8097 Loss_G: 0.8171 acc: 54.7%\n",
      "[BATCH 139/149] Loss_D: 0.7814 Loss_G: 0.8149 acc: 59.4%\n",
      "[BATCH 140/149] Loss_D: 0.7645 Loss_G: 0.8084 acc: 56.2%\n",
      "[BATCH 141/149] Loss_D: 0.7490 Loss_G: 0.8043 acc: 70.3%\n",
      "[BATCH 142/149] Loss_D: 0.7435 Loss_G: 0.7872 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7839 Loss_G: 0.7905 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7716 Loss_G: 0.8012 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.7651 Loss_G: 0.7880 acc: 70.3%\n",
      "[BATCH 146/149] Loss_D: 0.8402 Loss_G: 0.7957 acc: 73.4%\n",
      "[BATCH 147/149] Loss_D: 0.7770 Loss_G: 0.7935 acc: 67.2%\n",
      "[BATCH 148/149] Loss_D: 0.7836 Loss_G: 0.7934 acc: 64.1%\n",
      "[BATCH 149/149] Loss_D: 0.7911 Loss_G: 0.8093 acc: 75.0%\n",
      "-----THE [25/50] epoch end-----\n",
      "-----THE [26/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8205 Loss_G: 0.8382 acc: 75.0%\n",
      "[BATCH 2/149] Loss_D: 0.7744 Loss_G: 0.8235 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.8099 Loss_G: 0.8148 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.7791 Loss_G: 0.8064 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7710 Loss_G: 0.7975 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7906 Loss_G: 0.7959 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.7986 Loss_G: 0.7969 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.8205 Loss_G: 0.8126 acc: 56.2%\n",
      "[BATCH 9/149] Loss_D: 0.7835 Loss_G: 0.8054 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7718 Loss_G: 0.7956 acc: 59.4%\n",
      "[BATCH 11/149] Loss_D: 0.8738 Loss_G: 0.8186 acc: 70.3%\n",
      "[BATCH 12/149] Loss_D: 0.7772 Loss_G: 0.8172 acc: 68.8%\n",
      "[BATCH 13/149] Loss_D: 0.8298 Loss_G: 0.8258 acc: 76.6%\n",
      "[BATCH 14/149] Loss_D: 0.7801 Loss_G: 0.8201 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.8279 Loss_G: 0.8263 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7853 Loss_G: 0.8293 acc: 65.6%\n",
      "[BATCH 17/149] Loss_D: 0.8254 Loss_G: 0.8332 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.7692 Loss_G: 0.8224 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7836 Loss_G: 0.8174 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.7888 Loss_G: 0.8156 acc: 76.6%\n",
      "[BATCH 21/149] Loss_D: 0.7676 Loss_G: 0.8028 acc: 65.6%\n",
      "[BATCH 22/149] Loss_D: 0.7898 Loss_G: 0.8020 acc: 75.0%\n",
      "[BATCH 23/149] Loss_D: 0.7739 Loss_G: 0.7982 acc: 70.3%\n",
      "[BATCH 24/149] Loss_D: 0.8053 Loss_G: 0.8041 acc: 67.2%\n",
      "[BATCH 25/149] Loss_D: 0.7591 Loss_G: 0.8000 acc: 64.1%\n",
      "[EPOCH 3750] TEST ACC is : 75.6%\n",
      "[BATCH 26/149] Loss_D: 0.8170 Loss_G: 0.8135 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.8151 Loss_G: 0.8247 acc: 57.8%\n",
      "[BATCH 28/149] Loss_D: 0.8029 Loss_G: 0.8158 acc: 67.2%\n",
      "[BATCH 29/149] Loss_D: 0.8308 Loss_G: 0.8182 acc: 67.2%\n",
      "[BATCH 30/149] Loss_D: 0.7945 Loss_G: 0.8078 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7790 Loss_G: 0.8057 acc: 79.7%\n",
      "[BATCH 32/149] Loss_D: 0.7937 Loss_G: 0.8031 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.8070 Loss_G: 0.8036 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7840 Loss_G: 0.8165 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7365 Loss_G: 0.7955 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.7580 Loss_G: 0.7928 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7604 Loss_G: 0.7866 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.8480 Loss_G: 0.8032 acc: 59.4%\n",
      "[BATCH 39/149] Loss_D: 0.7725 Loss_G: 0.8012 acc: 60.9%\n",
      "[BATCH 40/149] Loss_D: 0.7842 Loss_G: 0.7987 acc: 62.5%\n",
      "[BATCH 41/149] Loss_D: 0.7439 Loss_G: 0.7899 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.8017 Loss_G: 0.8020 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.7840 Loss_G: 0.8157 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7694 Loss_G: 0.8024 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.7736 Loss_G: 0.7990 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.8280 Loss_G: 0.8052 acc: 64.1%\n",
      "[BATCH 47/149] Loss_D: 0.7460 Loss_G: 0.7967 acc: 75.0%\n",
      "[BATCH 48/149] Loss_D: 0.7610 Loss_G: 0.7953 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.7660 Loss_G: 0.8004 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.8263 Loss_G: 0.8137 acc: 65.6%\n",
      "[BATCH 51/149] Loss_D: 0.7838 Loss_G: 0.8206 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.7884 Loss_G: 0.8092 acc: 75.0%\n",
      "[BATCH 53/149] Loss_D: 0.7444 Loss_G: 0.7910 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.8213 Loss_G: 0.8047 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.7704 Loss_G: 0.8000 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.7625 Loss_G: 0.7904 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.7918 Loss_G: 0.7949 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7887 Loss_G: 0.7959 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.8298 Loss_G: 0.8196 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7663 Loss_G: 0.8238 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.7732 Loss_G: 0.8236 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.7563 Loss_G: 0.8062 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.7946 Loss_G: 0.8052 acc: 73.4%\n",
      "[BATCH 64/149] Loss_D: 0.7986 Loss_G: 0.8085 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.7957 Loss_G: 0.8053 acc: 57.8%\n",
      "[BATCH 66/149] Loss_D: 0.7833 Loss_G: 0.8149 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7702 Loss_G: 0.7938 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7684 Loss_G: 0.7905 acc: 56.2%\n",
      "[BATCH 69/149] Loss_D: 0.8140 Loss_G: 0.7981 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.7742 Loss_G: 0.8006 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.7576 Loss_G: 0.7969 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.7994 Loss_G: 0.7995 acc: 51.6%\n",
      "[BATCH 73/149] Loss_D: 0.7757 Loss_G: 0.8036 acc: 60.9%\n",
      "[BATCH 74/149] Loss_D: 0.7707 Loss_G: 0.7997 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.7626 Loss_G: 0.7927 acc: 62.5%\n",
      "[EPOCH 3800] TEST ACC is : 74.4%\n",
      "[BATCH 76/149] Loss_D: 0.7267 Loss_G: 0.7835 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.8036 Loss_G: 0.7929 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.7766 Loss_G: 0.7949 acc: 56.2%\n",
      "[BATCH 79/149] Loss_D: 0.8266 Loss_G: 0.8118 acc: 62.5%\n",
      "[BATCH 80/149] Loss_D: 0.7799 Loss_G: 0.8039 acc: 68.8%\n",
      "[BATCH 81/149] Loss_D: 0.7647 Loss_G: 0.7932 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.7509 Loss_G: 0.7857 acc: 54.7%\n",
      "[BATCH 83/149] Loss_D: 0.8277 Loss_G: 0.7973 acc: 51.6%\n",
      "[BATCH 84/149] Loss_D: 0.7753 Loss_G: 0.7994 acc: 54.7%\n",
      "[BATCH 85/149] Loss_D: 0.7640 Loss_G: 0.7974 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.7537 Loss_G: 0.7971 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.7965 Loss_G: 0.7978 acc: 57.8%\n",
      "[BATCH 88/149] Loss_D: 0.7838 Loss_G: 0.7991 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.7673 Loss_G: 0.8020 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.8131 Loss_G: 0.8168 acc: 56.2%\n",
      "[BATCH 91/149] Loss_D: 0.7707 Loss_G: 0.8077 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.7754 Loss_G: 0.7991 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.8594 Loss_G: 0.8215 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.7940 Loss_G: 0.8293 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7889 Loss_G: 0.8185 acc: 70.3%\n",
      "[BATCH 96/149] Loss_D: 0.7877 Loss_G: 0.8080 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7842 Loss_G: 0.8048 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7854 Loss_G: 0.8016 acc: 68.8%\n",
      "[BATCH 99/149] Loss_D: 0.7807 Loss_G: 0.8081 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.7588 Loss_G: 0.8079 acc: 73.4%\n",
      "[BATCH 101/149] Loss_D: 0.7466 Loss_G: 0.8010 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.7715 Loss_G: 0.7999 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7931 Loss_G: 0.8075 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7887 Loss_G: 0.8049 acc: 59.4%\n",
      "[BATCH 105/149] Loss_D: 0.8343 Loss_G: 0.8240 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7404 Loss_G: 0.8103 acc: 68.8%\n",
      "[BATCH 107/149] Loss_D: 0.8232 Loss_G: 0.8137 acc: 78.1%\n",
      "[BATCH 108/149] Loss_D: 0.7711 Loss_G: 0.7968 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.8036 Loss_G: 0.8050 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.7771 Loss_G: 0.8049 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.7610 Loss_G: 0.8026 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.7790 Loss_G: 0.8008 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.7767 Loss_G: 0.8083 acc: 73.4%\n",
      "[BATCH 114/149] Loss_D: 0.7622 Loss_G: 0.7990 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.7644 Loss_G: 0.7951 acc: 57.8%\n",
      "[BATCH 116/149] Loss_D: 0.7978 Loss_G: 0.7950 acc: 71.9%\n",
      "[BATCH 117/149] Loss_D: 0.8081 Loss_G: 0.8049 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7974 Loss_G: 0.8098 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.8146 Loss_G: 0.8225 acc: 70.3%\n",
      "[BATCH 120/149] Loss_D: 0.7545 Loss_G: 0.8207 acc: 78.1%\n",
      "[BATCH 121/149] Loss_D: 0.8669 Loss_G: 0.8374 acc: 65.6%\n",
      "[BATCH 122/149] Loss_D: 0.7992 Loss_G: 0.8327 acc: 56.2%\n",
      "[BATCH 123/149] Loss_D: 0.7915 Loss_G: 0.8142 acc: 73.4%\n",
      "[BATCH 124/149] Loss_D: 0.7864 Loss_G: 0.8147 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.8184 Loss_G: 0.8172 acc: 78.1%\n",
      "[EPOCH 3850] TEST ACC is : 74.6%\n",
      "[BATCH 126/149] Loss_D: 0.8430 Loss_G: 0.8314 acc: 67.2%\n",
      "[BATCH 127/149] Loss_D: 0.7705 Loss_G: 0.8068 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7705 Loss_G: 0.7959 acc: 57.8%\n",
      "[BATCH 129/149] Loss_D: 0.8107 Loss_G: 0.8034 acc: 78.1%\n",
      "[BATCH 130/149] Loss_D: 0.7907 Loss_G: 0.8131 acc: 53.1%\n",
      "[BATCH 131/149] Loss_D: 0.7644 Loss_G: 0.8090 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.7931 Loss_G: 0.8012 acc: 59.4%\n",
      "[BATCH 133/149] Loss_D: 0.8078 Loss_G: 0.8052 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7878 Loss_G: 0.8064 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.7854 Loss_G: 0.8066 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.8040 Loss_G: 0.8094 acc: 65.6%\n",
      "[BATCH 137/149] Loss_D: 0.7547 Loss_G: 0.7923 acc: 70.3%\n",
      "[BATCH 138/149] Loss_D: 0.7838 Loss_G: 0.7889 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.7690 Loss_G: 0.7863 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7778 Loss_G: 0.7871 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.7912 Loss_G: 0.7825 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7587 Loss_G: 0.7914 acc: 59.4%\n",
      "[BATCH 143/149] Loss_D: 0.8440 Loss_G: 0.8116 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7463 Loss_G: 0.7968 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.7452 Loss_G: 0.7867 acc: 71.9%\n",
      "[BATCH 146/149] Loss_D: 0.8090 Loss_G: 0.8003 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.8088 Loss_G: 0.8092 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.7901 Loss_G: 0.8036 acc: 59.4%\n",
      "[BATCH 149/149] Loss_D: 0.7828 Loss_G: 0.7997 acc: 65.6%\n",
      "-----THE [26/50] epoch end-----\n",
      "-----THE [27/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7793 Loss_G: 0.7961 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.8320 Loss_G: 0.8036 acc: 76.6%\n",
      "[BATCH 3/149] Loss_D: 0.7737 Loss_G: 0.7998 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.7630 Loss_G: 0.7928 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.8416 Loss_G: 0.8059 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.7577 Loss_G: 0.8112 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.7403 Loss_G: 0.7908 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.7543 Loss_G: 0.7840 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.7698 Loss_G: 0.7860 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.7512 Loss_G: 0.7883 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.8220 Loss_G: 0.8081 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.7658 Loss_G: 0.7996 acc: 51.6%\n",
      "[BATCH 13/149] Loss_D: 0.8109 Loss_G: 0.8066 acc: 65.6%\n",
      "[BATCH 14/149] Loss_D: 0.7689 Loss_G: 0.8030 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7875 Loss_G: 0.8099 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7607 Loss_G: 0.8043 acc: 59.4%\n",
      "[BATCH 17/149] Loss_D: 0.8033 Loss_G: 0.8096 acc: 54.7%\n",
      "[BATCH 18/149] Loss_D: 0.8010 Loss_G: 0.8116 acc: 71.9%\n",
      "[BATCH 19/149] Loss_D: 0.7811 Loss_G: 0.8088 acc: 53.1%\n",
      "[BATCH 20/149] Loss_D: 0.7692 Loss_G: 0.7969 acc: 75.0%\n",
      "[BATCH 21/149] Loss_D: 0.7467 Loss_G: 0.7798 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7459 Loss_G: 0.7781 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.8335 Loss_G: 0.7962 acc: 53.1%\n",
      "[BATCH 24/149] Loss_D: 0.7964 Loss_G: 0.8059 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.8133 Loss_G: 0.8132 acc: 67.2%\n",
      "[BATCH 26/149] Loss_D: 0.7743 Loss_G: 0.8028 acc: 62.5%\n",
      "[EPOCH 3900] TEST ACC is : 74.8%\n",
      "[BATCH 27/149] Loss_D: 0.7740 Loss_G: 0.7979 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.7940 Loss_G: 0.7997 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.7704 Loss_G: 0.7963 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.8926 Loss_G: 0.8266 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7703 Loss_G: 0.8213 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7876 Loss_G: 0.8075 acc: 51.6%\n",
      "[BATCH 33/149] Loss_D: 0.7810 Loss_G: 0.7997 acc: 54.7%\n",
      "[BATCH 34/149] Loss_D: 0.7769 Loss_G: 0.8011 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7908 Loss_G: 0.8087 acc: 71.9%\n",
      "[BATCH 36/149] Loss_D: 0.7829 Loss_G: 0.8161 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.7499 Loss_G: 0.8047 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.8055 Loss_G: 0.8045 acc: 70.3%\n",
      "[BATCH 39/149] Loss_D: 0.7490 Loss_G: 0.7924 acc: 76.6%\n",
      "[BATCH 40/149] Loss_D: 0.7697 Loss_G: 0.7864 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.8314 Loss_G: 0.8028 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.7589 Loss_G: 0.7946 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7952 Loss_G: 0.7917 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7843 Loss_G: 0.7967 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7599 Loss_G: 0.7835 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.8213 Loss_G: 0.8011 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7735 Loss_G: 0.8015 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.7507 Loss_G: 0.7918 acc: 57.8%\n",
      "[BATCH 49/149] Loss_D: 0.7743 Loss_G: 0.7883 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.7632 Loss_G: 0.7988 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.7736 Loss_G: 0.7995 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7971 Loss_G: 0.8001 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.8406 Loss_G: 0.8194 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.8054 Loss_G: 0.8094 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.7548 Loss_G: 0.8070 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.8024 Loss_G: 0.8072 acc: 59.4%\n",
      "[BATCH 57/149] Loss_D: 0.7832 Loss_G: 0.8089 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.7441 Loss_G: 0.8002 acc: 53.1%\n",
      "[BATCH 59/149] Loss_D: 0.8390 Loss_G: 0.8237 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7883 Loss_G: 0.8096 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.7670 Loss_G: 0.8000 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.7818 Loss_G: 0.7901 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.8171 Loss_G: 0.8014 acc: 71.9%\n",
      "[BATCH 64/149] Loss_D: 0.7892 Loss_G: 0.8025 acc: 64.1%\n",
      "[BATCH 65/149] Loss_D: 0.8111 Loss_G: 0.8154 acc: 68.8%\n",
      "[BATCH 66/149] Loss_D: 0.8218 Loss_G: 0.8296 acc: 59.4%\n",
      "[BATCH 67/149] Loss_D: 0.8228 Loss_G: 0.8256 acc: 62.5%\n",
      "[BATCH 68/149] Loss_D: 0.7601 Loss_G: 0.8255 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.8016 Loss_G: 0.8152 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.7933 Loss_G: 0.8049 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.8078 Loss_G: 0.8228 acc: 73.4%\n",
      "[BATCH 72/149] Loss_D: 0.7913 Loss_G: 0.8075 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7857 Loss_G: 0.8049 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7835 Loss_G: 0.7931 acc: 56.2%\n",
      "[BATCH 75/149] Loss_D: 0.8011 Loss_G: 0.7977 acc: 78.1%\n",
      "[BATCH 76/149] Loss_D: 0.8146 Loss_G: 0.8120 acc: 64.1%\n",
      "[EPOCH 3950] TEST ACC is : 74.8%\n",
      "[BATCH 77/149] Loss_D: 0.7485 Loss_G: 0.8066 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.7820 Loss_G: 0.8002 acc: 67.2%\n",
      "[BATCH 79/149] Loss_D: 0.8013 Loss_G: 0.8033 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7435 Loss_G: 0.7878 acc: 75.0%\n",
      "[BATCH 81/149] Loss_D: 0.7482 Loss_G: 0.7792 acc: 67.2%\n",
      "[BATCH 82/149] Loss_D: 0.7735 Loss_G: 0.7872 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7630 Loss_G: 0.7911 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.8623 Loss_G: 0.8100 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.7440 Loss_G: 0.8065 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.8137 Loss_G: 0.7948 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.8432 Loss_G: 0.8024 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7671 Loss_G: 0.8061 acc: 75.0%\n",
      "[BATCH 89/149] Loss_D: 0.7313 Loss_G: 0.8009 acc: 73.4%\n",
      "[BATCH 90/149] Loss_D: 0.7833 Loss_G: 0.7955 acc: 62.5%\n",
      "[BATCH 91/149] Loss_D: 0.7741 Loss_G: 0.8043 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.7902 Loss_G: 0.8053 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.8064 Loss_G: 0.8154 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7763 Loss_G: 0.8064 acc: 71.9%\n",
      "[BATCH 95/149] Loss_D: 0.8081 Loss_G: 0.8129 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7756 Loss_G: 0.8208 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.8063 Loss_G: 0.8238 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.8090 Loss_G: 0.8158 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7650 Loss_G: 0.8176 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.7917 Loss_G: 0.8215 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.7790 Loss_G: 0.8208 acc: 71.9%\n",
      "[BATCH 102/149] Loss_D: 0.8675 Loss_G: 0.8506 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.7878 Loss_G: 0.8271 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.7973 Loss_G: 0.8152 acc: 68.8%\n",
      "[BATCH 105/149] Loss_D: 0.7682 Loss_G: 0.8026 acc: 53.1%\n",
      "[BATCH 106/149] Loss_D: 0.7783 Loss_G: 0.7963 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.7987 Loss_G: 0.7992 acc: 53.1%\n",
      "[BATCH 108/149] Loss_D: 0.7768 Loss_G: 0.8012 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.8096 Loss_G: 0.8035 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.8051 Loss_G: 0.8107 acc: 73.4%\n",
      "[BATCH 111/149] Loss_D: 0.7755 Loss_G: 0.8110 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7494 Loss_G: 0.7981 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7383 Loss_G: 0.7907 acc: 78.1%\n",
      "[BATCH 114/149] Loss_D: 0.8634 Loss_G: 0.8134 acc: 60.9%\n",
      "[BATCH 115/149] Loss_D: 0.8376 Loss_G: 0.8279 acc: 70.3%\n",
      "[BATCH 116/149] Loss_D: 0.7735 Loss_G: 0.8124 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.7882 Loss_G: 0.7978 acc: 71.9%\n",
      "[BATCH 118/149] Loss_D: 0.7570 Loss_G: 0.8012 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.7833 Loss_G: 0.8058 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7615 Loss_G: 0.8073 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.8205 Loss_G: 0.8229 acc: 60.9%\n",
      "[BATCH 122/149] Loss_D: 0.7655 Loss_G: 0.8211 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.7975 Loss_G: 0.8139 acc: 57.8%\n",
      "[BATCH 124/149] Loss_D: 0.7664 Loss_G: 0.8127 acc: 71.9%\n",
      "[BATCH 125/149] Loss_D: 0.7680 Loss_G: 0.8042 acc: 75.0%\n",
      "[BATCH 126/149] Loss_D: 0.8271 Loss_G: 0.8150 acc: 65.6%\n",
      "[EPOCH 4000] TEST ACC is : 74.8%\n",
      "[BATCH 127/149] Loss_D: 0.7878 Loss_G: 0.8058 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.8084 Loss_G: 0.8070 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.7838 Loss_G: 0.8015 acc: 75.0%\n",
      "[BATCH 130/149] Loss_D: 0.7699 Loss_G: 0.7955 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7753 Loss_G: 0.7930 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.7592 Loss_G: 0.7930 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.7839 Loss_G: 0.7974 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.7959 Loss_G: 0.8001 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.8322 Loss_G: 0.8019 acc: 71.9%\n",
      "[BATCH 136/149] Loss_D: 0.8390 Loss_G: 0.8178 acc: 56.2%\n",
      "[BATCH 137/149] Loss_D: 0.7506 Loss_G: 0.7979 acc: 54.7%\n",
      "[BATCH 138/149] Loss_D: 0.7600 Loss_G: 0.7911 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.7800 Loss_G: 0.7874 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7759 Loss_G: 0.7970 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.8177 Loss_G: 0.8045 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.7901 Loss_G: 0.8079 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.8043 Loss_G: 0.8111 acc: 54.7%\n",
      "[BATCH 144/149] Loss_D: 0.7934 Loss_G: 0.8012 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.7998 Loss_G: 0.8034 acc: 68.8%\n",
      "[BATCH 146/149] Loss_D: 0.7764 Loss_G: 0.8033 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7805 Loss_G: 0.8061 acc: 64.1%\n",
      "[BATCH 148/149] Loss_D: 0.7584 Loss_G: 0.7999 acc: 59.4%\n",
      "[BATCH 149/149] Loss_D: 0.7941 Loss_G: 0.8011 acc: 71.9%\n",
      "-----THE [27/50] epoch end-----\n",
      "-----THE [28/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7989 Loss_G: 0.8208 acc: 59.4%\n",
      "[BATCH 2/149] Loss_D: 0.8570 Loss_G: 0.8352 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.7942 Loss_G: 0.8313 acc: 73.4%\n",
      "[BATCH 4/149] Loss_D: 0.8115 Loss_G: 0.8206 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.7909 Loss_G: 0.8185 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.8133 Loss_G: 0.8209 acc: 59.4%\n",
      "[BATCH 7/149] Loss_D: 0.7510 Loss_G: 0.8100 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.7912 Loss_G: 0.8063 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.8293 Loss_G: 0.8114 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7513 Loss_G: 0.8074 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.7728 Loss_G: 0.7983 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.7868 Loss_G: 0.7965 acc: 73.4%\n",
      "[BATCH 13/149] Loss_D: 0.8115 Loss_G: 0.7999 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7543 Loss_G: 0.7953 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7946 Loss_G: 0.8032 acc: 56.2%\n",
      "[BATCH 16/149] Loss_D: 0.8150 Loss_G: 0.8329 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.8132 Loss_G: 0.8344 acc: 71.9%\n",
      "[BATCH 18/149] Loss_D: 0.7721 Loss_G: 0.8144 acc: 57.8%\n",
      "[BATCH 19/149] Loss_D: 0.7780 Loss_G: 0.7982 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.8083 Loss_G: 0.8066 acc: 73.4%\n",
      "[BATCH 21/149] Loss_D: 0.7548 Loss_G: 0.8086 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.7555 Loss_G: 0.7994 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7975 Loss_G: 0.7911 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.8042 Loss_G: 0.7978 acc: 59.4%\n",
      "[BATCH 25/149] Loss_D: 0.7482 Loss_G: 0.7929 acc: 70.3%\n",
      "[BATCH 26/149] Loss_D: 0.7843 Loss_G: 0.7934 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7921 Loss_G: 0.7970 acc: 67.2%\n",
      "[EPOCH 4050] TEST ACC is : 75.4%\n",
      "[BATCH 28/149] Loss_D: 0.7993 Loss_G: 0.8102 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7576 Loss_G: 0.7923 acc: 57.8%\n",
      "[BATCH 30/149] Loss_D: 0.7531 Loss_G: 0.7929 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.8007 Loss_G: 0.8062 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.7401 Loss_G: 0.8042 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.7928 Loss_G: 0.7963 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.8249 Loss_G: 0.8145 acc: 57.8%\n",
      "[BATCH 35/149] Loss_D: 0.8007 Loss_G: 0.8149 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7508 Loss_G: 0.8056 acc: 71.9%\n",
      "[BATCH 37/149] Loss_D: 0.7608 Loss_G: 0.7984 acc: 54.7%\n",
      "[BATCH 38/149] Loss_D: 0.8505 Loss_G: 0.8140 acc: 67.2%\n",
      "[BATCH 39/149] Loss_D: 0.7858 Loss_G: 0.8184 acc: 56.2%\n",
      "[BATCH 40/149] Loss_D: 0.7770 Loss_G: 0.8082 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.7255 Loss_G: 0.7937 acc: 59.4%\n",
      "[BATCH 42/149] Loss_D: 0.7589 Loss_G: 0.7891 acc: 59.4%\n",
      "[BATCH 43/149] Loss_D: 0.8131 Loss_G: 0.7961 acc: 54.7%\n",
      "[BATCH 44/149] Loss_D: 0.7606 Loss_G: 0.8036 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7743 Loss_G: 0.8006 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.7559 Loss_G: 0.7857 acc: 57.8%\n",
      "[BATCH 47/149] Loss_D: 0.7671 Loss_G: 0.7848 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7737 Loss_G: 0.7899 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.7570 Loss_G: 0.7933 acc: 65.6%\n",
      "[BATCH 50/149] Loss_D: 0.8011 Loss_G: 0.7969 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7662 Loss_G: 0.8022 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.8100 Loss_G: 0.8162 acc: 62.5%\n",
      "[BATCH 53/149] Loss_D: 0.8234 Loss_G: 0.8176 acc: 57.8%\n",
      "[BATCH 54/149] Loss_D: 0.7644 Loss_G: 0.7961 acc: 73.4%\n",
      "[BATCH 55/149] Loss_D: 0.7807 Loss_G: 0.7939 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.8536 Loss_G: 0.8119 acc: 70.3%\n",
      "[BATCH 57/149] Loss_D: 0.8090 Loss_G: 0.8249 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.7987 Loss_G: 0.8223 acc: 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.8099 Loss_G: 0.8260 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7337 Loss_G: 0.8060 acc: 56.2%\n",
      "[BATCH 61/149] Loss_D: 0.8757 Loss_G: 0.8280 acc: 71.9%\n",
      "[BATCH 62/149] Loss_D: 0.7734 Loss_G: 0.8367 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.7850 Loss_G: 0.8196 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.8032 Loss_G: 0.8136 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7745 Loss_G: 0.8033 acc: 54.7%\n",
      "[BATCH 66/149] Loss_D: 0.7761 Loss_G: 0.8011 acc: 59.4%\n",
      "[BATCH 67/149] Loss_D: 0.8034 Loss_G: 0.8170 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.7574 Loss_G: 0.8001 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.7824 Loss_G: 0.7954 acc: 71.9%\n",
      "[BATCH 70/149] Loss_D: 0.8116 Loss_G: 0.8043 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7431 Loss_G: 0.7876 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.7789 Loss_G: 0.7890 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7915 Loss_G: 0.7931 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7926 Loss_G: 0.8040 acc: 71.9%\n",
      "[BATCH 75/149] Loss_D: 0.8035 Loss_G: 0.7987 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7939 Loss_G: 0.7990 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.7759 Loss_G: 0.7936 acc: 60.9%\n",
      "[EPOCH 4100] TEST ACC is : 75.4%\n",
      "[BATCH 78/149] Loss_D: 0.7500 Loss_G: 0.7859 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.7375 Loss_G: 0.7819 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.8236 Loss_G: 0.7954 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.8400 Loss_G: 0.8191 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.8208 Loss_G: 0.8228 acc: 73.4%\n",
      "[BATCH 83/149] Loss_D: 0.7656 Loss_G: 0.8151 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.7942 Loss_G: 0.8097 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.7826 Loss_G: 0.8112 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.8378 Loss_G: 0.8213 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.8500 Loss_G: 0.8376 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.8124 Loss_G: 0.8345 acc: 78.1%\n",
      "[BATCH 89/149] Loss_D: 0.8352 Loss_G: 0.8341 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7918 Loss_G: 0.8237 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.7579 Loss_G: 0.8047 acc: 57.8%\n",
      "[BATCH 92/149] Loss_D: 0.7973 Loss_G: 0.8023 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.7641 Loss_G: 0.8030 acc: 65.6%\n",
      "[BATCH 94/149] Loss_D: 0.8009 Loss_G: 0.8045 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.8045 Loss_G: 0.8050 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.7288 Loss_G: 0.7910 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7955 Loss_G: 0.7853 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7819 Loss_G: 0.7941 acc: 68.8%\n",
      "[BATCH 99/149] Loss_D: 0.7668 Loss_G: 0.7900 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.7916 Loss_G: 0.7930 acc: 59.4%\n",
      "[BATCH 101/149] Loss_D: 0.7950 Loss_G: 0.7903 acc: 67.2%\n",
      "[BATCH 102/149] Loss_D: 0.8053 Loss_G: 0.8024 acc: 71.9%\n",
      "[BATCH 103/149] Loss_D: 0.7569 Loss_G: 0.7977 acc: 76.6%\n",
      "[BATCH 104/149] Loss_D: 0.7870 Loss_G: 0.7972 acc: 59.4%\n",
      "[BATCH 105/149] Loss_D: 0.8128 Loss_G: 0.8160 acc: 71.9%\n",
      "[BATCH 106/149] Loss_D: 0.7914 Loss_G: 0.8102 acc: 68.8%\n",
      "[BATCH 107/149] Loss_D: 0.7742 Loss_G: 0.7975 acc: 57.8%\n",
      "[BATCH 108/149] Loss_D: 0.8240 Loss_G: 0.8053 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.7905 Loss_G: 0.8120 acc: 73.4%\n",
      "[BATCH 110/149] Loss_D: 0.7629 Loss_G: 0.8016 acc: 60.9%\n",
      "[BATCH 111/149] Loss_D: 0.7585 Loss_G: 0.7975 acc: 56.2%\n",
      "[BATCH 112/149] Loss_D: 0.7978 Loss_G: 0.7881 acc: 51.6%\n",
      "[BATCH 113/149] Loss_D: 0.7938 Loss_G: 0.7990 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.8116 Loss_G: 0.8103 acc: 50.0%\n",
      "[BATCH 115/149] Loss_D: 0.7569 Loss_G: 0.7940 acc: 71.9%\n",
      "[BATCH 116/149] Loss_D: 0.7807 Loss_G: 0.7903 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7945 Loss_G: 0.7956 acc: 48.4%\n",
      "[BATCH 118/149] Loss_D: 0.7863 Loss_G: 0.7949 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.7654 Loss_G: 0.7948 acc: 71.9%\n",
      "[BATCH 120/149] Loss_D: 0.7520 Loss_G: 0.7939 acc: 56.2%\n",
      "[BATCH 121/149] Loss_D: 0.7574 Loss_G: 0.8006 acc: 64.1%\n",
      "[BATCH 122/149] Loss_D: 0.7874 Loss_G: 0.8021 acc: 76.6%\n",
      "[BATCH 123/149] Loss_D: 0.7764 Loss_G: 0.7999 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7740 Loss_G: 0.8127 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.8280 Loss_G: 0.8231 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.7780 Loss_G: 0.8209 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.7848 Loss_G: 0.8162 acc: 68.8%\n",
      "[EPOCH 4150] TEST ACC is : 75.4%\n",
      "[BATCH 128/149] Loss_D: 0.8056 Loss_G: 0.8141 acc: 75.0%\n",
      "[BATCH 129/149] Loss_D: 0.7613 Loss_G: 0.8176 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7441 Loss_G: 0.8051 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.8027 Loss_G: 0.8053 acc: 76.6%\n",
      "[BATCH 132/149] Loss_D: 0.7811 Loss_G: 0.8023 acc: 71.9%\n",
      "[BATCH 133/149] Loss_D: 0.7666 Loss_G: 0.7958 acc: 57.8%\n",
      "[BATCH 134/149] Loss_D: 0.8067 Loss_G: 0.8029 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.7629 Loss_G: 0.7993 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7608 Loss_G: 0.7850 acc: 53.1%\n",
      "[BATCH 137/149] Loss_D: 0.8061 Loss_G: 0.7928 acc: 71.9%\n",
      "[BATCH 138/149] Loss_D: 0.7896 Loss_G: 0.7980 acc: 73.4%\n",
      "[BATCH 139/149] Loss_D: 0.7451 Loss_G: 0.7841 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.8009 Loss_G: 0.7847 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7985 Loss_G: 0.7919 acc: 71.9%\n",
      "[BATCH 142/149] Loss_D: 0.7693 Loss_G: 0.7947 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7995 Loss_G: 0.8050 acc: 71.9%\n",
      "[BATCH 144/149] Loss_D: 0.7944 Loss_G: 0.8025 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.7979 Loss_G: 0.8007 acc: 70.3%\n",
      "[BATCH 146/149] Loss_D: 0.7851 Loss_G: 0.7974 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7756 Loss_G: 0.7994 acc: 73.4%\n",
      "[BATCH 148/149] Loss_D: 0.7890 Loss_G: 0.7984 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7667 Loss_G: 0.8021 acc: 57.8%\n",
      "-----THE [28/50] epoch end-----\n",
      "-----THE [29/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7708 Loss_G: 0.7982 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.7899 Loss_G: 0.8079 acc: 79.7%\n",
      "[BATCH 3/149] Loss_D: 0.8069 Loss_G: 0.8151 acc: 71.9%\n",
      "[BATCH 4/149] Loss_D: 0.7760 Loss_G: 0.8166 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.7914 Loss_G: 0.8187 acc: 56.2%\n",
      "[BATCH 6/149] Loss_D: 0.7869 Loss_G: 0.8157 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.7989 Loss_G: 0.8121 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7526 Loss_G: 0.7966 acc: 53.1%\n",
      "[BATCH 9/149] Loss_D: 0.7769 Loss_G: 0.7956 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7603 Loss_G: 0.8022 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.8151 Loss_G: 0.7983 acc: 70.3%\n",
      "[BATCH 12/149] Loss_D: 0.7665 Loss_G: 0.8002 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7715 Loss_G: 0.7925 acc: 67.2%\n",
      "[BATCH 14/149] Loss_D: 0.7903 Loss_G: 0.8073 acc: 59.4%\n",
      "[BATCH 15/149] Loss_D: 0.7772 Loss_G: 0.8005 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.8423 Loss_G: 0.8180 acc: 70.3%\n",
      "[BATCH 17/149] Loss_D: 0.8068 Loss_G: 0.8239 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.8231 Loss_G: 0.8279 acc: 57.8%\n",
      "[BATCH 19/149] Loss_D: 0.7854 Loss_G: 0.8215 acc: 71.9%\n",
      "[BATCH 20/149] Loss_D: 0.8002 Loss_G: 0.8120 acc: 73.4%\n",
      "[BATCH 21/149] Loss_D: 0.7971 Loss_G: 0.8168 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7887 Loss_G: 0.8137 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.7948 Loss_G: 0.8136 acc: 53.1%\n",
      "[BATCH 24/149] Loss_D: 0.7778 Loss_G: 0.8100 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.8296 Loss_G: 0.8182 acc: 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.7998 Loss_G: 0.8284 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.7994 Loss_G: 0.8240 acc: 51.6%\n",
      "[BATCH 28/149] Loss_D: 0.7798 Loss_G: 0.8186 acc: 57.8%\n",
      "[EPOCH 4200] TEST ACC is : 74.4%\n",
      "[BATCH 29/149] Loss_D: 0.7813 Loss_G: 0.8080 acc: 75.0%\n",
      "[BATCH 30/149] Loss_D: 0.7850 Loss_G: 0.8065 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7922 Loss_G: 0.8069 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.7845 Loss_G: 0.8007 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.7872 Loss_G: 0.8023 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7727 Loss_G: 0.7977 acc: 54.7%\n",
      "[BATCH 35/149] Loss_D: 0.8118 Loss_G: 0.8007 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.7983 Loss_G: 0.8065 acc: 53.1%\n",
      "[BATCH 37/149] Loss_D: 0.8010 Loss_G: 0.8028 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.7388 Loss_G: 0.7920 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7824 Loss_G: 0.7866 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7923 Loss_G: 0.7930 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7838 Loss_G: 0.8020 acc: 54.7%\n",
      "[BATCH 42/149] Loss_D: 0.8184 Loss_G: 0.8070 acc: 57.8%\n",
      "[BATCH 43/149] Loss_D: 0.7519 Loss_G: 0.7863 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.7741 Loss_G: 0.7865 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7854 Loss_G: 0.7919 acc: 76.6%\n",
      "[BATCH 46/149] Loss_D: 0.7482 Loss_G: 0.7868 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.8244 Loss_G: 0.8011 acc: 70.3%\n",
      "[BATCH 48/149] Loss_D: 0.7902 Loss_G: 0.8151 acc: 56.2%\n",
      "[BATCH 49/149] Loss_D: 0.7983 Loss_G: 0.8067 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.7989 Loss_G: 0.8082 acc: 60.9%\n",
      "[BATCH 51/149] Loss_D: 0.8775 Loss_G: 0.8340 acc: 50.0%\n",
      "[BATCH 52/149] Loss_D: 0.7831 Loss_G: 0.8224 acc: 76.6%\n",
      "[BATCH 53/149] Loss_D: 0.7664 Loss_G: 0.8103 acc: 67.2%\n",
      "[BATCH 54/149] Loss_D: 0.7737 Loss_G: 0.7991 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.8671 Loss_G: 0.8183 acc: 54.7%\n",
      "[BATCH 56/149] Loss_D: 0.7651 Loss_G: 0.8066 acc: 62.5%\n",
      "[BATCH 57/149] Loss_D: 0.8007 Loss_G: 0.7972 acc: 65.6%\n",
      "[BATCH 58/149] Loss_D: 0.7577 Loss_G: 0.7930 acc: 68.8%\n",
      "[BATCH 59/149] Loss_D: 0.7543 Loss_G: 0.7849 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.8131 Loss_G: 0.8065 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.7867 Loss_G: 0.8221 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.7774 Loss_G: 0.8177 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.7591 Loss_G: 0.8133 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.7619 Loss_G: 0.8098 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.7708 Loss_G: 0.8129 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.7819 Loss_G: 0.8182 acc: 68.8%\n",
      "[BATCH 67/149] Loss_D: 0.7729 Loss_G: 0.8126 acc: 56.2%\n",
      "[BATCH 68/149] Loss_D: 0.7701 Loss_G: 0.8119 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.8069 Loss_G: 0.8143 acc: 68.8%\n",
      "[BATCH 70/149] Loss_D: 0.8069 Loss_G: 0.8093 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.8095 Loss_G: 0.8015 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7860 Loss_G: 0.8027 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7975 Loss_G: 0.8000 acc: 70.3%\n",
      "[BATCH 74/149] Loss_D: 0.7689 Loss_G: 0.7984 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.7849 Loss_G: 0.7971 acc: 59.4%\n",
      "[BATCH 76/149] Loss_D: 0.8104 Loss_G: 0.8047 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.7976 Loss_G: 0.8143 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.7802 Loss_G: 0.8127 acc: 60.9%\n",
      "[EPOCH 4250] TEST ACC is : 76.0%\n",
      "[BATCH 79/149] Loss_D: 0.7971 Loss_G: 0.8072 acc: 62.5%\n",
      "[BATCH 80/149] Loss_D: 0.8149 Loss_G: 0.8116 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.7715 Loss_G: 0.8011 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.7683 Loss_G: 0.7954 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.8016 Loss_G: 0.8021 acc: 53.1%\n",
      "[BATCH 84/149] Loss_D: 0.8051 Loss_G: 0.8050 acc: 68.8%\n",
      "[BATCH 85/149] Loss_D: 0.7825 Loss_G: 0.8024 acc: 71.9%\n",
      "[BATCH 86/149] Loss_D: 0.7660 Loss_G: 0.7967 acc: 78.1%\n",
      "[BATCH 87/149] Loss_D: 0.7482 Loss_G: 0.7882 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 0.7671 Loss_G: 0.7935 acc: 70.3%\n",
      "[BATCH 89/149] Loss_D: 0.7424 Loss_G: 0.7875 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.7778 Loss_G: 0.7948 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.7919 Loss_G: 0.8002 acc: 56.2%\n",
      "[BATCH 92/149] Loss_D: 0.7552 Loss_G: 0.7914 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.7839 Loss_G: 0.7907 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7909 Loss_G: 0.7979 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7600 Loss_G: 0.7905 acc: 76.6%\n",
      "[BATCH 96/149] Loss_D: 0.7535 Loss_G: 0.7840 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7882 Loss_G: 0.7914 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7809 Loss_G: 0.7921 acc: 51.6%\n",
      "[BATCH 99/149] Loss_D: 0.7775 Loss_G: 0.7944 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.7933 Loss_G: 0.7983 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.8123 Loss_G: 0.8197 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7682 Loss_G: 0.8142 acc: 75.0%\n",
      "[BATCH 103/149] Loss_D: 0.7956 Loss_G: 0.8157 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.8072 Loss_G: 0.8334 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7560 Loss_G: 0.8073 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7864 Loss_G: 0.8015 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.7733 Loss_G: 0.7982 acc: 75.0%\n",
      "[BATCH 108/149] Loss_D: 0.8143 Loss_G: 0.8030 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.7650 Loss_G: 0.8017 acc: 68.8%\n",
      "[BATCH 110/149] Loss_D: 0.8039 Loss_G: 0.7967 acc: 70.3%\n",
      "[BATCH 111/149] Loss_D: 0.7854 Loss_G: 0.7988 acc: 67.2%\n",
      "[BATCH 112/149] Loss_D: 0.8253 Loss_G: 0.8168 acc: 60.9%\n",
      "[BATCH 113/149] Loss_D: 0.8325 Loss_G: 0.8153 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.7643 Loss_G: 0.8015 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.7653 Loss_G: 0.7999 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.7516 Loss_G: 0.8060 acc: 81.2%\n",
      "[BATCH 117/149] Loss_D: 0.7666 Loss_G: 0.7971 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.7997 Loss_G: 0.8023 acc: 62.5%\n",
      "[BATCH 119/149] Loss_D: 0.7509 Loss_G: 0.8004 acc: 57.8%\n",
      "[BATCH 120/149] Loss_D: 0.7732 Loss_G: 0.7947 acc: 57.8%\n",
      "[BATCH 121/149] Loss_D: 0.7963 Loss_G: 0.8004 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.7840 Loss_G: 0.8206 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.8292 Loss_G: 0.8191 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7596 Loss_G: 0.8115 acc: 62.5%\n",
      "[BATCH 125/149] Loss_D: 0.8158 Loss_G: 0.8279 acc: 71.9%\n",
      "[BATCH 126/149] Loss_D: 0.7675 Loss_G: 0.8068 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7660 Loss_G: 0.8001 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.8482 Loss_G: 0.8051 acc: 64.1%\n",
      "[EPOCH 4300] TEST ACC is : 75.4%\n",
      "[BATCH 129/149] Loss_D: 0.7632 Loss_G: 0.8007 acc: 73.4%\n",
      "[BATCH 130/149] Loss_D: 0.7615 Loss_G: 0.7923 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7858 Loss_G: 0.7947 acc: 65.6%\n",
      "[BATCH 132/149] Loss_D: 0.7745 Loss_G: 0.7957 acc: 70.3%\n",
      "[BATCH 133/149] Loss_D: 0.7706 Loss_G: 0.7943 acc: 75.0%\n",
      "[BATCH 134/149] Loss_D: 0.7685 Loss_G: 0.7884 acc: 70.3%\n",
      "[BATCH 135/149] Loss_D: 0.7720 Loss_G: 0.7930 acc: 67.2%\n",
      "[BATCH 136/149] Loss_D: 0.8043 Loss_G: 0.8038 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.8147 Loss_G: 0.8161 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.7672 Loss_G: 0.8046 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.7807 Loss_G: 0.8046 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7733 Loss_G: 0.8071 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7382 Loss_G: 0.7987 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7595 Loss_G: 0.7881 acc: 71.9%\n",
      "[BATCH 143/149] Loss_D: 0.8025 Loss_G: 0.7965 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.8195 Loss_G: 0.8066 acc: 70.3%\n",
      "[BATCH 145/149] Loss_D: 0.7756 Loss_G: 0.8016 acc: 70.3%\n",
      "[BATCH 146/149] Loss_D: 0.7554 Loss_G: 0.7997 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.8696 Loss_G: 0.8249 acc: 73.4%\n",
      "[BATCH 148/149] Loss_D: 0.7832 Loss_G: 0.8200 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7492 Loss_G: 0.7992 acc: 59.4%\n",
      "-----THE [29/50] epoch end-----\n",
      "-----THE [30/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7768 Loss_G: 0.7955 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.7955 Loss_G: 0.8008 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7893 Loss_G: 0.7985 acc: 57.8%\n",
      "[BATCH 4/149] Loss_D: 0.8212 Loss_G: 0.8048 acc: 67.2%\n",
      "[BATCH 5/149] Loss_D: 0.7773 Loss_G: 0.8036 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.7834 Loss_G: 0.8006 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.7712 Loss_G: 0.7970 acc: 73.4%\n",
      "[BATCH 8/149] Loss_D: 0.7773 Loss_G: 0.7986 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.8046 Loss_G: 0.8107 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.7718 Loss_G: 0.8070 acc: 71.9%\n",
      "[BATCH 11/149] Loss_D: 0.8319 Loss_G: 0.8055 acc: 53.1%\n",
      "[BATCH 12/149] Loss_D: 0.7661 Loss_G: 0.7966 acc: 64.1%\n",
      "[BATCH 13/149] Loss_D: 0.7902 Loss_G: 0.8037 acc: 73.4%\n",
      "[BATCH 14/149] Loss_D: 0.7786 Loss_G: 0.8019 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7653 Loss_G: 0.7969 acc: 59.4%\n",
      "[BATCH 16/149] Loss_D: 0.7750 Loss_G: 0.8002 acc: 70.3%\n",
      "[BATCH 17/149] Loss_D: 0.7523 Loss_G: 0.7969 acc: 53.1%\n",
      "[BATCH 18/149] Loss_D: 0.7990 Loss_G: 0.8011 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.7743 Loss_G: 0.8012 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7539 Loss_G: 0.7988 acc: 59.4%\n",
      "[BATCH 21/149] Loss_D: 0.7474 Loss_G: 0.7833 acc: 64.1%\n",
      "[BATCH 22/149] Loss_D: 0.7861 Loss_G: 0.7871 acc: 59.4%\n",
      "[BATCH 23/149] Loss_D: 0.8071 Loss_G: 0.8103 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.8012 Loss_G: 0.8207 acc: 67.2%\n",
      "[BATCH 25/149] Loss_D: 0.7666 Loss_G: 0.7994 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.8089 Loss_G: 0.8142 acc: 59.4%\n",
      "[BATCH 27/149] Loss_D: 0.7496 Loss_G: 0.8053 acc: 71.9%\n",
      "[BATCH 28/149] Loss_D: 0.7518 Loss_G: 0.7939 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.8096 Loss_G: 0.8108 acc: 79.7%\n",
      "[EPOCH 4350] TEST ACC is : 75.2%\n",
      "[BATCH 30/149] Loss_D: 0.8218 Loss_G: 0.8183 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.8039 Loss_G: 0.8258 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.7622 Loss_G: 0.8015 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.7781 Loss_G: 0.8075 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.7532 Loss_G: 0.7961 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7713 Loss_G: 0.7900 acc: 70.3%\n",
      "[BATCH 36/149] Loss_D: 0.7818 Loss_G: 0.7971 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.8115 Loss_G: 0.8032 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7801 Loss_G: 0.8089 acc: 53.1%\n",
      "[BATCH 39/149] Loss_D: 0.7634 Loss_G: 0.7970 acc: 65.6%\n",
      "[BATCH 40/149] Loss_D: 0.7951 Loss_G: 0.8049 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.8194 Loss_G: 0.8125 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.8026 Loss_G: 0.8092 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.7869 Loss_G: 0.8026 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.8091 Loss_G: 0.8035 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.8144 Loss_G: 0.8030 acc: 56.2%\n",
      "[BATCH 46/149] Loss_D: 0.7386 Loss_G: 0.7958 acc: 64.1%\n",
      "[BATCH 47/149] Loss_D: 0.7935 Loss_G: 0.7927 acc: 56.2%\n",
      "[BATCH 48/149] Loss_D: 0.7919 Loss_G: 0.7982 acc: 71.9%\n",
      "[BATCH 49/149] Loss_D: 0.7979 Loss_G: 0.7965 acc: 70.3%\n",
      "[BATCH 50/149] Loss_D: 0.7781 Loss_G: 0.8015 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7741 Loss_G: 0.7983 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.7720 Loss_G: 0.7968 acc: 71.9%\n",
      "[BATCH 53/149] Loss_D: 0.8440 Loss_G: 0.8107 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.8042 Loss_G: 0.8165 acc: 73.4%\n",
      "[BATCH 55/149] Loss_D: 0.8166 Loss_G: 0.8152 acc: 73.4%\n",
      "[BATCH 56/149] Loss_D: 0.7662 Loss_G: 0.8046 acc: 59.4%\n",
      "[BATCH 57/149] Loss_D: 0.7648 Loss_G: 0.8040 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.7801 Loss_G: 0.8079 acc: 75.0%\n",
      "[BATCH 59/149] Loss_D: 0.8341 Loss_G: 0.8181 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7506 Loss_G: 0.8095 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.7665 Loss_G: 0.7902 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7875 Loss_G: 0.7947 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.7854 Loss_G: 0.7964 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.7672 Loss_G: 0.7899 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.7910 Loss_G: 0.7926 acc: 62.5%\n",
      "[BATCH 66/149] Loss_D: 0.7669 Loss_G: 0.7916 acc: 76.6%\n",
      "[BATCH 67/149] Loss_D: 0.7670 Loss_G: 0.7890 acc: 62.5%\n",
      "[BATCH 68/149] Loss_D: 0.7841 Loss_G: 0.7960 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.7940 Loss_G: 0.7988 acc: 53.1%\n",
      "[BATCH 70/149] Loss_D: 0.7837 Loss_G: 0.8035 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.8487 Loss_G: 0.8178 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.8297 Loss_G: 0.8285 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7866 Loss_G: 0.8175 acc: 70.3%\n",
      "[BATCH 74/149] Loss_D: 0.7483 Loss_G: 0.8037 acc: 71.9%\n",
      "[BATCH 75/149] Loss_D: 0.7901 Loss_G: 0.8036 acc: 51.6%\n",
      "[BATCH 76/149] Loss_D: 0.7704 Loss_G: 0.8064 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.8095 Loss_G: 0.8102 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7520 Loss_G: 0.8006 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.7515 Loss_G: 0.7927 acc: 60.9%\n",
      "[EPOCH 4400] TEST ACC is : 75.6%\n",
      "[BATCH 80/149] Loss_D: 0.7603 Loss_G: 0.7889 acc: 48.4%\n",
      "[BATCH 81/149] Loss_D: 0.8070 Loss_G: 0.7991 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.7958 Loss_G: 0.8162 acc: 70.3%\n",
      "[BATCH 83/149] Loss_D: 0.7458 Loss_G: 0.8069 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 0.7755 Loss_G: 0.8097 acc: 70.3%\n",
      "[BATCH 85/149] Loss_D: 0.8620 Loss_G: 0.8335 acc: 56.2%\n",
      "[BATCH 86/149] Loss_D: 0.8197 Loss_G: 0.8311 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.7996 Loss_G: 0.8122 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.7824 Loss_G: 0.8055 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.8004 Loss_G: 0.8016 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7900 Loss_G: 0.8032 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.9114 Loss_G: 0.8520 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.7663 Loss_G: 0.8337 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.7659 Loss_G: 0.7974 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.7884 Loss_G: 0.7968 acc: 53.1%\n",
      "[BATCH 95/149] Loss_D: 0.8400 Loss_G: 0.8153 acc: 57.8%\n",
      "[BATCH 96/149] Loss_D: 0.7809 Loss_G: 0.8206 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.7720 Loss_G: 0.8100 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7721 Loss_G: 0.7920 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.7777 Loss_G: 0.7911 acc: 53.1%\n",
      "[BATCH 100/149] Loss_D: 0.8528 Loss_G: 0.8035 acc: 71.9%\n",
      "[BATCH 101/149] Loss_D: 0.7717 Loss_G: 0.8123 acc: 76.6%\n",
      "[BATCH 102/149] Loss_D: 0.7818 Loss_G: 0.8115 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7780 Loss_G: 0.8090 acc: 65.6%\n",
      "[BATCH 104/149] Loss_D: 0.7955 Loss_G: 0.8083 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.7923 Loss_G: 0.8104 acc: 70.3%\n",
      "[BATCH 106/149] Loss_D: 0.7799 Loss_G: 0.8025 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.7617 Loss_G: 0.7923 acc: 73.4%\n",
      "[BATCH 108/149] Loss_D: 0.7653 Loss_G: 0.7955 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.7433 Loss_G: 0.7858 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.7819 Loss_G: 0.7904 acc: 73.4%\n",
      "[BATCH 111/149] Loss_D: 0.7554 Loss_G: 0.7844 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7786 Loss_G: 0.7844 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7813 Loss_G: 0.7859 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7774 Loss_G: 0.7913 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.7709 Loss_G: 0.7859 acc: 57.8%\n",
      "[BATCH 116/149] Loss_D: 0.7640 Loss_G: 0.7864 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7832 Loss_G: 0.7827 acc: 70.3%\n",
      "[BATCH 118/149] Loss_D: 0.7853 Loss_G: 0.7892 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.7893 Loss_G: 0.7945 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.7908 Loss_G: 0.7924 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.7933 Loss_G: 0.7923 acc: 59.4%\n",
      "[BATCH 122/149] Loss_D: 0.8200 Loss_G: 0.8033 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.7787 Loss_G: 0.8002 acc: 75.0%\n",
      "[BATCH 124/149] Loss_D: 0.8033 Loss_G: 0.7974 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.8311 Loss_G: 0.8211 acc: 56.2%\n",
      "[BATCH 126/149] Loss_D: 0.7540 Loss_G: 0.8100 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7625 Loss_G: 0.8034 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.8011 Loss_G: 0.8002 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.7658 Loss_G: 0.7901 acc: 67.2%\n",
      "[EPOCH 4450] TEST ACC is : 75.8%\n",
      "[BATCH 130/149] Loss_D: 0.7494 Loss_G: 0.7857 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.7698 Loss_G: 0.7914 acc: 68.8%\n",
      "[BATCH 132/149] Loss_D: 0.7696 Loss_G: 0.7917 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.7967 Loss_G: 0.7932 acc: 71.9%\n",
      "[BATCH 134/149] Loss_D: 0.7566 Loss_G: 0.7871 acc: 56.2%\n",
      "[BATCH 135/149] Loss_D: 0.7818 Loss_G: 0.7927 acc: 50.0%\n",
      "[BATCH 136/149] Loss_D: 0.7383 Loss_G: 0.7884 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.8051 Loss_G: 0.7959 acc: 71.9%\n",
      "[BATCH 138/149] Loss_D: 0.7447 Loss_G: 0.7986 acc: 70.3%\n",
      "[BATCH 139/149] Loss_D: 0.7829 Loss_G: 0.8042 acc: 59.4%\n",
      "[BATCH 140/149] Loss_D: 0.8013 Loss_G: 0.8142 acc: 65.6%\n",
      "[BATCH 141/149] Loss_D: 0.8355 Loss_G: 0.8034 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.8059 Loss_G: 0.8016 acc: 65.6%\n",
      "[BATCH 143/149] Loss_D: 0.8049 Loss_G: 0.8024 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.8144 Loss_G: 0.8111 acc: 62.5%\n",
      "[BATCH 145/149] Loss_D: 0.8026 Loss_G: 0.8151 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7537 Loss_G: 0.8037 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.7785 Loss_G: 0.7969 acc: 73.4%\n",
      "[BATCH 148/149] Loss_D: 0.7680 Loss_G: 0.7928 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7961 Loss_G: 0.7948 acc: 65.6%\n",
      "-----THE [30/50] epoch end-----\n",
      "-----THE [31/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7821 Loss_G: 0.7959 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.7849 Loss_G: 0.7959 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.8006 Loss_G: 0.7983 acc: 65.6%\n",
      "[BATCH 4/149] Loss_D: 0.7845 Loss_G: 0.7929 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7443 Loss_G: 0.7899 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.8057 Loss_G: 0.8024 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8033 Loss_G: 0.8019 acc: 56.2%\n",
      "[BATCH 8/149] Loss_D: 0.7685 Loss_G: 0.8051 acc: 59.4%\n",
      "[BATCH 9/149] Loss_D: 0.8115 Loss_G: 0.8082 acc: 45.3%\n",
      "[BATCH 10/149] Loss_D: 0.8360 Loss_G: 0.8238 acc: 53.1%\n",
      "[BATCH 11/149] Loss_D: 0.7904 Loss_G: 0.8193 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7827 Loss_G: 0.8038 acc: 64.1%\n",
      "[BATCH 13/149] Loss_D: 0.7480 Loss_G: 0.7905 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7647 Loss_G: 0.7891 acc: 68.8%\n",
      "[BATCH 15/149] Loss_D: 0.7691 Loss_G: 0.7909 acc: 60.9%\n",
      "[BATCH 16/149] Loss_D: 0.8235 Loss_G: 0.8109 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.7633 Loss_G: 0.8074 acc: 73.4%\n",
      "[BATCH 18/149] Loss_D: 0.7542 Loss_G: 0.7895 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.7845 Loss_G: 0.7898 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.7413 Loss_G: 0.7801 acc: 62.5%\n",
      "[BATCH 21/149] Loss_D: 0.7592 Loss_G: 0.7856 acc: 73.4%\n",
      "[BATCH 22/149] Loss_D: 0.7572 Loss_G: 0.7807 acc: 57.8%\n",
      "[BATCH 23/149] Loss_D: 0.8025 Loss_G: 0.7851 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.8038 Loss_G: 0.7898 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.8024 Loss_G: 0.7979 acc: 56.2%\n",
      "[BATCH 26/149] Loss_D: 0.7673 Loss_G: 0.7949 acc: 70.3%\n",
      "[BATCH 27/149] Loss_D: 0.7832 Loss_G: 0.8008 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.8034 Loss_G: 0.8202 acc: 68.8%\n",
      "[BATCH 29/149] Loss_D: 0.7518 Loss_G: 0.8138 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.7850 Loss_G: 0.8126 acc: 65.6%\n",
      "[EPOCH 4500] TEST ACC is : 75.0%\n",
      "[BATCH 31/149] Loss_D: 0.7795 Loss_G: 0.8014 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7531 Loss_G: 0.7893 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.7870 Loss_G: 0.7925 acc: 75.0%\n",
      "[BATCH 34/149] Loss_D: 0.7979 Loss_G: 0.8100 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7656 Loss_G: 0.8032 acc: 57.8%\n",
      "[BATCH 36/149] Loss_D: 0.8100 Loss_G: 0.8001 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.7722 Loss_G: 0.7974 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.7716 Loss_G: 0.7921 acc: 67.2%\n",
      "[BATCH 39/149] Loss_D: 0.7828 Loss_G: 0.7888 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.7910 Loss_G: 0.8029 acc: 71.9%\n",
      "[BATCH 41/149] Loss_D: 0.7595 Loss_G: 0.7939 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.8073 Loss_G: 0.8056 acc: 60.9%\n",
      "[BATCH 43/149] Loss_D: 0.7635 Loss_G: 0.8049 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7464 Loss_G: 0.8001 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7707 Loss_G: 0.7970 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7533 Loss_G: 0.7980 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.7689 Loss_G: 0.7939 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.7657 Loss_G: 0.7918 acc: 71.9%\n",
      "[BATCH 49/149] Loss_D: 0.7502 Loss_G: 0.7813 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.7963 Loss_G: 0.8002 acc: 53.1%\n",
      "[BATCH 51/149] Loss_D: 0.7672 Loss_G: 0.7944 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.7721 Loss_G: 0.7943 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.7948 Loss_G: 0.8110 acc: 54.7%\n",
      "[BATCH 54/149] Loss_D: 0.7491 Loss_G: 0.8251 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7805 Loss_G: 0.8311 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.8217 Loss_G: 0.8397 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.8255 Loss_G: 0.8209 acc: 67.2%\n",
      "[BATCH 58/149] Loss_D: 0.7639 Loss_G: 0.7965 acc: 62.5%\n",
      "[BATCH 59/149] Loss_D: 0.7790 Loss_G: 0.7951 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.7612 Loss_G: 0.7919 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.7517 Loss_G: 0.7914 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.8050 Loss_G: 0.8073 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7822 Loss_G: 0.8036 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.7664 Loss_G: 0.7960 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.8003 Loss_G: 0.7969 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.7928 Loss_G: 0.8063 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7701 Loss_G: 0.8044 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.8464 Loss_G: 0.8320 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.7732 Loss_G: 0.8270 acc: 59.4%\n",
      "[BATCH 70/149] Loss_D: 0.7789 Loss_G: 0.8225 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.7868 Loss_G: 0.8086 acc: 70.3%\n",
      "[BATCH 72/149] Loss_D: 0.8109 Loss_G: 0.8247 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7743 Loss_G: 0.8022 acc: 70.3%\n",
      "[BATCH 74/149] Loss_D: 0.8047 Loss_G: 0.8004 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.8112 Loss_G: 0.8138 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7838 Loss_G: 0.8148 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7661 Loss_G: 0.7864 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.8060 Loss_G: 0.7872 acc: 57.8%\n",
      "[BATCH 79/149] Loss_D: 0.8059 Loss_G: 0.7876 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7606 Loss_G: 0.7836 acc: 75.0%\n",
      "[EPOCH 4550] TEST ACC is : 76.4%\n",
      "[BATCH 81/149] Loss_D: 0.8224 Loss_G: 0.7993 acc: 54.7%\n",
      "[BATCH 82/149] Loss_D: 0.7945 Loss_G: 0.8123 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7795 Loss_G: 0.8076 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.7467 Loss_G: 0.7903 acc: 76.6%\n",
      "[BATCH 85/149] Loss_D: 0.8068 Loss_G: 0.7906 acc: 64.1%\n",
      "[BATCH 86/149] Loss_D: 0.7939 Loss_G: 0.7960 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.7928 Loss_G: 0.8037 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.8042 Loss_G: 0.8057 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.8125 Loss_G: 0.8265 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7679 Loss_G: 0.8041 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.8337 Loss_G: 0.8138 acc: 51.6%\n",
      "[BATCH 92/149] Loss_D: 0.8304 Loss_G: 0.8341 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.8170 Loss_G: 0.8413 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.7603 Loss_G: 0.8141 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.8137 Loss_G: 0.8080 acc: 65.6%\n",
      "[BATCH 96/149] Loss_D: 0.7545 Loss_G: 0.8041 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.7608 Loss_G: 0.8021 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7466 Loss_G: 0.7922 acc: 54.7%\n",
      "[BATCH 99/149] Loss_D: 0.8259 Loss_G: 0.7979 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.8020 Loss_G: 0.8091 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.7512 Loss_G: 0.7997 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.7571 Loss_G: 0.7959 acc: 57.8%\n",
      "[BATCH 103/149] Loss_D: 0.7785 Loss_G: 0.7956 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.7469 Loss_G: 0.7908 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.7562 Loss_G: 0.7865 acc: 62.5%\n",
      "[BATCH 106/149] Loss_D: 0.7662 Loss_G: 0.7858 acc: 68.8%\n",
      "[BATCH 107/149] Loss_D: 0.7692 Loss_G: 0.7853 acc: 71.9%\n",
      "[BATCH 108/149] Loss_D: 0.7971 Loss_G: 0.7953 acc: 60.9%\n",
      "[BATCH 109/149] Loss_D: 0.7616 Loss_G: 0.7972 acc: 76.6%\n",
      "[BATCH 110/149] Loss_D: 0.7808 Loss_G: 0.7922 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.7847 Loss_G: 0.8006 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7713 Loss_G: 0.7980 acc: 59.4%\n",
      "[BATCH 113/149] Loss_D: 0.8016 Loss_G: 0.8091 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.7865 Loss_G: 0.8208 acc: 67.2%\n",
      "[BATCH 115/149] Loss_D: 0.7790 Loss_G: 0.8078 acc: 73.4%\n",
      "[BATCH 116/149] Loss_D: 0.7880 Loss_G: 0.8011 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7696 Loss_G: 0.8055 acc: 54.7%\n",
      "[BATCH 118/149] Loss_D: 0.7692 Loss_G: 0.8000 acc: 62.5%\n",
      "[BATCH 119/149] Loss_D: 0.7602 Loss_G: 0.7903 acc: 71.9%\n",
      "[BATCH 120/149] Loss_D: 0.7702 Loss_G: 0.7895 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.8453 Loss_G: 0.8181 acc: 59.4%\n",
      "[BATCH 122/149] Loss_D: 0.7924 Loss_G: 0.8241 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7697 Loss_G: 0.8230 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7991 Loss_G: 0.8169 acc: 79.7%\n",
      "[BATCH 125/149] Loss_D: 0.7827 Loss_G: 0.8041 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.8076 Loss_G: 0.8073 acc: 54.7%\n",
      "[BATCH 127/149] Loss_D: 0.7657 Loss_G: 0.8164 acc: 68.8%\n",
      "[BATCH 128/149] Loss_D: 0.7882 Loss_G: 0.8164 acc: 57.8%\n",
      "[BATCH 129/149] Loss_D: 0.8136 Loss_G: 0.8137 acc: 67.2%\n",
      "[BATCH 130/149] Loss_D: 0.7669 Loss_G: 0.8069 acc: 64.1%\n",
      "[EPOCH 4600] TEST ACC is : 74.8%\n",
      "[BATCH 131/149] Loss_D: 0.7979 Loss_G: 0.7959 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.7588 Loss_G: 0.7852 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.7924 Loss_G: 0.7888 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.8090 Loss_G: 0.7960 acc: 67.2%\n",
      "[BATCH 135/149] Loss_D: 0.7583 Loss_G: 0.7952 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.8035 Loss_G: 0.7929 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.7938 Loss_G: 0.7942 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7661 Loss_G: 0.7892 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.8140 Loss_G: 0.7939 acc: 70.3%\n",
      "[BATCH 140/149] Loss_D: 0.8054 Loss_G: 0.7974 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7739 Loss_G: 0.7996 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8212 Loss_G: 0.8075 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.7904 Loss_G: 0.8103 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.8148 Loss_G: 0.8240 acc: 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.8009 Loss_G: 0.8117 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7826 Loss_G: 0.8054 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.8569 Loss_G: 0.8307 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.8150 Loss_G: 0.8275 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.8556 Loss_G: 0.8386 acc: 71.9%\n",
      "-----THE [31/50] epoch end-----\n",
      "-----THE [32/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7954 Loss_G: 0.8500 acc: 56.2%\n",
      "[BATCH 2/149] Loss_D: 0.7727 Loss_G: 0.8232 acc: 60.9%\n",
      "[BATCH 3/149] Loss_D: 0.7836 Loss_G: 0.8185 acc: 70.3%\n",
      "[BATCH 4/149] Loss_D: 0.7904 Loss_G: 0.8220 acc: 76.6%\n",
      "[BATCH 5/149] Loss_D: 0.8026 Loss_G: 0.8291 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.7576 Loss_G: 0.8108 acc: 73.4%\n",
      "[BATCH 7/149] Loss_D: 0.7813 Loss_G: 0.8018 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7584 Loss_G: 0.7948 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.7866 Loss_G: 0.7974 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8144 Loss_G: 0.8102 acc: 68.8%\n",
      "[BATCH 11/149] Loss_D: 0.7614 Loss_G: 0.8024 acc: 62.5%\n",
      "[BATCH 12/149] Loss_D: 0.8640 Loss_G: 0.8209 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7884 Loss_G: 0.7944 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.7914 Loss_G: 0.7962 acc: 54.7%\n",
      "[BATCH 15/149] Loss_D: 0.8240 Loss_G: 0.8161 acc: 54.7%\n",
      "[BATCH 16/149] Loss_D: 0.7903 Loss_G: 0.8136 acc: 59.4%\n",
      "[BATCH 17/149] Loss_D: 0.7868 Loss_G: 0.8046 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.7413 Loss_G: 0.7843 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.8145 Loss_G: 0.7933 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.7946 Loss_G: 0.8015 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.7767 Loss_G: 0.8032 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7550 Loss_G: 0.7866 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.7712 Loss_G: 0.7864 acc: 71.9%\n",
      "[BATCH 24/149] Loss_D: 0.7882 Loss_G: 0.7966 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.8083 Loss_G: 0.8028 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.7631 Loss_G: 0.8062 acc: 76.6%\n",
      "[BATCH 27/149] Loss_D: 0.8614 Loss_G: 0.8304 acc: 57.8%\n",
      "[BATCH 28/149] Loss_D: 0.7764 Loss_G: 0.8247 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.7850 Loss_G: 0.8039 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.8037 Loss_G: 0.8117 acc: 50.0%\n",
      "[BATCH 31/149] Loss_D: 0.8160 Loss_G: 0.8306 acc: 64.1%\n",
      "[EPOCH 4650] TEST ACC is : 75.6%\n",
      "[BATCH 32/149] Loss_D: 0.7655 Loss_G: 0.8215 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7577 Loss_G: 0.7954 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.7968 Loss_G: 0.8018 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7569 Loss_G: 0.7934 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.8442 Loss_G: 0.8131 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7626 Loss_G: 0.8215 acc: 62.5%\n",
      "[BATCH 38/149] Loss_D: 0.7778 Loss_G: 0.8009 acc: 56.2%\n",
      "[BATCH 39/149] Loss_D: 0.7763 Loss_G: 0.7988 acc: 54.7%\n",
      "[BATCH 40/149] Loss_D: 0.7673 Loss_G: 0.7853 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.8003 Loss_G: 0.7929 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7952 Loss_G: 0.7867 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.8233 Loss_G: 0.7993 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.7730 Loss_G: 0.8041 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7531 Loss_G: 0.7954 acc: 67.2%\n",
      "[BATCH 46/149] Loss_D: 0.7770 Loss_G: 0.7961 acc: 78.1%\n",
      "[BATCH 47/149] Loss_D: 0.7956 Loss_G: 0.8042 acc: 70.3%\n",
      "[BATCH 48/149] Loss_D: 0.7505 Loss_G: 0.7987 acc: 68.8%\n",
      "[BATCH 49/149] Loss_D: 0.7972 Loss_G: 0.8021 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7736 Loss_G: 0.8124 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.7743 Loss_G: 0.8052 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.8481 Loss_G: 0.8150 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.8397 Loss_G: 0.8308 acc: 67.2%\n",
      "[BATCH 54/149] Loss_D: 0.7834 Loss_G: 0.8208 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.7853 Loss_G: 0.8110 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.7788 Loss_G: 0.8030 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.7867 Loss_G: 0.7966 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.7881 Loss_G: 0.7981 acc: 57.8%\n",
      "[BATCH 59/149] Loss_D: 0.7713 Loss_G: 0.7995 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.7883 Loss_G: 0.8001 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.7567 Loss_G: 0.7942 acc: 71.9%\n",
      "[BATCH 62/149] Loss_D: 0.7922 Loss_G: 0.7937 acc: 71.9%\n",
      "[BATCH 63/149] Loss_D: 0.7865 Loss_G: 0.7927 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7919 Loss_G: 0.7946 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.7956 Loss_G: 0.8058 acc: 62.5%\n",
      "[BATCH 66/149] Loss_D: 0.7922 Loss_G: 0.8084 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7721 Loss_G: 0.7960 acc: 53.1%\n",
      "[BATCH 68/149] Loss_D: 0.7630 Loss_G: 0.7884 acc: 75.0%\n",
      "[BATCH 69/149] Loss_D: 0.7800 Loss_G: 0.7872 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.7943 Loss_G: 0.7935 acc: 62.5%\n",
      "[BATCH 71/149] Loss_D: 0.7935 Loss_G: 0.8015 acc: 53.1%\n",
      "[BATCH 72/149] Loss_D: 0.7683 Loss_G: 0.8040 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.8009 Loss_G: 0.8054 acc: 75.0%\n",
      "[BATCH 74/149] Loss_D: 0.7624 Loss_G: 0.7921 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.7425 Loss_G: 0.7857 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.7503 Loss_G: 0.7867 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.7715 Loss_G: 0.7923 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.7689 Loss_G: 0.7969 acc: 67.2%\n",
      "[BATCH 79/149] Loss_D: 0.8014 Loss_G: 0.8073 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.7875 Loss_G: 0.8113 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.7825 Loss_G: 0.8163 acc: 67.2%\n",
      "[EPOCH 4700] TEST ACC is : 73.8%\n",
      "[BATCH 82/149] Loss_D: 0.8456 Loss_G: 0.8298 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.7533 Loss_G: 0.8075 acc: 76.6%\n",
      "[BATCH 84/149] Loss_D: 0.8351 Loss_G: 0.8114 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.8153 Loss_G: 0.8087 acc: 71.9%\n",
      "[BATCH 86/149] Loss_D: 0.7921 Loss_G: 0.8017 acc: 75.0%\n",
      "[BATCH 87/149] Loss_D: 0.7596 Loss_G: 0.7950 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.8065 Loss_G: 0.8056 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7717 Loss_G: 0.8064 acc: 64.1%\n",
      "[BATCH 90/149] Loss_D: 0.7601 Loss_G: 0.7962 acc: 53.1%\n",
      "[BATCH 91/149] Loss_D: 0.7984 Loss_G: 0.8031 acc: 57.8%\n",
      "[BATCH 92/149] Loss_D: 0.8126 Loss_G: 0.8133 acc: 57.8%\n",
      "[BATCH 93/149] Loss_D: 0.7837 Loss_G: 0.8084 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.7786 Loss_G: 0.7963 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7720 Loss_G: 0.7952 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.7816 Loss_G: 0.8081 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7760 Loss_G: 0.7971 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.7600 Loss_G: 0.7877 acc: 65.6%\n",
      "[BATCH 99/149] Loss_D: 0.7575 Loss_G: 0.7832 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.7866 Loss_G: 0.7894 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.7638 Loss_G: 0.7861 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.7514 Loss_G: 0.7856 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.7718 Loss_G: 0.7872 acc: 70.3%\n",
      "[BATCH 104/149] Loss_D: 0.7674 Loss_G: 0.7865 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.7807 Loss_G: 0.7939 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7758 Loss_G: 0.8015 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.7870 Loss_G: 0.8020 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7665 Loss_G: 0.7949 acc: 75.0%\n",
      "[BATCH 109/149] Loss_D: 0.8446 Loss_G: 0.8170 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.7627 Loss_G: 0.8122 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.8059 Loss_G: 0.8079 acc: 67.2%\n",
      "[BATCH 112/149] Loss_D: 0.7834 Loss_G: 0.7988 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.7509 Loss_G: 0.8067 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.8014 Loss_G: 0.8116 acc: 56.2%\n",
      "[BATCH 115/149] Loss_D: 0.7434 Loss_G: 0.8047 acc: 70.3%\n",
      "[BATCH 116/149] Loss_D: 0.7507 Loss_G: 0.7938 acc: 53.1%\n",
      "[BATCH 117/149] Loss_D: 0.8279 Loss_G: 0.7989 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.8015 Loss_G: 0.8064 acc: 75.0%\n",
      "[BATCH 119/149] Loss_D: 0.7724 Loss_G: 0.8064 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7604 Loss_G: 0.7866 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.7977 Loss_G: 0.8020 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.7955 Loss_G: 0.8067 acc: 71.9%\n",
      "[BATCH 123/149] Loss_D: 0.8146 Loss_G: 0.8184 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7342 Loss_G: 0.8034 acc: 59.4%\n",
      "[BATCH 125/149] Loss_D: 0.7886 Loss_G: 0.7966 acc: 56.2%\n",
      "[BATCH 126/149] Loss_D: 0.7648 Loss_G: 0.8045 acc: 67.2%\n",
      "[BATCH 127/149] Loss_D: 0.7869 Loss_G: 0.8020 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.7774 Loss_G: 0.7973 acc: 71.9%\n",
      "[BATCH 129/149] Loss_D: 0.8033 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.8000 Loss_G: 0.8000 acc: 76.6%\n",
      "[BATCH 131/149] Loss_D: 0.7868 Loss_G: 0.7978 acc: 68.8%\n",
      "[EPOCH 4750] TEST ACC is : 74.8%\n",
      "[BATCH 132/149] Loss_D: 0.8184 Loss_G: 0.8130 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.7582 Loss_G: 0.7971 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.8063 Loss_G: 0.7939 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.7923 Loss_G: 0.7920 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.8036 Loss_G: 0.7911 acc: 59.4%\n",
      "[BATCH 137/149] Loss_D: 0.7898 Loss_G: 0.7960 acc: 67.2%\n",
      "[BATCH 138/149] Loss_D: 0.7734 Loss_G: 0.8051 acc: 48.4%\n",
      "[BATCH 139/149] Loss_D: 0.7898 Loss_G: 0.8199 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7984 Loss_G: 0.8049 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.8109 Loss_G: 0.8011 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8096 Loss_G: 0.8234 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7566 Loss_G: 0.8102 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7948 Loss_G: 0.7963 acc: 73.4%\n",
      "[BATCH 145/149] Loss_D: 0.7571 Loss_G: 0.7932 acc: 59.4%\n",
      "[BATCH 146/149] Loss_D: 0.7855 Loss_G: 0.8000 acc: 68.8%\n",
      "[BATCH 147/149] Loss_D: 0.7652 Loss_G: 0.8004 acc: 60.9%\n",
      "[BATCH 148/149] Loss_D: 0.7821 Loss_G: 0.7983 acc: 57.8%\n",
      "[BATCH 149/149] Loss_D: 0.8255 Loss_G: 0.8027 acc: 68.8%\n",
      "-----THE [32/50] epoch end-----\n",
      "-----THE [33/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7700 Loss_G: 0.7941 acc: 64.1%\n",
      "[BATCH 2/149] Loss_D: 0.7555 Loss_G: 0.7851 acc: 70.3%\n",
      "[BATCH 3/149] Loss_D: 0.7968 Loss_G: 0.7933 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.7881 Loss_G: 0.8012 acc: 67.2%\n",
      "[BATCH 5/149] Loss_D: 0.8038 Loss_G: 0.8096 acc: 62.5%\n",
      "[BATCH 6/149] Loss_D: 0.8173 Loss_G: 0.8211 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.7721 Loss_G: 0.8168 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.8098 Loss_G: 0.8042 acc: 62.5%\n",
      "[BATCH 9/149] Loss_D: 0.7615 Loss_G: 0.8006 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8077 Loss_G: 0.8081 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.7562 Loss_G: 0.7965 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7889 Loss_G: 0.7952 acc: 60.9%\n",
      "[BATCH 13/149] Loss_D: 0.7812 Loss_G: 0.7875 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.8140 Loss_G: 0.7972 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.8322 Loss_G: 0.8176 acc: 67.2%\n",
      "[BATCH 16/149] Loss_D: 0.8034 Loss_G: 0.8230 acc: 65.6%\n",
      "[BATCH 17/149] Loss_D: 0.7683 Loss_G: 0.8122 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.7953 Loss_G: 0.8124 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.8097 Loss_G: 0.8111 acc: 73.4%\n",
      "[BATCH 20/149] Loss_D: 0.8172 Loss_G: 0.8290 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.7821 Loss_G: 0.8200 acc: 71.9%\n",
      "[BATCH 22/149] Loss_D: 0.7875 Loss_G: 0.8170 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.7591 Loss_G: 0.7980 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7976 Loss_G: 0.7984 acc: 59.4%\n",
      "[BATCH 25/149] Loss_D: 0.7759 Loss_G: 0.8010 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7847 Loss_G: 0.8032 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7757 Loss_G: 0.8001 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.8297 Loss_G: 0.8094 acc: 65.6%\n",
      "[BATCH 29/149] Loss_D: 0.8033 Loss_G: 0.8124 acc: 75.0%\n",
      "[BATCH 30/149] Loss_D: 0.7798 Loss_G: 0.8054 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7603 Loss_G: 0.8070 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.7716 Loss_G: 0.7970 acc: 75.0%\n",
      "[EPOCH 4800] TEST ACC is : 75.4%\n",
      "[BATCH 33/149] Loss_D: 0.7611 Loss_G: 0.7932 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7791 Loss_G: 0.7864 acc: 54.7%\n",
      "[BATCH 35/149] Loss_D: 0.7722 Loss_G: 0.7973 acc: 70.3%\n",
      "[BATCH 36/149] Loss_D: 0.8261 Loss_G: 0.8209 acc: 67.2%\n",
      "[BATCH 37/149] Loss_D: 0.7743 Loss_G: 0.7980 acc: 68.8%\n",
      "[BATCH 38/149] Loss_D: 0.7960 Loss_G: 0.8066 acc: 78.1%\n",
      "[BATCH 39/149] Loss_D: 0.8037 Loss_G: 0.8046 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.7601 Loss_G: 0.7913 acc: 59.4%\n",
      "[BATCH 41/149] Loss_D: 0.8330 Loss_G: 0.7981 acc: 71.9%\n",
      "[BATCH 42/149] Loss_D: 0.7969 Loss_G: 0.8078 acc: 65.6%\n",
      "[BATCH 43/149] Loss_D: 0.7674 Loss_G: 0.7932 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7630 Loss_G: 0.7898 acc: 57.8%\n",
      "[BATCH 45/149] Loss_D: 0.7970 Loss_G: 0.7967 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.7468 Loss_G: 0.7994 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.7740 Loss_G: 0.7917 acc: 73.4%\n",
      "[BATCH 48/149] Loss_D: 0.7631 Loss_G: 0.7937 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.8517 Loss_G: 0.8020 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.7644 Loss_G: 0.8025 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.7567 Loss_G: 0.7911 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.7598 Loss_G: 0.7941 acc: 57.8%\n",
      "[BATCH 53/149] Loss_D: 0.7619 Loss_G: 0.7927 acc: 56.2%\n",
      "[BATCH 54/149] Loss_D: 0.7841 Loss_G: 0.8177 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.7898 Loss_G: 0.8132 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7820 Loss_G: 0.8190 acc: 59.4%\n",
      "[BATCH 57/149] Loss_D: 0.7590 Loss_G: 0.7903 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.7581 Loss_G: 0.7854 acc: 70.3%\n",
      "[BATCH 59/149] Loss_D: 0.7728 Loss_G: 0.7839 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7770 Loss_G: 0.7929 acc: 70.3%\n",
      "[BATCH 61/149] Loss_D: 0.7794 Loss_G: 0.8104 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.8193 Loss_G: 0.8091 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.8123 Loss_G: 0.8194 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.7717 Loss_G: 0.8076 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7612 Loss_G: 0.7939 acc: 54.7%\n",
      "[BATCH 66/149] Loss_D: 0.7629 Loss_G: 0.7979 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.7935 Loss_G: 0.8054 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.8218 Loss_G: 0.8157 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.8252 Loss_G: 0.8089 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.7418 Loss_G: 0.7942 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.8054 Loss_G: 0.7947 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7919 Loss_G: 0.8068 acc: 64.1%\n",
      "[BATCH 73/149] Loss_D: 0.7986 Loss_G: 0.8106 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7915 Loss_G: 0.8051 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.7908 Loss_G: 0.8066 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.7844 Loss_G: 0.8077 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7563 Loss_G: 0.7852 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.8036 Loss_G: 0.7963 acc: 56.2%\n",
      "[BATCH 79/149] Loss_D: 0.7721 Loss_G: 0.8038 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.8071 Loss_G: 0.8190 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.8094 Loss_G: 0.8206 acc: 76.6%\n",
      "[BATCH 82/149] Loss_D: 0.7803 Loss_G: 0.8023 acc: 57.8%\n",
      "[EPOCH 4850] TEST ACC is : 73.8%\n",
      "[BATCH 83/149] Loss_D: 0.7821 Loss_G: 0.8067 acc: 53.1%\n",
      "[BATCH 84/149] Loss_D: 0.8075 Loss_G: 0.8186 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.7783 Loss_G: 0.8062 acc: 50.0%\n",
      "[BATCH 86/149] Loss_D: 0.7881 Loss_G: 0.8003 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.7817 Loss_G: 0.8097 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.7927 Loss_G: 0.7988 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.7894 Loss_G: 0.8075 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7659 Loss_G: 0.7905 acc: 73.4%\n",
      "[BATCH 91/149] Loss_D: 0.7627 Loss_G: 0.7854 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.7793 Loss_G: 0.7966 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.8090 Loss_G: 0.7953 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.7542 Loss_G: 0.7997 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7449 Loss_G: 0.7921 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.7768 Loss_G: 0.7940 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.8273 Loss_G: 0.7998 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.7724 Loss_G: 0.7994 acc: 70.3%\n",
      "[BATCH 99/149] Loss_D: 0.7932 Loss_G: 0.7955 acc: 73.4%\n",
      "[BATCH 100/149] Loss_D: 0.7983 Loss_G: 0.8117 acc: 56.2%\n",
      "[BATCH 101/149] Loss_D: 0.7870 Loss_G: 0.8025 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.7609 Loss_G: 0.7984 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.7815 Loss_G: 0.8032 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.8154 Loss_G: 0.8154 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7493 Loss_G: 0.8053 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.7918 Loss_G: 0.8013 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.8128 Loss_G: 0.8112 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.8194 Loss_G: 0.8334 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.7549 Loss_G: 0.8112 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.7656 Loss_G: 0.8017 acc: 70.3%\n",
      "[BATCH 111/149] Loss_D: 0.8024 Loss_G: 0.7962 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7475 Loss_G: 0.7966 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7813 Loss_G: 0.7979 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7626 Loss_G: 0.8005 acc: 71.9%\n",
      "[BATCH 115/149] Loss_D: 0.7598 Loss_G: 0.7998 acc: 71.9%\n",
      "[BATCH 116/149] Loss_D: 0.7753 Loss_G: 0.7884 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7948 Loss_G: 0.7868 acc: 57.8%\n",
      "[BATCH 118/149] Loss_D: 0.7783 Loss_G: 0.7957 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.8555 Loss_G: 0.8212 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7702 Loss_G: 0.8201 acc: 54.7%\n",
      "[BATCH 121/149] Loss_D: 0.7551 Loss_G: 0.7930 acc: 59.4%\n",
      "[BATCH 122/149] Loss_D: 0.7773 Loss_G: 0.7911 acc: 57.8%\n",
      "[BATCH 123/149] Loss_D: 0.7537 Loss_G: 0.7947 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.8393 Loss_G: 0.8082 acc: 57.8%\n",
      "[BATCH 125/149] Loss_D: 0.8004 Loss_G: 0.8104 acc: 70.3%\n",
      "[BATCH 126/149] Loss_D: 0.8143 Loss_G: 0.8130 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7977 Loss_G: 0.8066 acc: 53.1%\n",
      "[BATCH 128/149] Loss_D: 0.7317 Loss_G: 0.7877 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 0.8095 Loss_G: 0.7956 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7610 Loss_G: 0.7947 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.7566 Loss_G: 0.7883 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.8533 Loss_G: 0.8099 acc: 67.2%\n",
      "[EPOCH 4900] TEST ACC is : 74.2%\n",
      "[BATCH 133/149] Loss_D: 0.8358 Loss_G: 0.8189 acc: 53.1%\n",
      "[BATCH 134/149] Loss_D: 0.7825 Loss_G: 0.8223 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.7451 Loss_G: 0.8039 acc: 67.2%\n",
      "[BATCH 136/149] Loss_D: 0.7935 Loss_G: 0.7997 acc: 56.2%\n",
      "[BATCH 137/149] Loss_D: 0.8183 Loss_G: 0.8145 acc: 71.9%\n",
      "[BATCH 138/149] Loss_D: 0.8037 Loss_G: 0.8263 acc: 75.0%\n",
      "[BATCH 139/149] Loss_D: 0.7454 Loss_G: 0.7978 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.8161 Loss_G: 0.7953 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7431 Loss_G: 0.7867 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.7836 Loss_G: 0.7918 acc: 56.2%\n",
      "[BATCH 143/149] Loss_D: 0.7769 Loss_G: 0.7926 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.8057 Loss_G: 0.7971 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.7846 Loss_G: 0.7952 acc: 57.8%\n",
      "[BATCH 146/149] Loss_D: 0.7823 Loss_G: 0.8007 acc: 59.4%\n",
      "[BATCH 147/149] Loss_D: 0.7880 Loss_G: 0.8140 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.7952 Loss_G: 0.8104 acc: 60.9%\n",
      "[BATCH 149/149] Loss_D: 0.7572 Loss_G: 0.7933 acc: 60.9%\n",
      "-----THE [33/50] epoch end-----\n",
      "-----THE [34/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7942 Loss_G: 0.8029 acc: 59.4%\n",
      "[BATCH 2/149] Loss_D: 0.7549 Loss_G: 0.7900 acc: 75.0%\n",
      "[BATCH 3/149] Loss_D: 0.8200 Loss_G: 0.8057 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.8304 Loss_G: 0.8325 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.7931 Loss_G: 0.8156 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7813 Loss_G: 0.8059 acc: 59.4%\n",
      "[BATCH 7/149] Loss_D: 0.7780 Loss_G: 0.7981 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7611 Loss_G: 0.7980 acc: 68.8%\n",
      "[BATCH 9/149] Loss_D: 0.7751 Loss_G: 0.7929 acc: 53.1%\n",
      "[BATCH 10/149] Loss_D: 0.7756 Loss_G: 0.8029 acc: 70.3%\n",
      "[BATCH 11/149] Loss_D: 0.7838 Loss_G: 0.8016 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7680 Loss_G: 0.7918 acc: 73.4%\n",
      "[BATCH 13/149] Loss_D: 0.7584 Loss_G: 0.7917 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7651 Loss_G: 0.7885 acc: 51.6%\n",
      "[BATCH 15/149] Loss_D: 0.8324 Loss_G: 0.8079 acc: 67.2%\n",
      "[BATCH 16/149] Loss_D: 0.7505 Loss_G: 0.8029 acc: 57.8%\n",
      "[BATCH 17/149] Loss_D: 0.7801 Loss_G: 0.8061 acc: 57.8%\n",
      "[BATCH 18/149] Loss_D: 0.7970 Loss_G: 0.7933 acc: 59.4%\n",
      "[BATCH 19/149] Loss_D: 0.7874 Loss_G: 0.7993 acc: 75.0%\n",
      "[BATCH 20/149] Loss_D: 0.7864 Loss_G: 0.7960 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.7699 Loss_G: 0.7945 acc: 59.4%\n",
      "[BATCH 22/149] Loss_D: 0.7686 Loss_G: 0.7956 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.8053 Loss_G: 0.8024 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7658 Loss_G: 0.8011 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.8055 Loss_G: 0.8094 acc: 75.0%\n",
      "[BATCH 26/149] Loss_D: 0.8574 Loss_G: 0.8299 acc: 73.4%\n",
      "[BATCH 27/149] Loss_D: 0.7722 Loss_G: 0.8279 acc: 75.0%\n",
      "[BATCH 28/149] Loss_D: 0.8110 Loss_G: 0.8308 acc: 67.2%\n",
      "[BATCH 29/149] Loss_D: 0.7825 Loss_G: 0.8202 acc: 57.8%\n",
      "[BATCH 30/149] Loss_D: 0.8056 Loss_G: 0.8183 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.7944 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.7442 Loss_G: 0.7918 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.7692 Loss_G: 0.7946 acc: 68.8%\n",
      "[EPOCH 4950] TEST ACC is : 75.2%\n",
      "[BATCH 34/149] Loss_D: 0.8379 Loss_G: 0.8124 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7764 Loss_G: 0.8095 acc: 67.2%\n",
      "[BATCH 36/149] Loss_D: 0.7767 Loss_G: 0.8021 acc: 71.9%\n",
      "[BATCH 37/149] Loss_D: 0.7686 Loss_G: 0.7921 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7827 Loss_G: 0.7952 acc: 70.3%\n",
      "[BATCH 39/149] Loss_D: 0.7633 Loss_G: 0.7983 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.8173 Loss_G: 0.7946 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7894 Loss_G: 0.7961 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.7982 Loss_G: 0.8052 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7823 Loss_G: 0.7966 acc: 65.6%\n",
      "[BATCH 44/149] Loss_D: 0.7609 Loss_G: 0.7896 acc: 57.8%\n",
      "[BATCH 45/149] Loss_D: 0.7533 Loss_G: 0.7843 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.7917 Loss_G: 0.8064 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.7475 Loss_G: 0.8062 acc: 60.9%\n",
      "[BATCH 48/149] Loss_D: 0.7610 Loss_G: 0.7915 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.7707 Loss_G: 0.7956 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7752 Loss_G: 0.8023 acc: 51.6%\n",
      "[BATCH 51/149] Loss_D: 0.8004 Loss_G: 0.8202 acc: 73.4%\n",
      "[BATCH 52/149] Loss_D: 0.7650 Loss_G: 0.8117 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7524 Loss_G: 0.8005 acc: 60.9%\n",
      "[BATCH 54/149] Loss_D: 0.7648 Loss_G: 0.8020 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.8117 Loss_G: 0.8121 acc: 68.8%\n",
      "[BATCH 56/149] Loss_D: 0.7292 Loss_G: 0.7897 acc: 59.4%\n",
      "[BATCH 57/149] Loss_D: 0.8296 Loss_G: 0.8011 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.7975 Loss_G: 0.8184 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.7720 Loss_G: 0.8112 acc: 56.2%\n",
      "[BATCH 60/149] Loss_D: 0.7798 Loss_G: 0.8036 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.7904 Loss_G: 0.8051 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.7893 Loss_G: 0.8084 acc: 56.2%\n",
      "[BATCH 63/149] Loss_D: 0.7615 Loss_G: 0.7970 acc: 56.2%\n",
      "[BATCH 64/149] Loss_D: 0.7797 Loss_G: 0.7951 acc: 64.1%\n",
      "[BATCH 65/149] Loss_D: 0.8037 Loss_G: 0.8046 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.8281 Loss_G: 0.8137 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7748 Loss_G: 0.8130 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7530 Loss_G: 0.7905 acc: 62.5%\n",
      "[BATCH 69/149] Loss_D: 0.7722 Loss_G: 0.7850 acc: 59.4%\n",
      "[BATCH 70/149] Loss_D: 0.7696 Loss_G: 0.7895 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.8055 Loss_G: 0.8049 acc: 70.3%\n",
      "[BATCH 72/149] Loss_D: 0.8327 Loss_G: 0.8102 acc: 70.3%\n",
      "[BATCH 73/149] Loss_D: 0.8219 Loss_G: 0.8174 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7655 Loss_G: 0.8119 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.7908 Loss_G: 0.7991 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.7587 Loss_G: 0.7976 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.7812 Loss_G: 0.7958 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.8101 Loss_G: 0.7995 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.7943 Loss_G: 0.8012 acc: 59.4%\n",
      "[BATCH 80/149] Loss_D: 0.7407 Loss_G: 0.7872 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7999 Loss_G: 0.7926 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.8102 Loss_G: 0.8030 acc: 65.6%\n",
      "[BATCH 83/149] Loss_D: 0.7819 Loss_G: 0.8056 acc: 60.9%\n",
      "[EPOCH 5000] TEST ACC is : 74.0%\n",
      "[BATCH 84/149] Loss_D: 0.7886 Loss_G: 0.7864 acc: 67.2%\n",
      "[BATCH 85/149] Loss_D: 0.7764 Loss_G: 0.7861 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.8236 Loss_G: 0.8059 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.8486 Loss_G: 0.8190 acc: 67.2%\n",
      "[BATCH 88/149] Loss_D: 0.7892 Loss_G: 0.8187 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.7627 Loss_G: 0.8088 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7905 Loss_G: 0.8031 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7470 Loss_G: 0.7921 acc: 56.2%\n",
      "[BATCH 92/149] Loss_D: 0.8462 Loss_G: 0.8170 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.8132 Loss_G: 0.8354 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 0.7585 Loss_G: 0.8045 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7540 Loss_G: 0.7913 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7835 Loss_G: 0.7790 acc: 70.3%\n",
      "[BATCH 97/149] Loss_D: 0.7816 Loss_G: 0.7896 acc: 76.6%\n",
      "[BATCH 98/149] Loss_D: 0.7892 Loss_G: 0.8044 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.8221 Loss_G: 0.8159 acc: 71.9%\n",
      "[BATCH 100/149] Loss_D: 0.8133 Loss_G: 0.8159 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7732 Loss_G: 0.8053 acc: 65.6%\n",
      "[BATCH 102/149] Loss_D: 0.7743 Loss_G: 0.8031 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.8202 Loss_G: 0.7968 acc: 59.4%\n",
      "[BATCH 104/149] Loss_D: 0.7496 Loss_G: 0.7920 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.8473 Loss_G: 0.8017 acc: 75.0%\n",
      "[BATCH 106/149] Loss_D: 0.7754 Loss_G: 0.8036 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.7383 Loss_G: 0.7905 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7973 Loss_G: 0.8008 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.7868 Loss_G: 0.7954 acc: 68.8%\n",
      "[BATCH 110/149] Loss_D: 0.7968 Loss_G: 0.8080 acc: 70.3%\n",
      "[BATCH 111/149] Loss_D: 0.7824 Loss_G: 0.8026 acc: 56.2%\n",
      "[BATCH 112/149] Loss_D: 0.7674 Loss_G: 0.8052 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.7828 Loss_G: 0.8035 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7858 Loss_G: 0.8022 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.7808 Loss_G: 0.7971 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7870 Loss_G: 0.7986 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7917 Loss_G: 0.8039 acc: 75.0%\n",
      "[BATCH 118/149] Loss_D: 0.7698 Loss_G: 0.7930 acc: 62.5%\n",
      "[BATCH 119/149] Loss_D: 0.7883 Loss_G: 0.7947 acc: 71.9%\n",
      "[BATCH 120/149] Loss_D: 0.7906 Loss_G: 0.7865 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.7623 Loss_G: 0.7854 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.7543 Loss_G: 0.7830 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.8007 Loss_G: 0.7979 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7606 Loss_G: 0.7839 acc: 56.2%\n",
      "[BATCH 125/149] Loss_D: 0.7859 Loss_G: 0.7890 acc: 75.0%\n",
      "[BATCH 126/149] Loss_D: 0.7423 Loss_G: 0.7829 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7516 Loss_G: 0.7745 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.8429 Loss_G: 0.7970 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.7840 Loss_G: 0.7989 acc: 68.8%\n",
      "[BATCH 130/149] Loss_D: 0.8580 Loss_G: 0.8221 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.8072 Loss_G: 0.8259 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.7584 Loss_G: 0.8164 acc: 78.1%\n",
      "[BATCH 133/149] Loss_D: 0.7836 Loss_G: 0.7897 acc: 68.8%\n",
      "[EPOCH 5050] TEST ACC is : 75.4%\n",
      "[BATCH 134/149] Loss_D: 0.8762 Loss_G: 0.8108 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.8095 Loss_G: 0.8096 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7567 Loss_G: 0.7910 acc: 73.4%\n",
      "[BATCH 137/149] Loss_D: 0.7871 Loss_G: 0.7989 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.7608 Loss_G: 0.7920 acc: 64.1%\n",
      "[BATCH 139/149] Loss_D: 0.7659 Loss_G: 0.7888 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7794 Loss_G: 0.7897 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.7598 Loss_G: 0.7873 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.8027 Loss_G: 0.7928 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.7875 Loss_G: 0.7931 acc: 56.2%\n",
      "[BATCH 144/149] Loss_D: 0.7525 Loss_G: 0.7896 acc: 56.2%\n",
      "[BATCH 145/149] Loss_D: 0.7666 Loss_G: 0.7864 acc: 79.7%\n",
      "[BATCH 146/149] Loss_D: 0.7462 Loss_G: 0.7741 acc: 71.9%\n",
      "[BATCH 147/149] Loss_D: 0.8194 Loss_G: 0.7997 acc: 57.8%\n",
      "[BATCH 148/149] Loss_D: 0.8251 Loss_G: 0.8116 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7468 Loss_G: 0.7954 acc: 57.8%\n",
      "-----THE [34/50] epoch end-----\n",
      "-----THE [35/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8103 Loss_G: 0.8053 acc: 64.1%\n",
      "[BATCH 2/149] Loss_D: 0.7521 Loss_G: 0.8004 acc: 54.7%\n",
      "[BATCH 3/149] Loss_D: 0.8025 Loss_G: 0.7980 acc: 70.3%\n",
      "[BATCH 4/149] Loss_D: 0.7583 Loss_G: 0.7808 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.7273 Loss_G: 0.7804 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7603 Loss_G: 0.7777 acc: 50.0%\n",
      "[BATCH 7/149] Loss_D: 0.8143 Loss_G: 0.7929 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7648 Loss_G: 0.8005 acc: 75.0%\n",
      "[BATCH 9/149] Loss_D: 0.7965 Loss_G: 0.8084 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7706 Loss_G: 0.7916 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.7705 Loss_G: 0.7816 acc: 62.5%\n",
      "[BATCH 12/149] Loss_D: 0.7933 Loss_G: 0.7926 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7832 Loss_G: 0.7988 acc: 60.9%\n",
      "[BATCH 14/149] Loss_D: 0.7980 Loss_G: 0.8021 acc: 73.4%\n",
      "[BATCH 15/149] Loss_D: 0.7636 Loss_G: 0.7949 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.7576 Loss_G: 0.7896 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7815 Loss_G: 0.7922 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.7701 Loss_G: 0.7935 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.8338 Loss_G: 0.8014 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.7599 Loss_G: 0.7977 acc: 54.7%\n",
      "[BATCH 21/149] Loss_D: 0.7656 Loss_G: 0.7969 acc: 75.0%\n",
      "[BATCH 22/149] Loss_D: 0.7584 Loss_G: 0.7890 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.7679 Loss_G: 0.7901 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.7571 Loss_G: 0.7916 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.7722 Loss_G: 0.7893 acc: 57.8%\n",
      "[BATCH 26/149] Loss_D: 0.7814 Loss_G: 0.7897 acc: 70.3%\n",
      "[BATCH 27/149] Loss_D: 0.8049 Loss_G: 0.8001 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7805 Loss_G: 0.8072 acc: 67.2%\n",
      "[BATCH 29/149] Loss_D: 0.7804 Loss_G: 0.8127 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.8473 Loss_G: 0.8266 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.7669 Loss_G: 0.8085 acc: 65.6%\n",
      "[BATCH 32/149] Loss_D: 0.7937 Loss_G: 0.8124 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.7765 Loss_G: 0.8087 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.7869 Loss_G: 0.8171 acc: 64.1%\n",
      "[EPOCH 5100] TEST ACC is : 74.6%\n",
      "[BATCH 35/149] Loss_D: 0.8102 Loss_G: 0.8075 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.7846 Loss_G: 0.8132 acc: 57.8%\n",
      "[BATCH 37/149] Loss_D: 0.7956 Loss_G: 0.8049 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.8425 Loss_G: 0.8263 acc: 75.0%\n",
      "[BATCH 39/149] Loss_D: 0.7327 Loss_G: 0.8186 acc: 68.8%\n",
      "[BATCH 40/149] Loss_D: 0.7534 Loss_G: 0.7939 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.8263 Loss_G: 0.8163 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.7577 Loss_G: 0.8172 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.8017 Loss_G: 0.8219 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.8015 Loss_G: 0.8139 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.8170 Loss_G: 0.8162 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7510 Loss_G: 0.7971 acc: 60.9%\n",
      "[BATCH 47/149] Loss_D: 0.7973 Loss_G: 0.7957 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.8019 Loss_G: 0.7934 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.7909 Loss_G: 0.7987 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7615 Loss_G: 0.7904 acc: 60.9%\n",
      "[BATCH 51/149] Loss_D: 0.7688 Loss_G: 0.7937 acc: 73.4%\n",
      "[BATCH 52/149] Loss_D: 0.7605 Loss_G: 0.7887 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7860 Loss_G: 0.7971 acc: 67.2%\n",
      "[BATCH 54/149] Loss_D: 0.7917 Loss_G: 0.8017 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.8089 Loss_G: 0.8202 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.8115 Loss_G: 0.8150 acc: 50.0%\n",
      "[BATCH 57/149] Loss_D: 0.7711 Loss_G: 0.7930 acc: 75.0%\n",
      "[BATCH 58/149] Loss_D: 0.7813 Loss_G: 0.7931 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.8004 Loss_G: 0.8036 acc: 60.9%\n",
      "[BATCH 60/149] Loss_D: 0.7580 Loss_G: 0.8024 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.7613 Loss_G: 0.7904 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.8047 Loss_G: 0.7924 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.7883 Loss_G: 0.8070 acc: 76.6%\n",
      "[BATCH 64/149] Loss_D: 0.7841 Loss_G: 0.7916 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.7906 Loss_G: 0.7964 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.7450 Loss_G: 0.7826 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.8297 Loss_G: 0.7977 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.7747 Loss_G: 0.8030 acc: 64.1%\n",
      "[BATCH 69/149] Loss_D: 0.7908 Loss_G: 0.8057 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.8125 Loss_G: 0.7997 acc: 54.7%\n",
      "[BATCH 71/149] Loss_D: 0.7807 Loss_G: 0.7991 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.7895 Loss_G: 0.8057 acc: 64.1%\n",
      "[BATCH 73/149] Loss_D: 0.7651 Loss_G: 0.7956 acc: 68.8%\n",
      "[BATCH 74/149] Loss_D: 0.7912 Loss_G: 0.7949 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.7952 Loss_G: 0.7938 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7713 Loss_G: 0.8023 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.8026 Loss_G: 0.8138 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.8055 Loss_G: 0.8213 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.8161 Loss_G: 0.8309 acc: 65.6%\n",
      "[BATCH 80/149] Loss_D: 0.7802 Loss_G: 0.8137 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7744 Loss_G: 0.7975 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.7751 Loss_G: 0.7983 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.7465 Loss_G: 0.7843 acc: 67.2%\n",
      "[BATCH 84/149] Loss_D: 0.7818 Loss_G: 0.7988 acc: 57.8%\n",
      "[EPOCH 5150] TEST ACC is : 74.2%\n",
      "[BATCH 85/149] Loss_D: 0.7882 Loss_G: 0.7922 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7937 Loss_G: 0.7991 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.7827 Loss_G: 0.8161 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.7467 Loss_G: 0.8018 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.7724 Loss_G: 0.7964 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.8554 Loss_G: 0.8198 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.8090 Loss_G: 0.8170 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7819 Loss_G: 0.8068 acc: 64.1%\n",
      "[BATCH 93/149] Loss_D: 0.7695 Loss_G: 0.7912 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.7733 Loss_G: 0.7903 acc: 56.2%\n",
      "[BATCH 95/149] Loss_D: 0.7879 Loss_G: 0.8002 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.7904 Loss_G: 0.7998 acc: 54.7%\n",
      "[BATCH 97/149] Loss_D: 0.7446 Loss_G: 0.7891 acc: 68.8%\n",
      "[BATCH 98/149] Loss_D: 0.7802 Loss_G: 0.7808 acc: 68.8%\n",
      "[BATCH 99/149] Loss_D: 0.7951 Loss_G: 0.8052 acc: 53.1%\n",
      "[BATCH 100/149] Loss_D: 0.7553 Loss_G: 0.7956 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.8200 Loss_G: 0.7971 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.7606 Loss_G: 0.7824 acc: 81.2%\n",
      "[BATCH 103/149] Loss_D: 0.8090 Loss_G: 0.7869 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.8053 Loss_G: 0.7961 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7866 Loss_G: 0.8037 acc: 57.8%\n",
      "[BATCH 106/149] Loss_D: 0.7617 Loss_G: 0.7888 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.8416 Loss_G: 0.8057 acc: 51.6%\n",
      "[BATCH 108/149] Loss_D: 0.7792 Loss_G: 0.8117 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.7900 Loss_G: 0.8073 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7780 Loss_G: 0.7967 acc: 70.3%\n",
      "[BATCH 111/149] Loss_D: 0.7841 Loss_G: 0.7959 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.7676 Loss_G: 0.7927 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7744 Loss_G: 0.7927 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.8022 Loss_G: 0.8058 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.8285 Loss_G: 0.8084 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.7905 Loss_G: 0.8331 acc: 50.0%\n",
      "[BATCH 117/149] Loss_D: 0.7569 Loss_G: 0.7927 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.7662 Loss_G: 0.7875 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.7892 Loss_G: 0.7922 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7522 Loss_G: 0.7922 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.7924 Loss_G: 0.7946 acc: 76.6%\n",
      "[BATCH 122/149] Loss_D: 0.7772 Loss_G: 0.7950 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.8195 Loss_G: 0.8082 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.8228 Loss_G: 0.8209 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.7543 Loss_G: 0.8215 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7562 Loss_G: 0.8061 acc: 54.7%\n",
      "[BATCH 127/149] Loss_D: 0.7816 Loss_G: 0.8196 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.8223 Loss_G: 0.8144 acc: 53.1%\n",
      "[BATCH 129/149] Loss_D: 0.7669 Loss_G: 0.8155 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.8028 Loss_G: 0.8100 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.8005 Loss_G: 0.8098 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.7789 Loss_G: 0.7963 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7935 Loss_G: 0.7994 acc: 57.8%\n",
      "[BATCH 134/149] Loss_D: 0.7733 Loss_G: 0.7902 acc: 51.6%\n",
      "[EPOCH 5200] TEST ACC is : 73.2%\n",
      "[BATCH 135/149] Loss_D: 0.8230 Loss_G: 0.7999 acc: 62.5%\n",
      "[BATCH 136/149] Loss_D: 0.7809 Loss_G: 0.8012 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7770 Loss_G: 0.7955 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.7749 Loss_G: 0.7865 acc: 56.2%\n",
      "[BATCH 139/149] Loss_D: 0.7974 Loss_G: 0.7916 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7907 Loss_G: 0.7984 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.7677 Loss_G: 0.8057 acc: 65.6%\n",
      "[BATCH 142/149] Loss_D: 0.8054 Loss_G: 0.8055 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.8004 Loss_G: 0.8084 acc: 56.2%\n",
      "[BATCH 144/149] Loss_D: 0.7451 Loss_G: 0.8118 acc: 57.8%\n",
      "[BATCH 145/149] Loss_D: 0.7725 Loss_G: 0.8068 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.8204 Loss_G: 0.8151 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7675 Loss_G: 0.7986 acc: 53.1%\n",
      "[BATCH 148/149] Loss_D: 0.7962 Loss_G: 0.7942 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7573 Loss_G: 0.7912 acc: 70.3%\n",
      "-----THE [35/50] epoch end-----\n",
      "-----THE [36/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8361 Loss_G: 0.8035 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.8504 Loss_G: 0.8312 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.7553 Loss_G: 0.8192 acc: 59.4%\n",
      "[BATCH 4/149] Loss_D: 0.7621 Loss_G: 0.8117 acc: 62.5%\n",
      "[BATCH 5/149] Loss_D: 0.7841 Loss_G: 0.7979 acc: 56.2%\n",
      "[BATCH 6/149] Loss_D: 0.7890 Loss_G: 0.8013 acc: 65.6%\n",
      "[BATCH 7/149] Loss_D: 0.7761 Loss_G: 0.7963 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.7759 Loss_G: 0.7943 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.8020 Loss_G: 0.8056 acc: 57.8%\n",
      "[BATCH 10/149] Loss_D: 0.7642 Loss_G: 0.8046 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.7865 Loss_G: 0.8011 acc: 54.7%\n",
      "[BATCH 12/149] Loss_D: 0.8100 Loss_G: 0.8020 acc: 53.1%\n",
      "[BATCH 13/149] Loss_D: 0.7947 Loss_G: 0.8025 acc: 57.8%\n",
      "[BATCH 14/149] Loss_D: 0.8121 Loss_G: 0.8118 acc: 59.4%\n",
      "[BATCH 15/149] Loss_D: 0.7519 Loss_G: 0.8036 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.7763 Loss_G: 0.7864 acc: 73.4%\n",
      "[BATCH 17/149] Loss_D: 0.7443 Loss_G: 0.7890 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.8007 Loss_G: 0.7907 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.7232 Loss_G: 0.7877 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.7992 Loss_G: 0.7899 acc: 67.2%\n",
      "[BATCH 21/149] Loss_D: 0.7981 Loss_G: 0.8053 acc: 57.8%\n",
      "[BATCH 22/149] Loss_D: 0.7592 Loss_G: 0.7979 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.7694 Loss_G: 0.7941 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.8217 Loss_G: 0.8172 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.8083 Loss_G: 0.8132 acc: 54.7%\n",
      "[BATCH 26/149] Loss_D: 0.7841 Loss_G: 0.8151 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.8156 Loss_G: 0.8093 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.7333 Loss_G: 0.7870 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.8461 Loss_G: 0.8046 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.7948 Loss_G: 0.8053 acc: 64.1%\n",
      "[BATCH 31/149] Loss_D: 0.7654 Loss_G: 0.7917 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7909 Loss_G: 0.7910 acc: 60.9%\n",
      "[BATCH 33/149] Loss_D: 0.7731 Loss_G: 0.8034 acc: 73.4%\n",
      "[BATCH 34/149] Loss_D: 0.7936 Loss_G: 0.7983 acc: 79.7%\n",
      "[BATCH 35/149] Loss_D: 0.7841 Loss_G: 0.8032 acc: 59.4%\n",
      "[EPOCH 5250] TEST ACC is : 74.6%\n",
      "[BATCH 36/149] Loss_D: 0.7500 Loss_G: 0.7949 acc: 68.8%\n",
      "[BATCH 37/149] Loss_D: 0.8086 Loss_G: 0.8055 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.7702 Loss_G: 0.8034 acc: 67.2%\n",
      "[BATCH 39/149] Loss_D: 0.7516 Loss_G: 0.7968 acc: 65.6%\n",
      "[BATCH 40/149] Loss_D: 0.7918 Loss_G: 0.7986 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7404 Loss_G: 0.7911 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.7532 Loss_G: 0.7882 acc: 65.6%\n",
      "[BATCH 43/149] Loss_D: 0.7546 Loss_G: 0.7776 acc: 57.8%\n",
      "[BATCH 44/149] Loss_D: 0.7777 Loss_G: 0.7863 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7878 Loss_G: 0.8027 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7794 Loss_G: 0.8020 acc: 59.4%\n",
      "[BATCH 47/149] Loss_D: 0.8142 Loss_G: 0.8116 acc: 57.8%\n",
      "[BATCH 48/149] Loss_D: 0.7956 Loss_G: 0.8158 acc: 56.2%\n",
      "[BATCH 49/149] Loss_D: 0.7476 Loss_G: 0.8033 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.8178 Loss_G: 0.8041 acc: 57.8%\n",
      "[BATCH 51/149] Loss_D: 0.8270 Loss_G: 0.8162 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.8061 Loss_G: 0.8279 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.8353 Loss_G: 0.8200 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.8266 Loss_G: 0.8261 acc: 57.8%\n",
      "[BATCH 55/149] Loss_D: 0.7969 Loss_G: 0.8025 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7760 Loss_G: 0.7967 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.7899 Loss_G: 0.7930 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.8146 Loss_G: 0.7971 acc: 68.8%\n",
      "[BATCH 59/149] Loss_D: 0.7763 Loss_G: 0.7873 acc: 60.9%\n",
      "[BATCH 60/149] Loss_D: 0.8486 Loss_G: 0.8076 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.8070 Loss_G: 0.8116 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7914 Loss_G: 0.8089 acc: 56.2%\n",
      "[BATCH 63/149] Loss_D: 0.7516 Loss_G: 0.8044 acc: 64.1%\n",
      "[BATCH 64/149] Loss_D: 0.7700 Loss_G: 0.8143 acc: 70.3%\n",
      "[BATCH 65/149] Loss_D: 0.7695 Loss_G: 0.8000 acc: 56.2%\n",
      "[BATCH 66/149] Loss_D: 0.7767 Loss_G: 0.8022 acc: 71.9%\n",
      "[BATCH 67/149] Loss_D: 0.7710 Loss_G: 0.8123 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7497 Loss_G: 0.7986 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.7972 Loss_G: 0.7889 acc: 56.2%\n",
      "[BATCH 70/149] Loss_D: 0.8103 Loss_G: 0.7979 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7908 Loss_G: 0.8009 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7907 Loss_G: 0.7923 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.8017 Loss_G: 0.7910 acc: 60.9%\n",
      "[BATCH 74/149] Loss_D: 0.7593 Loss_G: 0.7871 acc: 73.4%\n",
      "[BATCH 75/149] Loss_D: 0.7543 Loss_G: 0.7837 acc: 48.4%\n",
      "[BATCH 76/149] Loss_D: 0.8528 Loss_G: 0.8062 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7690 Loss_G: 0.8118 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.7800 Loss_G: 0.7990 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.7705 Loss_G: 0.7992 acc: 65.6%\n",
      "[BATCH 80/149] Loss_D: 0.7733 Loss_G: 0.7963 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7791 Loss_G: 0.7926 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.8576 Loss_G: 0.8118 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7676 Loss_G: 0.7994 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.7696 Loss_G: 0.7920 acc: 59.4%\n",
      "[BATCH 85/149] Loss_D: 0.7863 Loss_G: 0.8025 acc: 76.6%\n",
      "[EPOCH 5300] TEST ACC is : 76.2%\n",
      "[BATCH 86/149] Loss_D: 0.7763 Loss_G: 0.8051 acc: 62.5%\n",
      "[BATCH 87/149] Loss_D: 0.7758 Loss_G: 0.8008 acc: 57.8%\n",
      "[BATCH 88/149] Loss_D: 0.7799 Loss_G: 0.7954 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.7872 Loss_G: 0.8145 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7744 Loss_G: 0.8181 acc: 60.9%\n",
      "[BATCH 91/149] Loss_D: 0.7297 Loss_G: 0.8022 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.8463 Loss_G: 0.8283 acc: 57.8%\n",
      "[BATCH 93/149] Loss_D: 0.7532 Loss_G: 0.8097 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.7870 Loss_G: 0.8118 acc: 71.9%\n",
      "[BATCH 95/149] Loss_D: 0.7784 Loss_G: 0.8035 acc: 60.9%\n",
      "[BATCH 96/149] Loss_D: 0.7881 Loss_G: 0.8023 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7886 Loss_G: 0.8078 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.7896 Loss_G: 0.8074 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.7779 Loss_G: 0.8033 acc: 59.4%\n",
      "[BATCH 100/149] Loss_D: 0.7863 Loss_G: 0.8139 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.7641 Loss_G: 0.8095 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.7847 Loss_G: 0.8088 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.7648 Loss_G: 0.8059 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.8541 Loss_G: 0.8118 acc: 54.7%\n",
      "[BATCH 105/149] Loss_D: 0.7427 Loss_G: 0.8080 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7865 Loss_G: 0.7984 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.7936 Loss_G: 0.7966 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7344 Loss_G: 0.8011 acc: 75.0%\n",
      "[BATCH 109/149] Loss_D: 0.8000 Loss_G: 0.7969 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7667 Loss_G: 0.7970 acc: 67.2%\n",
      "[BATCH 111/149] Loss_D: 0.7644 Loss_G: 0.7926 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.7688 Loss_G: 0.7898 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.7365 Loss_G: 0.7901 acc: 54.7%\n",
      "[BATCH 114/149] Loss_D: 0.7436 Loss_G: 0.7844 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.8574 Loss_G: 0.8028 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.8114 Loss_G: 0.8089 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.8268 Loss_G: 0.8178 acc: 75.0%\n",
      "[BATCH 118/149] Loss_D: 0.7827 Loss_G: 0.8102 acc: 68.8%\n",
      "[BATCH 119/149] Loss_D: 0.7642 Loss_G: 0.8014 acc: 73.4%\n",
      "[BATCH 120/149] Loss_D: 0.7701 Loss_G: 0.7976 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.7901 Loss_G: 0.8080 acc: 71.9%\n",
      "[BATCH 122/149] Loss_D: 0.7584 Loss_G: 0.8094 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.7378 Loss_G: 0.7897 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.8000 Loss_G: 0.7990 acc: 56.2%\n",
      "[BATCH 125/149] Loss_D: 0.8219 Loss_G: 0.8179 acc: 60.9%\n",
      "[BATCH 126/149] Loss_D: 0.7883 Loss_G: 0.8275 acc: 65.6%\n",
      "[BATCH 127/149] Loss_D: 0.8012 Loss_G: 0.8300 acc: 60.9%\n",
      "[BATCH 128/149] Loss_D: 0.7646 Loss_G: 0.8061 acc: 64.1%\n",
      "[BATCH 129/149] Loss_D: 0.7877 Loss_G: 0.8048 acc: 59.4%\n",
      "[BATCH 130/149] Loss_D: 0.7835 Loss_G: 0.8129 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.7998 Loss_G: 0.8014 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.7947 Loss_G: 0.8048 acc: 71.9%\n",
      "[BATCH 133/149] Loss_D: 0.7432 Loss_G: 0.7985 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7550 Loss_G: 0.7868 acc: 70.3%\n",
      "[BATCH 135/149] Loss_D: 0.7477 Loss_G: 0.7909 acc: 67.2%\n",
      "[EPOCH 5350] TEST ACC is : 73.0%\n",
      "[BATCH 136/149] Loss_D: 0.7930 Loss_G: 0.7927 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.8194 Loss_G: 0.8098 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7629 Loss_G: 0.8065 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.8285 Loss_G: 0.8047 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7765 Loss_G: 0.7939 acc: 57.8%\n",
      "[BATCH 141/149] Loss_D: 0.7990 Loss_G: 0.8010 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7808 Loss_G: 0.7970 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7453 Loss_G: 0.7950 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.7731 Loss_G: 0.7924 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.8129 Loss_G: 0.7922 acc: 64.1%\n",
      "[BATCH 146/149] Loss_D: 0.7618 Loss_G: 0.7933 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.8037 Loss_G: 0.7995 acc: 54.7%\n",
      "[BATCH 148/149] Loss_D: 0.7678 Loss_G: 0.8020 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7702 Loss_G: 0.8038 acc: 67.2%\n",
      "-----THE [36/50] epoch end-----\n",
      "-----THE [37/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7919 Loss_G: 0.8009 acc: 75.0%\n",
      "[BATCH 2/149] Loss_D: 0.8065 Loss_G: 0.8130 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.7769 Loss_G: 0.8167 acc: 53.1%\n",
      "[BATCH 4/149] Loss_D: 0.7795 Loss_G: 0.8043 acc: 67.2%\n",
      "[BATCH 5/149] Loss_D: 0.7862 Loss_G: 0.7992 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.8175 Loss_G: 0.8059 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8366 Loss_G: 0.8235 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7873 Loss_G: 0.8275 acc: 59.4%\n",
      "[BATCH 9/149] Loss_D: 0.8017 Loss_G: 0.8254 acc: 73.4%\n",
      "[BATCH 10/149] Loss_D: 0.7388 Loss_G: 0.7984 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.7725 Loss_G: 0.7920 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7973 Loss_G: 0.7990 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.8142 Loss_G: 0.8153 acc: 65.6%\n",
      "[BATCH 14/149] Loss_D: 0.7912 Loss_G: 0.8058 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.8458 Loss_G: 0.8224 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.8055 Loss_G: 0.8221 acc: 51.6%\n",
      "[BATCH 17/149] Loss_D: 0.7394 Loss_G: 0.8016 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.8051 Loss_G: 0.8023 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.8002 Loss_G: 0.8016 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7479 Loss_G: 0.7949 acc: 70.3%\n",
      "[BATCH 21/149] Loss_D: 0.8171 Loss_G: 0.7974 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7278 Loss_G: 0.7921 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.8560 Loss_G: 0.8194 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7683 Loss_G: 0.8243 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.7911 Loss_G: 0.8217 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.7925 Loss_G: 0.8112 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7822 Loss_G: 0.8068 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.7613 Loss_G: 0.7956 acc: 64.1%\n",
      "[BATCH 29/149] Loss_D: 0.8124 Loss_G: 0.8091 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.7768 Loss_G: 0.8081 acc: 51.6%\n",
      "[BATCH 31/149] Loss_D: 0.7385 Loss_G: 0.7839 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.8067 Loss_G: 0.7872 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.7829 Loss_G: 0.7998 acc: 59.4%\n",
      "[BATCH 34/149] Loss_D: 0.8134 Loss_G: 0.8047 acc: 71.9%\n",
      "[BATCH 35/149] Loss_D: 0.7725 Loss_G: 0.8091 acc: 71.9%\n",
      "[BATCH 36/149] Loss_D: 0.7813 Loss_G: 0.8143 acc: 64.1%\n",
      "[EPOCH 5400] TEST ACC is : 75.2%\n",
      "[BATCH 37/149] Loss_D: 0.7597 Loss_G: 0.8036 acc: 62.5%\n",
      "[BATCH 38/149] Loss_D: 0.8015 Loss_G: 0.8051 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7479 Loss_G: 0.7999 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.7786 Loss_G: 0.8088 acc: 57.8%\n",
      "[BATCH 41/149] Loss_D: 0.7488 Loss_G: 0.7949 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7943 Loss_G: 0.7940 acc: 65.6%\n",
      "[BATCH 43/149] Loss_D: 0.7586 Loss_G: 0.7963 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7870 Loss_G: 0.7961 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.7854 Loss_G: 0.7938 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.7639 Loss_G: 0.7925 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.8097 Loss_G: 0.7991 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7392 Loss_G: 0.7900 acc: 53.1%\n",
      "[BATCH 49/149] Loss_D: 0.7453 Loss_G: 0.7799 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7645 Loss_G: 0.7855 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.7560 Loss_G: 0.7889 acc: 75.0%\n",
      "[BATCH 52/149] Loss_D: 0.7893 Loss_G: 0.8001 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.7498 Loss_G: 0.7918 acc: 57.8%\n",
      "[BATCH 54/149] Loss_D: 0.7382 Loss_G: 0.7869 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.8153 Loss_G: 0.8006 acc: 54.7%\n",
      "[BATCH 56/149] Loss_D: 0.8093 Loss_G: 0.8054 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.7868 Loss_G: 0.7948 acc: 70.3%\n",
      "[BATCH 58/149] Loss_D: 0.7558 Loss_G: 0.7804 acc: 68.8%\n",
      "[BATCH 59/149] Loss_D: 0.7632 Loss_G: 0.7800 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.8073 Loss_G: 0.7955 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.7766 Loss_G: 0.8013 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.7784 Loss_G: 0.7905 acc: 60.9%\n",
      "[BATCH 63/149] Loss_D: 0.7740 Loss_G: 0.7917 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.7563 Loss_G: 0.7782 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7788 Loss_G: 0.7814 acc: 76.6%\n",
      "[BATCH 66/149] Loss_D: 0.7840 Loss_G: 0.7917 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7771 Loss_G: 0.8069 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.7206 Loss_G: 0.7934 acc: 56.2%\n",
      "[BATCH 69/149] Loss_D: 0.7974 Loss_G: 0.8012 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.7767 Loss_G: 0.8052 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.8166 Loss_G: 0.8146 acc: 59.4%\n",
      "[BATCH 72/149] Loss_D: 0.7745 Loss_G: 0.8112 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.8001 Loss_G: 0.8088 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7711 Loss_G: 0.8030 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.7472 Loss_G: 0.7962 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.8104 Loss_G: 0.8005 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.8371 Loss_G: 0.8161 acc: 57.8%\n",
      "[BATCH 78/149] Loss_D: 0.8002 Loss_G: 0.8203 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.7825 Loss_G: 0.7975 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.8257 Loss_G: 0.8107 acc: 70.3%\n",
      "[BATCH 81/149] Loss_D: 0.7554 Loss_G: 0.8031 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.7616 Loss_G: 0.8065 acc: 59.4%\n",
      "[BATCH 83/149] Loss_D: 0.7906 Loss_G: 0.8060 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.7698 Loss_G: 0.8083 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.7769 Loss_G: 0.8079 acc: 76.6%\n",
      "[BATCH 86/149] Loss_D: 0.7816 Loss_G: 0.8056 acc: 68.8%\n",
      "[EPOCH 5450] TEST ACC is : 74.6%\n",
      "[BATCH 87/149] Loss_D: 0.7915 Loss_G: 0.8052 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7916 Loss_G: 0.8001 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.7885 Loss_G: 0.7988 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7719 Loss_G: 0.7973 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.8002 Loss_G: 0.8052 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7485 Loss_G: 0.7992 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.7602 Loss_G: 0.7871 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 0.7732 Loss_G: 0.7822 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.8063 Loss_G: 0.7931 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.7629 Loss_G: 0.7937 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.7637 Loss_G: 0.7877 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.7642 Loss_G: 0.7856 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7986 Loss_G: 0.8052 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.7721 Loss_G: 0.8062 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.8137 Loss_G: 0.8063 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7588 Loss_G: 0.7880 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.8513 Loss_G: 0.8031 acc: 65.6%\n",
      "[BATCH 104/149] Loss_D: 0.7997 Loss_G: 0.7987 acc: 57.8%\n",
      "[BATCH 105/149] Loss_D: 0.8216 Loss_G: 0.7988 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.8154 Loss_G: 0.8093 acc: 70.3%\n",
      "[BATCH 107/149] Loss_D: 0.7599 Loss_G: 0.8089 acc: 59.4%\n",
      "[BATCH 108/149] Loss_D: 0.7778 Loss_G: 0.8085 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.8251 Loss_G: 0.8257 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.8054 Loss_G: 0.8344 acc: 75.0%\n",
      "[BATCH 111/149] Loss_D: 0.7949 Loss_G: 0.8227 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.7814 Loss_G: 0.8144 acc: 71.9%\n",
      "[BATCH 113/149] Loss_D: 0.8063 Loss_G: 0.8132 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.7995 Loss_G: 0.8194 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7767 Loss_G: 0.8148 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.7998 Loss_G: 0.8098 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.7582 Loss_G: 0.7945 acc: 56.2%\n",
      "[BATCH 118/149] Loss_D: 0.7760 Loss_G: 0.7956 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.7825 Loss_G: 0.7921 acc: 59.4%\n",
      "[BATCH 120/149] Loss_D: 0.8093 Loss_G: 0.8068 acc: 73.4%\n",
      "[BATCH 121/149] Loss_D: 0.7370 Loss_G: 0.7997 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.7823 Loss_G: 0.7949 acc: 59.4%\n",
      "[BATCH 123/149] Loss_D: 0.7980 Loss_G: 0.8042 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.7672 Loss_G: 0.8036 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.7411 Loss_G: 0.8013 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.7677 Loss_G: 0.7922 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7719 Loss_G: 0.7929 acc: 56.2%\n",
      "[BATCH 128/149] Loss_D: 0.7718 Loss_G: 0.7958 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.7722 Loss_G: 0.8014 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7456 Loss_G: 0.8021 acc: 59.4%\n",
      "[BATCH 131/149] Loss_D: 0.7917 Loss_G: 0.8033 acc: 75.0%\n",
      "[BATCH 132/149] Loss_D: 0.7550 Loss_G: 0.7938 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7674 Loss_G: 0.7963 acc: 64.1%\n",
      "[BATCH 134/149] Loss_D: 0.8522 Loss_G: 0.8205 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.8158 Loss_G: 0.8281 acc: 67.2%\n",
      "[BATCH 136/149] Loss_D: 0.7852 Loss_G: 0.8277 acc: 67.2%\n",
      "[EPOCH 5500] TEST ACC is : 75.4%\n",
      "[BATCH 137/149] Loss_D: 0.7801 Loss_G: 0.8081 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.7632 Loss_G: 0.7979 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.8335 Loss_G: 0.8065 acc: 56.2%\n",
      "[BATCH 140/149] Loss_D: 0.7628 Loss_G: 0.8011 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.8027 Loss_G: 0.8081 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7747 Loss_G: 0.8103 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.8141 Loss_G: 0.8113 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7531 Loss_G: 0.7973 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.8341 Loss_G: 0.8153 acc: 54.7%\n",
      "[BATCH 146/149] Loss_D: 0.8386 Loss_G: 0.8276 acc: 70.3%\n",
      "[BATCH 147/149] Loss_D: 0.7513 Loss_G: 0.8066 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.7749 Loss_G: 0.8038 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7729 Loss_G: 0.7935 acc: 64.1%\n",
      "-----THE [37/50] epoch end-----\n",
      "-----THE [38/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8472 Loss_G: 0.8090 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7568 Loss_G: 0.7992 acc: 50.0%\n",
      "[BATCH 3/149] Loss_D: 0.7812 Loss_G: 0.7893 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.7502 Loss_G: 0.7895 acc: 64.1%\n",
      "[BATCH 5/149] Loss_D: 0.7654 Loss_G: 0.7903 acc: 57.8%\n",
      "[BATCH 6/149] Loss_D: 0.7776 Loss_G: 0.7936 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.8187 Loss_G: 0.8044 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7715 Loss_G: 0.7959 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.8285 Loss_G: 0.8049 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7595 Loss_G: 0.8085 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.7919 Loss_G: 0.8041 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.8144 Loss_G: 0.8276 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.8086 Loss_G: 0.8201 acc: 67.2%\n",
      "[BATCH 14/149] Loss_D: 0.7769 Loss_G: 0.8138 acc: 64.1%\n",
      "[BATCH 15/149] Loss_D: 0.7474 Loss_G: 0.8035 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7940 Loss_G: 0.8052 acc: 64.1%\n",
      "[BATCH 17/149] Loss_D: 0.8137 Loss_G: 0.8177 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.7695 Loss_G: 0.8012 acc: 62.5%\n",
      "[BATCH 19/149] Loss_D: 0.8118 Loss_G: 0.8084 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7843 Loss_G: 0.8033 acc: 62.5%\n",
      "[BATCH 21/149] Loss_D: 0.7634 Loss_G: 0.8022 acc: 65.6%\n",
      "[BATCH 22/149] Loss_D: 0.8032 Loss_G: 0.8029 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7747 Loss_G: 0.8043 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.7961 Loss_G: 0.7965 acc: 57.8%\n",
      "[BATCH 25/149] Loss_D: 0.8102 Loss_G: 0.8056 acc: 56.2%\n",
      "[BATCH 26/149] Loss_D: 0.8221 Loss_G: 0.8245 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7608 Loss_G: 0.8054 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.8131 Loss_G: 0.8040 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.7745 Loss_G: 0.8027 acc: 60.9%\n",
      "[BATCH 30/149] Loss_D: 0.7641 Loss_G: 0.7968 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7891 Loss_G: 0.7981 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.7539 Loss_G: 0.7906 acc: 56.2%\n",
      "[BATCH 33/149] Loss_D: 0.7948 Loss_G: 0.7936 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.7797 Loss_G: 0.7945 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7797 Loss_G: 0.8090 acc: 75.0%\n",
      "[BATCH 36/149] Loss_D: 0.7515 Loss_G: 0.7969 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7608 Loss_G: 0.7855 acc: 71.9%\n",
      "[EPOCH 5550] TEST ACC is : 75.0%\n",
      "[BATCH 38/149] Loss_D: 0.7806 Loss_G: 0.7894 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7693 Loss_G: 0.7803 acc: 60.9%\n",
      "[BATCH 40/149] Loss_D: 0.7785 Loss_G: 0.7873 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.8033 Loss_G: 0.7934 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.8261 Loss_G: 0.8052 acc: 59.4%\n",
      "[BATCH 43/149] Loss_D: 0.7780 Loss_G: 0.8031 acc: 60.9%\n",
      "[BATCH 44/149] Loss_D: 0.7882 Loss_G: 0.7946 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.8146 Loss_G: 0.8052 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.8261 Loss_G: 0.8236 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7665 Loss_G: 0.8089 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.7867 Loss_G: 0.7996 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.7708 Loss_G: 0.7913 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7452 Loss_G: 0.7977 acc: 70.3%\n",
      "[BATCH 51/149] Loss_D: 0.7769 Loss_G: 0.8010 acc: 70.3%\n",
      "[BATCH 52/149] Loss_D: 0.8074 Loss_G: 0.8162 acc: 62.5%\n",
      "[BATCH 53/149] Loss_D: 0.7688 Loss_G: 0.8046 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.7413 Loss_G: 0.7986 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.8170 Loss_G: 0.7997 acc: 68.8%\n",
      "[BATCH 56/149] Loss_D: 0.7736 Loss_G: 0.8147 acc: 57.8%\n",
      "[BATCH 57/149] Loss_D: 0.8223 Loss_G: 0.8444 acc: 60.9%\n",
      "[BATCH 58/149] Loss_D: 0.7545 Loss_G: 0.8136 acc: 70.3%\n",
      "[BATCH 59/149] Loss_D: 0.7610 Loss_G: 0.7953 acc: 64.1%\n",
      "[BATCH 60/149] Loss_D: 0.7761 Loss_G: 0.7912 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.7750 Loss_G: 0.7898 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7971 Loss_G: 0.8000 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.7796 Loss_G: 0.7980 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7894 Loss_G: 0.8075 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7524 Loss_G: 0.7941 acc: 68.8%\n",
      "[BATCH 66/149] Loss_D: 0.7941 Loss_G: 0.8070 acc: 67.2%\n",
      "[BATCH 67/149] Loss_D: 0.7672 Loss_G: 0.8053 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7547 Loss_G: 0.7994 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.8249 Loss_G: 0.8199 acc: 56.2%\n",
      "[BATCH 70/149] Loss_D: 0.7784 Loss_G: 0.8310 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.7669 Loss_G: 0.8047 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7970 Loss_G: 0.8035 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7483 Loss_G: 0.7927 acc: 68.8%\n",
      "[BATCH 74/149] Loss_D: 0.8039 Loss_G: 0.8035 acc: 56.2%\n",
      "[BATCH 75/149] Loss_D: 0.7724 Loss_G: 0.7968 acc: 51.6%\n",
      "[BATCH 76/149] Loss_D: 0.8077 Loss_G: 0.7998 acc: 64.1%\n",
      "[BATCH 77/149] Loss_D: 0.7863 Loss_G: 0.8047 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.7962 Loss_G: 0.8014 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.7996 Loss_G: 0.8169 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7711 Loss_G: 0.8099 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.7467 Loss_G: 0.8045 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.8385 Loss_G: 0.8278 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.8342 Loss_G: 0.8179 acc: 75.0%\n",
      "[BATCH 84/149] Loss_D: 0.7609 Loss_G: 0.8111 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.7391 Loss_G: 0.7805 acc: 71.9%\n",
      "[BATCH 86/149] Loss_D: 0.7646 Loss_G: 0.7869 acc: 71.9%\n",
      "[BATCH 87/149] Loss_D: 0.7731 Loss_G: 0.7866 acc: 59.4%\n",
      "[EPOCH 5600] TEST ACC is : 74.8%\n",
      "[BATCH 88/149] Loss_D: 0.8034 Loss_G: 0.7988 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.7996 Loss_G: 0.8108 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7664 Loss_G: 0.8099 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.7726 Loss_G: 0.8040 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.8099 Loss_G: 0.8173 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.7729 Loss_G: 0.8126 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.7814 Loss_G: 0.8111 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7762 Loss_G: 0.8070 acc: 56.2%\n",
      "[BATCH 96/149] Loss_D: 0.7568 Loss_G: 0.8083 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7950 Loss_G: 0.8083 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.7891 Loss_G: 0.8133 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7870 Loss_G: 0.8337 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.8423 Loss_G: 0.8434 acc: 71.9%\n",
      "[BATCH 101/149] Loss_D: 0.7962 Loss_G: 0.8424 acc: 70.3%\n",
      "[BATCH 102/149] Loss_D: 0.8020 Loss_G: 0.8187 acc: 62.5%\n",
      "[BATCH 103/149] Loss_D: 0.7713 Loss_G: 0.8094 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7549 Loss_G: 0.7989 acc: 73.4%\n",
      "[BATCH 105/149] Loss_D: 0.7630 Loss_G: 0.7958 acc: 65.6%\n",
      "[BATCH 106/149] Loss_D: 0.7655 Loss_G: 0.7928 acc: 57.8%\n",
      "[BATCH 107/149] Loss_D: 0.7803 Loss_G: 0.7990 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7945 Loss_G: 0.7896 acc: 62.5%\n",
      "[BATCH 109/149] Loss_D: 0.7585 Loss_G: 0.7895 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.8039 Loss_G: 0.7975 acc: 67.2%\n",
      "[BATCH 111/149] Loss_D: 0.7726 Loss_G: 0.7983 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7636 Loss_G: 0.7918 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.8004 Loss_G: 0.7898 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7767 Loss_G: 0.7990 acc: 67.2%\n",
      "[BATCH 115/149] Loss_D: 0.7516 Loss_G: 0.8049 acc: 73.4%\n",
      "[BATCH 116/149] Loss_D: 0.7743 Loss_G: 0.7969 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.7408 Loss_G: 0.7903 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.7700 Loss_G: 0.7949 acc: 54.7%\n",
      "[BATCH 119/149] Loss_D: 0.7948 Loss_G: 0.7946 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.8104 Loss_G: 0.8129 acc: 65.6%\n",
      "[BATCH 121/149] Loss_D: 0.8097 Loss_G: 0.8058 acc: 59.4%\n",
      "[BATCH 122/149] Loss_D: 0.7689 Loss_G: 0.7989 acc: 56.2%\n",
      "[BATCH 123/149] Loss_D: 0.7668 Loss_G: 0.7969 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.7396 Loss_G: 0.7935 acc: 81.2%\n",
      "[BATCH 125/149] Loss_D: 0.7882 Loss_G: 0.7952 acc: 78.1%\n",
      "[BATCH 126/149] Loss_D: 0.7740 Loss_G: 0.7979 acc: 71.9%\n",
      "[BATCH 127/149] Loss_D: 0.7320 Loss_G: 0.7827 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.7751 Loss_G: 0.7793 acc: 75.0%\n",
      "[BATCH 129/149] Loss_D: 0.7635 Loss_G: 0.7846 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7833 Loss_G: 0.8007 acc: 59.4%\n",
      "[BATCH 131/149] Loss_D: 0.7691 Loss_G: 0.8001 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.7715 Loss_G: 0.7927 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.7712 Loss_G: 0.7937 acc: 56.2%\n",
      "[BATCH 134/149] Loss_D: 0.8601 Loss_G: 0.8127 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.8737 Loss_G: 0.8656 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.7871 Loss_G: 0.8292 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7771 Loss_G: 0.8185 acc: 64.1%\n",
      "[EPOCH 5650] TEST ACC is : 75.4%\n",
      "[BATCH 138/149] Loss_D: 0.7860 Loss_G: 0.8129 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.8002 Loss_G: 0.8148 acc: 71.9%\n",
      "[BATCH 140/149] Loss_D: 0.7896 Loss_G: 0.8124 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.7655 Loss_G: 0.8128 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7648 Loss_G: 0.8090 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7743 Loss_G: 0.7955 acc: 57.8%\n",
      "[BATCH 144/149] Loss_D: 0.7866 Loss_G: 0.7944 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.8222 Loss_G: 0.8055 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7758 Loss_G: 0.8049 acc: 53.1%\n",
      "[BATCH 147/149] Loss_D: 0.7565 Loss_G: 0.7925 acc: 71.9%\n",
      "[BATCH 148/149] Loss_D: 0.7920 Loss_G: 0.8036 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.8179 Loss_G: 0.8170 acc: 68.8%\n",
      "-----THE [38/50] epoch end-----\n",
      "-----THE [39/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8322 Loss_G: 0.8281 acc: 59.4%\n",
      "[BATCH 2/149] Loss_D: 0.7720 Loss_G: 0.8168 acc: 60.9%\n",
      "[BATCH 3/149] Loss_D: 0.7820 Loss_G: 0.8061 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.7251 Loss_G: 0.7913 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.7394 Loss_G: 0.7811 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7604 Loss_G: 0.7808 acc: 65.6%\n",
      "[BATCH 7/149] Loss_D: 0.7740 Loss_G: 0.7869 acc: 59.4%\n",
      "[BATCH 8/149] Loss_D: 0.8029 Loss_G: 0.8038 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7933 Loss_G: 0.8131 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.7904 Loss_G: 0.8180 acc: 68.8%\n",
      "[BATCH 11/149] Loss_D: 0.7891 Loss_G: 0.8191 acc: 53.1%\n",
      "[BATCH 12/149] Loss_D: 0.7487 Loss_G: 0.8019 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7834 Loss_G: 0.7954 acc: 57.8%\n",
      "[BATCH 14/149] Loss_D: 0.7962 Loss_G: 0.7982 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7475 Loss_G: 0.7915 acc: 67.2%\n",
      "[BATCH 16/149] Loss_D: 0.7728 Loss_G: 0.7933 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7710 Loss_G: 0.8041 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.7878 Loss_G: 0.8130 acc: 78.1%\n",
      "[BATCH 19/149] Loss_D: 0.7772 Loss_G: 0.8056 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.7841 Loss_G: 0.8321 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.8066 Loss_G: 0.8273 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.8103 Loss_G: 0.8301 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.7828 Loss_G: 0.8064 acc: 65.6%\n",
      "[BATCH 24/149] Loss_D: 0.7457 Loss_G: 0.8115 acc: 73.4%\n",
      "[BATCH 25/149] Loss_D: 0.7874 Loss_G: 0.7967 acc: 54.7%\n",
      "[BATCH 26/149] Loss_D: 0.7491 Loss_G: 0.7947 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.7803 Loss_G: 0.7925 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.7670 Loss_G: 0.7929 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7832 Loss_G: 0.8013 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.7872 Loss_G: 0.8033 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.8427 Loss_G: 0.8241 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.7883 Loss_G: 0.8152 acc: 78.1%\n",
      "[BATCH 33/149] Loss_D: 0.7831 Loss_G: 0.8022 acc: 57.8%\n",
      "[BATCH 34/149] Loss_D: 0.7562 Loss_G: 0.7972 acc: 70.3%\n",
      "[BATCH 35/149] Loss_D: 0.7814 Loss_G: 0.7893 acc: 73.4%\n",
      "[BATCH 36/149] Loss_D: 0.7604 Loss_G: 0.7891 acc: 59.4%\n",
      "[BATCH 37/149] Loss_D: 0.7914 Loss_G: 0.7904 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.7928 Loss_G: 0.8053 acc: 65.6%\n",
      "[EPOCH 5700] TEST ACC is : 75.6%\n",
      "[BATCH 39/149] Loss_D: 0.8296 Loss_G: 0.8275 acc: 60.9%\n",
      "[BATCH 40/149] Loss_D: 0.8135 Loss_G: 0.8179 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.7616 Loss_G: 0.8106 acc: 56.2%\n",
      "[BATCH 42/149] Loss_D: 0.8103 Loss_G: 0.8063 acc: 60.9%\n",
      "[BATCH 43/149] Loss_D: 0.7613 Loss_G: 0.8102 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7725 Loss_G: 0.7999 acc: 56.2%\n",
      "[BATCH 45/149] Loss_D: 0.8195 Loss_G: 0.8056 acc: 73.4%\n",
      "[BATCH 46/149] Loss_D: 0.7504 Loss_G: 0.8004 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.7536 Loss_G: 0.7937 acc: 73.4%\n",
      "[BATCH 48/149] Loss_D: 0.7862 Loss_G: 0.7974 acc: 56.2%\n",
      "[BATCH 49/149] Loss_D: 0.7549 Loss_G: 0.7952 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.7337 Loss_G: 0.7993 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.8007 Loss_G: 0.7967 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7862 Loss_G: 0.8172 acc: 67.2%\n",
      "[BATCH 53/149] Loss_D: 0.7527 Loss_G: 0.8004 acc: 67.2%\n",
      "[BATCH 54/149] Loss_D: 0.7728 Loss_G: 0.7835 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.8285 Loss_G: 0.8018 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.7773 Loss_G: 0.8143 acc: 60.9%\n",
      "[BATCH 57/149] Loss_D: 0.7806 Loss_G: 0.8013 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.7701 Loss_G: 0.7935 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.7827 Loss_G: 0.7935 acc: 64.1%\n",
      "[BATCH 60/149] Loss_D: 0.7588 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.7591 Loss_G: 0.8046 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.7684 Loss_G: 0.8096 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.8374 Loss_G: 0.8485 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7953 Loss_G: 0.8245 acc: 64.1%\n",
      "[BATCH 65/149] Loss_D: 0.7608 Loss_G: 0.7989 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.7819 Loss_G: 0.7956 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.7810 Loss_G: 0.7952 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7796 Loss_G: 0.7959 acc: 56.2%\n",
      "[BATCH 69/149] Loss_D: 0.8091 Loss_G: 0.8085 acc: 59.4%\n",
      "[BATCH 70/149] Loss_D: 0.7805 Loss_G: 0.8103 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.7721 Loss_G: 0.7976 acc: 59.4%\n",
      "[BATCH 72/149] Loss_D: 0.7485 Loss_G: 0.7897 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7434 Loss_G: 0.7912 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.8029 Loss_G: 0.7991 acc: 56.2%\n",
      "[BATCH 75/149] Loss_D: 0.7962 Loss_G: 0.8039 acc: 57.8%\n",
      "[BATCH 76/149] Loss_D: 0.8015 Loss_G: 0.8095 acc: 70.3%\n",
      "[BATCH 77/149] Loss_D: 0.7680 Loss_G: 0.8204 acc: 57.8%\n",
      "[BATCH 78/149] Loss_D: 0.7578 Loss_G: 0.8025 acc: 67.2%\n",
      "[BATCH 79/149] Loss_D: 0.7547 Loss_G: 0.8012 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.8024 Loss_G: 0.8101 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7889 Loss_G: 0.8236 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.7815 Loss_G: 0.8108 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.8173 Loss_G: 0.8055 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.7463 Loss_G: 0.7912 acc: 54.7%\n",
      "[BATCH 85/149] Loss_D: 0.7896 Loss_G: 0.7918 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.8417 Loss_G: 0.8096 acc: 62.5%\n",
      "[BATCH 87/149] Loss_D: 0.7455 Loss_G: 0.8099 acc: 67.2%\n",
      "[BATCH 88/149] Loss_D: 0.7816 Loss_G: 0.8077 acc: 68.8%\n",
      "[EPOCH 5750] TEST ACC is : 72.1%\n",
      "[BATCH 89/149] Loss_D: 0.8405 Loss_G: 0.8179 acc: 64.1%\n",
      "[BATCH 90/149] Loss_D: 0.7944 Loss_G: 0.8113 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.7314 Loss_G: 0.7956 acc: 68.8%\n",
      "[BATCH 92/149] Loss_D: 0.7618 Loss_G: 0.7932 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.8399 Loss_G: 0.8005 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7863 Loss_G: 0.8077 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7992 Loss_G: 0.8112 acc: 76.6%\n",
      "[BATCH 96/149] Loss_D: 0.7755 Loss_G: 0.8115 acc: 67.2%\n",
      "[BATCH 97/149] Loss_D: 0.8086 Loss_G: 0.8159 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.8215 Loss_G: 0.8357 acc: 71.9%\n",
      "[BATCH 99/149] Loss_D: 0.7902 Loss_G: 0.8210 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.7710 Loss_G: 0.8159 acc: 67.2%\n",
      "[BATCH 101/149] Loss_D: 0.8087 Loss_G: 0.8195 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.8018 Loss_G: 0.8373 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.8251 Loss_G: 0.8338 acc: 78.1%\n",
      "[BATCH 104/149] Loss_D: 0.7319 Loss_G: 0.8223 acc: 73.4%\n",
      "[BATCH 105/149] Loss_D: 0.7990 Loss_G: 0.8108 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.7946 Loss_G: 0.8096 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.7836 Loss_G: 0.7991 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.8142 Loss_G: 0.8004 acc: 62.5%\n",
      "[BATCH 109/149] Loss_D: 0.8267 Loss_G: 0.8055 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.7804 Loss_G: 0.8050 acc: 60.9%\n",
      "[BATCH 111/149] Loss_D: 0.7828 Loss_G: 0.8025 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.8010 Loss_G: 0.8194 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.8060 Loss_G: 0.8390 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.7791 Loss_G: 0.8114 acc: 76.6%\n",
      "[BATCH 115/149] Loss_D: 0.7841 Loss_G: 0.8150 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7604 Loss_G: 0.8037 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.8503 Loss_G: 0.8196 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.7716 Loss_G: 0.8148 acc: 73.4%\n",
      "[BATCH 119/149] Loss_D: 0.7729 Loss_G: 0.7976 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.7550 Loss_G: 0.7882 acc: 57.8%\n",
      "[BATCH 121/149] Loss_D: 0.7490 Loss_G: 0.7836 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.7904 Loss_G: 0.7897 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.8000 Loss_G: 0.7840 acc: 70.3%\n",
      "[BATCH 124/149] Loss_D: 0.7926 Loss_G: 0.7899 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.7981 Loss_G: 0.7922 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.8002 Loss_G: 0.7966 acc: 64.1%\n",
      "[BATCH 127/149] Loss_D: 0.7602 Loss_G: 0.7927 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.7834 Loss_G: 0.7901 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.7895 Loss_G: 0.7972 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7996 Loss_G: 0.8105 acc: 59.4%\n",
      "[BATCH 131/149] Loss_D: 0.8054 Loss_G: 0.8119 acc: 59.4%\n",
      "[BATCH 132/149] Loss_D: 0.7777 Loss_G: 0.8206 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.7774 Loss_G: 0.8107 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.7901 Loss_G: 0.8121 acc: 70.3%\n",
      "[BATCH 135/149] Loss_D: 0.7505 Loss_G: 0.7994 acc: 67.2%\n",
      "[BATCH 136/149] Loss_D: 0.7858 Loss_G: 0.8098 acc: 70.3%\n",
      "[BATCH 137/149] Loss_D: 0.7731 Loss_G: 0.8169 acc: 71.9%\n",
      "[BATCH 138/149] Loss_D: 0.8057 Loss_G: 0.8181 acc: 54.7%\n",
      "[EPOCH 5800] TEST ACC is : 75.0%\n",
      "[BATCH 139/149] Loss_D: 0.7787 Loss_G: 0.8232 acc: 57.8%\n",
      "[BATCH 140/149] Loss_D: 0.7517 Loss_G: 0.8047 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7625 Loss_G: 0.8097 acc: 70.3%\n",
      "[BATCH 142/149] Loss_D: 0.7729 Loss_G: 0.8138 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.7879 Loss_G: 0.8083 acc: 59.4%\n",
      "[BATCH 144/149] Loss_D: 0.7790 Loss_G: 0.8075 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.7881 Loss_G: 0.7999 acc: 59.4%\n",
      "[BATCH 146/149] Loss_D: 0.8254 Loss_G: 0.8054 acc: 59.4%\n",
      "[BATCH 147/149] Loss_D: 0.7752 Loss_G: 0.8063 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7448 Loss_G: 0.7948 acc: 68.8%\n",
      "[BATCH 149/149] Loss_D: 0.7751 Loss_G: 0.7964 acc: 62.5%\n",
      "-----THE [39/50] epoch end-----\n",
      "-----THE [40/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7868 Loss_G: 0.7988 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.8066 Loss_G: 0.8152 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7973 Loss_G: 0.8145 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.7862 Loss_G: 0.8040 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.8088 Loss_G: 0.7977 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7849 Loss_G: 0.8102 acc: 64.1%\n",
      "[BATCH 7/149] Loss_D: 0.7445 Loss_G: 0.7913 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.7940 Loss_G: 0.7919 acc: 78.1%\n",
      "[BATCH 9/149] Loss_D: 0.7699 Loss_G: 0.7971 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.8202 Loss_G: 0.8169 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.7894 Loss_G: 0.8130 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.7903 Loss_G: 0.8172 acc: 57.8%\n",
      "[BATCH 13/149] Loss_D: 0.7412 Loss_G: 0.8079 acc: 70.3%\n",
      "[BATCH 14/149] Loss_D: 0.7379 Loss_G: 0.7905 acc: 59.4%\n",
      "[BATCH 15/149] Loss_D: 0.7525 Loss_G: 0.7824 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.7651 Loss_G: 0.7889 acc: 68.8%\n",
      "[BATCH 17/149] Loss_D: 0.7879 Loss_G: 0.7921 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.7707 Loss_G: 0.7925 acc: 71.9%\n",
      "[BATCH 19/149] Loss_D: 0.7446 Loss_G: 0.7837 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7805 Loss_G: 0.7889 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.8583 Loss_G: 0.8189 acc: 64.1%\n",
      "[BATCH 22/149] Loss_D: 0.8549 Loss_G: 0.8278 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7800 Loss_G: 0.8039 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7813 Loss_G: 0.8031 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.7451 Loss_G: 0.7916 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7633 Loss_G: 0.7870 acc: 60.9%\n",
      "[BATCH 27/149] Loss_D: 0.7825 Loss_G: 0.7950 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7582 Loss_G: 0.7930 acc: 67.2%\n",
      "[BATCH 29/149] Loss_D: 0.7967 Loss_G: 0.7946 acc: 54.7%\n",
      "[BATCH 30/149] Loss_D: 0.7902 Loss_G: 0.8174 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7975 Loss_G: 0.8013 acc: 73.4%\n",
      "[BATCH 32/149] Loss_D: 0.7527 Loss_G: 0.8025 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.7881 Loss_G: 0.8063 acc: 75.0%\n",
      "[BATCH 34/149] Loss_D: 0.8013 Loss_G: 0.8082 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7554 Loss_G: 0.8108 acc: 71.9%\n",
      "[BATCH 36/149] Loss_D: 0.8078 Loss_G: 0.8055 acc: 57.8%\n",
      "[BATCH 37/149] Loss_D: 0.7853 Loss_G: 0.8063 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.8081 Loss_G: 0.8092 acc: 73.4%\n",
      "[BATCH 39/149] Loss_D: 0.7745 Loss_G: 0.8105 acc: 64.1%\n",
      "[EPOCH 5850] TEST ACC is : 75.8%\n",
      "[BATCH 40/149] Loss_D: 0.7927 Loss_G: 0.8069 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.8484 Loss_G: 0.8339 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.8114 Loss_G: 0.8422 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7654 Loss_G: 0.8159 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7835 Loss_G: 0.8092 acc: 62.5%\n",
      "[BATCH 45/149] Loss_D: 0.7711 Loss_G: 0.8109 acc: 68.8%\n",
      "[BATCH 46/149] Loss_D: 0.8127 Loss_G: 0.8159 acc: 54.7%\n",
      "[BATCH 47/149] Loss_D: 0.7858 Loss_G: 0.8152 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7983 Loss_G: 0.8095 acc: 68.8%\n",
      "[BATCH 49/149] Loss_D: 0.7789 Loss_G: 0.7955 acc: 75.0%\n",
      "[BATCH 50/149] Loss_D: 0.7643 Loss_G: 0.7882 acc: 65.6%\n",
      "[BATCH 51/149] Loss_D: 0.7701 Loss_G: 0.7916 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.7603 Loss_G: 0.7922 acc: 60.9%\n",
      "[BATCH 53/149] Loss_D: 0.7551 Loss_G: 0.7840 acc: 53.1%\n",
      "[BATCH 54/149] Loss_D: 0.8122 Loss_G: 0.7863 acc: 76.6%\n",
      "[BATCH 55/149] Loss_D: 0.8510 Loss_G: 0.8115 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7509 Loss_G: 0.8067 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.8131 Loss_G: 0.8062 acc: 73.4%\n",
      "[BATCH 58/149] Loss_D: 0.7836 Loss_G: 0.8040 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.7756 Loss_G: 0.7934 acc: 64.1%\n",
      "[BATCH 60/149] Loss_D: 0.7930 Loss_G: 0.7902 acc: 68.8%\n",
      "[BATCH 61/149] Loss_D: 0.7357 Loss_G: 0.7879 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7857 Loss_G: 0.7940 acc: 73.4%\n",
      "[BATCH 63/149] Loss_D: 0.7525 Loss_G: 0.7984 acc: 67.2%\n",
      "[BATCH 64/149] Loss_D: 0.7522 Loss_G: 0.7957 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.7235 Loss_G: 0.7980 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.8245 Loss_G: 0.8036 acc: 67.2%\n",
      "[BATCH 67/149] Loss_D: 0.7957 Loss_G: 0.8109 acc: 71.9%\n",
      "[BATCH 68/149] Loss_D: 0.7877 Loss_G: 0.8180 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.7839 Loss_G: 0.8024 acc: 75.0%\n",
      "[BATCH 70/149] Loss_D: 0.7633 Loss_G: 0.8031 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.7659 Loss_G: 0.7972 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7631 Loss_G: 0.7952 acc: 70.3%\n",
      "[BATCH 73/149] Loss_D: 0.8006 Loss_G: 0.7942 acc: 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.8083 Loss_G: 0.8092 acc: 73.4%\n",
      "[BATCH 75/149] Loss_D: 0.8680 Loss_G: 0.8376 acc: 71.9%\n",
      "[BATCH 76/149] Loss_D: 0.8241 Loss_G: 0.8237 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.7671 Loss_G: 0.8129 acc: 59.4%\n",
      "[BATCH 78/149] Loss_D: 0.7754 Loss_G: 0.8013 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.7817 Loss_G: 0.7980 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.7592 Loss_G: 0.7910 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7981 Loss_G: 0.7934 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.7764 Loss_G: 0.8068 acc: 56.2%\n",
      "[BATCH 83/149] Loss_D: 0.8254 Loss_G: 0.8088 acc: 57.8%\n",
      "[BATCH 84/149] Loss_D: 0.7892 Loss_G: 0.8054 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.8044 Loss_G: 0.8143 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.7428 Loss_G: 0.7988 acc: 64.1%\n",
      "[BATCH 87/149] Loss_D: 0.7514 Loss_G: 0.7864 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.7520 Loss_G: 0.7861 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.8374 Loss_G: 0.8105 acc: 71.9%\n",
      "[EPOCH 5900] TEST ACC is : 73.4%\n",
      "[BATCH 90/149] Loss_D: 0.7711 Loss_G: 0.8010 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7781 Loss_G: 0.7996 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.7947 Loss_G: 0.7945 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.7687 Loss_G: 0.7982 acc: 59.4%\n",
      "[BATCH 94/149] Loss_D: 0.7871 Loss_G: 0.8039 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.7987 Loss_G: 0.8214 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7762 Loss_G: 0.8249 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7443 Loss_G: 0.7992 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7628 Loss_G: 0.8100 acc: 68.8%\n",
      "[BATCH 99/149] Loss_D: 0.8345 Loss_G: 0.8261 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.8221 Loss_G: 0.8390 acc: 73.4%\n",
      "[BATCH 101/149] Loss_D: 0.7738 Loss_G: 0.8203 acc: 57.8%\n",
      "[BATCH 102/149] Loss_D: 0.7941 Loss_G: 0.8112 acc: 71.9%\n",
      "[BATCH 103/149] Loss_D: 0.7466 Loss_G: 0.7905 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7532 Loss_G: 0.7824 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.8081 Loss_G: 0.8008 acc: 76.6%\n",
      "[BATCH 106/149] Loss_D: 0.8040 Loss_G: 0.8135 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7826 Loss_G: 0.8019 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7984 Loss_G: 0.7990 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.7652 Loss_G: 0.7963 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.7646 Loss_G: 0.7941 acc: 75.0%\n",
      "[BATCH 111/149] Loss_D: 0.8195 Loss_G: 0.8029 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.7674 Loss_G: 0.8028 acc: 75.0%\n",
      "[BATCH 113/149] Loss_D: 0.7569 Loss_G: 0.7970 acc: 59.4%\n",
      "[BATCH 114/149] Loss_D: 0.7607 Loss_G: 0.7891 acc: 60.9%\n",
      "[BATCH 115/149] Loss_D: 0.8136 Loss_G: 0.7990 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.7876 Loss_G: 0.8068 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.7588 Loss_G: 0.8111 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.7578 Loss_G: 0.7931 acc: 53.1%\n",
      "[BATCH 119/149] Loss_D: 0.7702 Loss_G: 0.7894 acc: 56.2%\n",
      "[BATCH 120/149] Loss_D: 0.8297 Loss_G: 0.8052 acc: 60.9%\n",
      "[BATCH 121/149] Loss_D: 0.7419 Loss_G: 0.7926 acc: 56.2%\n",
      "[BATCH 122/149] Loss_D: 0.8048 Loss_G: 0.7963 acc: 48.4%\n",
      "[BATCH 123/149] Loss_D: 0.7601 Loss_G: 0.8006 acc: 62.5%\n",
      "[BATCH 124/149] Loss_D: 0.8036 Loss_G: 0.7937 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7839 Loss_G: 0.7986 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.7960 Loss_G: 0.8007 acc: 75.0%\n",
      "[BATCH 127/149] Loss_D: 0.8204 Loss_G: 0.8149 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.7570 Loss_G: 0.8113 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.7518 Loss_G: 0.8035 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7745 Loss_G: 0.8039 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7519 Loss_G: 0.8032 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.7671 Loss_G: 0.7963 acc: 70.3%\n",
      "[BATCH 133/149] Loss_D: 0.7649 Loss_G: 0.7943 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.7512 Loss_G: 0.7847 acc: 56.2%\n",
      "[BATCH 135/149] Loss_D: 0.7940 Loss_G: 0.7946 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7911 Loss_G: 0.7943 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.7873 Loss_G: 0.7913 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.7653 Loss_G: 0.7765 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7848 Loss_G: 0.7807 acc: 64.1%\n",
      "[EPOCH 5950] TEST ACC is : 73.8%\n",
      "[BATCH 140/149] Loss_D: 0.8290 Loss_G: 0.8068 acc: 68.8%\n",
      "[BATCH 141/149] Loss_D: 0.7778 Loss_G: 0.7954 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7614 Loss_G: 0.7925 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.7906 Loss_G: 0.8060 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.7846 Loss_G: 0.8097 acc: 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.7300 Loss_G: 0.7945 acc: 57.8%\n",
      "[BATCH 146/149] Loss_D: 0.7907 Loss_G: 0.8025 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.8172 Loss_G: 0.8108 acc: 67.2%\n",
      "[BATCH 148/149] Loss_D: 0.7590 Loss_G: 0.8051 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7779 Loss_G: 0.7986 acc: 56.2%\n",
      "-----THE [40/50] epoch end-----\n",
      "-----THE [41/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8001 Loss_G: 0.8146 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7777 Loss_G: 0.8144 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.7851 Loss_G: 0.8145 acc: 54.7%\n",
      "[BATCH 4/149] Loss_D: 0.8191 Loss_G: 0.8081 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.7898 Loss_G: 0.7994 acc: 57.8%\n",
      "[BATCH 6/149] Loss_D: 0.8443 Loss_G: 0.8126 acc: 59.4%\n",
      "[BATCH 7/149] Loss_D: 0.8081 Loss_G: 0.8145 acc: 65.6%\n",
      "[BATCH 8/149] Loss_D: 0.7723 Loss_G: 0.8104 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.7739 Loss_G: 0.8012 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.8207 Loss_G: 0.7972 acc: 57.8%\n",
      "[BATCH 11/149] Loss_D: 0.7736 Loss_G: 0.7929 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7821 Loss_G: 0.7981 acc: 73.4%\n",
      "[BATCH 13/149] Loss_D: 0.7791 Loss_G: 0.8002 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7547 Loss_G: 0.7930 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7542 Loss_G: 0.7914 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.8288 Loss_G: 0.8061 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.7966 Loss_G: 0.8330 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.7408 Loss_G: 0.7932 acc: 57.8%\n",
      "[BATCH 19/149] Loss_D: 0.7771 Loss_G: 0.7905 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.8297 Loss_G: 0.8040 acc: 65.6%\n",
      "[BATCH 21/149] Loss_D: 0.7978 Loss_G: 0.8205 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7593 Loss_G: 0.8074 acc: 65.6%\n",
      "[BATCH 23/149] Loss_D: 0.7900 Loss_G: 0.8096 acc: 75.0%\n",
      "[BATCH 24/149] Loss_D: 0.7810 Loss_G: 0.8001 acc: 54.7%\n",
      "[BATCH 25/149] Loss_D: 0.7742 Loss_G: 0.7993 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.7692 Loss_G: 0.8022 acc: 60.9%\n",
      "[BATCH 27/149] Loss_D: 0.7564 Loss_G: 0.8006 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.7984 Loss_G: 0.8124 acc: 56.2%\n",
      "[BATCH 29/149] Loss_D: 0.7831 Loss_G: 0.8172 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7321 Loss_G: 0.8043 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7925 Loss_G: 0.8027 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.7542 Loss_G: 0.8064 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.8243 Loss_G: 0.8165 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.8242 Loss_G: 0.8356 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7803 Loss_G: 0.8193 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7685 Loss_G: 0.8044 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.7886 Loss_G: 0.8105 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.8364 Loss_G: 0.8284 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.7940 Loss_G: 0.8245 acc: 54.7%\n",
      "[BATCH 40/149] Loss_D: 0.7520 Loss_G: 0.8052 acc: 67.2%\n",
      "[EPOCH 6000] TEST ACC is : 74.2%\n",
      "[BATCH 41/149] Loss_D: 0.8177 Loss_G: 0.8091 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.7835 Loss_G: 0.8021 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7924 Loss_G: 0.8087 acc: 71.9%\n",
      "[BATCH 44/149] Loss_D: 0.7362 Loss_G: 0.8079 acc: 62.5%\n",
      "[BATCH 45/149] Loss_D: 0.7417 Loss_G: 0.7935 acc: 64.1%\n",
      "[BATCH 46/149] Loss_D: 0.7853 Loss_G: 0.7877 acc: 70.3%\n",
      "[BATCH 47/149] Loss_D: 0.7696 Loss_G: 0.7919 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.7572 Loss_G: 0.7896 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.7695 Loss_G: 0.8072 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.7486 Loss_G: 0.7889 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.7700 Loss_G: 0.7904 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.8099 Loss_G: 0.7917 acc: 56.2%\n",
      "[BATCH 53/149] Loss_D: 0.7710 Loss_G: 0.7937 acc: 60.9%\n",
      "[BATCH 54/149] Loss_D: 0.8095 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.7665 Loss_G: 0.7968 acc: 53.1%\n",
      "[BATCH 56/149] Loss_D: 0.7760 Loss_G: 0.7964 acc: 48.4%\n",
      "[BATCH 57/149] Loss_D: 0.8057 Loss_G: 0.8093 acc: 60.9%\n",
      "[BATCH 58/149] Loss_D: 0.7681 Loss_G: 0.8017 acc: 75.0%\n",
      "[BATCH 59/149] Loss_D: 0.7748 Loss_G: 0.7966 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7736 Loss_G: 0.7977 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.8039 Loss_G: 0.8127 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.8227 Loss_G: 0.8312 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7918 Loss_G: 0.8249 acc: 71.9%\n",
      "[BATCH 64/149] Loss_D: 0.7616 Loss_G: 0.8151 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7851 Loss_G: 0.8021 acc: 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.7741 Loss_G: 0.8035 acc: 71.9%\n",
      "[BATCH 67/149] Loss_D: 0.7949 Loss_G: 0.8006 acc: 78.1%\n",
      "[BATCH 68/149] Loss_D: 0.8331 Loss_G: 0.8168 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.7520 Loss_G: 0.8131 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.8286 Loss_G: 0.8090 acc: 57.8%\n",
      "[BATCH 71/149] Loss_D: 0.7903 Loss_G: 0.8056 acc: 75.0%\n",
      "[BATCH 72/149] Loss_D: 0.8080 Loss_G: 0.8094 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7937 Loss_G: 0.8161 acc: 57.8%\n",
      "[BATCH 74/149] Loss_D: 0.7588 Loss_G: 0.8046 acc: 57.8%\n",
      "[BATCH 75/149] Loss_D: 0.7838 Loss_G: 0.8018 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.8255 Loss_G: 0.8173 acc: 57.8%\n",
      "[BATCH 77/149] Loss_D: 0.7972 Loss_G: 0.8120 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.7821 Loss_G: 0.8005 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.7767 Loss_G: 0.7930 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.8039 Loss_G: 0.7949 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7677 Loss_G: 0.7980 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.7631 Loss_G: 0.7918 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7711 Loss_G: 0.7918 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.8243 Loss_G: 0.8080 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.8344 Loss_G: 0.8229 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.7931 Loss_G: 0.8282 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.7908 Loss_G: 0.8145 acc: 56.2%\n",
      "[BATCH 88/149] Loss_D: 0.7872 Loss_G: 0.8040 acc: 73.4%\n",
      "[BATCH 89/149] Loss_D: 0.7680 Loss_G: 0.7953 acc: 76.6%\n",
      "[BATCH 90/149] Loss_D: 0.7861 Loss_G: 0.7994 acc: 56.2%\n",
      "[EPOCH 6050] TEST ACC is : 75.4%\n",
      "[BATCH 91/149] Loss_D: 0.8137 Loss_G: 0.8218 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.8315 Loss_G: 0.8229 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.7840 Loss_G: 0.8106 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.7565 Loss_G: 0.8113 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.7592 Loss_G: 0.7942 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.7562 Loss_G: 0.7916 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7568 Loss_G: 0.7877 acc: 68.8%\n",
      "[BATCH 98/149] Loss_D: 0.7683 Loss_G: 0.7914 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.7484 Loss_G: 0.7826 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.7526 Loss_G: 0.7791 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.7194 Loss_G: 0.7720 acc: 71.9%\n",
      "[BATCH 102/149] Loss_D: 0.8075 Loss_G: 0.7839 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.7793 Loss_G: 0.7944 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.7507 Loss_G: 0.7896 acc: 75.0%\n",
      "[BATCH 105/149] Loss_D: 0.7604 Loss_G: 0.8013 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7482 Loss_G: 0.8071 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.7460 Loss_G: 0.7887 acc: 59.4%\n",
      "[BATCH 108/149] Loss_D: 0.7805 Loss_G: 0.8109 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.7566 Loss_G: 0.7950 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.8007 Loss_G: 0.7960 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7645 Loss_G: 0.8010 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.7803 Loss_G: 0.7977 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.8050 Loss_G: 0.8164 acc: 68.8%\n",
      "[BATCH 114/149] Loss_D: 0.7778 Loss_G: 0.8131 acc: 67.2%\n",
      "[BATCH 115/149] Loss_D: 0.7840 Loss_G: 0.8280 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8493 Loss_G: 0.8523 acc: 62.5%\n",
      "[BATCH 117/149] Loss_D: 0.7904 Loss_G: 0.8129 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7899 Loss_G: 0.7964 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.7884 Loss_G: 0.8017 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7702 Loss_G: 0.8031 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.7759 Loss_G: 0.8056 acc: 60.9%\n",
      "[BATCH 122/149] Loss_D: 0.7507 Loss_G: 0.7943 acc: 50.0%\n",
      "[BATCH 123/149] Loss_D: 0.7450 Loss_G: 0.7876 acc: 65.6%\n",
      "[BATCH 124/149] Loss_D: 0.8240 Loss_G: 0.8003 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.7436 Loss_G: 0.7903 acc: 59.4%\n",
      "[BATCH 126/149] Loss_D: 0.7795 Loss_G: 0.7880 acc: 68.8%\n",
      "[BATCH 127/149] Loss_D: 0.7949 Loss_G: 0.7952 acc: 75.0%\n",
      "[BATCH 128/149] Loss_D: 0.7718 Loss_G: 0.7965 acc: 68.8%\n",
      "[BATCH 129/149] Loss_D: 0.7746 Loss_G: 0.8007 acc: 68.8%\n",
      "[BATCH 130/149] Loss_D: 0.7937 Loss_G: 0.8126 acc: 62.5%\n",
      "[BATCH 131/149] Loss_D: 0.8383 Loss_G: 0.8222 acc: 65.6%\n",
      "[BATCH 132/149] Loss_D: 0.7928 Loss_G: 0.8058 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.7692 Loss_G: 0.7953 acc: 53.1%\n",
      "[BATCH 134/149] Loss_D: 0.7596 Loss_G: 0.7834 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.7443 Loss_G: 0.7796 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.7925 Loss_G: 0.7871 acc: 68.8%\n",
      "[BATCH 137/149] Loss_D: 0.7834 Loss_G: 0.7929 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.7736 Loss_G: 0.7907 acc: 62.5%\n",
      "[BATCH 139/149] Loss_D: 0.7689 Loss_G: 0.7824 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7521 Loss_G: 0.7832 acc: 67.2%\n",
      "[EPOCH 6100] TEST ACC is : 75.0%\n",
      "[BATCH 141/149] Loss_D: 0.7758 Loss_G: 0.7901 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.7958 Loss_G: 0.8074 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.7828 Loss_G: 0.8078 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.8339 Loss_G: 0.8145 acc: 53.1%\n",
      "[BATCH 145/149] Loss_D: 0.7549 Loss_G: 0.8197 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7543 Loss_G: 0.8027 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7558 Loss_G: 0.7908 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.8032 Loss_G: 0.8045 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7724 Loss_G: 0.8062 acc: 60.9%\n",
      "-----THE [41/50] epoch end-----\n",
      "-----THE [42/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7904 Loss_G: 0.8075 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7980 Loss_G: 0.8087 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.7629 Loss_G: 0.8102 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.8298 Loss_G: 0.8114 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.8158 Loss_G: 0.8107 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.8092 Loss_G: 0.8145 acc: 68.8%\n",
      "[BATCH 7/149] Loss_D: 0.7521 Loss_G: 0.8021 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.7811 Loss_G: 0.7910 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.7544 Loss_G: 0.7918 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7536 Loss_G: 0.7869 acc: 73.4%\n",
      "[BATCH 11/149] Loss_D: 0.7851 Loss_G: 0.7981 acc: 59.4%\n",
      "[BATCH 12/149] Loss_D: 0.8023 Loss_G: 0.8026 acc: 64.1%\n",
      "[BATCH 13/149] Loss_D: 0.7781 Loss_G: 0.8035 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7870 Loss_G: 0.8064 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7688 Loss_G: 0.7989 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.7930 Loss_G: 0.7892 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.8213 Loss_G: 0.8049 acc: 76.6%\n",
      "[BATCH 18/149] Loss_D: 0.7782 Loss_G: 0.8025 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7624 Loss_G: 0.7920 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.7709 Loss_G: 0.7930 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.7485 Loss_G: 0.7895 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.7689 Loss_G: 0.7926 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7898 Loss_G: 0.8023 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.7901 Loss_G: 0.8052 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.8006 Loss_G: 0.8019 acc: 53.1%\n",
      "[BATCH 26/149] Loss_D: 0.7946 Loss_G: 0.8036 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.8049 Loss_G: 0.8039 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.7729 Loss_G: 0.8017 acc: 64.1%\n",
      "[BATCH 29/149] Loss_D: 0.7972 Loss_G: 0.8067 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7552 Loss_G: 0.7993 acc: 76.6%\n",
      "[BATCH 31/149] Loss_D: 0.7480 Loss_G: 0.7907 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.7455 Loss_G: 0.7913 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.8016 Loss_G: 0.7998 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.8096 Loss_G: 0.8139 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7825 Loss_G: 0.8151 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.7748 Loss_G: 0.8091 acc: 75.0%\n",
      "[BATCH 37/149] Loss_D: 0.7738 Loss_G: 0.7996 acc: 60.9%\n",
      "[BATCH 38/149] Loss_D: 0.7769 Loss_G: 0.7859 acc: 62.5%\n",
      "[BATCH 39/149] Loss_D: 0.7912 Loss_G: 0.8006 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.7864 Loss_G: 0.7967 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.7991 Loss_G: 0.8055 acc: 50.0%\n",
      "[EPOCH 6150] TEST ACC is : 76.0%\n",
      "[BATCH 42/149] Loss_D: 0.8038 Loss_G: 0.7961 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7786 Loss_G: 0.7927 acc: 67.2%\n",
      "[BATCH 44/149] Loss_D: 0.7765 Loss_G: 0.7902 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.7592 Loss_G: 0.7865 acc: 67.2%\n",
      "[BATCH 46/149] Loss_D: 0.7589 Loss_G: 0.7872 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.7796 Loss_G: 0.7968 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.7535 Loss_G: 0.7898 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.7764 Loss_G: 0.7868 acc: 64.1%\n",
      "[BATCH 50/149] Loss_D: 0.7793 Loss_G: 0.7889 acc: 71.9%\n",
      "[BATCH 51/149] Loss_D: 0.7532 Loss_G: 0.7957 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7776 Loss_G: 0.7942 acc: 75.0%\n",
      "[BATCH 53/149] Loss_D: 0.7617 Loss_G: 0.7984 acc: 56.2%\n",
      "[BATCH 54/149] Loss_D: 0.8018 Loss_G: 0.8137 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7796 Loss_G: 0.8099 acc: 56.2%\n",
      "[BATCH 56/149] Loss_D: 0.7732 Loss_G: 0.8028 acc: 57.8%\n",
      "[BATCH 57/149] Loss_D: 0.7951 Loss_G: 0.8096 acc: 67.2%\n",
      "[BATCH 58/149] Loss_D: 0.7896 Loss_G: 0.8183 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.8026 Loss_G: 0.8148 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7729 Loss_G: 0.7968 acc: 56.2%\n",
      "[BATCH 61/149] Loss_D: 0.7729 Loss_G: 0.7897 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.7899 Loss_G: 0.7993 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7775 Loss_G: 0.7948 acc: 67.2%\n",
      "[BATCH 64/149] Loss_D: 0.8304 Loss_G: 0.8091 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.7700 Loss_G: 0.8037 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.7693 Loss_G: 0.8050 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7963 Loss_G: 0.8084 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.8012 Loss_G: 0.8101 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.8140 Loss_G: 0.8155 acc: 57.8%\n",
      "[BATCH 70/149] Loss_D: 0.7604 Loss_G: 0.8008 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.7684 Loss_G: 0.7975 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8063 Loss_G: 0.8124 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.7905 Loss_G: 0.8198 acc: 56.2%\n",
      "[BATCH 74/149] Loss_D: 0.7831 Loss_G: 0.8127 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.7970 Loss_G: 0.8117 acc: 71.9%\n",
      "[BATCH 76/149] Loss_D: 0.7703 Loss_G: 0.8046 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.7647 Loss_G: 0.8012 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.7437 Loss_G: 0.7909 acc: 71.9%\n",
      "[BATCH 79/149] Loss_D: 0.7535 Loss_G: 0.7878 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7705 Loss_G: 0.7867 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7822 Loss_G: 0.7885 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.8262 Loss_G: 0.8056 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.8182 Loss_G: 0.8184 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.7603 Loss_G: 0.7941 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.8347 Loss_G: 0.8061 acc: 75.0%\n",
      "[BATCH 86/149] Loss_D: 0.7709 Loss_G: 0.8163 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.8231 Loss_G: 0.8192 acc: 73.4%\n",
      "[BATCH 88/149] Loss_D: 0.7943 Loss_G: 0.8134 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.7937 Loss_G: 0.8146 acc: 64.1%\n",
      "[BATCH 90/149] Loss_D: 0.7534 Loss_G: 0.7954 acc: 60.9%\n",
      "[BATCH 91/149] Loss_D: 0.7923 Loss_G: 0.7995 acc: 64.1%\n",
      "[EPOCH 6200] TEST ACC is : 73.8%\n",
      "[BATCH 92/149] Loss_D: 0.7591 Loss_G: 0.8059 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.8117 Loss_G: 0.8133 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.7789 Loss_G: 0.8097 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7581 Loss_G: 0.8055 acc: 71.9%\n",
      "[BATCH 96/149] Loss_D: 0.7751 Loss_G: 0.8079 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7735 Loss_G: 0.7931 acc: 56.2%\n",
      "[BATCH 98/149] Loss_D: 0.8187 Loss_G: 0.7947 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7636 Loss_G: 0.7946 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.7871 Loss_G: 0.7924 acc: 56.2%\n",
      "[BATCH 101/149] Loss_D: 0.7530 Loss_G: 0.7774 acc: 70.3%\n",
      "[BATCH 102/149] Loss_D: 0.7859 Loss_G: 0.7848 acc: 54.7%\n",
      "[BATCH 103/149] Loss_D: 0.7639 Loss_G: 0.7829 acc: 75.0%\n",
      "[BATCH 104/149] Loss_D: 0.7649 Loss_G: 0.7794 acc: 70.3%\n",
      "[BATCH 105/149] Loss_D: 0.7848 Loss_G: 0.7910 acc: 62.5%\n",
      "[BATCH 106/149] Loss_D: 0.7470 Loss_G: 0.7822 acc: 76.6%\n",
      "[BATCH 107/149] Loss_D: 0.7700 Loss_G: 0.7940 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.8225 Loss_G: 0.8089 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.7951 Loss_G: 0.8171 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.7573 Loss_G: 0.7934 acc: 67.2%\n",
      "[BATCH 111/149] Loss_D: 0.7818 Loss_G: 0.7948 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8024 Loss_G: 0.8062 acc: 53.1%\n",
      "[BATCH 113/149] Loss_D: 0.7662 Loss_G: 0.8014 acc: 73.4%\n",
      "[BATCH 114/149] Loss_D: 0.7891 Loss_G: 0.8030 acc: 70.3%\n",
      "[BATCH 115/149] Loss_D: 0.7852 Loss_G: 0.7985 acc: 51.6%\n",
      "[BATCH 116/149] Loss_D: 0.7984 Loss_G: 0.7978 acc: 62.5%\n",
      "[BATCH 117/149] Loss_D: 0.7858 Loss_G: 0.8002 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7679 Loss_G: 0.8034 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.7580 Loss_G: 0.7920 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.7813 Loss_G: 0.7909 acc: 51.6%\n",
      "[BATCH 121/149] Loss_D: 0.7711 Loss_G: 0.7979 acc: 56.2%\n",
      "[BATCH 122/149] Loss_D: 0.7765 Loss_G: 0.7969 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.7868 Loss_G: 0.7951 acc: 56.2%\n",
      "[BATCH 124/149] Loss_D: 0.7535 Loss_G: 0.7946 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.7874 Loss_G: 0.7963 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7548 Loss_G: 0.7921 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.8426 Loss_G: 0.8104 acc: 59.4%\n",
      "[BATCH 128/149] Loss_D: 0.7896 Loss_G: 0.8194 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7660 Loss_G: 0.8056 acc: 67.2%\n",
      "[BATCH 130/149] Loss_D: 0.7972 Loss_G: 0.8071 acc: 68.8%\n",
      "[BATCH 131/149] Loss_D: 0.7792 Loss_G: 0.8130 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.7822 Loss_G: 0.8131 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.7920 Loss_G: 0.8134 acc: 71.9%\n",
      "[BATCH 134/149] Loss_D: 0.7651 Loss_G: 0.8090 acc: 62.5%\n",
      "[BATCH 135/149] Loss_D: 0.7521 Loss_G: 0.7975 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.8016 Loss_G: 0.7993 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7535 Loss_G: 0.7919 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.7731 Loss_G: 0.8075 acc: 70.3%\n",
      "[BATCH 139/149] Loss_D: 0.8066 Loss_G: 0.7989 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.7939 Loss_G: 0.8193 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7875 Loss_G: 0.8112 acc: 71.9%\n",
      "[EPOCH 6250] TEST ACC is : 75.0%\n",
      "[BATCH 142/149] Loss_D: 0.7810 Loss_G: 0.8065 acc: 71.9%\n",
      "[BATCH 143/149] Loss_D: 0.7828 Loss_G: 0.7998 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.7984 Loss_G: 0.8107 acc: 54.7%\n",
      "[BATCH 145/149] Loss_D: 0.7826 Loss_G: 0.8126 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7746 Loss_G: 0.8066 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.7414 Loss_G: 0.8019 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.7945 Loss_G: 0.7988 acc: 64.1%\n",
      "[BATCH 149/149] Loss_D: 0.7859 Loss_G: 0.8198 acc: 57.8%\n",
      "-----THE [42/50] epoch end-----\n",
      "-----THE [43/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8125 Loss_G: 0.8394 acc: 60.9%\n",
      "[BATCH 2/149] Loss_D: 0.7866 Loss_G: 0.8156 acc: 73.4%\n",
      "[BATCH 3/149] Loss_D: 0.8038 Loss_G: 0.8073 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.8043 Loss_G: 0.8084 acc: 64.1%\n",
      "[BATCH 5/149] Loss_D: 0.7867 Loss_G: 0.8045 acc: 68.8%\n",
      "[BATCH 6/149] Loss_D: 0.7870 Loss_G: 0.8127 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.7839 Loss_G: 0.8116 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.8183 Loss_G: 0.8145 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7720 Loss_G: 0.8075 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7681 Loss_G: 0.7959 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.7762 Loss_G: 0.7940 acc: 70.3%\n",
      "[BATCH 12/149] Loss_D: 0.7527 Loss_G: 0.7907 acc: 71.9%\n",
      "[BATCH 13/149] Loss_D: 0.8343 Loss_G: 0.8198 acc: 73.4%\n",
      "[BATCH 14/149] Loss_D: 0.7713 Loss_G: 0.8083 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.7370 Loss_G: 0.7898 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.8012 Loss_G: 0.7935 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7969 Loss_G: 0.7997 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.7690 Loss_G: 0.7935 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.8068 Loss_G: 0.7991 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.7626 Loss_G: 0.7870 acc: 67.2%\n",
      "[BATCH 21/149] Loss_D: 0.7676 Loss_G: 0.7933 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7426 Loss_G: 0.7875 acc: 59.4%\n",
      "[BATCH 23/149] Loss_D: 0.7529 Loss_G: 0.7851 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7554 Loss_G: 0.7870 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.7905 Loss_G: 0.7971 acc: 56.2%\n",
      "[BATCH 26/149] Loss_D: 0.7938 Loss_G: 0.8066 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7624 Loss_G: 0.8026 acc: 53.1%\n",
      "[BATCH 28/149] Loss_D: 0.8116 Loss_G: 0.8135 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.7833 Loss_G: 0.8141 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7507 Loss_G: 0.8044 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.7447 Loss_G: 0.7902 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.7995 Loss_G: 0.8024 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.8047 Loss_G: 0.8162 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.7865 Loss_G: 0.8048 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7580 Loss_G: 0.7850 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7900 Loss_G: 0.7947 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7816 Loss_G: 0.7921 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.8073 Loss_G: 0.7887 acc: 62.5%\n",
      "[BATCH 39/149] Loss_D: 0.7773 Loss_G: 0.8024 acc: 68.8%\n",
      "[BATCH 40/149] Loss_D: 0.7547 Loss_G: 0.7893 acc: 75.0%\n",
      "[BATCH 41/149] Loss_D: 0.7725 Loss_G: 0.7935 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.7839 Loss_G: 0.7994 acc: 57.8%\n",
      "[EPOCH 6300] TEST ACC is : 75.2%\n",
      "[BATCH 43/149] Loss_D: 0.7751 Loss_G: 0.7917 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7364 Loss_G: 0.7803 acc: 71.9%\n",
      "[BATCH 45/149] Loss_D: 0.7358 Loss_G: 0.7748 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.8015 Loss_G: 0.7864 acc: 73.4%\n",
      "[BATCH 47/149] Loss_D: 0.7648 Loss_G: 0.7932 acc: 60.9%\n",
      "[BATCH 48/149] Loss_D: 0.7807 Loss_G: 0.8031 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7777 Loss_G: 0.7940 acc: 75.0%\n",
      "[BATCH 50/149] Loss_D: 0.7679 Loss_G: 0.8064 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.7828 Loss_G: 0.8080 acc: 71.9%\n",
      "[BATCH 52/149] Loss_D: 0.7977 Loss_G: 0.8157 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7820 Loss_G: 0.8120 acc: 64.1%\n",
      "[BATCH 54/149] Loss_D: 0.7770 Loss_G: 0.8207 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.7577 Loss_G: 0.8016 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7610 Loss_G: 0.7916 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.7854 Loss_G: 0.8124 acc: 73.4%\n",
      "[BATCH 58/149] Loss_D: 0.7723 Loss_G: 0.7916 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7540 Loss_G: 0.7872 acc: 73.4%\n",
      "[BATCH 60/149] Loss_D: 0.7824 Loss_G: 0.7915 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.7733 Loss_G: 0.7931 acc: 60.9%\n",
      "[BATCH 62/149] Loss_D: 0.7478 Loss_G: 0.7878 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.8175 Loss_G: 0.7964 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.8082 Loss_G: 0.8051 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.8029 Loss_G: 0.7975 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.7628 Loss_G: 0.7968 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.7509 Loss_G: 0.7866 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.8160 Loss_G: 0.7964 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.7849 Loss_G: 0.7992 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.8140 Loss_G: 0.8032 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.8198 Loss_G: 0.8071 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.8297 Loss_G: 0.8156 acc: 59.4%\n",
      "[BATCH 73/149] Loss_D: 0.8050 Loss_G: 0.8183 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7753 Loss_G: 0.8166 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.7988 Loss_G: 0.8139 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.7899 Loss_G: 0.8068 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.7634 Loss_G: 0.7934 acc: 64.1%\n",
      "[BATCH 78/149] Loss_D: 0.7666 Loss_G: 0.7848 acc: 67.2%\n",
      "[BATCH 79/149] Loss_D: 0.7788 Loss_G: 0.7913 acc: 78.1%\n",
      "[BATCH 80/149] Loss_D: 0.7947 Loss_G: 0.7904 acc: 56.2%\n",
      "[BATCH 81/149] Loss_D: 0.7821 Loss_G: 0.7908 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.8381 Loss_G: 0.8124 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.8224 Loss_G: 0.8134 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.7892 Loss_G: 0.7991 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.7537 Loss_G: 0.8045 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7875 Loss_G: 0.8092 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.7737 Loss_G: 0.8002 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7431 Loss_G: 0.7949 acc: 70.3%\n",
      "[BATCH 89/149] Loss_D: 0.7805 Loss_G: 0.8009 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7992 Loss_G: 0.8088 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.8556 Loss_G: 0.8320 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7467 Loss_G: 0.8039 acc: 64.1%\n",
      "[EPOCH 6350] TEST ACC is : 76.0%\n",
      "[BATCH 93/149] Loss_D: 0.7628 Loss_G: 0.7875 acc: 71.9%\n",
      "[BATCH 94/149] Loss_D: 0.7485 Loss_G: 0.7816 acc: 59.4%\n",
      "[BATCH 95/149] Loss_D: 0.7644 Loss_G: 0.7830 acc: 75.0%\n",
      "[BATCH 96/149] Loss_D: 0.7592 Loss_G: 0.7836 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7997 Loss_G: 0.7857 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.7974 Loss_G: 0.7953 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7686 Loss_G: 0.8011 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.7618 Loss_G: 0.8052 acc: 71.9%\n",
      "[BATCH 101/149] Loss_D: 0.8215 Loss_G: 0.8191 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7475 Loss_G: 0.8198 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.7733 Loss_G: 0.8040 acc: 70.3%\n",
      "[BATCH 104/149] Loss_D: 0.7468 Loss_G: 0.7943 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7536 Loss_G: 0.7940 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7490 Loss_G: 0.7914 acc: 60.9%\n",
      "[BATCH 107/149] Loss_D: 0.7915 Loss_G: 0.7960 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.7484 Loss_G: 0.7956 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.7541 Loss_G: 0.7947 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7771 Loss_G: 0.7938 acc: 60.9%\n",
      "[BATCH 111/149] Loss_D: 0.8047 Loss_G: 0.7907 acc: 75.0%\n",
      "[BATCH 112/149] Loss_D: 0.7498 Loss_G: 0.7879 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7850 Loss_G: 0.7882 acc: 75.0%\n",
      "[BATCH 114/149] Loss_D: 0.7902 Loss_G: 0.7892 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.7931 Loss_G: 0.7973 acc: 73.4%\n",
      "[BATCH 116/149] Loss_D: 0.8176 Loss_G: 0.8136 acc: 68.8%\n",
      "[BATCH 117/149] Loss_D: 0.8094 Loss_G: 0.8142 acc: 71.9%\n",
      "[BATCH 118/149] Loss_D: 0.7540 Loss_G: 0.7991 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.7588 Loss_G: 0.7874 acc: 70.3%\n",
      "[BATCH 120/149] Loss_D: 0.8302 Loss_G: 0.8050 acc: 70.3%\n",
      "[BATCH 121/149] Loss_D: 0.7698 Loss_G: 0.8018 acc: 60.9%\n",
      "[BATCH 122/149] Loss_D: 0.7707 Loss_G: 0.7958 acc: 64.1%\n",
      "[BATCH 123/149] Loss_D: 0.7434 Loss_G: 0.7835 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.7835 Loss_G: 0.7785 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7568 Loss_G: 0.7810 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.8320 Loss_G: 0.8012 acc: 67.2%\n",
      "[BATCH 127/149] Loss_D: 0.8000 Loss_G: 0.8078 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.8234 Loss_G: 0.8103 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.7547 Loss_G: 0.8107 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7974 Loss_G: 0.8110 acc: 60.9%\n",
      "[BATCH 131/149] Loss_D: 0.7427 Loss_G: 0.7983 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.8019 Loss_G: 0.8017 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7630 Loss_G: 0.8002 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.7573 Loss_G: 0.7960 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.7967 Loss_G: 0.7853 acc: 53.1%\n",
      "[BATCH 136/149] Loss_D: 0.8041 Loss_G: 0.7948 acc: 68.8%\n",
      "[BATCH 137/149] Loss_D: 0.8040 Loss_G: 0.8248 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7909 Loss_G: 0.8150 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.8096 Loss_G: 0.8253 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7863 Loss_G: 0.8382 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7518 Loss_G: 0.8121 acc: 57.8%\n",
      "[BATCH 142/149] Loss_D: 0.7520 Loss_G: 0.7940 acc: 53.1%\n",
      "[EPOCH 6400] TEST ACC is : 73.8%\n",
      "[BATCH 143/149] Loss_D: 0.8266 Loss_G: 0.8064 acc: 60.9%\n",
      "[BATCH 144/149] Loss_D: 0.7946 Loss_G: 0.8160 acc: 57.8%\n",
      "[BATCH 145/149] Loss_D: 0.7968 Loss_G: 0.8159 acc: 57.8%\n",
      "[BATCH 146/149] Loss_D: 0.8087 Loss_G: 0.8214 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7710 Loss_G: 0.8161 acc: 73.4%\n",
      "[BATCH 148/149] Loss_D: 0.7815 Loss_G: 0.8057 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.8189 Loss_G: 0.8138 acc: 73.4%\n",
      "-----THE [43/50] epoch end-----\n",
      "-----THE [44/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7819 Loss_G: 0.8132 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7758 Loss_G: 0.8089 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.8030 Loss_G: 0.8086 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.8096 Loss_G: 0.8155 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.8166 Loss_G: 0.8151 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7686 Loss_G: 0.8046 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.7644 Loss_G: 0.7922 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7312 Loss_G: 0.7725 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.8105 Loss_G: 0.7923 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.7864 Loss_G: 0.8000 acc: 71.9%\n",
      "[BATCH 11/149] Loss_D: 0.7648 Loss_G: 0.7832 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7655 Loss_G: 0.7808 acc: 60.9%\n",
      "[BATCH 13/149] Loss_D: 0.7791 Loss_G: 0.7832 acc: 57.8%\n",
      "[BATCH 14/149] Loss_D: 0.7495 Loss_G: 0.7826 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.8173 Loss_G: 0.7889 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.7648 Loss_G: 0.7930 acc: 59.4%\n",
      "[BATCH 17/149] Loss_D: 0.7863 Loss_G: 0.8009 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.7578 Loss_G: 0.7880 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.7933 Loss_G: 0.7891 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.7793 Loss_G: 0.7985 acc: 62.5%\n",
      "[BATCH 21/149] Loss_D: 0.7811 Loss_G: 0.7932 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.8324 Loss_G: 0.8086 acc: 60.9%\n",
      "[BATCH 23/149] Loss_D: 0.7786 Loss_G: 0.8117 acc: 70.3%\n",
      "[BATCH 24/149] Loss_D: 0.7854 Loss_G: 0.8061 acc: 56.2%\n",
      "[BATCH 25/149] Loss_D: 0.7599 Loss_G: 0.8046 acc: 67.2%\n",
      "[BATCH 26/149] Loss_D: 0.7575 Loss_G: 0.7985 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.7801 Loss_G: 0.7983 acc: 56.2%\n",
      "[BATCH 28/149] Loss_D: 0.7642 Loss_G: 0.7933 acc: 68.8%\n",
      "[BATCH 29/149] Loss_D: 0.8082 Loss_G: 0.7993 acc: 71.9%\n",
      "[BATCH 30/149] Loss_D: 0.7918 Loss_G: 0.8029 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7773 Loss_G: 0.8060 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.8103 Loss_G: 0.8080 acc: 51.6%\n",
      "[BATCH 33/149] Loss_D: 0.7872 Loss_G: 0.8175 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.7924 Loss_G: 0.8032 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7444 Loss_G: 0.7908 acc: 76.6%\n",
      "[BATCH 36/149] Loss_D: 0.7624 Loss_G: 0.7849 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.7683 Loss_G: 0.7972 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7453 Loss_G: 0.7898 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.7352 Loss_G: 0.7862 acc: 78.1%\n",
      "[BATCH 40/149] Loss_D: 0.7613 Loss_G: 0.7830 acc: 59.4%\n",
      "[BATCH 41/149] Loss_D: 0.7960 Loss_G: 0.7983 acc: 71.9%\n",
      "[BATCH 42/149] Loss_D: 0.7667 Loss_G: 0.7936 acc: 65.6%\n",
      "[BATCH 43/149] Loss_D: 0.8011 Loss_G: 0.7984 acc: 57.8%\n",
      "[EPOCH 6450] TEST ACC is : 74.6%\n",
      "[BATCH 44/149] Loss_D: 0.8110 Loss_G: 0.7978 acc: 60.9%\n",
      "[BATCH 45/149] Loss_D: 0.8090 Loss_G: 0.8151 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7771 Loss_G: 0.8053 acc: 76.6%\n",
      "[BATCH 47/149] Loss_D: 0.7436 Loss_G: 0.7919 acc: 82.8%\n",
      "[BATCH 48/149] Loss_D: 0.7227 Loss_G: 0.7772 acc: 51.6%\n",
      "[BATCH 49/149] Loss_D: 0.7747 Loss_G: 0.7734 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.7934 Loss_G: 0.7902 acc: 70.3%\n",
      "[BATCH 51/149] Loss_D: 0.7862 Loss_G: 0.7920 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.7510 Loss_G: 0.7783 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7829 Loss_G: 0.7782 acc: 57.8%\n",
      "[BATCH 54/149] Loss_D: 0.7644 Loss_G: 0.7873 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.8098 Loss_G: 0.8043 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.7554 Loss_G: 0.8016 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.8087 Loss_G: 0.8007 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.7572 Loss_G: 0.7960 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.7466 Loss_G: 0.7910 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.8055 Loss_G: 0.8086 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7580 Loss_G: 0.8043 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.7777 Loss_G: 0.8048 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.7907 Loss_G: 0.8004 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.7624 Loss_G: 0.7989 acc: 78.1%\n",
      "[BATCH 65/149] Loss_D: 0.7894 Loss_G: 0.7912 acc: 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.8175 Loss_G: 0.8032 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7690 Loss_G: 0.7982 acc: 64.1%\n",
      "[BATCH 68/149] Loss_D: 0.7683 Loss_G: 0.7993 acc: 59.4%\n",
      "[BATCH 69/149] Loss_D: 0.7422 Loss_G: 0.7818 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.8042 Loss_G: 0.7943 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.7927 Loss_G: 0.7974 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.7538 Loss_G: 0.7842 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.8412 Loss_G: 0.8023 acc: 73.4%\n",
      "[BATCH 74/149] Loss_D: 0.7783 Loss_G: 0.7985 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.7847 Loss_G: 0.8110 acc: 60.9%\n",
      "[BATCH 76/149] Loss_D: 0.8090 Loss_G: 0.8037 acc: 75.0%\n",
      "[BATCH 77/149] Loss_D: 0.8185 Loss_G: 0.8042 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.7908 Loss_G: 0.8013 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.7853 Loss_G: 0.8017 acc: 57.8%\n",
      "[BATCH 80/149] Loss_D: 0.8315 Loss_G: 0.8133 acc: 75.0%\n",
      "[BATCH 81/149] Loss_D: 0.7535 Loss_G: 0.8024 acc: 75.0%\n",
      "[BATCH 82/149] Loss_D: 0.7994 Loss_G: 0.8095 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.7551 Loss_G: 0.8105 acc: 67.2%\n",
      "[BATCH 84/149] Loss_D: 0.7633 Loss_G: 0.7980 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.7978 Loss_G: 0.8021 acc: 59.4%\n",
      "[BATCH 86/149] Loss_D: 0.7488 Loss_G: 0.8096 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.7369 Loss_G: 0.7949 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7800 Loss_G: 0.7958 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7625 Loss_G: 0.8021 acc: 59.4%\n",
      "[BATCH 90/149] Loss_D: 0.7957 Loss_G: 0.8123 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.7688 Loss_G: 0.8104 acc: 59.4%\n",
      "[BATCH 92/149] Loss_D: 0.7977 Loss_G: 0.8129 acc: 75.0%\n",
      "[BATCH 93/149] Loss_D: 0.7732 Loss_G: 0.8116 acc: 68.8%\n",
      "[EPOCH 6500] TEST ACC is : 74.2%\n",
      "[BATCH 94/149] Loss_D: 0.7868 Loss_G: 0.8135 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7943 Loss_G: 0.8166 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.7674 Loss_G: 0.8146 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.7788 Loss_G: 0.7974 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.7831 Loss_G: 0.7927 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7665 Loss_G: 0.7919 acc: 78.1%\n",
      "[BATCH 100/149] Loss_D: 0.7815 Loss_G: 0.7964 acc: 67.2%\n",
      "[BATCH 101/149] Loss_D: 0.7801 Loss_G: 0.8008 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.7639 Loss_G: 0.8055 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.7820 Loss_G: 0.8078 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.8203 Loss_G: 0.8216 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.7809 Loss_G: 0.8136 acc: 53.1%\n",
      "[BATCH 106/149] Loss_D: 0.7527 Loss_G: 0.8058 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.8489 Loss_G: 0.8280 acc: 62.5%\n",
      "[BATCH 108/149] Loss_D: 0.7684 Loss_G: 0.8328 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.7973 Loss_G: 0.8108 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.7381 Loss_G: 0.8024 acc: 45.3%\n",
      "[BATCH 111/149] Loss_D: 0.7879 Loss_G: 0.8011 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7619 Loss_G: 0.8061 acc: 71.9%\n",
      "[BATCH 113/149] Loss_D: 0.8096 Loss_G: 0.8061 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7967 Loss_G: 0.8071 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.7988 Loss_G: 0.7975 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.7909 Loss_G: 0.8051 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7841 Loss_G: 0.7948 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7919 Loss_G: 0.7996 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.8005 Loss_G: 0.8052 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.8045 Loss_G: 0.8295 acc: 60.9%\n",
      "[BATCH 121/149] Loss_D: 0.7786 Loss_G: 0.8320 acc: 71.9%\n",
      "[BATCH 122/149] Loss_D: 0.7753 Loss_G: 0.8117 acc: 73.4%\n",
      "[BATCH 123/149] Loss_D: 0.7779 Loss_G: 0.8056 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7439 Loss_G: 0.7918 acc: 62.5%\n",
      "[BATCH 125/149] Loss_D: 0.8067 Loss_G: 0.8008 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.7560 Loss_G: 0.7944 acc: 78.1%\n",
      "[BATCH 127/149] Loss_D: 0.7714 Loss_G: 0.7973 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.7606 Loss_G: 0.7942 acc: 81.2%\n",
      "[BATCH 129/149] Loss_D: 0.7922 Loss_G: 0.7976 acc: 59.4%\n",
      "[BATCH 130/149] Loss_D: 0.7640 Loss_G: 0.7975 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.7813 Loss_G: 0.7948 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.7418 Loss_G: 0.7917 acc: 54.7%\n",
      "[BATCH 133/149] Loss_D: 0.7866 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.7556 Loss_G: 0.7894 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.8224 Loss_G: 0.7893 acc: 71.9%\n",
      "[BATCH 136/149] Loss_D: 0.7865 Loss_G: 0.7971 acc: 65.6%\n",
      "[BATCH 137/149] Loss_D: 0.8124 Loss_G: 0.8054 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.8059 Loss_G: 0.8064 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.7766 Loss_G: 0.8048 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7594 Loss_G: 0.7890 acc: 65.6%\n",
      "[BATCH 141/149] Loss_D: 0.8121 Loss_G: 0.8000 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7868 Loss_G: 0.8007 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.7628 Loss_G: 0.7970 acc: 62.5%\n",
      "[EPOCH 6550] TEST ACC is : 75.4%\n",
      "[BATCH 144/149] Loss_D: 0.7911 Loss_G: 0.7990 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.8068 Loss_G: 0.8050 acc: 70.3%\n",
      "[BATCH 146/149] Loss_D: 0.7842 Loss_G: 0.8083 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.7856 Loss_G: 0.8077 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.8377 Loss_G: 0.8347 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7816 Loss_G: 0.8321 acc: 71.9%\n",
      "-----THE [44/50] epoch end-----\n",
      "-----THE [45/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7667 Loss_G: 0.8101 acc: 57.8%\n",
      "[BATCH 2/149] Loss_D: 0.7704 Loss_G: 0.7988 acc: 73.4%\n",
      "[BATCH 3/149] Loss_D: 0.7942 Loss_G: 0.8055 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.7771 Loss_G: 0.8083 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.7956 Loss_G: 0.8061 acc: 51.6%\n",
      "[BATCH 6/149] Loss_D: 0.7676 Loss_G: 0.7983 acc: 62.5%\n",
      "[BATCH 7/149] Loss_D: 0.7693 Loss_G: 0.7987 acc: 53.1%\n",
      "[BATCH 8/149] Loss_D: 0.7457 Loss_G: 0.7947 acc: 65.6%\n",
      "[BATCH 9/149] Loss_D: 0.7807 Loss_G: 0.7863 acc: 71.9%\n",
      "[BATCH 10/149] Loss_D: 0.7698 Loss_G: 0.7941 acc: 56.2%\n",
      "[BATCH 11/149] Loss_D: 0.8139 Loss_G: 0.8131 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7582 Loss_G: 0.7905 acc: 75.0%\n",
      "[BATCH 13/149] Loss_D: 0.7508 Loss_G: 0.7800 acc: 64.1%\n",
      "[BATCH 14/149] Loss_D: 0.7783 Loss_G: 0.7817 acc: 64.1%\n",
      "[BATCH 15/149] Loss_D: 0.7721 Loss_G: 0.7894 acc: 67.2%\n",
      "[BATCH 16/149] Loss_D: 0.7610 Loss_G: 0.7974 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.7406 Loss_G: 0.7964 acc: 73.4%\n",
      "[BATCH 18/149] Loss_D: 0.7777 Loss_G: 0.8080 acc: 64.1%\n",
      "[BATCH 19/149] Loss_D: 0.7426 Loss_G: 0.8042 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7611 Loss_G: 0.7860 acc: 75.0%\n",
      "[BATCH 21/149] Loss_D: 0.7806 Loss_G: 0.7926 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7697 Loss_G: 0.8009 acc: 76.6%\n",
      "[BATCH 23/149] Loss_D: 0.7782 Loss_G: 0.7989 acc: 50.0%\n",
      "[BATCH 24/149] Loss_D: 0.7495 Loss_G: 0.7886 acc: 73.4%\n",
      "[BATCH 25/149] Loss_D: 0.7409 Loss_G: 0.7821 acc: 59.4%\n",
      "[BATCH 26/149] Loss_D: 0.7250 Loss_G: 0.7729 acc: 67.2%\n",
      "[BATCH 27/149] Loss_D: 0.7588 Loss_G: 0.7729 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7991 Loss_G: 0.7916 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.7709 Loss_G: 0.8088 acc: 53.1%\n",
      "[BATCH 30/149] Loss_D: 0.8001 Loss_G: 0.8089 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.8129 Loss_G: 0.8119 acc: 64.1%\n",
      "[BATCH 32/149] Loss_D: 0.7822 Loss_G: 0.8043 acc: 54.7%\n",
      "[BATCH 33/149] Loss_D: 0.8508 Loss_G: 0.8164 acc: 57.8%\n",
      "[BATCH 34/149] Loss_D: 0.7671 Loss_G: 0.8011 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7719 Loss_G: 0.7944 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.7860 Loss_G: 0.7967 acc: 67.2%\n",
      "[BATCH 37/149] Loss_D: 0.8319 Loss_G: 0.8153 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7865 Loss_G: 0.8118 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.8278 Loss_G: 0.8179 acc: 59.4%\n",
      "[BATCH 40/149] Loss_D: 0.7744 Loss_G: 0.8081 acc: 62.5%\n",
      "[BATCH 41/149] Loss_D: 0.7866 Loss_G: 0.8035 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7712 Loss_G: 0.8027 acc: 56.2%\n",
      "[BATCH 43/149] Loss_D: 0.8115 Loss_G: 0.8081 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.7555 Loss_G: 0.8057 acc: 65.6%\n",
      "[EPOCH 6600] TEST ACC is : 75.2%\n",
      "[BATCH 45/149] Loss_D: 0.7909 Loss_G: 0.8010 acc: 70.3%\n",
      "[BATCH 46/149] Loss_D: 0.7728 Loss_G: 0.7947 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.7817 Loss_G: 0.7949 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.7756 Loss_G: 0.7988 acc: 59.4%\n",
      "[BATCH 49/149] Loss_D: 0.7935 Loss_G: 0.7956 acc: 60.9%\n",
      "[BATCH 50/149] Loss_D: 0.7722 Loss_G: 0.7944 acc: 71.9%\n",
      "[BATCH 51/149] Loss_D: 0.7716 Loss_G: 0.7884 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.8002 Loss_G: 0.8002 acc: 67.2%\n",
      "[BATCH 53/149] Loss_D: 0.7981 Loss_G: 0.8116 acc: 60.9%\n",
      "[BATCH 54/149] Loss_D: 0.7547 Loss_G: 0.7976 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.7464 Loss_G: 0.7920 acc: 70.3%\n",
      "[BATCH 56/149] Loss_D: 0.7726 Loss_G: 0.7952 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.7294 Loss_G: 0.7835 acc: 75.0%\n",
      "[BATCH 58/149] Loss_D: 0.7558 Loss_G: 0.7803 acc: 56.2%\n",
      "[BATCH 59/149] Loss_D: 0.7883 Loss_G: 0.7880 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7493 Loss_G: 0.7888 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.7499 Loss_G: 0.7781 acc: 78.1%\n",
      "[BATCH 62/149] Loss_D: 0.8199 Loss_G: 0.7941 acc: 71.9%\n",
      "[BATCH 63/149] Loss_D: 0.7605 Loss_G: 0.7965 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7721 Loss_G: 0.7988 acc: 75.0%\n",
      "[BATCH 65/149] Loss_D: 0.7872 Loss_G: 0.8031 acc: 53.1%\n",
      "[BATCH 66/149] Loss_D: 0.7851 Loss_G: 0.8232 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7567 Loss_G: 0.7981 acc: 59.4%\n",
      "[BATCH 68/149] Loss_D: 0.7739 Loss_G: 0.7893 acc: 62.5%\n",
      "[BATCH 69/149] Loss_D: 0.8243 Loss_G: 0.7952 acc: 62.5%\n",
      "[BATCH 70/149] Loss_D: 0.7800 Loss_G: 0.7983 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7713 Loss_G: 0.8004 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7621 Loss_G: 0.8004 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.7765 Loss_G: 0.7951 acc: 65.6%\n",
      "[BATCH 74/149] Loss_D: 0.8016 Loss_G: 0.8021 acc: 71.9%\n",
      "[BATCH 75/149] Loss_D: 0.8114 Loss_G: 0.8071 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.8059 Loss_G: 0.8043 acc: 62.5%\n",
      "[BATCH 77/149] Loss_D: 0.7826 Loss_G: 0.8015 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7835 Loss_G: 0.8049 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.7941 Loss_G: 0.8020 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.7777 Loss_G: 0.7991 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.8161 Loss_G: 0.8167 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.8000 Loss_G: 0.8157 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.7884 Loss_G: 0.8264 acc: 67.2%\n",
      "[BATCH 84/149] Loss_D: 0.7890 Loss_G: 0.8126 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.7925 Loss_G: 0.8071 acc: 60.9%\n",
      "[BATCH 86/149] Loss_D: 0.8386 Loss_G: 0.8078 acc: 57.8%\n",
      "[BATCH 87/149] Loss_D: 0.7614 Loss_G: 0.7949 acc: 70.3%\n",
      "[BATCH 88/149] Loss_D: 0.8035 Loss_G: 0.7987 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.7877 Loss_G: 0.8017 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7717 Loss_G: 0.7998 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7737 Loss_G: 0.7912 acc: 75.0%\n",
      "[BATCH 92/149] Loss_D: 0.7948 Loss_G: 0.7917 acc: 53.1%\n",
      "[BATCH 93/149] Loss_D: 0.7526 Loss_G: 0.7902 acc: 56.2%\n",
      "[BATCH 94/149] Loss_D: 0.7844 Loss_G: 0.7896 acc: 56.2%\n",
      "[EPOCH 6650] TEST ACC is : 75.2%\n",
      "[BATCH 95/149] Loss_D: 0.8048 Loss_G: 0.7966 acc: 65.6%\n",
      "[BATCH 96/149] Loss_D: 0.7693 Loss_G: 0.8059 acc: 57.8%\n",
      "[BATCH 97/149] Loss_D: 0.8077 Loss_G: 0.8070 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7646 Loss_G: 0.7996 acc: 62.5%\n",
      "[BATCH 99/149] Loss_D: 0.7789 Loss_G: 0.8058 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.8019 Loss_G: 0.8062 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.7908 Loss_G: 0.8146 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.7531 Loss_G: 0.8070 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.7981 Loss_G: 0.8063 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7650 Loss_G: 0.7998 acc: 59.4%\n",
      "[BATCH 105/149] Loss_D: 0.8267 Loss_G: 0.8149 acc: 70.3%\n",
      "[BATCH 106/149] Loss_D: 0.7737 Loss_G: 0.8119 acc: 70.3%\n",
      "[BATCH 107/149] Loss_D: 0.8629 Loss_G: 0.8133 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.7772 Loss_G: 0.8131 acc: 53.1%\n",
      "[BATCH 109/149] Loss_D: 0.8040 Loss_G: 0.8097 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7849 Loss_G: 0.8018 acc: 53.1%\n",
      "[BATCH 111/149] Loss_D: 0.8178 Loss_G: 0.8112 acc: 60.9%\n",
      "[BATCH 112/149] Loss_D: 0.8023 Loss_G: 0.8169 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.7685 Loss_G: 0.8094 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7539 Loss_G: 0.7992 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.7653 Loss_G: 0.7954 acc: 62.5%\n",
      "[BATCH 116/149] Loss_D: 0.7606 Loss_G: 0.7988 acc: 76.6%\n",
      "[BATCH 117/149] Loss_D: 0.7578 Loss_G: 0.7988 acc: 59.4%\n",
      "[BATCH 118/149] Loss_D: 0.7826 Loss_G: 0.8049 acc: 46.9%\n",
      "[BATCH 119/149] Loss_D: 0.8141 Loss_G: 0.8055 acc: 75.0%\n",
      "[BATCH 120/149] Loss_D: 0.7683 Loss_G: 0.8039 acc: 57.8%\n",
      "[BATCH 121/149] Loss_D: 0.7944 Loss_G: 0.8096 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.7226 Loss_G: 0.7881 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.7966 Loss_G: 0.7952 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7695 Loss_G: 0.7893 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.7636 Loss_G: 0.7943 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7538 Loss_G: 0.7908 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.7917 Loss_G: 0.7922 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.8095 Loss_G: 0.8042 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.7540 Loss_G: 0.8082 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.7847 Loss_G: 0.8050 acc: 65.6%\n",
      "[BATCH 131/149] Loss_D: 0.8171 Loss_G: 0.8106 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7590 Loss_G: 0.8011 acc: 56.2%\n",
      "[BATCH 133/149] Loss_D: 0.8058 Loss_G: 0.8012 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.7945 Loss_G: 0.8140 acc: 62.5%\n",
      "[BATCH 135/149] Loss_D: 0.7956 Loss_G: 0.8103 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.7976 Loss_G: 0.8211 acc: 68.8%\n",
      "[BATCH 137/149] Loss_D: 0.7830 Loss_G: 0.8072 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.8214 Loss_G: 0.8184 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7978 Loss_G: 0.8311 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.7869 Loss_G: 0.8225 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.7588 Loss_G: 0.7957 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.7884 Loss_G: 0.7989 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7753 Loss_G: 0.8082 acc: 71.9%\n",
      "[BATCH 144/149] Loss_D: 0.7827 Loss_G: 0.8130 acc: 70.3%\n",
      "[EPOCH 6700] TEST ACC is : 75.2%\n",
      "[BATCH 145/149] Loss_D: 0.7635 Loss_G: 0.8033 acc: 56.2%\n",
      "[BATCH 146/149] Loss_D: 0.7549 Loss_G: 0.7934 acc: 56.2%\n",
      "[BATCH 147/149] Loss_D: 0.7787 Loss_G: 0.7954 acc: 67.2%\n",
      "[BATCH 148/149] Loss_D: 0.7627 Loss_G: 0.7943 acc: 64.1%\n",
      "[BATCH 149/149] Loss_D: 0.7757 Loss_G: 0.7975 acc: 67.2%\n",
      "-----THE [45/50] epoch end-----\n",
      "-----THE [46/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7589 Loss_G: 0.8005 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7445 Loss_G: 0.7974 acc: 60.9%\n",
      "[BATCH 3/149] Loss_D: 0.7450 Loss_G: 0.7910 acc: 70.3%\n",
      "[BATCH 4/149] Loss_D: 0.7885 Loss_G: 0.7942 acc: 56.2%\n",
      "[BATCH 5/149] Loss_D: 0.7548 Loss_G: 0.7865 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7489 Loss_G: 0.7762 acc: 73.4%\n",
      "[BATCH 7/149] Loss_D: 0.7912 Loss_G: 0.7817 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.7790 Loss_G: 0.7912 acc: 68.8%\n",
      "[BATCH 9/149] Loss_D: 0.8840 Loss_G: 0.8250 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.7969 Loss_G: 0.8048 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.7996 Loss_G: 0.7985 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7429 Loss_G: 0.7904 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7902 Loss_G: 0.7969 acc: 60.9%\n",
      "[BATCH 14/149] Loss_D: 0.7661 Loss_G: 0.8043 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.8051 Loss_G: 0.8067 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7763 Loss_G: 0.8038 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.7644 Loss_G: 0.8042 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.8083 Loss_G: 0.8127 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.7886 Loss_G: 0.7983 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.7715 Loss_G: 0.7938 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.8331 Loss_G: 0.8298 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.7769 Loss_G: 0.8200 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.7664 Loss_G: 0.8094 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7878 Loss_G: 0.8029 acc: 71.9%\n",
      "[BATCH 25/149] Loss_D: 0.7783 Loss_G: 0.7942 acc: 54.7%\n",
      "[BATCH 26/149] Loss_D: 0.7767 Loss_G: 0.7868 acc: 54.7%\n",
      "[BATCH 27/149] Loss_D: 0.7859 Loss_G: 0.7932 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7933 Loss_G: 0.8024 acc: 76.6%\n",
      "[BATCH 29/149] Loss_D: 0.7573 Loss_G: 0.7895 acc: 62.5%\n",
      "[BATCH 30/149] Loss_D: 0.8043 Loss_G: 0.8013 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7751 Loss_G: 0.7977 acc: 57.8%\n",
      "[BATCH 32/149] Loss_D: 0.7688 Loss_G: 0.8015 acc: 57.8%\n",
      "[BATCH 33/149] Loss_D: 0.8189 Loss_G: 0.8114 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.7878 Loss_G: 0.8263 acc: 70.3%\n",
      "[BATCH 35/149] Loss_D: 0.7824 Loss_G: 0.8454 acc: 68.8%\n",
      "[BATCH 36/149] Loss_D: 0.7914 Loss_G: 0.8123 acc: 68.8%\n",
      "[BATCH 37/149] Loss_D: 0.7512 Loss_G: 0.8040 acc: 62.5%\n",
      "[BATCH 38/149] Loss_D: 0.7715 Loss_G: 0.8164 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.7673 Loss_G: 0.7981 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.7712 Loss_G: 0.7957 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.7572 Loss_G: 0.7803 acc: 56.2%\n",
      "[BATCH 42/149] Loss_D: 0.7919 Loss_G: 0.7899 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.7891 Loss_G: 0.7910 acc: 73.4%\n",
      "[BATCH 44/149] Loss_D: 0.7787 Loss_G: 0.7993 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7657 Loss_G: 0.8048 acc: 60.9%\n",
      "[EPOCH 6750] TEST ACC is : 74.6%\n",
      "[BATCH 46/149] Loss_D: 0.7900 Loss_G: 0.8091 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7379 Loss_G: 0.8064 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.7864 Loss_G: 0.8057 acc: 56.2%\n",
      "[BATCH 49/149] Loss_D: 0.7317 Loss_G: 0.7946 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7478 Loss_G: 0.7843 acc: 70.3%\n",
      "[BATCH 51/149] Loss_D: 0.7662 Loss_G: 0.7874 acc: 70.3%\n",
      "[BATCH 52/149] Loss_D: 0.7750 Loss_G: 0.7898 acc: 57.8%\n",
      "[BATCH 53/149] Loss_D: 0.8303 Loss_G: 0.8133 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.7779 Loss_G: 0.8081 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.7922 Loss_G: 0.8056 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.8191 Loss_G: 0.8161 acc: 56.2%\n",
      "[BATCH 57/149] Loss_D: 0.7542 Loss_G: 0.8041 acc: 60.9%\n",
      "[BATCH 58/149] Loss_D: 0.7543 Loss_G: 0.7879 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.8149 Loss_G: 0.7981 acc: 64.1%\n",
      "[BATCH 60/149] Loss_D: 0.7596 Loss_G: 0.7898 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.7707 Loss_G: 0.7891 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.7547 Loss_G: 0.7805 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7959 Loss_G: 0.7919 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.7598 Loss_G: 0.7937 acc: 64.1%\n",
      "[BATCH 65/149] Loss_D: 0.7602 Loss_G: 0.7896 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.8470 Loss_G: 0.8161 acc: 71.9%\n",
      "[BATCH 67/149] Loss_D: 0.7928 Loss_G: 0.8178 acc: 71.9%\n",
      "[BATCH 68/149] Loss_D: 0.7850 Loss_G: 0.8029 acc: 57.8%\n",
      "[BATCH 69/149] Loss_D: 0.7280 Loss_G: 0.7841 acc: 51.6%\n",
      "[BATCH 70/149] Loss_D: 0.8066 Loss_G: 0.7916 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.7408 Loss_G: 0.7870 acc: 67.2%\n",
      "[BATCH 72/149] Loss_D: 0.7556 Loss_G: 0.7935 acc: 73.4%\n",
      "[BATCH 73/149] Loss_D: 0.7247 Loss_G: 0.7782 acc: 75.0%\n",
      "[BATCH 74/149] Loss_D: 0.7891 Loss_G: 0.7777 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.8053 Loss_G: 0.7895 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.7899 Loss_G: 0.7922 acc: 64.1%\n",
      "[BATCH 77/149] Loss_D: 0.7538 Loss_G: 0.7883 acc: 76.6%\n",
      "[BATCH 78/149] Loss_D: 0.7620 Loss_G: 0.7892 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.7982 Loss_G: 0.8018 acc: 65.6%\n",
      "[BATCH 80/149] Loss_D: 0.8032 Loss_G: 0.8064 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7776 Loss_G: 0.8034 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.8004 Loss_G: 0.8071 acc: 53.1%\n",
      "[BATCH 83/149] Loss_D: 0.8092 Loss_G: 0.8194 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.7605 Loss_G: 0.8123 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.7537 Loss_G: 0.7986 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7944 Loss_G: 0.7971 acc: 57.8%\n",
      "[BATCH 87/149] Loss_D: 0.7918 Loss_G: 0.8046 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7655 Loss_G: 0.7954 acc: 62.5%\n",
      "[BATCH 89/149] Loss_D: 0.8048 Loss_G: 0.8030 acc: 73.4%\n",
      "[BATCH 90/149] Loss_D: 0.7783 Loss_G: 0.8064 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.8239 Loss_G: 0.8235 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7978 Loss_G: 0.8176 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.8040 Loss_G: 0.8165 acc: 57.8%\n",
      "[BATCH 94/149] Loss_D: 0.8024 Loss_G: 0.8464 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.8022 Loss_G: 0.8398 acc: 67.2%\n",
      "[EPOCH 6800] TEST ACC is : 75.2%\n",
      "[BATCH 96/149] Loss_D: 0.7887 Loss_G: 0.8180 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.8311 Loss_G: 0.8085 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.7650 Loss_G: 0.8034 acc: 75.0%\n",
      "[BATCH 99/149] Loss_D: 0.7635 Loss_G: 0.8029 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.7730 Loss_G: 0.7984 acc: 59.4%\n",
      "[BATCH 101/149] Loss_D: 0.7981 Loss_G: 0.8154 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.7559 Loss_G: 0.8101 acc: 56.2%\n",
      "[BATCH 103/149] Loss_D: 0.8309 Loss_G: 0.8074 acc: 54.7%\n",
      "[BATCH 104/149] Loss_D: 0.7411 Loss_G: 0.7875 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.8581 Loss_G: 0.8186 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7725 Loss_G: 0.8016 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.7681 Loss_G: 0.7909 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.7460 Loss_G: 0.7855 acc: 62.5%\n",
      "[BATCH 109/149] Loss_D: 0.7636 Loss_G: 0.7792 acc: 59.4%\n",
      "[BATCH 110/149] Loss_D: 0.8264 Loss_G: 0.7995 acc: 75.0%\n",
      "[BATCH 111/149] Loss_D: 0.7956 Loss_G: 0.8061 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.7883 Loss_G: 0.8070 acc: 60.9%\n",
      "[BATCH 113/149] Loss_D: 0.7839 Loss_G: 0.8017 acc: 71.9%\n",
      "[BATCH 114/149] Loss_D: 0.7786 Loss_G: 0.7971 acc: 53.1%\n",
      "[BATCH 115/149] Loss_D: 0.8132 Loss_G: 0.8113 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.7655 Loss_G: 0.7979 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.8117 Loss_G: 0.8060 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.8201 Loss_G: 0.8226 acc: 59.4%\n",
      "[BATCH 119/149] Loss_D: 0.8106 Loss_G: 0.8223 acc: 71.9%\n",
      "[BATCH 120/149] Loss_D: 0.8483 Loss_G: 0.8367 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.7602 Loss_G: 0.8189 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.8162 Loss_G: 0.8098 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.7676 Loss_G: 0.8024 acc: 48.4%\n",
      "[BATCH 124/149] Loss_D: 0.7527 Loss_G: 0.7980 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.7658 Loss_G: 0.7920 acc: 73.4%\n",
      "[BATCH 126/149] Loss_D: 0.7749 Loss_G: 0.7987 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7642 Loss_G: 0.7897 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.7746 Loss_G: 0.7872 acc: 51.6%\n",
      "[BATCH 129/149] Loss_D: 0.7613 Loss_G: 0.7869 acc: 60.9%\n",
      "[BATCH 130/149] Loss_D: 0.8050 Loss_G: 0.7889 acc: 65.6%\n",
      "[BATCH 131/149] Loss_D: 0.7463 Loss_G: 0.7936 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.8095 Loss_G: 0.7959 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.7757 Loss_G: 0.7921 acc: 54.7%\n",
      "[BATCH 134/149] Loss_D: 0.7823 Loss_G: 0.7989 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7992 Loss_G: 0.8094 acc: 71.9%\n",
      "[BATCH 136/149] Loss_D: 0.7689 Loss_G: 0.8094 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.7595 Loss_G: 0.7967 acc: 68.8%\n",
      "[BATCH 138/149] Loss_D: 0.7479 Loss_G: 0.7875 acc: 62.5%\n",
      "[BATCH 139/149] Loss_D: 0.7807 Loss_G: 0.7755 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7453 Loss_G: 0.7788 acc: 64.1%\n",
      "[BATCH 141/149] Loss_D: 0.7385 Loss_G: 0.7771 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.7641 Loss_G: 0.7847 acc: 71.9%\n",
      "[BATCH 143/149] Loss_D: 0.7654 Loss_G: 0.7855 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.8001 Loss_G: 0.7881 acc: 53.1%\n",
      "[BATCH 145/149] Loss_D: 0.7861 Loss_G: 0.7868 acc: 81.2%\n",
      "[EPOCH 6850] TEST ACC is : 74.8%\n",
      "[BATCH 146/149] Loss_D: 0.8002 Loss_G: 0.7934 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.7519 Loss_G: 0.7896 acc: 64.1%\n",
      "[BATCH 148/149] Loss_D: 0.7446 Loss_G: 0.7864 acc: 59.4%\n",
      "[BATCH 149/149] Loss_D: 0.7819 Loss_G: 0.8031 acc: 64.1%\n",
      "-----THE [46/50] epoch end-----\n",
      "-----THE [47/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8020 Loss_G: 0.8076 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7797 Loss_G: 0.8181 acc: 59.4%\n",
      "[BATCH 3/149] Loss_D: 0.7932 Loss_G: 0.8192 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.7860 Loss_G: 0.8190 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.8222 Loss_G: 0.8240 acc: 62.5%\n",
      "[BATCH 6/149] Loss_D: 0.8111 Loss_G: 0.8196 acc: 59.4%\n",
      "[BATCH 7/149] Loss_D: 0.7666 Loss_G: 0.8148 acc: 67.2%\n",
      "[BATCH 8/149] Loss_D: 0.7433 Loss_G: 0.7985 acc: 62.5%\n",
      "[BATCH 9/149] Loss_D: 0.8172 Loss_G: 0.8055 acc: 57.8%\n",
      "[BATCH 10/149] Loss_D: 0.7705 Loss_G: 0.8034 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.7562 Loss_G: 0.7962 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.8086 Loss_G: 0.8117 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7446 Loss_G: 0.8057 acc: 73.4%\n",
      "[BATCH 14/149] Loss_D: 0.7744 Loss_G: 0.7988 acc: 71.9%\n",
      "[BATCH 15/149] Loss_D: 0.7558 Loss_G: 0.7993 acc: 62.5%\n",
      "[BATCH 16/149] Loss_D: 0.8018 Loss_G: 0.8005 acc: 73.4%\n",
      "[BATCH 17/149] Loss_D: 0.7581 Loss_G: 0.7936 acc: 56.2%\n",
      "[BATCH 18/149] Loss_D: 0.7553 Loss_G: 0.7910 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.7699 Loss_G: 0.7919 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7915 Loss_G: 0.7861 acc: 57.8%\n",
      "[BATCH 21/149] Loss_D: 0.7694 Loss_G: 0.7941 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7730 Loss_G: 0.7916 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.7843 Loss_G: 0.8008 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7353 Loss_G: 0.7921 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.8314 Loss_G: 0.8056 acc: 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.7819 Loss_G: 0.7929 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.7813 Loss_G: 0.8047 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7581 Loss_G: 0.7970 acc: 67.2%\n",
      "[BATCH 29/149] Loss_D: 0.7601 Loss_G: 0.7928 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.7830 Loss_G: 0.7863 acc: 60.9%\n",
      "[BATCH 31/149] Loss_D: 0.7609 Loss_G: 0.7854 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.8125 Loss_G: 0.8004 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.7994 Loss_G: 0.8041 acc: 73.4%\n",
      "[BATCH 34/149] Loss_D: 0.7848 Loss_G: 0.8028 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7820 Loss_G: 0.7971 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.7821 Loss_G: 0.7984 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.7933 Loss_G: 0.8031 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7949 Loss_G: 0.8247 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.7944 Loss_G: 0.8059 acc: 65.6%\n",
      "[BATCH 40/149] Loss_D: 0.8053 Loss_G: 0.7989 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.7576 Loss_G: 0.7869 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7577 Loss_G: 0.7878 acc: 73.4%\n",
      "[BATCH 43/149] Loss_D: 0.8034 Loss_G: 0.7970 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.7500 Loss_G: 0.7828 acc: 70.3%\n",
      "[BATCH 45/149] Loss_D: 0.7941 Loss_G: 0.7806 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7798 Loss_G: 0.7857 acc: 59.4%\n",
      "[EPOCH 6900] TEST ACC is : 72.5%\n",
      "[BATCH 47/149] Loss_D: 0.7779 Loss_G: 0.7862 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7641 Loss_G: 0.7843 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.7955 Loss_G: 0.7876 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.7612 Loss_G: 0.7907 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.7786 Loss_G: 0.7904 acc: 76.6%\n",
      "[BATCH 52/149] Loss_D: 0.7540 Loss_G: 0.7819 acc: 54.7%\n",
      "[BATCH 53/149] Loss_D: 0.7911 Loss_G: 0.7889 acc: 57.8%\n",
      "[BATCH 54/149] Loss_D: 0.7981 Loss_G: 0.8107 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.7745 Loss_G: 0.8046 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.7867 Loss_G: 0.8061 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.7674 Loss_G: 0.8029 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7738 Loss_G: 0.8060 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7902 Loss_G: 0.8045 acc: 64.1%\n",
      "[BATCH 60/149] Loss_D: 0.7576 Loss_G: 0.8052 acc: 56.2%\n",
      "[BATCH 61/149] Loss_D: 0.8030 Loss_G: 0.8069 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7858 Loss_G: 0.8069 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.7985 Loss_G: 0.8159 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.7615 Loss_G: 0.8046 acc: 67.2%\n",
      "[BATCH 65/149] Loss_D: 0.7802 Loss_G: 0.8057 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.7773 Loss_G: 0.7990 acc: 46.9%\n",
      "[BATCH 67/149] Loss_D: 0.8064 Loss_G: 0.8016 acc: 75.0%\n",
      "[BATCH 68/149] Loss_D: 0.7423 Loss_G: 0.7880 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.7800 Loss_G: 0.7886 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.7757 Loss_G: 0.7993 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.8679 Loss_G: 0.8164 acc: 57.8%\n",
      "[BATCH 72/149] Loss_D: 0.7802 Loss_G: 0.8082 acc: 70.3%\n",
      "[BATCH 73/149] Loss_D: 0.8033 Loss_G: 0.8055 acc: 65.6%\n",
      "[BATCH 74/149] Loss_D: 0.7738 Loss_G: 0.7915 acc: 70.3%\n",
      "[BATCH 75/149] Loss_D: 0.7383 Loss_G: 0.7853 acc: 73.4%\n",
      "[BATCH 76/149] Loss_D: 0.7809 Loss_G: 0.7923 acc: 71.9%\n",
      "[BATCH 77/149] Loss_D: 0.7641 Loss_G: 0.7891 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.7852 Loss_G: 0.7967 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.7559 Loss_G: 0.8056 acc: 71.9%\n",
      "[BATCH 80/149] Loss_D: 0.7279 Loss_G: 0.7833 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.8089 Loss_G: 0.7925 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.8323 Loss_G: 0.8140 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.8149 Loss_G: 0.8190 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.7829 Loss_G: 0.8108 acc: 73.4%\n",
      "[BATCH 85/149] Loss_D: 0.7740 Loss_G: 0.8049 acc: 54.7%\n",
      "[BATCH 86/149] Loss_D: 0.7880 Loss_G: 0.7974 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.7995 Loss_G: 0.7974 acc: 56.2%\n",
      "[BATCH 88/149] Loss_D: 0.7805 Loss_G: 0.7950 acc: 60.9%\n",
      "[BATCH 89/149] Loss_D: 0.7752 Loss_G: 0.7959 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.8415 Loss_G: 0.8317 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.7960 Loss_G: 0.8106 acc: 56.2%\n",
      "[BATCH 92/149] Loss_D: 0.7344 Loss_G: 0.7834 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.7876 Loss_G: 0.7842 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7836 Loss_G: 0.7980 acc: 78.1%\n",
      "[BATCH 95/149] Loss_D: 0.7583 Loss_G: 0.8011 acc: 60.9%\n",
      "[BATCH 96/149] Loss_D: 0.8189 Loss_G: 0.8133 acc: 62.5%\n",
      "[EPOCH 6950] TEST ACC is : 75.2%\n",
      "[BATCH 97/149] Loss_D: 0.8274 Loss_G: 0.8254 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7637 Loss_G: 0.8088 acc: 57.8%\n",
      "[BATCH 99/149] Loss_D: 0.7936 Loss_G: 0.8121 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.7663 Loss_G: 0.8076 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.7887 Loss_G: 0.8083 acc: 54.7%\n",
      "[BATCH 102/149] Loss_D: 0.7538 Loss_G: 0.8099 acc: 62.5%\n",
      "[BATCH 103/149] Loss_D: 0.7805 Loss_G: 0.8082 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.7659 Loss_G: 0.7967 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.7624 Loss_G: 0.7947 acc: 56.2%\n",
      "[BATCH 106/149] Loss_D: 0.7683 Loss_G: 0.7968 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.8099 Loss_G: 0.8100 acc: 62.5%\n",
      "[BATCH 108/149] Loss_D: 0.7581 Loss_G: 0.8078 acc: 54.7%\n",
      "[BATCH 109/149] Loss_D: 0.8113 Loss_G: 0.8152 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.7641 Loss_G: 0.8022 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.8112 Loss_G: 0.8048 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7942 Loss_G: 0.8044 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.7655 Loss_G: 0.8094 acc: 78.1%\n",
      "[BATCH 114/149] Loss_D: 0.8010 Loss_G: 0.8064 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.7770 Loss_G: 0.8082 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.7812 Loss_G: 0.8158 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7978 Loss_G: 0.8083 acc: 56.2%\n",
      "[BATCH 118/149] Loss_D: 0.7708 Loss_G: 0.7995 acc: 56.2%\n",
      "[BATCH 119/149] Loss_D: 0.7794 Loss_G: 0.7959 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7868 Loss_G: 0.7959 acc: 67.2%\n",
      "[BATCH 121/149] Loss_D: 0.7463 Loss_G: 0.7905 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.8177 Loss_G: 0.7917 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.7653 Loss_G: 0.7983 acc: 65.6%\n",
      "[BATCH 124/149] Loss_D: 0.7492 Loss_G: 0.7855 acc: 56.2%\n",
      "[BATCH 125/149] Loss_D: 0.7780 Loss_G: 0.7881 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.7318 Loss_G: 0.7850 acc: 65.6%\n",
      "[BATCH 127/149] Loss_D: 0.7631 Loss_G: 0.7889 acc: 68.8%\n",
      "[BATCH 128/149] Loss_D: 0.7765 Loss_G: 0.7930 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.7562 Loss_G: 0.7973 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7699 Loss_G: 0.7833 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.7714 Loss_G: 0.7889 acc: 59.4%\n",
      "[BATCH 132/149] Loss_D: 0.7828 Loss_G: 0.7972 acc: 71.9%\n",
      "[BATCH 133/149] Loss_D: 0.8403 Loss_G: 0.8175 acc: 62.5%\n",
      "[BATCH 134/149] Loss_D: 0.7912 Loss_G: 0.8177 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.7828 Loss_G: 0.8095 acc: 62.5%\n",
      "[BATCH 136/149] Loss_D: 0.7417 Loss_G: 0.7932 acc: 68.8%\n",
      "[BATCH 137/149] Loss_D: 0.7963 Loss_G: 0.8030 acc: 73.4%\n",
      "[BATCH 138/149] Loss_D: 0.7548 Loss_G: 0.8026 acc: 60.9%\n",
      "[BATCH 139/149] Loss_D: 0.7385 Loss_G: 0.7939 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7666 Loss_G: 0.7884 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.7652 Loss_G: 0.7851 acc: 78.1%\n",
      "[BATCH 142/149] Loss_D: 0.7746 Loss_G: 0.7982 acc: 59.4%\n",
      "[BATCH 143/149] Loss_D: 0.7877 Loss_G: 0.8010 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7702 Loss_G: 0.7940 acc: 56.2%\n",
      "[BATCH 145/149] Loss_D: 0.7811 Loss_G: 0.7951 acc: 64.1%\n",
      "[BATCH 146/149] Loss_D: 0.7743 Loss_G: 0.7903 acc: 65.6%\n",
      "[EPOCH 7000] TEST ACC is : 74.4%\n",
      "[BATCH 147/149] Loss_D: 0.8063 Loss_G: 0.8019 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7982 Loss_G: 0.7987 acc: 60.9%\n",
      "[BATCH 149/149] Loss_D: 0.7923 Loss_G: 0.8046 acc: 70.3%\n",
      "-----THE [47/50] epoch end-----\n",
      "-----THE [48/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7799 Loss_G: 0.8059 acc: 73.4%\n",
      "[BATCH 2/149] Loss_D: 0.7634 Loss_G: 0.7992 acc: 59.4%\n",
      "[BATCH 3/149] Loss_D: 0.7796 Loss_G: 0.7988 acc: 54.7%\n",
      "[BATCH 4/149] Loss_D: 0.8078 Loss_G: 0.8140 acc: 64.1%\n",
      "[BATCH 5/149] Loss_D: 0.7552 Loss_G: 0.8061 acc: 62.5%\n",
      "[BATCH 6/149] Loss_D: 0.7948 Loss_G: 0.7981 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.8126 Loss_G: 0.8053 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7896 Loss_G: 0.8057 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7965 Loss_G: 0.8005 acc: 73.4%\n",
      "[BATCH 10/149] Loss_D: 0.7770 Loss_G: 0.7965 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.8068 Loss_G: 0.7978 acc: 62.5%\n",
      "[BATCH 12/149] Loss_D: 0.7727 Loss_G: 0.7976 acc: 62.5%\n",
      "[BATCH 13/149] Loss_D: 0.8100 Loss_G: 0.8071 acc: 70.3%\n",
      "[BATCH 14/149] Loss_D: 0.7625 Loss_G: 0.8122 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.8038 Loss_G: 0.8161 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.7743 Loss_G: 0.8235 acc: 71.9%\n",
      "[BATCH 17/149] Loss_D: 0.7623 Loss_G: 0.8044 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.7444 Loss_G: 0.8000 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7707 Loss_G: 0.7942 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.7296 Loss_G: 0.7822 acc: 71.9%\n",
      "[BATCH 21/149] Loss_D: 0.7831 Loss_G: 0.7958 acc: 65.6%\n",
      "[BATCH 22/149] Loss_D: 0.8008 Loss_G: 0.8056 acc: 60.9%\n",
      "[BATCH 23/149] Loss_D: 0.7540 Loss_G: 0.7967 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.7601 Loss_G: 0.7797 acc: 59.4%\n",
      "[BATCH 25/149] Loss_D: 0.8125 Loss_G: 0.7863 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7935 Loss_G: 0.7958 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.7738 Loss_G: 0.7902 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.7981 Loss_G: 0.8058 acc: 68.8%\n",
      "[BATCH 29/149] Loss_D: 0.7414 Loss_G: 0.7968 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.7907 Loss_G: 0.8007 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7772 Loss_G: 0.8013 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7925 Loss_G: 0.8005 acc: 60.9%\n",
      "[BATCH 33/149] Loss_D: 0.7997 Loss_G: 0.7982 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.8074 Loss_G: 0.7989 acc: 53.1%\n",
      "[BATCH 35/149] Loss_D: 0.7454 Loss_G: 0.7992 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7602 Loss_G: 0.8002 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7838 Loss_G: 0.7930 acc: 68.8%\n",
      "[BATCH 38/149] Loss_D: 0.7461 Loss_G: 0.7949 acc: 57.8%\n",
      "[BATCH 39/149] Loss_D: 0.7779 Loss_G: 0.7958 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.7835 Loss_G: 0.7996 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.7629 Loss_G: 0.8062 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7903 Loss_G: 0.8224 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.7538 Loss_G: 0.8117 acc: 53.1%\n",
      "[BATCH 44/149] Loss_D: 0.7525 Loss_G: 0.7942 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7641 Loss_G: 0.7985 acc: 70.3%\n",
      "[BATCH 46/149] Loss_D: 0.7703 Loss_G: 0.7848 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.7979 Loss_G: 0.7916 acc: 71.9%\n",
      "[EPOCH 7050] TEST ACC is : 74.4%\n",
      "[BATCH 48/149] Loss_D: 0.7860 Loss_G: 0.8097 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.8046 Loss_G: 0.8076 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.8480 Loss_G: 0.8250 acc: 54.7%\n",
      "[BATCH 51/149] Loss_D: 0.7801 Loss_G: 0.8195 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.8044 Loss_G: 0.8209 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.8663 Loss_G: 0.8449 acc: 67.2%\n",
      "[BATCH 54/149] Loss_D: 0.8014 Loss_G: 0.8342 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.7433 Loss_G: 0.8085 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.7816 Loss_G: 0.7942 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.7485 Loss_G: 0.7888 acc: 57.8%\n",
      "[BATCH 58/149] Loss_D: 0.7572 Loss_G: 0.7887 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.7389 Loss_G: 0.7770 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7762 Loss_G: 0.7867 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7805 Loss_G: 0.8003 acc: 57.8%\n",
      "[BATCH 62/149] Loss_D: 0.7531 Loss_G: 0.7905 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7826 Loss_G: 0.7876 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.7573 Loss_G: 0.7905 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7631 Loss_G: 0.7878 acc: 54.7%\n",
      "[BATCH 66/149] Loss_D: 0.7982 Loss_G: 0.8040 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.8107 Loss_G: 0.8216 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.8240 Loss_G: 0.8198 acc: 53.1%\n",
      "[BATCH 69/149] Loss_D: 0.7604 Loss_G: 0.8164 acc: 76.6%\n",
      "[BATCH 70/149] Loss_D: 0.7964 Loss_G: 0.8095 acc: 59.4%\n",
      "[BATCH 71/149] Loss_D: 0.7665 Loss_G: 0.8042 acc: 67.2%\n",
      "[BATCH 72/149] Loss_D: 0.7729 Loss_G: 0.8002 acc: 56.2%\n",
      "[BATCH 73/149] Loss_D: 0.7825 Loss_G: 0.8031 acc: 75.0%\n",
      "[BATCH 74/149] Loss_D: 0.7950 Loss_G: 0.8027 acc: 68.8%\n",
      "[BATCH 75/149] Loss_D: 0.7566 Loss_G: 0.7947 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.8006 Loss_G: 0.7976 acc: 70.3%\n",
      "[BATCH 77/149] Loss_D: 0.7868 Loss_G: 0.8025 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.8140 Loss_G: 0.8086 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.7685 Loss_G: 0.8047 acc: 62.5%\n",
      "[BATCH 80/149] Loss_D: 0.7542 Loss_G: 0.7961 acc: 67.2%\n",
      "[BATCH 81/149] Loss_D: 0.7799 Loss_G: 0.7926 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.7520 Loss_G: 0.7910 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7605 Loss_G: 0.7905 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.8023 Loss_G: 0.8045 acc: 70.3%\n",
      "[BATCH 85/149] Loss_D: 0.7889 Loss_G: 0.8045 acc: 73.4%\n",
      "[BATCH 86/149] Loss_D: 0.8253 Loss_G: 0.8149 acc: 70.3%\n",
      "[BATCH 87/149] Loss_D: 0.8063 Loss_G: 0.8151 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 0.8044 Loss_G: 0.8179 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.7569 Loss_G: 0.8168 acc: 75.0%\n",
      "[BATCH 90/149] Loss_D: 0.7436 Loss_G: 0.7926 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.7597 Loss_G: 0.7884 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.7415 Loss_G: 0.7836 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.7459 Loss_G: 0.7808 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.7838 Loss_G: 0.7955 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.8138 Loss_G: 0.8073 acc: 70.3%\n",
      "[BATCH 96/149] Loss_D: 0.7702 Loss_G: 0.7997 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.7500 Loss_G: 0.7891 acc: 73.4%\n",
      "[EPOCH 7100] TEST ACC is : 76.6%\n",
      "[BATCH 98/149] Loss_D: 0.7776 Loss_G: 0.7830 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7208 Loss_G: 0.7764 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.7976 Loss_G: 0.7762 acc: 73.4%\n",
      "[BATCH 101/149] Loss_D: 0.8090 Loss_G: 0.7904 acc: 65.6%\n",
      "[BATCH 102/149] Loss_D: 0.8013 Loss_G: 0.8028 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.8087 Loss_G: 0.8097 acc: 57.8%\n",
      "[BATCH 104/149] Loss_D: 0.8014 Loss_G: 0.8072 acc: 73.4%\n",
      "[BATCH 105/149] Loss_D: 0.7812 Loss_G: 0.8003 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7749 Loss_G: 0.7993 acc: 70.3%\n",
      "[BATCH 107/149] Loss_D: 0.7564 Loss_G: 0.7995 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.7984 Loss_G: 0.8032 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.7801 Loss_G: 0.8049 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.7535 Loss_G: 0.8030 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.7529 Loss_G: 0.7980 acc: 54.7%\n",
      "[BATCH 112/149] Loss_D: 0.8099 Loss_G: 0.8153 acc: 60.9%\n",
      "[BATCH 113/149] Loss_D: 0.7952 Loss_G: 0.8163 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.7739 Loss_G: 0.8085 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.7505 Loss_G: 0.7980 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.8063 Loss_G: 0.8078 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.7362 Loss_G: 0.7931 acc: 60.9%\n",
      "[BATCH 118/149] Loss_D: 0.7564 Loss_G: 0.7853 acc: 51.6%\n",
      "[BATCH 119/149] Loss_D: 0.7544 Loss_G: 0.7868 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7798 Loss_G: 0.7902 acc: 56.2%\n",
      "[BATCH 121/149] Loss_D: 0.7445 Loss_G: 0.7901 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.8224 Loss_G: 0.8006 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7915 Loss_G: 0.8068 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.7673 Loss_G: 0.8054 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7666 Loss_G: 0.7990 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.7902 Loss_G: 0.8006 acc: 81.2%\n",
      "[BATCH 127/149] Loss_D: 0.7998 Loss_G: 0.8080 acc: 68.8%\n",
      "[BATCH 128/149] Loss_D: 0.7866 Loss_G: 0.8025 acc: 56.2%\n",
      "[BATCH 129/149] Loss_D: 0.7593 Loss_G: 0.7981 acc: 67.2%\n",
      "[BATCH 130/149] Loss_D: 0.7873 Loss_G: 0.7962 acc: 67.2%\n",
      "[BATCH 131/149] Loss_D: 0.7989 Loss_G: 0.7989 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.8212 Loss_G: 0.8084 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.7603 Loss_G: 0.7910 acc: 64.1%\n",
      "[BATCH 134/149] Loss_D: 0.7705 Loss_G: 0.7920 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.7989 Loss_G: 0.7897 acc: 50.0%\n",
      "[BATCH 136/149] Loss_D: 0.7926 Loss_G: 0.7935 acc: 54.7%\n",
      "[BATCH 137/149] Loss_D: 0.8678 Loss_G: 0.8274 acc: 68.8%\n",
      "[BATCH 138/149] Loss_D: 0.7682 Loss_G: 0.8025 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.7717 Loss_G: 0.7902 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7708 Loss_G: 0.7924 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.7631 Loss_G: 0.7941 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.8036 Loss_G: 0.8034 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7498 Loss_G: 0.8003 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7509 Loss_G: 0.7896 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.7555 Loss_G: 0.7880 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7893 Loss_G: 0.7839 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.8127 Loss_G: 0.8029 acc: 76.6%\n",
      "[EPOCH 7150] TEST ACC is : 75.4%\n",
      "[BATCH 148/149] Loss_D: 0.7911 Loss_G: 0.8018 acc: 57.8%\n",
      "[BATCH 149/149] Loss_D: 0.8019 Loss_G: 0.8154 acc: 68.8%\n",
      "-----THE [48/50] epoch end-----\n",
      "-----THE [49/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7807 Loss_G: 0.8172 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7570 Loss_G: 0.8041 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.8447 Loss_G: 0.8288 acc: 57.8%\n",
      "[BATCH 4/149] Loss_D: 0.7892 Loss_G: 0.8355 acc: 67.2%\n",
      "[BATCH 5/149] Loss_D: 0.7566 Loss_G: 0.7999 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7749 Loss_G: 0.7973 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.7856 Loss_G: 0.7992 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7348 Loss_G: 0.7955 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.7433 Loss_G: 0.7875 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.7717 Loss_G: 0.7884 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.7777 Loss_G: 0.8048 acc: 70.3%\n",
      "[BATCH 12/149] Loss_D: 0.7744 Loss_G: 0.8109 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.8347 Loss_G: 0.8259 acc: 57.8%\n",
      "[BATCH 14/149] Loss_D: 0.7509 Loss_G: 0.8064 acc: 71.9%\n",
      "[BATCH 15/149] Loss_D: 0.7847 Loss_G: 0.8119 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7922 Loss_G: 0.8020 acc: 65.6%\n",
      "[BATCH 17/149] Loss_D: 0.8103 Loss_G: 0.8145 acc: 53.1%\n",
      "[BATCH 18/149] Loss_D: 0.7923 Loss_G: 0.8032 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.7418 Loss_G: 0.7887 acc: 53.1%\n",
      "[BATCH 20/149] Loss_D: 0.8218 Loss_G: 0.8045 acc: 75.0%\n",
      "[BATCH 21/149] Loss_D: 0.7920 Loss_G: 0.8330 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.8060 Loss_G: 0.8221 acc: 65.6%\n",
      "[BATCH 23/149] Loss_D: 0.8240 Loss_G: 0.8202 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7629 Loss_G: 0.8072 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.7793 Loss_G: 0.8131 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7744 Loss_G: 0.8043 acc: 70.3%\n",
      "[BATCH 27/149] Loss_D: 0.7748 Loss_G: 0.8066 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.7717 Loss_G: 0.8060 acc: 65.6%\n",
      "[BATCH 29/149] Loss_D: 0.7968 Loss_G: 0.8010 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.8027 Loss_G: 0.8047 acc: 71.9%\n",
      "[BATCH 31/149] Loss_D: 0.7644 Loss_G: 0.8099 acc: 54.7%\n",
      "[BATCH 32/149] Loss_D: 0.7737 Loss_G: 0.8053 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7610 Loss_G: 0.7968 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.7902 Loss_G: 0.7941 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7757 Loss_G: 0.7999 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7716 Loss_G: 0.8038 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.7567 Loss_G: 0.7811 acc: 60.9%\n",
      "[BATCH 38/149] Loss_D: 0.7665 Loss_G: 0.7790 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7451 Loss_G: 0.7780 acc: 62.5%\n",
      "[BATCH 40/149] Loss_D: 0.7875 Loss_G: 0.7881 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.8004 Loss_G: 0.8068 acc: 60.9%\n",
      "[BATCH 42/149] Loss_D: 0.8024 Loss_G: 0.8088 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.7813 Loss_G: 0.8003 acc: 59.4%\n",
      "[BATCH 44/149] Loss_D: 0.7450 Loss_G: 0.7783 acc: 71.9%\n",
      "[BATCH 45/149] Loss_D: 0.7857 Loss_G: 0.7910 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7538 Loss_G: 0.7882 acc: 57.8%\n",
      "[BATCH 47/149] Loss_D: 0.7631 Loss_G: 0.7789 acc: 67.2%\n",
      "[BATCH 48/149] Loss_D: 0.7403 Loss_G: 0.7766 acc: 68.8%\n",
      "[EPOCH 7200] TEST ACC is : 73.8%\n",
      "[BATCH 49/149] Loss_D: 0.7396 Loss_G: 0.7725 acc: 60.9%\n",
      "[BATCH 50/149] Loss_D: 0.7889 Loss_G: 0.7876 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.7746 Loss_G: 0.7893 acc: 59.4%\n",
      "[BATCH 52/149] Loss_D: 0.7807 Loss_G: 0.7885 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7978 Loss_G: 0.7978 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.7957 Loss_G: 0.8074 acc: 59.4%\n",
      "[BATCH 55/149] Loss_D: 0.7655 Loss_G: 0.8077 acc: 73.4%\n",
      "[BATCH 56/149] Loss_D: 0.7362 Loss_G: 0.7899 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.7603 Loss_G: 0.7879 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.8122 Loss_G: 0.7992 acc: 60.9%\n",
      "[BATCH 59/149] Loss_D: 0.7899 Loss_G: 0.8070 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7749 Loss_G: 0.7981 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.7602 Loss_G: 0.7921 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.7781 Loss_G: 0.7953 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.7825 Loss_G: 0.8054 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7923 Loss_G: 0.8111 acc: 59.4%\n",
      "[BATCH 65/149] Loss_D: 0.7835 Loss_G: 0.8032 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.8138 Loss_G: 0.8185 acc: 71.9%\n",
      "[BATCH 67/149] Loss_D: 0.7749 Loss_G: 0.8122 acc: 59.4%\n",
      "[BATCH 68/149] Loss_D: 0.7531 Loss_G: 0.8089 acc: 79.7%\n",
      "[BATCH 69/149] Loss_D: 0.7892 Loss_G: 0.8073 acc: 75.0%\n",
      "[BATCH 70/149] Loss_D: 0.7954 Loss_G: 0.8149 acc: 70.3%\n",
      "[BATCH 71/149] Loss_D: 0.7659 Loss_G: 0.8024 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7714 Loss_G: 0.7974 acc: 73.4%\n",
      "[BATCH 73/149] Loss_D: 0.7533 Loss_G: 0.7926 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7619 Loss_G: 0.7899 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.7565 Loss_G: 0.7866 acc: 59.4%\n",
      "[BATCH 76/149] Loss_D: 0.7791 Loss_G: 0.7978 acc: 67.2%\n",
      "[BATCH 77/149] Loss_D: 0.7654 Loss_G: 0.8133 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.7581 Loss_G: 0.8094 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.7681 Loss_G: 0.7997 acc: 67.2%\n",
      "[BATCH 80/149] Loss_D: 0.7749 Loss_G: 0.7908 acc: 79.7%\n",
      "[BATCH 81/149] Loss_D: 0.8135 Loss_G: 0.8117 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.7493 Loss_G: 0.8001 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.7982 Loss_G: 0.7989 acc: 57.8%\n",
      "[BATCH 84/149] Loss_D: 0.7652 Loss_G: 0.7867 acc: 53.1%\n",
      "[BATCH 85/149] Loss_D: 0.8039 Loss_G: 0.7992 acc: 46.9%\n",
      "[BATCH 86/149] Loss_D: 0.8263 Loss_G: 0.8113 acc: 57.8%\n",
      "[BATCH 87/149] Loss_D: 0.8015 Loss_G: 0.8089 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.8229 Loss_G: 0.8142 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.7662 Loss_G: 0.8036 acc: 65.6%\n",
      "[BATCH 90/149] Loss_D: 0.8263 Loss_G: 0.8113 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.7679 Loss_G: 0.8097 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7367 Loss_G: 0.7995 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.7888 Loss_G: 0.7979 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7656 Loss_G: 0.7947 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.7561 Loss_G: 0.7963 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7566 Loss_G: 0.7906 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7643 Loss_G: 0.7979 acc: 68.8%\n",
      "[BATCH 98/149] Loss_D: 0.8017 Loss_G: 0.8072 acc: 70.3%\n",
      "[EPOCH 7250] TEST ACC is : 73.8%\n",
      "[BATCH 99/149] Loss_D: 0.7471 Loss_G: 0.7926 acc: 68.8%\n",
      "[BATCH 100/149] Loss_D: 0.8158 Loss_G: 0.8110 acc: 67.2%\n",
      "[BATCH 101/149] Loss_D: 0.7254 Loss_G: 0.8042 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.7784 Loss_G: 0.7948 acc: 57.8%\n",
      "[BATCH 103/149] Loss_D: 0.7904 Loss_G: 0.8150 acc: 75.0%\n",
      "[BATCH 104/149] Loss_D: 0.7996 Loss_G: 0.8043 acc: 70.3%\n",
      "[BATCH 105/149] Loss_D: 0.7513 Loss_G: 0.7949 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.7762 Loss_G: 0.7899 acc: 59.4%\n",
      "[BATCH 107/149] Loss_D: 0.7775 Loss_G: 0.7875 acc: 64.1%\n",
      "[BATCH 108/149] Loss_D: 0.7685 Loss_G: 0.7868 acc: 62.5%\n",
      "[BATCH 109/149] Loss_D: 0.7429 Loss_G: 0.7832 acc: 65.6%\n",
      "[BATCH 110/149] Loss_D: 0.7666 Loss_G: 0.7818 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.7683 Loss_G: 0.7832 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.7376 Loss_G: 0.7747 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.7958 Loss_G: 0.7838 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.7613 Loss_G: 0.7894 acc: 56.2%\n",
      "[BATCH 115/149] Loss_D: 0.7743 Loss_G: 0.7921 acc: 70.3%\n",
      "[BATCH 116/149] Loss_D: 0.8056 Loss_G: 0.8065 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7810 Loss_G: 0.8049 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.8472 Loss_G: 0.8319 acc: 65.6%\n",
      "[BATCH 119/149] Loss_D: 0.7999 Loss_G: 0.8404 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7684 Loss_G: 0.8133 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.7991 Loss_G: 0.8154 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.7566 Loss_G: 0.8041 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.7526 Loss_G: 0.7968 acc: 73.4%\n",
      "[BATCH 124/149] Loss_D: 0.7642 Loss_G: 0.7888 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7838 Loss_G: 0.7932 acc: 75.0%\n",
      "[BATCH 126/149] Loss_D: 0.7771 Loss_G: 0.7954 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.7662 Loss_G: 0.7981 acc: 59.4%\n",
      "[BATCH 128/149] Loss_D: 0.7662 Loss_G: 0.7996 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.7998 Loss_G: 0.8057 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7508 Loss_G: 0.7972 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7793 Loss_G: 0.7909 acc: 71.9%\n",
      "[BATCH 132/149] Loss_D: 0.8429 Loss_G: 0.8166 acc: 64.1%\n",
      "[BATCH 133/149] Loss_D: 0.8125 Loss_G: 0.8296 acc: 64.1%\n",
      "[BATCH 134/149] Loss_D: 0.8473 Loss_G: 0.8356 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.7788 Loss_G: 0.8147 acc: 62.5%\n",
      "[BATCH 136/149] Loss_D: 0.7870 Loss_G: 0.8120 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7864 Loss_G: 0.8158 acc: 60.9%\n",
      "[BATCH 138/149] Loss_D: 0.7850 Loss_G: 0.8104 acc: 53.1%\n",
      "[BATCH 139/149] Loss_D: 0.8526 Loss_G: 0.8236 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7774 Loss_G: 0.8248 acc: 56.2%\n",
      "[BATCH 141/149] Loss_D: 0.7821 Loss_G: 0.8387 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.7673 Loss_G: 0.8255 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7848 Loss_G: 0.8132 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7580 Loss_G: 0.7965 acc: 73.4%\n",
      "[BATCH 145/149] Loss_D: 0.8471 Loss_G: 0.8061 acc: 59.4%\n",
      "[BATCH 146/149] Loss_D: 0.7854 Loss_G: 0.8076 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.7642 Loss_G: 0.7980 acc: 64.1%\n",
      "[BATCH 148/149] Loss_D: 0.7702 Loss_G: 0.7961 acc: 67.2%\n",
      "[EPOCH 7300] TEST ACC is : 74.8%\n",
      "[BATCH 149/149] Loss_D: 0.8074 Loss_G: 0.7981 acc: 65.6%\n",
      "-----THE [49/50] epoch end-----\n",
      "-----THE [50/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8191 Loss_G: 0.8127 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7614 Loss_G: 0.8186 acc: 60.9%\n",
      "[BATCH 3/149] Loss_D: 0.7419 Loss_G: 0.7993 acc: 54.7%\n",
      "[BATCH 4/149] Loss_D: 0.7958 Loss_G: 0.7982 acc: 60.9%\n",
      "[BATCH 5/149] Loss_D: 0.7983 Loss_G: 0.8058 acc: 62.5%\n",
      "[BATCH 6/149] Loss_D: 0.7904 Loss_G: 0.8128 acc: 59.4%\n",
      "[BATCH 7/149] Loss_D: 0.7981 Loss_G: 0.8214 acc: 65.6%\n",
      "[BATCH 8/149] Loss_D: 0.7990 Loss_G: 0.8136 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.7991 Loss_G: 0.8036 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7881 Loss_G: 0.8114 acc: 59.4%\n",
      "[BATCH 11/149] Loss_D: 0.7756 Loss_G: 0.8057 acc: 56.2%\n",
      "[BATCH 12/149] Loss_D: 0.8041 Loss_G: 0.7940 acc: 71.9%\n",
      "[BATCH 13/149] Loss_D: 0.8100 Loss_G: 0.8021 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.7383 Loss_G: 0.7962 acc: 68.8%\n",
      "[BATCH 15/149] Loss_D: 0.7727 Loss_G: 0.7891 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.8009 Loss_G: 0.7976 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.8183 Loss_G: 0.8185 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.7532 Loss_G: 0.8059 acc: 76.6%\n",
      "[BATCH 19/149] Loss_D: 0.7609 Loss_G: 0.7956 acc: 75.0%\n",
      "[BATCH 20/149] Loss_D: 0.7843 Loss_G: 0.7943 acc: 65.6%\n",
      "[BATCH 21/149] Loss_D: 0.8051 Loss_G: 0.7993 acc: 75.0%\n",
      "[BATCH 22/149] Loss_D: 0.7429 Loss_G: 0.7937 acc: 71.9%\n",
      "[BATCH 23/149] Loss_D: 0.7873 Loss_G: 0.7852 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7610 Loss_G: 0.7889 acc: 73.4%\n",
      "[BATCH 25/149] Loss_D: 0.7886 Loss_G: 0.7912 acc: 73.4%\n",
      "[BATCH 26/149] Loss_D: 0.8284 Loss_G: 0.8139 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.7760 Loss_G: 0.8121 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.7518 Loss_G: 0.7947 acc: 65.6%\n",
      "[BATCH 29/149] Loss_D: 0.7683 Loss_G: 0.7943 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.7906 Loss_G: 0.8011 acc: 60.9%\n",
      "[BATCH 31/149] Loss_D: 0.7792 Loss_G: 0.7964 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7719 Loss_G: 0.7897 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.7389 Loss_G: 0.7846 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.7567 Loss_G: 0.7927 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7930 Loss_G: 0.8086 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.7380 Loss_G: 0.8050 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.7902 Loss_G: 0.8008 acc: 62.5%\n",
      "[BATCH 38/149] Loss_D: 0.7348 Loss_G: 0.7906 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8257 Loss_G: 0.7993 acc: 53.1%\n",
      "[BATCH 40/149] Loss_D: 0.7626 Loss_G: 0.8008 acc: 53.1%\n",
      "[BATCH 41/149] Loss_D: 0.7887 Loss_G: 0.7931 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7841 Loss_G: 0.7987 acc: 70.3%\n",
      "[BATCH 43/149] Loss_D: 0.7580 Loss_G: 0.7992 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7588 Loss_G: 0.7948 acc: 75.0%\n",
      "[BATCH 45/149] Loss_D: 0.7628 Loss_G: 0.7936 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.7660 Loss_G: 0.7931 acc: 54.7%\n",
      "[BATCH 47/149] Loss_D: 0.7957 Loss_G: 0.7983 acc: 67.2%\n",
      "[BATCH 48/149] Loss_D: 0.7782 Loss_G: 0.7986 acc: 54.7%\n",
      "[BATCH 49/149] Loss_D: 0.7919 Loss_G: 0.8050 acc: 54.7%\n",
      "[EPOCH 7350] TEST ACC is : 75.2%\n",
      "[BATCH 50/149] Loss_D: 0.7883 Loss_G: 0.8047 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7643 Loss_G: 0.7971 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7801 Loss_G: 0.7896 acc: 57.8%\n",
      "[BATCH 53/149] Loss_D: 0.8012 Loss_G: 0.8024 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.7540 Loss_G: 0.7948 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7856 Loss_G: 0.7944 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.7785 Loss_G: 0.7940 acc: 54.7%\n",
      "[BATCH 57/149] Loss_D: 0.7575 Loss_G: 0.7982 acc: 60.9%\n",
      "[BATCH 58/149] Loss_D: 0.8049 Loss_G: 0.8105 acc: 53.1%\n",
      "[BATCH 59/149] Loss_D: 0.7916 Loss_G: 0.8235 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.8148 Loss_G: 0.8232 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.7845 Loss_G: 0.8085 acc: 73.4%\n",
      "[BATCH 62/149] Loss_D: 0.7703 Loss_G: 0.8085 acc: 73.4%\n",
      "[BATCH 63/149] Loss_D: 0.7811 Loss_G: 0.8049 acc: 71.9%\n",
      "[BATCH 64/149] Loss_D: 0.8134 Loss_G: 0.8141 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7402 Loss_G: 0.7984 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.7520 Loss_G: 0.7910 acc: 59.4%\n",
      "[BATCH 67/149] Loss_D: 0.8009 Loss_G: 0.7934 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.8391 Loss_G: 0.8072 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.8215 Loss_G: 0.8300 acc: 67.2%\n",
      "[BATCH 70/149] Loss_D: 0.7677 Loss_G: 0.8077 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7767 Loss_G: 0.7992 acc: 67.2%\n",
      "[BATCH 72/149] Loss_D: 0.8009 Loss_G: 0.8061 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7402 Loss_G: 0.7997 acc: 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.7652 Loss_G: 0.7934 acc: 76.6%\n",
      "[BATCH 75/149] Loss_D: 0.7507 Loss_G: 0.7851 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.7490 Loss_G: 0.7788 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.7942 Loss_G: 0.7812 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.7650 Loss_G: 0.7876 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.8040 Loss_G: 0.7978 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7903 Loss_G: 0.8049 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.7576 Loss_G: 0.7942 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.8382 Loss_G: 0.8120 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.8086 Loss_G: 0.8049 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.7975 Loss_G: 0.8080 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.7468 Loss_G: 0.7923 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.7966 Loss_G: 0.7885 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.7891 Loss_G: 0.8021 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.8155 Loss_G: 0.8163 acc: 51.6%\n",
      "[BATCH 89/149] Loss_D: 0.7466 Loss_G: 0.8046 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7800 Loss_G: 0.7988 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.7929 Loss_G: 0.7989 acc: 62.5%\n",
      "[BATCH 92/149] Loss_D: 0.7981 Loss_G: 0.8054 acc: 73.4%\n",
      "[BATCH 93/149] Loss_D: 0.7746 Loss_G: 0.8068 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7382 Loss_G: 0.8053 acc: 75.0%\n",
      "[BATCH 95/149] Loss_D: 0.7675 Loss_G: 0.7944 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.7843 Loss_G: 0.7938 acc: 67.2%\n",
      "[BATCH 97/149] Loss_D: 0.7771 Loss_G: 0.7924 acc: 60.9%\n",
      "[BATCH 98/149] Loss_D: 0.8022 Loss_G: 0.7994 acc: 65.6%\n",
      "[BATCH 99/149] Loss_D: 0.7871 Loss_G: 0.8012 acc: 75.0%\n",
      "[EPOCH 7400] TEST ACC is : 74.0%\n",
      "[BATCH 100/149] Loss_D: 0.7418 Loss_G: 0.7910 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.7786 Loss_G: 0.7797 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7814 Loss_G: 0.7910 acc: 62.5%\n",
      "[BATCH 103/149] Loss_D: 0.7512 Loss_G: 0.7844 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.8129 Loss_G: 0.8081 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7538 Loss_G: 0.8053 acc: 56.2%\n",
      "[BATCH 106/149] Loss_D: 0.7679 Loss_G: 0.7989 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.7629 Loss_G: 0.7919 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7665 Loss_G: 0.7935 acc: 53.1%\n",
      "[BATCH 109/149] Loss_D: 0.7528 Loss_G: 0.7864 acc: 70.3%\n",
      "[BATCH 110/149] Loss_D: 0.7979 Loss_G: 0.8022 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.7857 Loss_G: 0.8216 acc: 65.6%\n",
      "[BATCH 112/149] Loss_D: 0.7947 Loss_G: 0.8202 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7599 Loss_G: 0.7939 acc: 71.9%\n",
      "[BATCH 114/149] Loss_D: 0.7513 Loss_G: 0.7986 acc: 67.2%\n",
      "[BATCH 115/149] Loss_D: 0.8021 Loss_G: 0.8061 acc: 68.8%\n",
      "[BATCH 116/149] Loss_D: 0.7840 Loss_G: 0.7966 acc: 50.0%\n",
      "[BATCH 117/149] Loss_D: 0.7888 Loss_G: 0.7995 acc: 75.0%\n",
      "[BATCH 118/149] Loss_D: 0.7914 Loss_G: 0.7966 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.7897 Loss_G: 0.7899 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.7793 Loss_G: 0.7904 acc: 56.2%\n",
      "[BATCH 121/149] Loss_D: 0.7488 Loss_G: 0.7873 acc: 64.1%\n",
      "[BATCH 122/149] Loss_D: 0.7795 Loss_G: 0.7909 acc: 71.9%\n",
      "[BATCH 123/149] Loss_D: 0.7889 Loss_G: 0.8038 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.8138 Loss_G: 0.8184 acc: 75.0%\n",
      "[BATCH 125/149] Loss_D: 0.8162 Loss_G: 0.8145 acc: 57.8%\n",
      "[BATCH 126/149] Loss_D: 0.7666 Loss_G: 0.8003 acc: 67.2%\n",
      "[BATCH 127/149] Loss_D: 0.7626 Loss_G: 0.7932 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.7636 Loss_G: 0.7932 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.7477 Loss_G: 0.7858 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7532 Loss_G: 0.7828 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.8326 Loss_G: 0.7905 acc: 68.8%\n",
      "[BATCH 132/149] Loss_D: 0.8072 Loss_G: 0.8290 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7599 Loss_G: 0.8011 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7762 Loss_G: 0.7919 acc: 57.8%\n",
      "[BATCH 135/149] Loss_D: 0.7800 Loss_G: 0.7907 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.7978 Loss_G: 0.8000 acc: 53.1%\n",
      "[BATCH 137/149] Loss_D: 0.7423 Loss_G: 0.7998 acc: 70.3%\n",
      "[BATCH 138/149] Loss_D: 0.7891 Loss_G: 0.8071 acc: 60.9%\n",
      "[BATCH 139/149] Loss_D: 0.7398 Loss_G: 0.8008 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7916 Loss_G: 0.8146 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.8008 Loss_G: 0.8163 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7587 Loss_G: 0.7997 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7969 Loss_G: 0.8059 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.7771 Loss_G: 0.7999 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.7667 Loss_G: 0.7972 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.7573 Loss_G: 0.7930 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7608 Loss_G: 0.7957 acc: 68.8%\n",
      "[BATCH 148/149] Loss_D: 0.7519 Loss_G: 0.7855 acc: 68.8%\n",
      "[BATCH 149/149] Loss_D: 0.8330 Loss_G: 0.8030 acc: 75.0%\n",
      "[EPOCH 7450] TEST ACC is : 72.7%\n",
      "-----THE [50/50] epoch end-----\n",
      "The 3 * 50 epochs train starts:\n",
      "-----THE [1/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7506 Loss_G: 0.8018 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7649 Loss_G: 0.7887 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.8048 Loss_G: 0.7915 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.7576 Loss_G: 0.7912 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.8069 Loss_G: 0.7915 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7575 Loss_G: 0.7868 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.7495 Loss_G: 0.7811 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7612 Loss_G: 0.7812 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7728 Loss_G: 0.7737 acc: 79.7%\n",
      "[BATCH 10/149] Loss_D: 0.7707 Loss_G: 0.7837 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.7479 Loss_G: 0.7864 acc: 59.4%\n",
      "[BATCH 12/149] Loss_D: 0.7846 Loss_G: 0.7933 acc: 56.2%\n",
      "[BATCH 13/149] Loss_D: 0.7951 Loss_G: 0.8087 acc: 67.2%\n",
      "[BATCH 14/149] Loss_D: 0.7770 Loss_G: 0.8122 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.8394 Loss_G: 0.8236 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7842 Loss_G: 0.8115 acc: 68.8%\n",
      "[BATCH 17/149] Loss_D: 0.8350 Loss_G: 0.8177 acc: 54.7%\n",
      "[BATCH 18/149] Loss_D: 0.7899 Loss_G: 0.8144 acc: 73.4%\n",
      "[BATCH 19/149] Loss_D: 0.7330 Loss_G: 0.7961 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.7625 Loss_G: 0.8022 acc: 64.1%\n",
      "[BATCH 21/149] Loss_D: 0.7832 Loss_G: 0.8114 acc: 51.6%\n",
      "[BATCH 22/149] Loss_D: 0.8066 Loss_G: 0.8324 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.7554 Loss_G: 0.8111 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7597 Loss_G: 0.7898 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.8063 Loss_G: 0.7925 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7738 Loss_G: 0.7932 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.7453 Loss_G: 0.7937 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.7317 Loss_G: 0.7782 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.7792 Loss_G: 0.7791 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.7861 Loss_G: 0.7962 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7845 Loss_G: 0.7972 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.8031 Loss_G: 0.7957 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7677 Loss_G: 0.8009 acc: 76.6%\n",
      "[BATCH 34/149] Loss_D: 0.8093 Loss_G: 0.8004 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7937 Loss_G: 0.8084 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.8055 Loss_G: 0.8026 acc: 71.9%\n",
      "[BATCH 37/149] Loss_D: 0.8017 Loss_G: 0.8000 acc: 62.5%\n",
      "[BATCH 38/149] Loss_D: 0.7636 Loss_G: 0.7948 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.7634 Loss_G: 0.7927 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.7862 Loss_G: 0.7926 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.8090 Loss_G: 0.8083 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.7473 Loss_G: 0.8169 acc: 59.4%\n",
      "[BATCH 43/149] Loss_D: 0.7581 Loss_G: 0.7807 acc: 65.6%\n",
      "[BATCH 44/149] Loss_D: 0.7839 Loss_G: 0.7837 acc: 62.5%\n",
      "[BATCH 45/149] Loss_D: 0.8036 Loss_G: 0.8024 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7628 Loss_G: 0.7965 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.7876 Loss_G: 0.8023 acc: 56.2%\n",
      "[BATCH 48/149] Loss_D: 0.7689 Loss_G: 0.8042 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7447 Loss_G: 0.7972 acc: 70.3%\n",
      "[BATCH 50/149] Loss_D: 0.7773 Loss_G: 0.7978 acc: 78.1%\n",
      "[EPOCH 50] TEST ACC is : 73.0%\n",
      "[BATCH 51/149] Loss_D: 0.7855 Loss_G: 0.7955 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7698 Loss_G: 0.7935 acc: 59.4%\n",
      "[BATCH 53/149] Loss_D: 0.8126 Loss_G: 0.8005 acc: 71.9%\n",
      "[BATCH 54/149] Loss_D: 0.7373 Loss_G: 0.7878 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.7686 Loss_G: 0.7909 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7952 Loss_G: 0.8058 acc: 59.4%\n",
      "[BATCH 57/149] Loss_D: 0.7700 Loss_G: 0.7932 acc: 79.7%\n",
      "[BATCH 58/149] Loss_D: 0.7663 Loss_G: 0.7998 acc: 70.3%\n",
      "[BATCH 59/149] Loss_D: 0.8022 Loss_G: 0.8083 acc: 60.9%\n",
      "[BATCH 60/149] Loss_D: 0.7991 Loss_G: 0.8154 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7732 Loss_G: 0.8094 acc: 71.9%\n",
      "[BATCH 62/149] Loss_D: 0.8063 Loss_G: 0.8194 acc: 50.0%\n",
      "[BATCH 63/149] Loss_D: 0.7961 Loss_G: 0.8119 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.7836 Loss_G: 0.7963 acc: 68.8%\n",
      "[BATCH 65/149] Loss_D: 0.7898 Loss_G: 0.8098 acc: 78.1%\n",
      "[BATCH 66/149] Loss_D: 0.7777 Loss_G: 0.8109 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7683 Loss_G: 0.7998 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7488 Loss_G: 0.7942 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.7869 Loss_G: 0.7931 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.7918 Loss_G: 0.7959 acc: 48.4%\n",
      "[BATCH 71/149] Loss_D: 0.7397 Loss_G: 0.7872 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7805 Loss_G: 0.7952 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.7612 Loss_G: 0.7878 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.8308 Loss_G: 0.8047 acc: 76.6%\n",
      "[BATCH 75/149] Loss_D: 0.8130 Loss_G: 0.8350 acc: 56.2%\n",
      "[BATCH 76/149] Loss_D: 0.7720 Loss_G: 0.8123 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.7603 Loss_G: 0.7916 acc: 73.4%\n",
      "[BATCH 78/149] Loss_D: 0.8304 Loss_G: 0.8070 acc: 67.2%\n",
      "[BATCH 79/149] Loss_D: 0.7469 Loss_G: 0.8062 acc: 56.2%\n",
      "[BATCH 80/149] Loss_D: 0.7531 Loss_G: 0.7893 acc: 56.2%\n",
      "[BATCH 81/149] Loss_D: 0.7919 Loss_G: 0.7881 acc: 67.2%\n",
      "[BATCH 82/149] Loss_D: 0.7840 Loss_G: 0.7940 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7981 Loss_G: 0.7983 acc: 56.2%\n",
      "[BATCH 84/149] Loss_D: 0.7933 Loss_G: 0.8073 acc: 68.8%\n",
      "[BATCH 85/149] Loss_D: 0.7751 Loss_G: 0.8053 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.7910 Loss_G: 0.7997 acc: 64.1%\n",
      "[BATCH 87/149] Loss_D: 0.7755 Loss_G: 0.8043 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7549 Loss_G: 0.7979 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.7544 Loss_G: 0.7995 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7848 Loss_G: 0.8022 acc: 76.6%\n",
      "[BATCH 91/149] Loss_D: 0.7620 Loss_G: 0.8032 acc: 71.9%\n",
      "[BATCH 92/149] Loss_D: 0.7859 Loss_G: 0.8006 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.8025 Loss_G: 0.8077 acc: 73.4%\n",
      "[BATCH 94/149] Loss_D: 0.8159 Loss_G: 0.8226 acc: 75.0%\n",
      "[BATCH 95/149] Loss_D: 0.7544 Loss_G: 0.8079 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.8055 Loss_G: 0.7969 acc: 67.2%\n",
      "[BATCH 97/149] Loss_D: 0.7625 Loss_G: 0.7946 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.7359 Loss_G: 0.7799 acc: 56.2%\n",
      "[BATCH 99/149] Loss_D: 0.7582 Loss_G: 0.7805 acc: 75.0%\n",
      "[BATCH 100/149] Loss_D: 0.8042 Loss_G: 0.7965 acc: 70.3%\n",
      "[EPOCH 100] TEST ACC is : 73.8%\n",
      "[BATCH 101/149] Loss_D: 0.7796 Loss_G: 0.8087 acc: 67.2%\n",
      "[BATCH 102/149] Loss_D: 0.7952 Loss_G: 0.8056 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.7623 Loss_G: 0.8066 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.7650 Loss_G: 0.7907 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7888 Loss_G: 0.7937 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7582 Loss_G: 0.7909 acc: 68.8%\n",
      "[BATCH 107/149] Loss_D: 0.7792 Loss_G: 0.7871 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.7597 Loss_G: 0.7948 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.7775 Loss_G: 0.7964 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.7911 Loss_G: 0.8025 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7944 Loss_G: 0.8040 acc: 67.2%\n",
      "[BATCH 112/149] Loss_D: 0.8064 Loss_G: 0.7966 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7514 Loss_G: 0.7898 acc: 59.4%\n",
      "[BATCH 114/149] Loss_D: 0.8281 Loss_G: 0.8180 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7507 Loss_G: 0.7976 acc: 68.8%\n",
      "[BATCH 116/149] Loss_D: 0.7687 Loss_G: 0.7861 acc: 64.1%\n",
      "[BATCH 117/149] Loss_D: 0.8060 Loss_G: 0.7942 acc: 71.9%\n",
      "[BATCH 118/149] Loss_D: 0.7645 Loss_G: 0.7992 acc: 60.9%\n",
      "[BATCH 119/149] Loss_D: 0.7575 Loss_G: 0.8022 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7900 Loss_G: 0.7992 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.7474 Loss_G: 0.8003 acc: 60.9%\n",
      "[BATCH 122/149] Loss_D: 0.7416 Loss_G: 0.7965 acc: 56.2%\n",
      "[BATCH 123/149] Loss_D: 0.7392 Loss_G: 0.7886 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7802 Loss_G: 0.7950 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.7703 Loss_G: 0.8017 acc: 54.7%\n",
      "[BATCH 126/149] Loss_D: 0.7908 Loss_G: 0.7973 acc: 70.3%\n",
      "[BATCH 127/149] Loss_D: 0.7831 Loss_G: 0.8114 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7707 Loss_G: 0.8259 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7899 Loss_G: 0.8006 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7682 Loss_G: 0.7870 acc: 67.2%\n",
      "[BATCH 131/149] Loss_D: 0.7444 Loss_G: 0.7943 acc: 68.8%\n",
      "[BATCH 132/149] Loss_D: 0.8217 Loss_G: 0.8090 acc: 60.9%\n",
      "[BATCH 133/149] Loss_D: 0.7981 Loss_G: 0.8223 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.8540 Loss_G: 0.8230 acc: 51.6%\n",
      "[BATCH 135/149] Loss_D: 0.7516 Loss_G: 0.7963 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7954 Loss_G: 0.8023 acc: 70.3%\n",
      "[BATCH 137/149] Loss_D: 0.7637 Loss_G: 0.7856 acc: 56.2%\n",
      "[BATCH 138/149] Loss_D: 0.8391 Loss_G: 0.8072 acc: 67.2%\n",
      "[BATCH 139/149] Loss_D: 0.7560 Loss_G: 0.8052 acc: 59.4%\n",
      "[BATCH 140/149] Loss_D: 0.8116 Loss_G: 0.8101 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.8217 Loss_G: 0.8165 acc: 65.6%\n",
      "[BATCH 142/149] Loss_D: 0.7701 Loss_G: 0.8128 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7498 Loss_G: 0.8067 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7778 Loss_G: 0.8053 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.7578 Loss_G: 0.7969 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.8471 Loss_G: 0.8097 acc: 54.7%\n",
      "[BATCH 147/149] Loss_D: 0.7695 Loss_G: 0.8034 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.7787 Loss_G: 0.7952 acc: 64.1%\n",
      "[BATCH 149/149] Loss_D: 0.7973 Loss_G: 0.7928 acc: 62.5%\n",
      "-----THE [1/50] epoch end-----\n",
      "-----THE [2/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7468 Loss_G: 0.7869 acc: 64.1%\n",
      "[EPOCH 150] TEST ACC is : 75.0%\n",
      "[BATCH 2/149] Loss_D: 0.7925 Loss_G: 0.8000 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.8090 Loss_G: 0.7986 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.7619 Loss_G: 0.7854 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7551 Loss_G: 0.7824 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7817 Loss_G: 0.7979 acc: 64.1%\n",
      "[BATCH 7/149] Loss_D: 0.7813 Loss_G: 0.8053 acc: 59.4%\n",
      "[BATCH 8/149] Loss_D: 0.7856 Loss_G: 0.7994 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.7662 Loss_G: 0.7987 acc: 59.4%\n",
      "[BATCH 10/149] Loss_D: 0.7565 Loss_G: 0.7931 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.7514 Loss_G: 0.7902 acc: 70.3%\n",
      "[BATCH 12/149] Loss_D: 0.7579 Loss_G: 0.7822 acc: 56.2%\n",
      "[BATCH 13/149] Loss_D: 0.7673 Loss_G: 0.7824 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.7887 Loss_G: 0.7885 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.7611 Loss_G: 0.7870 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7758 Loss_G: 0.7901 acc: 68.8%\n",
      "[BATCH 17/149] Loss_D: 0.8210 Loss_G: 0.8077 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.7717 Loss_G: 0.8017 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.7862 Loss_G: 0.7948 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7609 Loss_G: 0.7960 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.8237 Loss_G: 0.8049 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7863 Loss_G: 0.8120 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.8020 Loss_G: 0.8108 acc: 53.1%\n",
      "[BATCH 24/149] Loss_D: 0.7656 Loss_G: 0.7970 acc: 56.2%\n",
      "[BATCH 25/149] Loss_D: 0.8159 Loss_G: 0.8053 acc: 70.3%\n",
      "[BATCH 26/149] Loss_D: 0.7646 Loss_G: 0.7940 acc: 57.8%\n",
      "[BATCH 27/149] Loss_D: 0.7933 Loss_G: 0.8030 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7533 Loss_G: 0.7950 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.8611 Loss_G: 0.8091 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.7465 Loss_G: 0.7979 acc: 71.9%\n",
      "[BATCH 31/149] Loss_D: 0.7787 Loss_G: 0.7879 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.7901 Loss_G: 0.7910 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.7594 Loss_G: 0.7931 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.8044 Loss_G: 0.7999 acc: 68.8%\n",
      "[BATCH 35/149] Loss_D: 0.7880 Loss_G: 0.8008 acc: 71.9%\n",
      "[BATCH 36/149] Loss_D: 0.7946 Loss_G: 0.8074 acc: 60.9%\n",
      "[BATCH 37/149] Loss_D: 0.7655 Loss_G: 0.7982 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7566 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.7393 Loss_G: 0.7861 acc: 56.2%\n",
      "[BATCH 40/149] Loss_D: 0.7447 Loss_G: 0.7769 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.7767 Loss_G: 0.7809 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.7540 Loss_G: 0.7866 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7385 Loss_G: 0.7744 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7778 Loss_G: 0.7747 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.8356 Loss_G: 0.7919 acc: 75.0%\n",
      "[BATCH 46/149] Loss_D: 0.8168 Loss_G: 0.7984 acc: 60.9%\n",
      "[BATCH 47/149] Loss_D: 0.7691 Loss_G: 0.7940 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.8137 Loss_G: 0.7998 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.7397 Loss_G: 0.7911 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7707 Loss_G: 0.7953 acc: 56.2%\n",
      "[BATCH 51/149] Loss_D: 0.7750 Loss_G: 0.7882 acc: 73.4%\n",
      "[EPOCH 200] TEST ACC is : 73.8%\n",
      "[BATCH 52/149] Loss_D: 0.7638 Loss_G: 0.7850 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.7959 Loss_G: 0.7858 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.7877 Loss_G: 0.7887 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.7454 Loss_G: 0.7898 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7774 Loss_G: 0.7836 acc: 60.9%\n",
      "[BATCH 57/149] Loss_D: 0.7570 Loss_G: 0.7791 acc: 53.1%\n",
      "[BATCH 58/149] Loss_D: 0.7562 Loss_G: 0.7794 acc: 68.8%\n",
      "[BATCH 59/149] Loss_D: 0.8506 Loss_G: 0.8059 acc: 71.9%\n",
      "[BATCH 60/149] Loss_D: 0.7654 Loss_G: 0.8079 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.7692 Loss_G: 0.7943 acc: 64.1%\n",
      "[BATCH 62/149] Loss_D: 0.8134 Loss_G: 0.7878 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7528 Loss_G: 0.7865 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.7544 Loss_G: 0.7871 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.7955 Loss_G: 0.8071 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.8106 Loss_G: 0.8143 acc: 67.2%\n",
      "[BATCH 67/149] Loss_D: 0.8086 Loss_G: 0.8231 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.7726 Loss_G: 0.8039 acc: 57.8%\n",
      "[BATCH 69/149] Loss_D: 0.7914 Loss_G: 0.8037 acc: 68.8%\n",
      "[BATCH 70/149] Loss_D: 0.7686 Loss_G: 0.8024 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.7924 Loss_G: 0.7995 acc: 64.1%\n",
      "[BATCH 72/149] Loss_D: 0.7715 Loss_G: 0.8073 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7789 Loss_G: 0.8028 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7798 Loss_G: 0.7927 acc: 68.8%\n",
      "[BATCH 75/149] Loss_D: 0.8023 Loss_G: 0.8103 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.7667 Loss_G: 0.8051 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7467 Loss_G: 0.7957 acc: 57.8%\n",
      "[BATCH 78/149] Loss_D: 0.7706 Loss_G: 0.7937 acc: 67.2%\n",
      "[BATCH 79/149] Loss_D: 0.7576 Loss_G: 0.7814 acc: 65.6%\n",
      "[BATCH 80/149] Loss_D: 0.7577 Loss_G: 0.7710 acc: 73.4%\n",
      "[BATCH 81/149] Loss_D: 0.7913 Loss_G: 0.7855 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.7681 Loss_G: 0.7960 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.8236 Loss_G: 0.8200 acc: 67.2%\n",
      "[BATCH 84/149] Loss_D: 0.7681 Loss_G: 0.8088 acc: 62.5%\n",
      "[BATCH 85/149] Loss_D: 0.8140 Loss_G: 0.8111 acc: 57.8%\n",
      "[BATCH 86/149] Loss_D: 0.7847 Loss_G: 0.8059 acc: 70.3%\n",
      "[BATCH 87/149] Loss_D: 0.7696 Loss_G: 0.8056 acc: 71.9%\n",
      "[BATCH 88/149] Loss_D: 0.8037 Loss_G: 0.8133 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.8236 Loss_G: 0.8270 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7992 Loss_G: 0.8228 acc: 57.8%\n",
      "[BATCH 91/149] Loss_D: 0.7733 Loss_G: 0.8190 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.8017 Loss_G: 0.8151 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.7494 Loss_G: 0.8143 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7676 Loss_G: 0.7921 acc: 65.6%\n",
      "[BATCH 95/149] Loss_D: 0.7569 Loss_G: 0.8036 acc: 57.8%\n",
      "[BATCH 96/149] Loss_D: 0.7951 Loss_G: 0.8173 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.7560 Loss_G: 0.7996 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7712 Loss_G: 0.8002 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7924 Loss_G: 0.7941 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.7677 Loss_G: 0.7987 acc: 57.8%\n",
      "[BATCH 101/149] Loss_D: 0.7376 Loss_G: 0.7849 acc: 65.6%\n",
      "[EPOCH 250] TEST ACC is : 73.4%\n",
      "[BATCH 102/149] Loss_D: 0.8124 Loss_G: 0.7892 acc: 53.1%\n",
      "[BATCH 103/149] Loss_D: 0.8088 Loss_G: 0.7966 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.7867 Loss_G: 0.8034 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7736 Loss_G: 0.7909 acc: 76.6%\n",
      "[BATCH 106/149] Loss_D: 0.7675 Loss_G: 0.7921 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.7630 Loss_G: 0.7890 acc: 73.4%\n",
      "[BATCH 108/149] Loss_D: 0.7472 Loss_G: 0.7894 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.7696 Loss_G: 0.7893 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7645 Loss_G: 0.7914 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.8095 Loss_G: 0.7987 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.8283 Loss_G: 0.8092 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.7953 Loss_G: 0.8275 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.7690 Loss_G: 0.8032 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.7584 Loss_G: 0.7945 acc: 68.8%\n",
      "[BATCH 116/149] Loss_D: 0.7514 Loss_G: 0.7926 acc: 59.4%\n",
      "[BATCH 117/149] Loss_D: 0.7649 Loss_G: 0.7909 acc: 56.2%\n",
      "[BATCH 118/149] Loss_D: 0.7782 Loss_G: 0.7920 acc: 71.9%\n",
      "[BATCH 119/149] Loss_D: 0.7999 Loss_G: 0.7967 acc: 56.2%\n",
      "[BATCH 120/149] Loss_D: 0.7579 Loss_G: 0.7907 acc: 54.7%\n",
      "[BATCH 121/149] Loss_D: 0.8117 Loss_G: 0.7888 acc: 76.6%\n",
      "[BATCH 122/149] Loss_D: 0.7560 Loss_G: 0.7884 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.7381 Loss_G: 0.7852 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7941 Loss_G: 0.7772 acc: 76.6%\n",
      "[BATCH 125/149] Loss_D: 0.8057 Loss_G: 0.8013 acc: 71.9%\n",
      "[BATCH 126/149] Loss_D: 0.8003 Loss_G: 0.8059 acc: 57.8%\n",
      "[BATCH 127/149] Loss_D: 0.7545 Loss_G: 0.8093 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7467 Loss_G: 0.7967 acc: 59.4%\n",
      "[BATCH 129/149] Loss_D: 0.8129 Loss_G: 0.8006 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7430 Loss_G: 0.8004 acc: 51.6%\n",
      "[BATCH 131/149] Loss_D: 0.8218 Loss_G: 0.8135 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.7526 Loss_G: 0.8030 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.7793 Loss_G: 0.7984 acc: 59.4%\n",
      "[BATCH 134/149] Loss_D: 0.7589 Loss_G: 0.7975 acc: 60.9%\n",
      "[BATCH 135/149] Loss_D: 0.8064 Loss_G: 0.8110 acc: 56.2%\n",
      "[BATCH 136/149] Loss_D: 0.7888 Loss_G: 0.8141 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7632 Loss_G: 0.8029 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.8189 Loss_G: 0.8066 acc: 75.0%\n",
      "[BATCH 139/149] Loss_D: 0.7690 Loss_G: 0.8077 acc: 70.3%\n",
      "[BATCH 140/149] Loss_D: 0.7860 Loss_G: 0.7993 acc: 62.5%\n",
      "[BATCH 141/149] Loss_D: 0.7951 Loss_G: 0.8094 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.8013 Loss_G: 0.8081 acc: 67.2%\n",
      "[BATCH 143/149] Loss_D: 0.7548 Loss_G: 0.7957 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7889 Loss_G: 0.7982 acc: 68.8%\n",
      "[BATCH 145/149] Loss_D: 0.8056 Loss_G: 0.8094 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.7487 Loss_G: 0.8004 acc: 71.9%\n",
      "[BATCH 147/149] Loss_D: 0.7836 Loss_G: 0.7964 acc: 67.2%\n",
      "[BATCH 148/149] Loss_D: 0.7639 Loss_G: 0.7849 acc: 71.9%\n",
      "[BATCH 149/149] Loss_D: 0.7607 Loss_G: 0.8014 acc: 70.3%\n",
      "-----THE [2/50] epoch end-----\n",
      "-----THE [3/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7477 Loss_G: 0.7813 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.7715 Loss_G: 0.7887 acc: 67.2%\n",
      "[EPOCH 300] TEST ACC is : 75.2%\n",
      "[BATCH 3/149] Loss_D: 0.8008 Loss_G: 0.7976 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.7879 Loss_G: 0.8092 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.7800 Loss_G: 0.8152 acc: 64.1%\n",
      "[BATCH 6/149] Loss_D: 0.7426 Loss_G: 0.7899 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.8084 Loss_G: 0.7999 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7495 Loss_G: 0.7878 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.7543 Loss_G: 0.7836 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.8054 Loss_G: 0.7984 acc: 73.4%\n",
      "[BATCH 11/149] Loss_D: 0.7603 Loss_G: 0.7908 acc: 67.2%\n",
      "[BATCH 12/149] Loss_D: 0.7797 Loss_G: 0.7902 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.8227 Loss_G: 0.8074 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7905 Loss_G: 0.8066 acc: 57.8%\n",
      "[BATCH 15/149] Loss_D: 0.7799 Loss_G: 0.7990 acc: 56.2%\n",
      "[BATCH 16/149] Loss_D: 0.8171 Loss_G: 0.7987 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7811 Loss_G: 0.7904 acc: 53.1%\n",
      "[BATCH 18/149] Loss_D: 0.7473 Loss_G: 0.7885 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7641 Loss_G: 0.7823 acc: 62.5%\n",
      "[BATCH 20/149] Loss_D: 0.7911 Loss_G: 0.7854 acc: 60.9%\n",
      "[BATCH 21/149] Loss_D: 0.7559 Loss_G: 0.7792 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.7389 Loss_G: 0.7747 acc: 60.9%\n",
      "[BATCH 23/149] Loss_D: 0.7592 Loss_G: 0.7772 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7444 Loss_G: 0.7740 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.7776 Loss_G: 0.7825 acc: 73.4%\n",
      "[BATCH 26/149] Loss_D: 0.8020 Loss_G: 0.7953 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.7786 Loss_G: 0.7974 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.8099 Loss_G: 0.8129 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7934 Loss_G: 0.8167 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.8218 Loss_G: 0.8217 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7414 Loss_G: 0.7999 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.7636 Loss_G: 0.7926 acc: 71.9%\n",
      "[BATCH 33/149] Loss_D: 0.7587 Loss_G: 0.7870 acc: 60.9%\n",
      "[BATCH 34/149] Loss_D: 0.8171 Loss_G: 0.8160 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7961 Loss_G: 0.8229 acc: 75.0%\n",
      "[BATCH 36/149] Loss_D: 0.7908 Loss_G: 0.8230 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7943 Loss_G: 0.8054 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7804 Loss_G: 0.7997 acc: 60.9%\n",
      "[BATCH 39/149] Loss_D: 0.8022 Loss_G: 0.7983 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.7807 Loss_G: 0.8088 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7822 Loss_G: 0.8095 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.7770 Loss_G: 0.7949 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7731 Loss_G: 0.8026 acc: 57.8%\n",
      "[BATCH 44/149] Loss_D: 0.7520 Loss_G: 0.7953 acc: 70.3%\n",
      "[BATCH 45/149] Loss_D: 0.7793 Loss_G: 0.7954 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.8003 Loss_G: 0.8131 acc: 73.4%\n",
      "[BATCH 47/149] Loss_D: 0.7661 Loss_G: 0.7966 acc: 67.2%\n",
      "[BATCH 48/149] Loss_D: 0.7967 Loss_G: 0.7993 acc: 71.9%\n",
      "[BATCH 49/149] Loss_D: 0.7482 Loss_G: 0.7929 acc: 56.2%\n",
      "[BATCH 50/149] Loss_D: 0.7608 Loss_G: 0.7916 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7853 Loss_G: 0.7981 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.7232 Loss_G: 0.7878 acc: 56.2%\n",
      "[EPOCH 350] TEST ACC is : 74.4%\n",
      "[BATCH 53/149] Loss_D: 0.8031 Loss_G: 0.7873 acc: 75.0%\n",
      "[BATCH 54/149] Loss_D: 0.7822 Loss_G: 0.7851 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.7590 Loss_G: 0.7918 acc: 65.6%\n",
      "[BATCH 56/149] Loss_D: 0.8146 Loss_G: 0.8116 acc: 70.3%\n",
      "[BATCH 57/149] Loss_D: 0.7733 Loss_G: 0.8007 acc: 62.5%\n",
      "[BATCH 58/149] Loss_D: 0.7782 Loss_G: 0.8193 acc: 59.4%\n",
      "[BATCH 59/149] Loss_D: 0.7782 Loss_G: 0.8040 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7787 Loss_G: 0.7994 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.7551 Loss_G: 0.7891 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.7826 Loss_G: 0.7929 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7976 Loss_G: 0.8018 acc: 57.8%\n",
      "[BATCH 64/149] Loss_D: 0.7543 Loss_G: 0.7849 acc: 79.7%\n",
      "[BATCH 65/149] Loss_D: 0.7730 Loss_G: 0.7804 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7973 Loss_G: 0.7858 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7746 Loss_G: 0.7934 acc: 70.3%\n",
      "[BATCH 68/149] Loss_D: 0.7372 Loss_G: 0.7797 acc: 75.0%\n",
      "[BATCH 69/149] Loss_D: 0.7726 Loss_G: 0.7783 acc: 54.7%\n",
      "[BATCH 70/149] Loss_D: 0.7777 Loss_G: 0.7781 acc: 62.5%\n",
      "[BATCH 71/149] Loss_D: 0.7833 Loss_G: 0.7886 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.8093 Loss_G: 0.7987 acc: 56.2%\n",
      "[BATCH 73/149] Loss_D: 0.7676 Loss_G: 0.7921 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7802 Loss_G: 0.7925 acc: 62.5%\n",
      "[BATCH 75/149] Loss_D: 0.7588 Loss_G: 0.7834 acc: 71.9%\n",
      "[BATCH 76/149] Loss_D: 0.7869 Loss_G: 0.7931 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7834 Loss_G: 0.8030 acc: 56.2%\n",
      "[BATCH 78/149] Loss_D: 0.8367 Loss_G: 0.8155 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.7812 Loss_G: 0.8026 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7943 Loss_G: 0.7993 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.8555 Loss_G: 0.8137 acc: 75.0%\n",
      "[BATCH 82/149] Loss_D: 0.7727 Loss_G: 0.8088 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7946 Loss_G: 0.8145 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.7669 Loss_G: 0.8165 acc: 67.2%\n",
      "[BATCH 85/149] Loss_D: 0.8138 Loss_G: 0.8153 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.7395 Loss_G: 0.8033 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.8054 Loss_G: 0.7997 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7784 Loss_G: 0.8073 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.7615 Loss_G: 0.8097 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.7669 Loss_G: 0.8122 acc: 60.9%\n",
      "[BATCH 91/149] Loss_D: 0.7738 Loss_G: 0.7900 acc: 50.0%\n",
      "[BATCH 92/149] Loss_D: 0.7903 Loss_G: 0.7884 acc: 59.4%\n",
      "[BATCH 93/149] Loss_D: 0.7841 Loss_G: 0.7897 acc: 71.9%\n",
      "[BATCH 94/149] Loss_D: 0.7675 Loss_G: 0.7908 acc: 57.8%\n",
      "[BATCH 95/149] Loss_D: 0.7618 Loss_G: 0.7859 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.7972 Loss_G: 0.7880 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7852 Loss_G: 0.7957 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.8052 Loss_G: 0.7977 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.7927 Loss_G: 0.8022 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.7207 Loss_G: 0.7923 acc: 56.2%\n",
      "[BATCH 101/149] Loss_D: 0.7951 Loss_G: 0.7981 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.7941 Loss_G: 0.8086 acc: 68.8%\n",
      "[EPOCH 400] TEST ACC is : 73.4%\n",
      "[BATCH 103/149] Loss_D: 0.7655 Loss_G: 0.8010 acc: 70.3%\n",
      "[BATCH 104/149] Loss_D: 0.7961 Loss_G: 0.8270 acc: 76.6%\n",
      "[BATCH 105/149] Loss_D: 0.8026 Loss_G: 0.8309 acc: 65.6%\n",
      "[BATCH 106/149] Loss_D: 0.8107 Loss_G: 0.8225 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.7333 Loss_G: 0.8110 acc: 57.8%\n",
      "[BATCH 108/149] Loss_D: 0.7499 Loss_G: 0.7881 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.7932 Loss_G: 0.7890 acc: 68.8%\n",
      "[BATCH 110/149] Loss_D: 0.7599 Loss_G: 0.7841 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.7857 Loss_G: 0.7953 acc: 67.2%\n",
      "[BATCH 112/149] Loss_D: 0.7761 Loss_G: 0.7935 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.8054 Loss_G: 0.7994 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.8275 Loss_G: 0.8278 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.8016 Loss_G: 0.8378 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8178 Loss_G: 0.8235 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7986 Loss_G: 0.8171 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7781 Loss_G: 0.7968 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.8031 Loss_G: 0.8032 acc: 60.9%\n",
      "[BATCH 120/149] Loss_D: 0.8183 Loss_G: 0.8062 acc: 60.9%\n",
      "[BATCH 121/149] Loss_D: 0.7463 Loss_G: 0.7982 acc: 73.4%\n",
      "[BATCH 122/149] Loss_D: 0.7786 Loss_G: 0.8043 acc: 73.4%\n",
      "[BATCH 123/149] Loss_D: 0.7842 Loss_G: 0.8062 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.7379 Loss_G: 0.7993 acc: 60.9%\n",
      "[BATCH 125/149] Loss_D: 0.7440 Loss_G: 0.7812 acc: 70.3%\n",
      "[BATCH 126/149] Loss_D: 0.7943 Loss_G: 0.7777 acc: 68.8%\n",
      "[BATCH 127/149] Loss_D: 0.7517 Loss_G: 0.7730 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.8037 Loss_G: 0.7862 acc: 65.6%\n",
      "[BATCH 129/149] Loss_D: 0.7554 Loss_G: 0.7940 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7833 Loss_G: 0.8030 acc: 51.6%\n",
      "[BATCH 131/149] Loss_D: 0.7509 Loss_G: 0.7984 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7617 Loss_G: 0.7929 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.7609 Loss_G: 0.7842 acc: 73.4%\n",
      "[BATCH 134/149] Loss_D: 0.7748 Loss_G: 0.7940 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.8361 Loss_G: 0.8186 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7607 Loss_G: 0.7986 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7761 Loss_G: 0.7986 acc: 65.6%\n",
      "[BATCH 138/149] Loss_D: 0.7687 Loss_G: 0.7946 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7789 Loss_G: 0.8005 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.7865 Loss_G: 0.7981 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.7711 Loss_G: 0.7993 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7476 Loss_G: 0.7992 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.7829 Loss_G: 0.8025 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7548 Loss_G: 0.8026 acc: 65.6%\n",
      "[BATCH 145/149] Loss_D: 0.7634 Loss_G: 0.7920 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.7910 Loss_G: 0.8017 acc: 62.5%\n",
      "[BATCH 147/149] Loss_D: 0.7702 Loss_G: 0.7979 acc: 64.1%\n",
      "[BATCH 148/149] Loss_D: 0.7670 Loss_G: 0.7898 acc: 68.8%\n",
      "[BATCH 149/149] Loss_D: 0.7461 Loss_G: 0.7771 acc: 60.9%\n",
      "-----THE [3/50] epoch end-----\n",
      "-----THE [4/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7517 Loss_G: 0.7842 acc: 76.6%\n",
      "[BATCH 2/149] Loss_D: 0.7762 Loss_G: 0.7801 acc: 62.5%\n",
      "[BATCH 3/149] Loss_D: 0.8400 Loss_G: 0.7951 acc: 59.4%\n",
      "[EPOCH 450] TEST ACC is : 74.8%\n",
      "[BATCH 4/149] Loss_D: 0.8073 Loss_G: 0.8008 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.8015 Loss_G: 0.7963 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7870 Loss_G: 0.7982 acc: 65.6%\n",
      "[BATCH 7/149] Loss_D: 0.7378 Loss_G: 0.7845 acc: 67.2%\n",
      "[BATCH 8/149] Loss_D: 0.7505 Loss_G: 0.7766 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.8011 Loss_G: 0.7902 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7765 Loss_G: 0.8047 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.7750 Loss_G: 0.8026 acc: 67.2%\n",
      "[BATCH 12/149] Loss_D: 0.7781 Loss_G: 0.7974 acc: 62.5%\n",
      "[BATCH 13/149] Loss_D: 0.8032 Loss_G: 0.7953 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.7536 Loss_G: 0.7943 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7367 Loss_G: 0.7809 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7759 Loss_G: 0.7910 acc: 76.6%\n",
      "[BATCH 17/149] Loss_D: 0.7566 Loss_G: 0.7891 acc: 71.9%\n",
      "[BATCH 18/149] Loss_D: 0.7753 Loss_G: 0.7979 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7392 Loss_G: 0.7857 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.7714 Loss_G: 0.7887 acc: 57.8%\n",
      "[BATCH 21/149] Loss_D: 0.7599 Loss_G: 0.7875 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.7536 Loss_G: 0.7827 acc: 59.4%\n",
      "[BATCH 23/149] Loss_D: 0.7643 Loss_G: 0.7854 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.7503 Loss_G: 0.7777 acc: 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.7846 Loss_G: 0.7882 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7581 Loss_G: 0.7852 acc: 71.9%\n",
      "[BATCH 27/149] Loss_D: 0.7646 Loss_G: 0.7831 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.7881 Loss_G: 0.7954 acc: 54.7%\n",
      "[BATCH 29/149] Loss_D: 0.7660 Loss_G: 0.7914 acc: 71.9%\n",
      "[BATCH 30/149] Loss_D: 0.7611 Loss_G: 0.7890 acc: 75.0%\n",
      "[BATCH 31/149] Loss_D: 0.7978 Loss_G: 0.7982 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.7800 Loss_G: 0.7988 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7940 Loss_G: 0.8029 acc: 73.4%\n",
      "[BATCH 34/149] Loss_D: 0.7800 Loss_G: 0.8012 acc: 71.9%\n",
      "[BATCH 35/149] Loss_D: 0.8256 Loss_G: 0.8150 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.8059 Loss_G: 0.8102 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.7822 Loss_G: 0.8137 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7785 Loss_G: 0.8064 acc: 65.6%\n",
      "[BATCH 39/149] Loss_D: 0.7748 Loss_G: 0.7990 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.7485 Loss_G: 0.7918 acc: 73.4%\n",
      "[BATCH 41/149] Loss_D: 0.7665 Loss_G: 0.7931 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7819 Loss_G: 0.7916 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.7948 Loss_G: 0.8035 acc: 56.2%\n",
      "[BATCH 44/149] Loss_D: 0.8136 Loss_G: 0.8110 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.8279 Loss_G: 0.8195 acc: 67.2%\n",
      "[BATCH 46/149] Loss_D: 0.8000 Loss_G: 0.8255 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.7591 Loss_G: 0.8146 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.7545 Loss_G: 0.7980 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7799 Loss_G: 0.7982 acc: 65.6%\n",
      "[BATCH 50/149] Loss_D: 0.7786 Loss_G: 0.8099 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7680 Loss_G: 0.8000 acc: 65.6%\n",
      "[BATCH 52/149] Loss_D: 0.7684 Loss_G: 0.7968 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7360 Loss_G: 0.7851 acc: 62.5%\n",
      "[EPOCH 500] TEST ACC is : 75.0%\n",
      "[BATCH 54/149] Loss_D: 0.8045 Loss_G: 0.7888 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.7558 Loss_G: 0.7882 acc: 57.8%\n",
      "[BATCH 56/149] Loss_D: 0.7824 Loss_G: 0.7825 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.7738 Loss_G: 0.7816 acc: 67.2%\n",
      "[BATCH 58/149] Loss_D: 0.7769 Loss_G: 0.7878 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7697 Loss_G: 0.7965 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7966 Loss_G: 0.7977 acc: 68.8%\n",
      "[BATCH 61/149] Loss_D: 0.7896 Loss_G: 0.8082 acc: 78.1%\n",
      "[BATCH 62/149] Loss_D: 0.7776 Loss_G: 0.8103 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.7842 Loss_G: 0.8017 acc: 59.4%\n",
      "[BATCH 64/149] Loss_D: 0.8432 Loss_G: 0.8257 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.7601 Loss_G: 0.8093 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.7656 Loss_G: 0.7917 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.7809 Loss_G: 0.8012 acc: 62.5%\n",
      "[BATCH 68/149] Loss_D: 0.7726 Loss_G: 0.8026 acc: 59.4%\n",
      "[BATCH 69/149] Loss_D: 0.7886 Loss_G: 0.8073 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.7274 Loss_G: 0.7919 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.8060 Loss_G: 0.8014 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7818 Loss_G: 0.8095 acc: 64.1%\n",
      "[BATCH 73/149] Loss_D: 0.8028 Loss_G: 0.8130 acc: 56.2%\n",
      "[BATCH 74/149] Loss_D: 0.7661 Loss_G: 0.8001 acc: 57.8%\n",
      "[BATCH 75/149] Loss_D: 0.7698 Loss_G: 0.8067 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.8038 Loss_G: 0.8138 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7586 Loss_G: 0.8027 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.7769 Loss_G: 0.7978 acc: 67.2%\n",
      "[BATCH 79/149] Loss_D: 0.7787 Loss_G: 0.7946 acc: 60.9%\n",
      "[BATCH 80/149] Loss_D: 0.7707 Loss_G: 0.7924 acc: 53.1%\n",
      "[BATCH 81/149] Loss_D: 0.7761 Loss_G: 0.7858 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.8260 Loss_G: 0.8009 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.8115 Loss_G: 0.8069 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.8240 Loss_G: 0.8166 acc: 70.3%\n",
      "[BATCH 85/149] Loss_D: 0.7565 Loss_G: 0.7957 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.7789 Loss_G: 0.7850 acc: 73.4%\n",
      "[BATCH 87/149] Loss_D: 0.7453 Loss_G: 0.7818 acc: 54.7%\n",
      "[BATCH 88/149] Loss_D: 0.7654 Loss_G: 0.7887 acc: 53.1%\n",
      "[BATCH 89/149] Loss_D: 0.8183 Loss_G: 0.7968 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.7908 Loss_G: 0.7952 acc: 67.2%\n",
      "[BATCH 91/149] Loss_D: 0.7439 Loss_G: 0.7830 acc: 68.8%\n",
      "[BATCH 92/149] Loss_D: 0.7555 Loss_G: 0.7832 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.7837 Loss_G: 0.7886 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7478 Loss_G: 0.7803 acc: 71.9%\n",
      "[BATCH 95/149] Loss_D: 0.7847 Loss_G: 0.7934 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.7765 Loss_G: 0.8047 acc: 56.2%\n",
      "[BATCH 97/149] Loss_D: 0.7538 Loss_G: 0.7886 acc: 57.8%\n",
      "[BATCH 98/149] Loss_D: 0.7573 Loss_G: 0.7776 acc: 56.2%\n",
      "[BATCH 99/149] Loss_D: 0.7967 Loss_G: 0.7900 acc: 51.6%\n",
      "[BATCH 100/149] Loss_D: 0.7741 Loss_G: 0.7834 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.8234 Loss_G: 0.8100 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.8042 Loss_G: 0.8141 acc: 71.9%\n",
      "[BATCH 103/149] Loss_D: 0.8041 Loss_G: 0.8188 acc: 56.2%\n",
      "[EPOCH 550] TEST ACC is : 74.8%\n",
      "[BATCH 104/149] Loss_D: 0.8037 Loss_G: 0.8196 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.7671 Loss_G: 0.8066 acc: 59.4%\n",
      "[BATCH 106/149] Loss_D: 0.7677 Loss_G: 0.8041 acc: 70.3%\n",
      "[BATCH 107/149] Loss_D: 0.8211 Loss_G: 0.8172 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.8095 Loss_G: 0.8329 acc: 57.8%\n",
      "[BATCH 109/149] Loss_D: 0.7852 Loss_G: 0.8209 acc: 71.9%\n",
      "[BATCH 110/149] Loss_D: 0.7494 Loss_G: 0.8036 acc: 57.8%\n",
      "[BATCH 111/149] Loss_D: 0.7929 Loss_G: 0.7960 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.7657 Loss_G: 0.7943 acc: 71.9%\n",
      "[BATCH 113/149] Loss_D: 0.7525 Loss_G: 0.7928 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7571 Loss_G: 0.7836 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.7512 Loss_G: 0.7794 acc: 73.4%\n",
      "[BATCH 116/149] Loss_D: 0.7864 Loss_G: 0.7855 acc: 62.5%\n",
      "[BATCH 117/149] Loss_D: 0.8037 Loss_G: 0.8022 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.7614 Loss_G: 0.7954 acc: 71.9%\n",
      "[BATCH 119/149] Loss_D: 0.7926 Loss_G: 0.7867 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7672 Loss_G: 0.7899 acc: 70.3%\n",
      "[BATCH 121/149] Loss_D: 0.8096 Loss_G: 0.7984 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.7869 Loss_G: 0.7915 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.7769 Loss_G: 0.7920 acc: 54.7%\n",
      "[BATCH 124/149] Loss_D: 0.7672 Loss_G: 0.7888 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.7577 Loss_G: 0.7879 acc: 71.9%\n",
      "[BATCH 126/149] Loss_D: 0.7743 Loss_G: 0.8058 acc: 71.9%\n",
      "[BATCH 127/149] Loss_D: 0.8049 Loss_G: 0.8010 acc: 57.8%\n",
      "[BATCH 128/149] Loss_D: 0.7372 Loss_G: 0.7930 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.8011 Loss_G: 0.7956 acc: 68.8%\n",
      "[BATCH 130/149] Loss_D: 0.7404 Loss_G: 0.7887 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.7490 Loss_G: 0.7772 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7554 Loss_G: 0.7718 acc: 65.6%\n",
      "[BATCH 133/149] Loss_D: 0.7902 Loss_G: 0.7792 acc: 75.0%\n",
      "[BATCH 134/149] Loss_D: 0.8521 Loss_G: 0.8097 acc: 57.8%\n",
      "[BATCH 135/149] Loss_D: 0.7769 Loss_G: 0.8043 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.7577 Loss_G: 0.7971 acc: 75.0%\n",
      "[BATCH 137/149] Loss_D: 0.7563 Loss_G: 0.7887 acc: 53.1%\n",
      "[BATCH 138/149] Loss_D: 0.7647 Loss_G: 0.7827 acc: 57.8%\n",
      "[BATCH 139/149] Loss_D: 0.7669 Loss_G: 0.7926 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7562 Loss_G: 0.7802 acc: 68.8%\n",
      "[BATCH 141/149] Loss_D: 0.7340 Loss_G: 0.7773 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.8020 Loss_G: 0.7903 acc: 60.9%\n",
      "[BATCH 143/149] Loss_D: 0.8167 Loss_G: 0.8071 acc: 73.4%\n",
      "[BATCH 144/149] Loss_D: 0.7666 Loss_G: 0.8045 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.7978 Loss_G: 0.8144 acc: 68.8%\n",
      "[BATCH 146/149] Loss_D: 0.7600 Loss_G: 0.8050 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.8144 Loss_G: 0.8031 acc: 64.1%\n",
      "[BATCH 148/149] Loss_D: 0.7947 Loss_G: 0.7999 acc: 50.0%\n",
      "[BATCH 149/149] Loss_D: 0.7772 Loss_G: 0.7980 acc: 73.4%\n",
      "-----THE [4/50] epoch end-----\n",
      "-----THE [5/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7949 Loss_G: 0.8001 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.7588 Loss_G: 0.7923 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.7940 Loss_G: 0.8000 acc: 81.2%\n",
      "[BATCH 4/149] Loss_D: 0.7458 Loss_G: 0.7939 acc: 59.4%\n",
      "[EPOCH 600] TEST ACC is : 73.6%\n",
      "[BATCH 5/149] Loss_D: 0.7471 Loss_G: 0.7841 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7714 Loss_G: 0.7838 acc: 64.1%\n",
      "[BATCH 7/149] Loss_D: 0.7640 Loss_G: 0.7792 acc: 73.4%\n",
      "[BATCH 8/149] Loss_D: 0.7707 Loss_G: 0.7871 acc: 59.4%\n",
      "[BATCH 9/149] Loss_D: 0.7701 Loss_G: 0.7845 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7304 Loss_G: 0.7771 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.8187 Loss_G: 0.7793 acc: 75.0%\n",
      "[BATCH 12/149] Loss_D: 0.8017 Loss_G: 0.8086 acc: 62.5%\n",
      "[BATCH 13/149] Loss_D: 0.7681 Loss_G: 0.8141 acc: 51.6%\n",
      "[BATCH 14/149] Loss_D: 0.7936 Loss_G: 0.8054 acc: 71.9%\n",
      "[BATCH 15/149] Loss_D: 0.7746 Loss_G: 0.8034 acc: 59.4%\n",
      "[BATCH 16/149] Loss_D: 0.8245 Loss_G: 0.8167 acc: 65.6%\n",
      "[BATCH 17/149] Loss_D: 0.8230 Loss_G: 0.8236 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.7704 Loss_G: 0.7997 acc: 60.9%\n",
      "[BATCH 19/149] Loss_D: 0.7858 Loss_G: 0.7959 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.8009 Loss_G: 0.8063 acc: 67.2%\n",
      "[BATCH 21/149] Loss_D: 0.7600 Loss_G: 0.7967 acc: 65.6%\n",
      "[BATCH 22/149] Loss_D: 0.8231 Loss_G: 0.7967 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.7540 Loss_G: 0.7930 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7839 Loss_G: 0.7899 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.7683 Loss_G: 0.7879 acc: 62.5%\n",
      "[BATCH 26/149] Loss_D: 0.7792 Loss_G: 0.7829 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.8111 Loss_G: 0.8030 acc: 53.1%\n",
      "[BATCH 28/149] Loss_D: 0.7836 Loss_G: 0.8207 acc: 68.8%\n",
      "[BATCH 29/149] Loss_D: 0.7585 Loss_G: 0.8063 acc: 67.2%\n",
      "[BATCH 30/149] Loss_D: 0.7633 Loss_G: 0.8008 acc: 71.9%\n",
      "[BATCH 31/149] Loss_D: 0.7601 Loss_G: 0.8043 acc: 60.9%\n",
      "[BATCH 32/149] Loss_D: 0.7809 Loss_G: 0.8069 acc: 71.9%\n",
      "[BATCH 33/149] Loss_D: 0.7712 Loss_G: 0.7996 acc: 70.3%\n",
      "[BATCH 34/149] Loss_D: 0.7489 Loss_G: 0.7964 acc: 71.9%\n",
      "[BATCH 35/149] Loss_D: 0.7983 Loss_G: 0.7910 acc: 75.0%\n",
      "[BATCH 36/149] Loss_D: 0.7520 Loss_G: 0.7919 acc: 59.4%\n",
      "[BATCH 37/149] Loss_D: 0.7531 Loss_G: 0.7850 acc: 57.8%\n",
      "[BATCH 38/149] Loss_D: 0.7872 Loss_G: 0.7962 acc: 59.4%\n",
      "[BATCH 39/149] Loss_D: 0.7965 Loss_G: 0.8191 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.8139 Loss_G: 0.8227 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.7766 Loss_G: 0.8067 acc: 75.0%\n",
      "[BATCH 42/149] Loss_D: 0.8204 Loss_G: 0.8251 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.7504 Loss_G: 0.8013 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7350 Loss_G: 0.7924 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7669 Loss_G: 0.7930 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.7762 Loss_G: 0.7970 acc: 70.3%\n",
      "[BATCH 47/149] Loss_D: 0.7410 Loss_G: 0.7871 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.8184 Loss_G: 0.7868 acc: 62.5%\n",
      "[BATCH 49/149] Loss_D: 0.8196 Loss_G: 0.7960 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7490 Loss_G: 0.7825 acc: 75.0%\n",
      "[BATCH 51/149] Loss_D: 0.7563 Loss_G: 0.7797 acc: 78.1%\n",
      "[BATCH 52/149] Loss_D: 0.7548 Loss_G: 0.7833 acc: 51.6%\n",
      "[BATCH 53/149] Loss_D: 0.7881 Loss_G: 0.7903 acc: 67.2%\n",
      "[BATCH 54/149] Loss_D: 0.8051 Loss_G: 0.7999 acc: 56.2%\n",
      "[EPOCH 650] TEST ACC is : 73.0%\n",
      "[BATCH 55/149] Loss_D: 0.7433 Loss_G: 0.7917 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7434 Loss_G: 0.7796 acc: 60.9%\n",
      "[BATCH 57/149] Loss_D: 0.8306 Loss_G: 0.7968 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.7992 Loss_G: 0.8060 acc: 68.8%\n",
      "[BATCH 59/149] Loss_D: 0.8229 Loss_G: 0.8014 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7853 Loss_G: 0.8012 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.8502 Loss_G: 0.8112 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7660 Loss_G: 0.7995 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7541 Loss_G: 0.8047 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.7801 Loss_G: 0.7980 acc: 59.4%\n",
      "[BATCH 65/149] Loss_D: 0.8226 Loss_G: 0.8045 acc: 73.4%\n",
      "[BATCH 66/149] Loss_D: 0.7613 Loss_G: 0.8003 acc: 62.5%\n",
      "[BATCH 67/149] Loss_D: 0.7937 Loss_G: 0.8061 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.8078 Loss_G: 0.8051 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.7779 Loss_G: 0.7983 acc: 57.8%\n",
      "[BATCH 70/149] Loss_D: 0.7876 Loss_G: 0.7986 acc: 56.2%\n",
      "[BATCH 71/149] Loss_D: 0.7672 Loss_G: 0.7924 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.8191 Loss_G: 0.8056 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.7727 Loss_G: 0.7997 acc: 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.8126 Loss_G: 0.8048 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.7585 Loss_G: 0.7956 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.7954 Loss_G: 0.7858 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.7879 Loss_G: 0.7882 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.7379 Loss_G: 0.7827 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.7501 Loss_G: 0.7832 acc: 56.2%\n",
      "[BATCH 80/149] Loss_D: 0.7488 Loss_G: 0.7764 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7983 Loss_G: 0.7839 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.8083 Loss_G: 0.8104 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.7801 Loss_G: 0.8121 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.7618 Loss_G: 0.8044 acc: 68.8%\n",
      "[BATCH 85/149] Loss_D: 0.7628 Loss_G: 0.7970 acc: 70.3%\n",
      "[BATCH 86/149] Loss_D: 0.7536 Loss_G: 0.7894 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.7984 Loss_G: 0.7837 acc: 60.9%\n",
      "[BATCH 88/149] Loss_D: 0.7667 Loss_G: 0.7955 acc: 51.6%\n",
      "[BATCH 89/149] Loss_D: 0.8439 Loss_G: 0.8209 acc: 60.9%\n",
      "[BATCH 90/149] Loss_D: 0.7828 Loss_G: 0.8296 acc: 71.9%\n",
      "[BATCH 91/149] Loss_D: 0.7628 Loss_G: 0.8221 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.7932 Loss_G: 0.8204 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.7507 Loss_G: 0.8015 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7552 Loss_G: 0.7806 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.8037 Loss_G: 0.7955 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.7866 Loss_G: 0.7901 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7557 Loss_G: 0.7867 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.7956 Loss_G: 0.7867 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7841 Loss_G: 0.7927 acc: 68.8%\n",
      "[BATCH 100/149] Loss_D: 0.8350 Loss_G: 0.8168 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7731 Loss_G: 0.8120 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.7674 Loss_G: 0.8001 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.7704 Loss_G: 0.7949 acc: 76.6%\n",
      "[BATCH 104/149] Loss_D: 0.7942 Loss_G: 0.8020 acc: 60.9%\n",
      "[EPOCH 700] TEST ACC is : 73.4%\n",
      "[BATCH 105/149] Loss_D: 0.7670 Loss_G: 0.7912 acc: 71.9%\n",
      "[BATCH 106/149] Loss_D: 0.8248 Loss_G: 0.8101 acc: 68.8%\n",
      "[BATCH 107/149] Loss_D: 0.7396 Loss_G: 0.7953 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7867 Loss_G: 0.7950 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.8109 Loss_G: 0.7982 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.7377 Loss_G: 0.8036 acc: 73.4%\n",
      "[BATCH 111/149] Loss_D: 0.7741 Loss_G: 0.7940 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7873 Loss_G: 0.7898 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.7643 Loss_G: 0.7873 acc: 59.4%\n",
      "[BATCH 114/149] Loss_D: 0.8189 Loss_G: 0.8041 acc: 62.5%\n",
      "[BATCH 115/149] Loss_D: 0.7765 Loss_G: 0.7945 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.7701 Loss_G: 0.7915 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.7901 Loss_G: 0.8016 acc: 78.1%\n",
      "[BATCH 118/149] Loss_D: 0.7347 Loss_G: 0.7950 acc: 68.8%\n",
      "[BATCH 119/149] Loss_D: 0.7740 Loss_G: 0.7858 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7660 Loss_G: 0.7872 acc: 71.9%\n",
      "[BATCH 121/149] Loss_D: 0.7706 Loss_G: 0.7923 acc: 59.4%\n",
      "[BATCH 122/149] Loss_D: 0.7541 Loss_G: 0.7800 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.7689 Loss_G: 0.7879 acc: 70.3%\n",
      "[BATCH 124/149] Loss_D: 0.7879 Loss_G: 0.7963 acc: 59.4%\n",
      "[BATCH 125/149] Loss_D: 0.7998 Loss_G: 0.8089 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.7892 Loss_G: 0.8052 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7987 Loss_G: 0.8032 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7479 Loss_G: 0.7947 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.7549 Loss_G: 0.7759 acc: 56.2%\n",
      "[BATCH 130/149] Loss_D: 0.7810 Loss_G: 0.7820 acc: 71.9%\n",
      "[BATCH 131/149] Loss_D: 0.7846 Loss_G: 0.7879 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.7594 Loss_G: 0.7915 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7979 Loss_G: 0.7937 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7570 Loss_G: 0.7951 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.7644 Loss_G: 0.7807 acc: 70.3%\n",
      "[BATCH 136/149] Loss_D: 0.7456 Loss_G: 0.7812 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.7746 Loss_G: 0.7865 acc: 56.2%\n",
      "[BATCH 138/149] Loss_D: 0.7256 Loss_G: 0.7730 acc: 62.5%\n",
      "[BATCH 139/149] Loss_D: 0.7697 Loss_G: 0.7759 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7550 Loss_G: 0.8003 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.7410 Loss_G: 0.7859 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.7714 Loss_G: 0.7916 acc: 71.9%\n",
      "[BATCH 143/149] Loss_D: 0.7443 Loss_G: 0.7894 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7522 Loss_G: 0.7830 acc: 62.5%\n",
      "[BATCH 145/149] Loss_D: 0.7973 Loss_G: 0.7951 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7984 Loss_G: 0.8170 acc: 71.9%\n",
      "[BATCH 147/149] Loss_D: 0.8151 Loss_G: 0.8100 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7612 Loss_G: 0.7981 acc: 65.6%\n",
      "[BATCH 149/149] Loss_D: 0.7615 Loss_G: 0.7931 acc: 67.2%\n",
      "-----THE [5/50] epoch end-----\n",
      "-----THE [6/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8300 Loss_G: 0.8194 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.8215 Loss_G: 0.8385 acc: 78.1%\n",
      "[BATCH 3/149] Loss_D: 0.7734 Loss_G: 0.8174 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.7367 Loss_G: 0.7951 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.7378 Loss_G: 0.7848 acc: 71.9%\n",
      "[EPOCH 750] TEST ACC is : 73.8%\n",
      "[BATCH 6/149] Loss_D: 0.7753 Loss_G: 0.7878 acc: 76.6%\n",
      "[BATCH 7/149] Loss_D: 0.7391 Loss_G: 0.7813 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.7991 Loss_G: 0.7925 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.7917 Loss_G: 0.8072 acc: 60.9%\n",
      "[BATCH 10/149] Loss_D: 0.7849 Loss_G: 0.8134 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.7992 Loss_G: 0.8157 acc: 59.4%\n",
      "[BATCH 12/149] Loss_D: 0.7907 Loss_G: 0.8124 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.7835 Loss_G: 0.8023 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7941 Loss_G: 0.8100 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.7971 Loss_G: 0.8203 acc: 59.4%\n",
      "[BATCH 16/149] Loss_D: 0.7509 Loss_G: 0.8083 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.7532 Loss_G: 0.8039 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.7802 Loss_G: 0.7930 acc: 68.8%\n",
      "[BATCH 19/149] Loss_D: 0.7312 Loss_G: 0.7845 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.7688 Loss_G: 0.7907 acc: 59.4%\n",
      "[BATCH 21/149] Loss_D: 0.7780 Loss_G: 0.7917 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7753 Loss_G: 0.7903 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7658 Loss_G: 0.7930 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.8036 Loss_G: 0.7935 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.8117 Loss_G: 0.8104 acc: 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.7585 Loss_G: 0.7989 acc: 62.5%\n",
      "[BATCH 27/149] Loss_D: 0.8203 Loss_G: 0.8058 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.7610 Loss_G: 0.7958 acc: 65.6%\n",
      "[BATCH 29/149] Loss_D: 0.7781 Loss_G: 0.8011 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.7916 Loss_G: 0.8031 acc: 62.5%\n",
      "[BATCH 31/149] Loss_D: 0.7627 Loss_G: 0.7966 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.8415 Loss_G: 0.8167 acc: 62.5%\n",
      "[BATCH 33/149] Loss_D: 0.7547 Loss_G: 0.7970 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.7721 Loss_G: 0.7952 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7832 Loss_G: 0.7960 acc: 71.9%\n",
      "[BATCH 36/149] Loss_D: 0.7672 Loss_G: 0.7942 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.7359 Loss_G: 0.7805 acc: 70.3%\n",
      "[BATCH 38/149] Loss_D: 0.7690 Loss_G: 0.7753 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7388 Loss_G: 0.7726 acc: 64.1%\n",
      "[BATCH 40/149] Loss_D: 0.7781 Loss_G: 0.7758 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.7639 Loss_G: 0.7825 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7661 Loss_G: 0.7759 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.7589 Loss_G: 0.7771 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.7572 Loss_G: 0.7776 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.8174 Loss_G: 0.7916 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.8043 Loss_G: 0.8018 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.8179 Loss_G: 0.8105 acc: 67.2%\n",
      "[BATCH 48/149] Loss_D: 0.8219 Loss_G: 0.8367 acc: 73.4%\n",
      "[BATCH 49/149] Loss_D: 0.7658 Loss_G: 0.8194 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.7878 Loss_G: 0.8055 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.7735 Loss_G: 0.8153 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.7846 Loss_G: 0.8118 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.7630 Loss_G: 0.7944 acc: 73.4%\n",
      "[BATCH 54/149] Loss_D: 0.7986 Loss_G: 0.8016 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.7924 Loss_G: 0.8077 acc: 59.4%\n",
      "[EPOCH 800] TEST ACC is : 72.7%\n",
      "[BATCH 56/149] Loss_D: 0.7828 Loss_G: 0.8029 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7355 Loss_G: 0.7920 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.7855 Loss_G: 0.7865 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.7591 Loss_G: 0.7899 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7538 Loss_G: 0.7872 acc: 76.6%\n",
      "[BATCH 61/149] Loss_D: 0.7748 Loss_G: 0.7938 acc: 71.9%\n",
      "[BATCH 62/149] Loss_D: 0.7886 Loss_G: 0.8012 acc: 57.8%\n",
      "[BATCH 63/149] Loss_D: 0.7702 Loss_G: 0.7856 acc: 67.2%\n",
      "[BATCH 64/149] Loss_D: 0.7708 Loss_G: 0.7848 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.7689 Loss_G: 0.7802 acc: 62.5%\n",
      "[BATCH 66/149] Loss_D: 0.7867 Loss_G: 0.7782 acc: 76.6%\n",
      "[BATCH 67/149] Loss_D: 0.7669 Loss_G: 0.7786 acc: 60.9%\n",
      "[BATCH 68/149] Loss_D: 0.7604 Loss_G: 0.7853 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.7887 Loss_G: 0.7866 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.8055 Loss_G: 0.7945 acc: 70.3%\n",
      "[BATCH 71/149] Loss_D: 0.7823 Loss_G: 0.7934 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7804 Loss_G: 0.7853 acc: 64.1%\n",
      "[BATCH 73/149] Loss_D: 0.7872 Loss_G: 0.7861 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.7841 Loss_G: 0.8071 acc: 67.2%\n",
      "[BATCH 75/149] Loss_D: 0.7850 Loss_G: 0.8063 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.8391 Loss_G: 0.8257 acc: 56.2%\n",
      "[BATCH 77/149] Loss_D: 0.7932 Loss_G: 0.8214 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.7987 Loss_G: 0.8061 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.7742 Loss_G: 0.7941 acc: 56.2%\n",
      "[BATCH 80/149] Loss_D: 0.8154 Loss_G: 0.8020 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7854 Loss_G: 0.8011 acc: 62.5%\n",
      "[BATCH 82/149] Loss_D: 0.7866 Loss_G: 0.7910 acc: 46.9%\n",
      "[BATCH 83/149] Loss_D: 0.7927 Loss_G: 0.7938 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 0.7447 Loss_G: 0.7882 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.7723 Loss_G: 0.7825 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7471 Loss_G: 0.7808 acc: 62.5%\n",
      "[BATCH 87/149] Loss_D: 0.7982 Loss_G: 0.7841 acc: 57.8%\n",
      "[BATCH 88/149] Loss_D: 0.8150 Loss_G: 0.7969 acc: 79.7%\n",
      "[BATCH 89/149] Loss_D: 0.8145 Loss_G: 0.8238 acc: 59.4%\n",
      "[BATCH 90/149] Loss_D: 0.7787 Loss_G: 0.8120 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7309 Loss_G: 0.7939 acc: 62.5%\n",
      "[BATCH 92/149] Loss_D: 0.7809 Loss_G: 0.7906 acc: 62.5%\n",
      "[BATCH 93/149] Loss_D: 0.7663 Loss_G: 0.7801 acc: 68.8%\n",
      "[BATCH 94/149] Loss_D: 0.7916 Loss_G: 0.7862 acc: 75.0%\n",
      "[BATCH 95/149] Loss_D: 0.7741 Loss_G: 0.7907 acc: 71.9%\n",
      "[BATCH 96/149] Loss_D: 0.7501 Loss_G: 0.7861 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7642 Loss_G: 0.7902 acc: 59.4%\n",
      "[BATCH 98/149] Loss_D: 0.7372 Loss_G: 0.7901 acc: 71.9%\n",
      "[BATCH 99/149] Loss_D: 0.7773 Loss_G: 0.7951 acc: 59.4%\n",
      "[BATCH 100/149] Loss_D: 0.7712 Loss_G: 0.7997 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.7873 Loss_G: 0.8434 acc: 73.4%\n",
      "[BATCH 102/149] Loss_D: 0.7917 Loss_G: 0.8251 acc: 59.4%\n",
      "[BATCH 103/149] Loss_D: 0.7804 Loss_G: 0.7997 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.7594 Loss_G: 0.7929 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.7679 Loss_G: 0.7942 acc: 68.8%\n",
      "[EPOCH 850] TEST ACC is : 72.3%\n",
      "[BATCH 106/149] Loss_D: 0.8035 Loss_G: 0.7984 acc: 68.8%\n",
      "[BATCH 107/149] Loss_D: 0.7430 Loss_G: 0.8013 acc: 76.6%\n",
      "[BATCH 108/149] Loss_D: 0.8086 Loss_G: 0.7936 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.7692 Loss_G: 0.7956 acc: 60.9%\n",
      "[BATCH 110/149] Loss_D: 0.7549 Loss_G: 0.7979 acc: 57.8%\n",
      "[BATCH 111/149] Loss_D: 0.7617 Loss_G: 0.7859 acc: 79.7%\n",
      "[BATCH 112/149] Loss_D: 0.7961 Loss_G: 0.7930 acc: 73.4%\n",
      "[BATCH 113/149] Loss_D: 0.7539 Loss_G: 0.7934 acc: 53.1%\n",
      "[BATCH 114/149] Loss_D: 0.7534 Loss_G: 0.7906 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.7643 Loss_G: 0.8002 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.8136 Loss_G: 0.8124 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7842 Loss_G: 0.8078 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.7632 Loss_G: 0.7927 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.7754 Loss_G: 0.7800 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.8166 Loss_G: 0.8137 acc: 73.4%\n",
      "[BATCH 121/149] Loss_D: 0.7753 Loss_G: 0.8000 acc: 57.8%\n",
      "[BATCH 122/149] Loss_D: 0.7365 Loss_G: 0.7848 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.7792 Loss_G: 0.7815 acc: 68.8%\n",
      "[BATCH 124/149] Loss_D: 0.7596 Loss_G: 0.7859 acc: 57.8%\n",
      "[BATCH 125/149] Loss_D: 0.7435 Loss_G: 0.7831 acc: 65.6%\n",
      "[BATCH 126/149] Loss_D: 0.7787 Loss_G: 0.7795 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.7757 Loss_G: 0.7842 acc: 57.8%\n",
      "[BATCH 128/149] Loss_D: 0.7592 Loss_G: 0.7814 acc: 60.9%\n",
      "[BATCH 129/149] Loss_D: 0.7677 Loss_G: 0.7808 acc: 57.8%\n",
      "[BATCH 130/149] Loss_D: 0.7710 Loss_G: 0.7832 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.7721 Loss_G: 0.7912 acc: 59.4%\n",
      "[BATCH 132/149] Loss_D: 0.8064 Loss_G: 0.7960 acc: 59.4%\n",
      "[BATCH 133/149] Loss_D: 0.7637 Loss_G: 0.7948 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.7808 Loss_G: 0.8015 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.7785 Loss_G: 0.7990 acc: 64.1%\n",
      "[BATCH 136/149] Loss_D: 0.7911 Loss_G: 0.8031 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.7871 Loss_G: 0.8109 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.7624 Loss_G: 0.8029 acc: 67.2%\n",
      "[BATCH 139/149] Loss_D: 0.8056 Loss_G: 0.8132 acc: 70.3%\n",
      "[BATCH 140/149] Loss_D: 0.7617 Loss_G: 0.8024 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7654 Loss_G: 0.7989 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.7990 Loss_G: 0.8008 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.7599 Loss_G: 0.7944 acc: 60.9%\n",
      "[BATCH 144/149] Loss_D: 0.8135 Loss_G: 0.7997 acc: 59.4%\n",
      "[BATCH 145/149] Loss_D: 0.7524 Loss_G: 0.7913 acc: 78.1%\n",
      "[BATCH 146/149] Loss_D: 0.7853 Loss_G: 0.7836 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7979 Loss_G: 0.7935 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.7832 Loss_G: 0.7971 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7437 Loss_G: 0.7808 acc: 57.8%\n",
      "-----THE [6/50] epoch end-----\n",
      "-----THE [7/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7391 Loss_G: 0.7785 acc: 71.9%\n",
      "[BATCH 2/149] Loss_D: 0.7454 Loss_G: 0.7719 acc: 64.1%\n",
      "[BATCH 3/149] Loss_D: 0.8016 Loss_G: 0.7794 acc: 73.4%\n",
      "[BATCH 4/149] Loss_D: 0.7589 Loss_G: 0.7858 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.7715 Loss_G: 0.7788 acc: 71.9%\n",
      "[BATCH 6/149] Loss_D: 0.7712 Loss_G: 0.7845 acc: 60.9%\n",
      "[EPOCH 900] TEST ACC is : 73.2%\n",
      "[BATCH 7/149] Loss_D: 0.7875 Loss_G: 0.7933 acc: 56.2%\n",
      "[BATCH 8/149] Loss_D: 0.7877 Loss_G: 0.8024 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.8047 Loss_G: 0.8099 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7869 Loss_G: 0.8263 acc: 70.3%\n",
      "[BATCH 11/149] Loss_D: 0.7928 Loss_G: 0.8043 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.8001 Loss_G: 0.8014 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.7653 Loss_G: 0.7888 acc: 59.4%\n",
      "[BATCH 14/149] Loss_D: 0.7603 Loss_G: 0.7845 acc: 70.3%\n",
      "[BATCH 15/149] Loss_D: 0.7929 Loss_G: 0.8017 acc: 56.2%\n",
      "[BATCH 16/149] Loss_D: 0.7953 Loss_G: 0.8018 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7698 Loss_G: 0.7968 acc: 70.3%\n",
      "[BATCH 18/149] Loss_D: 0.7645 Loss_G: 0.7929 acc: 56.2%\n",
      "[BATCH 19/149] Loss_D: 0.7556 Loss_G: 0.7887 acc: 57.8%\n",
      "[BATCH 20/149] Loss_D: 0.8042 Loss_G: 0.8004 acc: 75.0%\n",
      "[BATCH 21/149] Loss_D: 0.7438 Loss_G: 0.7960 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.7503 Loss_G: 0.7896 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.8428 Loss_G: 0.8040 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7856 Loss_G: 0.8227 acc: 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.8070 Loss_G: 0.8124 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7573 Loss_G: 0.7849 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.7559 Loss_G: 0.7910 acc: 65.6%\n",
      "[BATCH 28/149] Loss_D: 0.7976 Loss_G: 0.7976 acc: 78.1%\n",
      "[BATCH 29/149] Loss_D: 0.7804 Loss_G: 0.8025 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.7550 Loss_G: 0.7994 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.8003 Loss_G: 0.8029 acc: 57.8%\n",
      "[BATCH 32/149] Loss_D: 0.7461 Loss_G: 0.7929 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.8050 Loss_G: 0.7896 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.8036 Loss_G: 0.7981 acc: 62.5%\n",
      "[BATCH 35/149] Loss_D: 0.7964 Loss_G: 0.8040 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.7966 Loss_G: 0.7956 acc: 67.2%\n",
      "[BATCH 37/149] Loss_D: 0.8039 Loss_G: 0.7939 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.7691 Loss_G: 0.7951 acc: 73.4%\n",
      "[BATCH 39/149] Loss_D: 0.8234 Loss_G: 0.8150 acc: 65.6%\n",
      "[BATCH 40/149] Loss_D: 0.7773 Loss_G: 0.8131 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.7562 Loss_G: 0.8016 acc: 56.2%\n",
      "[BATCH 42/149] Loss_D: 0.7243 Loss_G: 0.7801 acc: 65.6%\n",
      "[BATCH 43/149] Loss_D: 0.7883 Loss_G: 0.7861 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.7964 Loss_G: 0.7966 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7809 Loss_G: 0.8057 acc: 68.8%\n",
      "[BATCH 46/149] Loss_D: 0.7661 Loss_G: 0.7902 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.7605 Loss_G: 0.7802 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7447 Loss_G: 0.7813 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.7941 Loss_G: 0.7863 acc: 54.7%\n",
      "[BATCH 50/149] Loss_D: 0.7899 Loss_G: 0.8106 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.8186 Loss_G: 0.8181 acc: 54.7%\n",
      "[BATCH 52/149] Loss_D: 0.7920 Loss_G: 0.8234 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.7961 Loss_G: 0.8174 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.7687 Loss_G: 0.8139 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.7960 Loss_G: 0.8176 acc: 68.8%\n",
      "[BATCH 56/149] Loss_D: 0.7941 Loss_G: 0.8072 acc: 75.0%\n",
      "[EPOCH 950] TEST ACC is : 70.5%\n",
      "[BATCH 57/149] Loss_D: 0.7345 Loss_G: 0.7997 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.7854 Loss_G: 0.8023 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.7250 Loss_G: 0.7843 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7743 Loss_G: 0.7842 acc: 71.9%\n",
      "[BATCH 61/149] Loss_D: 0.7829 Loss_G: 0.7986 acc: 67.2%\n",
      "[BATCH 62/149] Loss_D: 0.7845 Loss_G: 0.7983 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7672 Loss_G: 0.7927 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.7785 Loss_G: 0.7925 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7744 Loss_G: 0.7997 acc: 73.4%\n",
      "[BATCH 66/149] Loss_D: 0.7894 Loss_G: 0.7962 acc: 57.8%\n",
      "[BATCH 67/149] Loss_D: 0.7567 Loss_G: 0.7874 acc: 67.2%\n",
      "[BATCH 68/149] Loss_D: 0.8166 Loss_G: 0.8028 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.7553 Loss_G: 0.8021 acc: 57.8%\n",
      "[BATCH 70/149] Loss_D: 0.7605 Loss_G: 0.7860 acc: 68.8%\n",
      "[BATCH 71/149] Loss_D: 0.7920 Loss_G: 0.7994 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7499 Loss_G: 0.7866 acc: 75.0%\n",
      "[BATCH 73/149] Loss_D: 0.8259 Loss_G: 0.8049 acc: 57.8%\n",
      "[BATCH 74/149] Loss_D: 0.7599 Loss_G: 0.8044 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.7557 Loss_G: 0.7887 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7582 Loss_G: 0.7820 acc: 62.5%\n",
      "[BATCH 77/149] Loss_D: 0.7356 Loss_G: 0.7756 acc: 60.9%\n",
      "[BATCH 78/149] Loss_D: 0.7788 Loss_G: 0.7820 acc: 59.4%\n",
      "[BATCH 79/149] Loss_D: 0.7724 Loss_G: 0.7945 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.8316 Loss_G: 0.8028 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7711 Loss_G: 0.7969 acc: 65.6%\n",
      "[BATCH 82/149] Loss_D: 0.7526 Loss_G: 0.7920 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7555 Loss_G: 0.7906 acc: 57.8%\n",
      "[BATCH 84/149] Loss_D: 0.7301 Loss_G: 0.7819 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.7806 Loss_G: 0.7899 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.7512 Loss_G: 0.7875 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.7650 Loss_G: 0.7903 acc: 65.6%\n",
      "[BATCH 88/149] Loss_D: 0.7489 Loss_G: 0.7835 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7744 Loss_G: 0.7913 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.8109 Loss_G: 0.8066 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.7794 Loss_G: 0.8171 acc: 71.9%\n",
      "[BATCH 92/149] Loss_D: 0.7471 Loss_G: 0.7965 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.7727 Loss_G: 0.7858 acc: 65.6%\n",
      "[BATCH 94/149] Loss_D: 0.8111 Loss_G: 0.7927 acc: 60.9%\n",
      "[BATCH 95/149] Loss_D: 0.7998 Loss_G: 0.8132 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.8061 Loss_G: 0.8170 acc: 56.2%\n",
      "[BATCH 97/149] Loss_D: 0.7657 Loss_G: 0.7954 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7559 Loss_G: 0.7862 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.8022 Loss_G: 0.7875 acc: 73.4%\n",
      "[BATCH 100/149] Loss_D: 0.7793 Loss_G: 0.7977 acc: 71.9%\n",
      "[BATCH 101/149] Loss_D: 0.7859 Loss_G: 0.7936 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.7738 Loss_G: 0.8009 acc: 60.9%\n",
      "[BATCH 103/149] Loss_D: 0.7740 Loss_G: 0.7981 acc: 60.9%\n",
      "[BATCH 104/149] Loss_D: 0.7726 Loss_G: 0.7927 acc: 68.8%\n",
      "[BATCH 105/149] Loss_D: 0.7397 Loss_G: 0.7835 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7958 Loss_G: 0.7896 acc: 75.0%\n",
      "[EPOCH 1000] TEST ACC is : 72.3%\n",
      "[BATCH 107/149] Loss_D: 0.7436 Loss_G: 0.7940 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.7355 Loss_G: 0.7774 acc: 73.4%\n",
      "[BATCH 109/149] Loss_D: 0.7620 Loss_G: 0.7786 acc: 68.8%\n",
      "[BATCH 110/149] Loss_D: 0.7932 Loss_G: 0.7938 acc: 64.1%\n",
      "[BATCH 111/149] Loss_D: 0.7599 Loss_G: 0.7950 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.7235 Loss_G: 0.7839 acc: 62.5%\n",
      "[BATCH 113/149] Loss_D: 0.7777 Loss_G: 0.7901 acc: 60.9%\n",
      "[BATCH 114/149] Loss_D: 0.8131 Loss_G: 0.8017 acc: 71.9%\n",
      "[BATCH 115/149] Loss_D: 0.7931 Loss_G: 0.8039 acc: 73.4%\n",
      "[BATCH 116/149] Loss_D: 0.8154 Loss_G: 0.8167 acc: 56.2%\n",
      "[BATCH 117/149] Loss_D: 0.7882 Loss_G: 0.7996 acc: 65.6%\n",
      "[BATCH 118/149] Loss_D: 0.7479 Loss_G: 0.7939 acc: 53.1%\n",
      "[BATCH 119/149] Loss_D: 0.7890 Loss_G: 0.7966 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7900 Loss_G: 0.8045 acc: 75.0%\n",
      "[BATCH 121/149] Loss_D: 0.7902 Loss_G: 0.8080 acc: 65.6%\n",
      "[BATCH 122/149] Loss_D: 0.7488 Loss_G: 0.7973 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.8023 Loss_G: 0.7999 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.7871 Loss_G: 0.8026 acc: 56.2%\n",
      "[BATCH 125/149] Loss_D: 0.7919 Loss_G: 0.8058 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7478 Loss_G: 0.7963 acc: 54.7%\n",
      "[BATCH 127/149] Loss_D: 0.7537 Loss_G: 0.7854 acc: 64.1%\n",
      "[BATCH 128/149] Loss_D: 0.8515 Loss_G: 0.8103 acc: 65.6%\n",
      "[BATCH 129/149] Loss_D: 0.7626 Loss_G: 0.7966 acc: 67.2%\n",
      "[BATCH 130/149] Loss_D: 0.7567 Loss_G: 0.7874 acc: 68.8%\n",
      "[BATCH 131/149] Loss_D: 0.7481 Loss_G: 0.7813 acc: 56.2%\n",
      "[BATCH 132/149] Loss_D: 0.7586 Loss_G: 0.7832 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.7809 Loss_G: 0.7864 acc: 54.7%\n",
      "[BATCH 134/149] Loss_D: 0.7538 Loss_G: 0.7866 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.7604 Loss_G: 0.7784 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.7627 Loss_G: 0.7847 acc: 57.8%\n",
      "[BATCH 137/149] Loss_D: 0.7548 Loss_G: 0.7955 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7582 Loss_G: 0.7849 acc: 62.5%\n",
      "[BATCH 139/149] Loss_D: 0.7673 Loss_G: 0.7910 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7748 Loss_G: 0.7866 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.7901 Loss_G: 0.7843 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.7593 Loss_G: 0.7890 acc: 65.6%\n",
      "[BATCH 143/149] Loss_D: 0.7807 Loss_G: 0.7882 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.8553 Loss_G: 0.8223 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.8246 Loss_G: 0.8350 acc: 71.9%\n",
      "[BATCH 146/149] Loss_D: 0.7891 Loss_G: 0.8173 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.8198 Loss_G: 0.8181 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.8133 Loss_G: 0.8282 acc: 75.0%\n",
      "[BATCH 149/149] Loss_D: 0.7622 Loss_G: 0.8231 acc: 67.2%\n",
      "-----THE [7/50] epoch end-----\n",
      "-----THE [8/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8111 Loss_G: 0.8242 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.7804 Loss_G: 0.8225 acc: 79.7%\n",
      "[BATCH 3/149] Loss_D: 0.8133 Loss_G: 0.8254 acc: 65.6%\n",
      "[BATCH 4/149] Loss_D: 0.7459 Loss_G: 0.8061 acc: 56.2%\n",
      "[BATCH 5/149] Loss_D: 0.7896 Loss_G: 0.7997 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.7867 Loss_G: 0.7990 acc: 60.9%\n",
      "[BATCH 7/149] Loss_D: 0.7884 Loss_G: 0.8162 acc: 56.2%\n",
      "[EPOCH 1050] TEST ACC is : 73.6%\n",
      "[BATCH 8/149] Loss_D: 0.7782 Loss_G: 0.8084 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.7719 Loss_G: 0.7999 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7662 Loss_G: 0.7941 acc: 65.6%\n",
      "[BATCH 11/149] Loss_D: 0.7646 Loss_G: 0.7950 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.7602 Loss_G: 0.7789 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.8294 Loss_G: 0.7868 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7745 Loss_G: 0.7928 acc: 75.0%\n",
      "[BATCH 15/149] Loss_D: 0.7927 Loss_G: 0.7915 acc: 67.2%\n",
      "[BATCH 16/149] Loss_D: 0.7678 Loss_G: 0.7881 acc: 70.3%\n",
      "[BATCH 17/149] Loss_D: 0.7568 Loss_G: 0.7867 acc: 45.3%\n",
      "[BATCH 18/149] Loss_D: 0.7950 Loss_G: 0.7931 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.7785 Loss_G: 0.7955 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7523 Loss_G: 0.7899 acc: 57.8%\n",
      "[BATCH 21/149] Loss_D: 0.7727 Loss_G: 0.7921 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.7771 Loss_G: 0.7956 acc: 48.4%\n",
      "[BATCH 23/149] Loss_D: 0.7639 Loss_G: 0.8052 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.7875 Loss_G: 0.8023 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.7391 Loss_G: 0.7971 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7505 Loss_G: 0.7898 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.7871 Loss_G: 0.7946 acc: 60.9%\n",
      "[BATCH 28/149] Loss_D: 0.7772 Loss_G: 0.7928 acc: 67.2%\n",
      "[BATCH 29/149] Loss_D: 0.8091 Loss_G: 0.7981 acc: 71.9%\n",
      "[BATCH 30/149] Loss_D: 0.7920 Loss_G: 0.8074 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.8127 Loss_G: 0.8201 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.8188 Loss_G: 0.8120 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.7579 Loss_G: 0.7981 acc: 51.6%\n",
      "[BATCH 34/149] Loss_D: 0.7314 Loss_G: 0.7838 acc: 70.3%\n",
      "[BATCH 35/149] Loss_D: 0.7566 Loss_G: 0.7816 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.7773 Loss_G: 0.7841 acc: 68.8%\n",
      "[BATCH 37/149] Loss_D: 0.7510 Loss_G: 0.7833 acc: 68.8%\n",
      "[BATCH 38/149] Loss_D: 0.7721 Loss_G: 0.7793 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.7341 Loss_G: 0.7725 acc: 70.3%\n",
      "[BATCH 40/149] Loss_D: 0.8226 Loss_G: 0.7950 acc: 57.8%\n",
      "[BATCH 41/149] Loss_D: 0.7333 Loss_G: 0.7861 acc: 62.5%\n",
      "[BATCH 42/149] Loss_D: 0.7654 Loss_G: 0.7918 acc: 60.9%\n",
      "[BATCH 43/149] Loss_D: 0.7855 Loss_G: 0.7854 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7980 Loss_G: 0.8027 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.8325 Loss_G: 0.8289 acc: 68.8%\n",
      "[BATCH 46/149] Loss_D: 0.7760 Loss_G: 0.8142 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.7667 Loss_G: 0.8047 acc: 75.0%\n",
      "[BATCH 48/149] Loss_D: 0.7582 Loss_G: 0.7937 acc: 68.8%\n",
      "[BATCH 49/149] Loss_D: 0.7493 Loss_G: 0.7986 acc: 65.6%\n",
      "[BATCH 50/149] Loss_D: 0.7698 Loss_G: 0.8002 acc: 71.9%\n",
      "[BATCH 51/149] Loss_D: 0.7620 Loss_G: 0.8054 acc: 60.9%\n",
      "[BATCH 52/149] Loss_D: 0.7934 Loss_G: 0.8006 acc: 50.0%\n",
      "[BATCH 53/149] Loss_D: 0.7605 Loss_G: 0.7990 acc: 53.1%\n",
      "[BATCH 54/149] Loss_D: 0.7608 Loss_G: 0.7872 acc: 60.9%\n",
      "[BATCH 55/149] Loss_D: 0.8268 Loss_G: 0.8057 acc: 73.4%\n",
      "[BATCH 56/149] Loss_D: 0.7589 Loss_G: 0.7904 acc: 62.5%\n",
      "[BATCH 57/149] Loss_D: 0.7269 Loss_G: 0.7792 acc: 64.1%\n",
      "[EPOCH 1100] TEST ACC is : 69.9%\n",
      "[BATCH 58/149] Loss_D: 0.7393 Loss_G: 0.7760 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.8129 Loss_G: 0.7843 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7327 Loss_G: 0.7863 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.7550 Loss_G: 0.7797 acc: 73.4%\n",
      "[BATCH 62/149] Loss_D: 0.7571 Loss_G: 0.7840 acc: 51.6%\n",
      "[BATCH 63/149] Loss_D: 0.7972 Loss_G: 0.7967 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.7712 Loss_G: 0.8061 acc: 62.5%\n",
      "[BATCH 65/149] Loss_D: 0.7986 Loss_G: 0.8029 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.7518 Loss_G: 0.8120 acc: 71.9%\n",
      "[BATCH 67/149] Loss_D: 0.7496 Loss_G: 0.7921 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7650 Loss_G: 0.7926 acc: 68.8%\n",
      "[BATCH 69/149] Loss_D: 0.7822 Loss_G: 0.7944 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.7733 Loss_G: 0.7937 acc: 67.2%\n",
      "[BATCH 71/149] Loss_D: 0.7788 Loss_G: 0.7934 acc: 62.5%\n",
      "[BATCH 72/149] Loss_D: 0.7866 Loss_G: 0.7945 acc: 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.7793 Loss_G: 0.7962 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.7587 Loss_G: 0.7979 acc: 65.6%\n",
      "[BATCH 75/149] Loss_D: 0.8050 Loss_G: 0.7981 acc: 56.2%\n",
      "[BATCH 76/149] Loss_D: 0.7390 Loss_G: 0.7860 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7698 Loss_G: 0.7883 acc: 65.6%\n",
      "[BATCH 78/149] Loss_D: 0.7805 Loss_G: 0.7857 acc: 57.8%\n",
      "[BATCH 79/149] Loss_D: 0.7634 Loss_G: 0.7857 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.7716 Loss_G: 0.7910 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.7876 Loss_G: 0.8050 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.8461 Loss_G: 0.8435 acc: 64.1%\n",
      "[BATCH 83/149] Loss_D: 0.7778 Loss_G: 0.8341 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.8112 Loss_G: 0.8161 acc: 65.6%\n",
      "[BATCH 85/149] Loss_D: 0.7582 Loss_G: 0.8087 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.7757 Loss_G: 0.8054 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.7649 Loss_G: 0.8109 acc: 53.1%\n",
      "[BATCH 88/149] Loss_D: 0.7829 Loss_G: 0.7985 acc: 71.9%\n",
      "[BATCH 89/149] Loss_D: 0.7781 Loss_G: 0.7938 acc: 65.6%\n",
      "[BATCH 90/149] Loss_D: 0.7624 Loss_G: 0.7953 acc: 57.8%\n",
      "[BATCH 91/149] Loss_D: 0.7814 Loss_G: 0.7935 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.8227 Loss_G: 0.8070 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.8113 Loss_G: 0.8194 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.7456 Loss_G: 0.8009 acc: 57.8%\n",
      "[BATCH 95/149] Loss_D: 0.7802 Loss_G: 0.7998 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.7556 Loss_G: 0.7984 acc: 59.4%\n",
      "[BATCH 97/149] Loss_D: 0.7716 Loss_G: 0.7938 acc: 57.8%\n",
      "[BATCH 98/149] Loss_D: 0.7639 Loss_G: 0.7932 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7507 Loss_G: 0.7848 acc: 62.5%\n",
      "[BATCH 100/149] Loss_D: 0.7904 Loss_G: 0.7859 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7933 Loss_G: 0.7855 acc: 60.9%\n",
      "[BATCH 102/149] Loss_D: 0.7591 Loss_G: 0.7881 acc: 53.1%\n",
      "[BATCH 103/149] Loss_D: 0.8234 Loss_G: 0.8056 acc: 65.6%\n",
      "[BATCH 104/149] Loss_D: 0.7529 Loss_G: 0.7899 acc: 62.5%\n",
      "[BATCH 105/149] Loss_D: 0.7674 Loss_G: 0.7859 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.7546 Loss_G: 0.7829 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.7732 Loss_G: 0.7895 acc: 57.8%\n",
      "[EPOCH 1150] TEST ACC is : 70.7%\n",
      "[BATCH 108/149] Loss_D: 0.7500 Loss_G: 0.7895 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.7707 Loss_G: 0.7878 acc: 60.9%\n",
      "[BATCH 110/149] Loss_D: 0.7987 Loss_G: 0.7927 acc: 56.2%\n",
      "[BATCH 111/149] Loss_D: 0.7464 Loss_G: 0.7944 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.8022 Loss_G: 0.8122 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.7815 Loss_G: 0.8092 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7595 Loss_G: 0.8034 acc: 59.4%\n",
      "[BATCH 115/149] Loss_D: 0.8036 Loss_G: 0.8218 acc: 73.4%\n",
      "[BATCH 116/149] Loss_D: 0.7538 Loss_G: 0.8231 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7865 Loss_G: 0.8218 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.7838 Loss_G: 0.7968 acc: 53.1%\n",
      "[BATCH 119/149] Loss_D: 0.7819 Loss_G: 0.7922 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.7946 Loss_G: 0.7928 acc: 65.6%\n",
      "[BATCH 121/149] Loss_D: 0.7675 Loss_G: 0.7844 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.7673 Loss_G: 0.7769 acc: 65.6%\n",
      "[BATCH 123/149] Loss_D: 0.7734 Loss_G: 0.7867 acc: 65.6%\n",
      "[BATCH 124/149] Loss_D: 0.7331 Loss_G: 0.7832 acc: 57.8%\n",
      "[BATCH 125/149] Loss_D: 0.7815 Loss_G: 0.7878 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.8393 Loss_G: 0.8027 acc: 50.0%\n",
      "[BATCH 127/149] Loss_D: 0.7957 Loss_G: 0.8134 acc: 60.9%\n",
      "[BATCH 128/149] Loss_D: 0.7971 Loss_G: 0.8038 acc: 68.8%\n",
      "[BATCH 129/149] Loss_D: 0.8143 Loss_G: 0.8098 acc: 73.4%\n",
      "[BATCH 130/149] Loss_D: 0.8003 Loss_G: 0.8123 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.7720 Loss_G: 0.8095 acc: 65.6%\n",
      "[BATCH 132/149] Loss_D: 0.7913 Loss_G: 0.8066 acc: 53.1%\n",
      "[BATCH 133/149] Loss_D: 0.7773 Loss_G: 0.8053 acc: 57.8%\n",
      "[BATCH 134/149] Loss_D: 0.7598 Loss_G: 0.7968 acc: 59.4%\n",
      "[BATCH 135/149] Loss_D: 0.7548 Loss_G: 0.7929 acc: 78.1%\n",
      "[BATCH 136/149] Loss_D: 0.7582 Loss_G: 0.7965 acc: 57.8%\n",
      "[BATCH 137/149] Loss_D: 0.7863 Loss_G: 0.8021 acc: 53.1%\n",
      "[BATCH 138/149] Loss_D: 0.8289 Loss_G: 0.8103 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7671 Loss_G: 0.8097 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.7685 Loss_G: 0.7932 acc: 57.8%\n",
      "[BATCH 141/149] Loss_D: 0.7665 Loss_G: 0.7904 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.8039 Loss_G: 0.8005 acc: 65.6%\n",
      "[BATCH 143/149] Loss_D: 0.7955 Loss_G: 0.8150 acc: 59.4%\n",
      "[BATCH 144/149] Loss_D: 0.7541 Loss_G: 0.8076 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.7911 Loss_G: 0.8040 acc: 70.3%\n",
      "[BATCH 146/149] Loss_D: 0.7568 Loss_G: 0.8004 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7917 Loss_G: 0.8139 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.8116 Loss_G: 0.8150 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7539 Loss_G: 0.8012 acc: 56.2%\n",
      "-----THE [8/50] epoch end-----\n",
      "-----THE [9/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7697 Loss_G: 0.7885 acc: 56.2%\n",
      "[BATCH 2/149] Loss_D: 0.7457 Loss_G: 0.7810 acc: 73.4%\n",
      "[BATCH 3/149] Loss_D: 0.7419 Loss_G: 0.7808 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.8363 Loss_G: 0.8050 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.7705 Loss_G: 0.7884 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7544 Loss_G: 0.7821 acc: 56.2%\n",
      "[BATCH 7/149] Loss_D: 0.7898 Loss_G: 0.7948 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.8074 Loss_G: 0.8183 acc: 68.8%\n",
      "[EPOCH 1200] TEST ACC is : 70.7%\n",
      "[BATCH 9/149] Loss_D: 0.7924 Loss_G: 0.8242 acc: 57.8%\n",
      "[BATCH 10/149] Loss_D: 0.7794 Loss_G: 0.8157 acc: 57.8%\n",
      "[BATCH 11/149] Loss_D: 0.8140 Loss_G: 0.8105 acc: 53.1%\n",
      "[BATCH 12/149] Loss_D: 0.7676 Loss_G: 0.8069 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.8417 Loss_G: 0.8179 acc: 56.2%\n",
      "[BATCH 14/149] Loss_D: 0.8064 Loss_G: 0.8114 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.7352 Loss_G: 0.7948 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7572 Loss_G: 0.7872 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.7776 Loss_G: 0.7907 acc: 73.4%\n",
      "[BATCH 18/149] Loss_D: 0.7718 Loss_G: 0.7907 acc: 65.6%\n",
      "[BATCH 19/149] Loss_D: 0.7675 Loss_G: 0.7790 acc: 60.9%\n",
      "[BATCH 20/149] Loss_D: 0.7865 Loss_G: 0.7911 acc: 57.8%\n",
      "[BATCH 21/149] Loss_D: 0.7690 Loss_G: 0.7881 acc: 73.4%\n",
      "[BATCH 22/149] Loss_D: 0.8317 Loss_G: 0.8082 acc: 62.5%\n",
      "[BATCH 23/149] Loss_D: 0.7808 Loss_G: 0.8026 acc: 57.8%\n",
      "[BATCH 24/149] Loss_D: 0.7475 Loss_G: 0.7959 acc: 53.1%\n",
      "[BATCH 25/149] Loss_D: 0.7897 Loss_G: 0.7903 acc: 62.5%\n",
      "[BATCH 26/149] Loss_D: 0.7490 Loss_G: 0.7827 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.7547 Loss_G: 0.7842 acc: 64.1%\n",
      "[BATCH 28/149] Loss_D: 0.7676 Loss_G: 0.7815 acc: 57.8%\n",
      "[BATCH 29/149] Loss_D: 0.7673 Loss_G: 0.7897 acc: 65.6%\n",
      "[BATCH 30/149] Loss_D: 0.7607 Loss_G: 0.7911 acc: 65.6%\n",
      "[BATCH 31/149] Loss_D: 0.7807 Loss_G: 0.8068 acc: 65.6%\n",
      "[BATCH 32/149] Loss_D: 0.7825 Loss_G: 0.8054 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.7686 Loss_G: 0.8023 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.7352 Loss_G: 0.7870 acc: 60.9%\n",
      "[BATCH 35/149] Loss_D: 0.7662 Loss_G: 0.7897 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.7701 Loss_G: 0.8003 acc: 62.5%\n",
      "[BATCH 37/149] Loss_D: 0.7622 Loss_G: 0.7825 acc: 70.3%\n",
      "[BATCH 38/149] Loss_D: 0.7762 Loss_G: 0.7847 acc: 73.4%\n",
      "[BATCH 39/149] Loss_D: 0.7342 Loss_G: 0.7758 acc: 60.9%\n",
      "[BATCH 40/149] Loss_D: 0.7243 Loss_G: 0.7737 acc: 53.1%\n",
      "[BATCH 41/149] Loss_D: 0.7739 Loss_G: 0.7740 acc: 54.7%\n",
      "[BATCH 42/149] Loss_D: 0.7957 Loss_G: 0.7968 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7729 Loss_G: 0.7891 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.7398 Loss_G: 0.7763 acc: 59.4%\n",
      "[BATCH 45/149] Loss_D: 0.7955 Loss_G: 0.7932 acc: 62.5%\n",
      "[BATCH 46/149] Loss_D: 0.7611 Loss_G: 0.7875 acc: 75.0%\n",
      "[BATCH 47/149] Loss_D: 0.7471 Loss_G: 0.7901 acc: 67.2%\n",
      "[BATCH 48/149] Loss_D: 0.7805 Loss_G: 0.7885 acc: 75.0%\n",
      "[BATCH 49/149] Loss_D: 0.7795 Loss_G: 0.7971 acc: 60.9%\n",
      "[BATCH 50/149] Loss_D: 0.7789 Loss_G: 0.8115 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7837 Loss_G: 0.8152 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7637 Loss_G: 0.8126 acc: 67.2%\n",
      "[BATCH 53/149] Loss_D: 0.7853 Loss_G: 0.8063 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.7844 Loss_G: 0.8170 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.7774 Loss_G: 0.8033 acc: 60.9%\n",
      "[BATCH 56/149] Loss_D: 0.7346 Loss_G: 0.7832 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.7548 Loss_G: 0.7816 acc: 54.7%\n",
      "[BATCH 58/149] Loss_D: 0.7468 Loss_G: 0.7738 acc: 73.4%\n",
      "[EPOCH 1250] TEST ACC is : 69.7%\n",
      "[BATCH 59/149] Loss_D: 0.7856 Loss_G: 0.7929 acc: 79.7%\n",
      "[BATCH 60/149] Loss_D: 0.7259 Loss_G: 0.7907 acc: 64.1%\n",
      "[BATCH 61/149] Loss_D: 0.8018 Loss_G: 0.7941 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.8122 Loss_G: 0.8046 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.8163 Loss_G: 0.8202 acc: 62.5%\n",
      "[BATCH 64/149] Loss_D: 0.7678 Loss_G: 0.8108 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.8225 Loss_G: 0.8273 acc: 64.1%\n",
      "[BATCH 66/149] Loss_D: 0.7879 Loss_G: 0.8181 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7601 Loss_G: 0.8214 acc: 51.6%\n",
      "[BATCH 68/149] Loss_D: 0.7559 Loss_G: 0.7977 acc: 60.9%\n",
      "[BATCH 69/149] Loss_D: 0.7708 Loss_G: 0.7929 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.7457 Loss_G: 0.7844 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.7780 Loss_G: 0.7894 acc: 70.3%\n",
      "[BATCH 72/149] Loss_D: 0.7714 Loss_G: 0.7920 acc: 75.0%\n",
      "[BATCH 73/149] Loss_D: 0.7509 Loss_G: 0.7906 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7462 Loss_G: 0.7813 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.7518 Loss_G: 0.7820 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.7668 Loss_G: 0.7800 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7875 Loss_G: 0.7837 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.7935 Loss_G: 0.7906 acc: 62.5%\n",
      "[BATCH 79/149] Loss_D: 0.7738 Loss_G: 0.7922 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7854 Loss_G: 0.7916 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.7671 Loss_G: 0.7937 acc: 62.5%\n",
      "[BATCH 82/149] Loss_D: 0.7786 Loss_G: 0.7992 acc: 65.6%\n",
      "[BATCH 83/149] Loss_D: 0.7855 Loss_G: 0.7984 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.7635 Loss_G: 0.7991 acc: 70.3%\n",
      "[BATCH 85/149] Loss_D: 0.8147 Loss_G: 0.8121 acc: 65.6%\n",
      "[BATCH 86/149] Loss_D: 0.7790 Loss_G: 0.8044 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.7395 Loss_G: 0.7855 acc: 62.5%\n",
      "[BATCH 88/149] Loss_D: 0.7596 Loss_G: 0.7836 acc: 57.8%\n",
      "[BATCH 89/149] Loss_D: 0.7766 Loss_G: 0.7834 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.7698 Loss_G: 0.7895 acc: 59.4%\n",
      "[BATCH 91/149] Loss_D: 0.7818 Loss_G: 0.8002 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.8135 Loss_G: 0.7985 acc: 53.1%\n",
      "[BATCH 93/149] Loss_D: 0.8612 Loss_G: 0.8161 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.7930 Loss_G: 0.8157 acc: 73.4%\n",
      "[BATCH 95/149] Loss_D: 0.8064 Loss_G: 0.8126 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7720 Loss_G: 0.8114 acc: 60.9%\n",
      "[BATCH 97/149] Loss_D: 0.7844 Loss_G: 0.8149 acc: 68.8%\n",
      "[BATCH 98/149] Loss_D: 0.7865 Loss_G: 0.8223 acc: 59.4%\n",
      "[BATCH 99/149] Loss_D: 0.7697 Loss_G: 0.8129 acc: 60.9%\n",
      "[BATCH 100/149] Loss_D: 0.7791 Loss_G: 0.8044 acc: 56.2%\n",
      "[BATCH 101/149] Loss_D: 0.8226 Loss_G: 0.8150 acc: 57.8%\n",
      "[BATCH 102/149] Loss_D: 0.7936 Loss_G: 0.8147 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.8562 Loss_G: 0.8129 acc: 75.0%\n",
      "[BATCH 104/149] Loss_D: 0.7564 Loss_G: 0.7944 acc: 65.6%\n",
      "[BATCH 105/149] Loss_D: 0.7452 Loss_G: 0.7868 acc: 60.9%\n",
      "[BATCH 106/149] Loss_D: 0.7572 Loss_G: 0.7785 acc: 81.2%\n",
      "[BATCH 107/149] Loss_D: 0.7541 Loss_G: 0.7827 acc: 56.2%\n",
      "[BATCH 108/149] Loss_D: 0.8115 Loss_G: 0.7939 acc: 65.6%\n",
      "[EPOCH 1300] TEST ACC is : 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.7530 Loss_G: 0.7919 acc: 51.6%\n",
      "[BATCH 110/149] Loss_D: 0.7433 Loss_G: 0.7815 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7507 Loss_G: 0.7855 acc: 76.6%\n",
      "[BATCH 112/149] Loss_D: 0.7867 Loss_G: 0.7926 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7797 Loss_G: 0.8059 acc: 51.6%\n",
      "[BATCH 114/149] Loss_D: 0.7539 Loss_G: 0.7880 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.7907 Loss_G: 0.7935 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.8189 Loss_G: 0.8001 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7781 Loss_G: 0.7955 acc: 53.1%\n",
      "[BATCH 118/149] Loss_D: 0.7987 Loss_G: 0.7963 acc: 64.1%\n",
      "[BATCH 119/149] Loss_D: 0.7563 Loss_G: 0.7869 acc: 64.1%\n",
      "[BATCH 120/149] Loss_D: 0.7873 Loss_G: 0.7866 acc: 67.2%\n",
      "[BATCH 121/149] Loss_D: 0.7772 Loss_G: 0.7932 acc: 51.6%\n",
      "[BATCH 122/149] Loss_D: 0.7700 Loss_G: 0.7827 acc: 70.3%\n",
      "[BATCH 123/149] Loss_D: 0.7922 Loss_G: 0.7938 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7688 Loss_G: 0.7935 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7659 Loss_G: 0.8005 acc: 62.5%\n",
      "[BATCH 126/149] Loss_D: 0.8414 Loss_G: 0.8185 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7645 Loss_G: 0.8116 acc: 60.9%\n",
      "[BATCH 128/149] Loss_D: 0.7354 Loss_G: 0.7907 acc: 68.8%\n",
      "[BATCH 129/149] Loss_D: 0.7806 Loss_G: 0.8058 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7638 Loss_G: 0.7997 acc: 57.8%\n",
      "[BATCH 131/149] Loss_D: 0.7796 Loss_G: 0.8054 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7913 Loss_G: 0.7961 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7602 Loss_G: 0.7902 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.7672 Loss_G: 0.7914 acc: 53.1%\n",
      "[BATCH 135/149] Loss_D: 0.8169 Loss_G: 0.8009 acc: 51.6%\n",
      "[BATCH 136/149] Loss_D: 0.8149 Loss_G: 0.8051 acc: 56.2%\n",
      "[BATCH 137/149] Loss_D: 0.7630 Loss_G: 0.7981 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.7966 Loss_G: 0.7970 acc: 60.9%\n",
      "[BATCH 139/149] Loss_D: 0.7465 Loss_G: 0.7880 acc: 59.4%\n",
      "[BATCH 140/149] Loss_D: 0.7712 Loss_G: 0.7873 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7854 Loss_G: 0.7917 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7460 Loss_G: 0.7882 acc: 57.8%\n",
      "[BATCH 143/149] Loss_D: 0.7963 Loss_G: 0.7944 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7738 Loss_G: 0.7916 acc: 75.0%\n",
      "[BATCH 145/149] Loss_D: 0.7900 Loss_G: 0.7934 acc: 76.6%\n",
      "[BATCH 146/149] Loss_D: 0.7531 Loss_G: 0.7965 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.7918 Loss_G: 0.8045 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.7623 Loss_G: 0.8019 acc: 57.8%\n",
      "[BATCH 149/149] Loss_D: 0.7866 Loss_G: 0.8091 acc: 56.2%\n",
      "-----THE [9/50] epoch end-----\n",
      "-----THE [10/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7849 Loss_G: 0.7985 acc: 75.0%\n",
      "[BATCH 2/149] Loss_D: 0.7712 Loss_G: 0.8084 acc: 54.7%\n",
      "[BATCH 3/149] Loss_D: 0.7746 Loss_G: 0.8005 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.7551 Loss_G: 0.8127 acc: 50.0%\n",
      "[BATCH 5/149] Loss_D: 0.8033 Loss_G: 0.8068 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7402 Loss_G: 0.7984 acc: 57.8%\n",
      "[BATCH 7/149] Loss_D: 0.7850 Loss_G: 0.7927 acc: 62.5%\n",
      "[BATCH 8/149] Loss_D: 0.7639 Loss_G: 0.8039 acc: 62.5%\n",
      "[BATCH 9/149] Loss_D: 0.7670 Loss_G: 0.7979 acc: 60.9%\n",
      "[EPOCH 1350] TEST ACC is : 68.2%\n",
      "[BATCH 10/149] Loss_D: 0.7685 Loss_G: 0.8058 acc: 57.8%\n",
      "[BATCH 11/149] Loss_D: 0.7526 Loss_G: 0.7837 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.8021 Loss_G: 0.7794 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.7289 Loss_G: 0.7766 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7596 Loss_G: 0.7766 acc: 62.5%\n",
      "[BATCH 15/149] Loss_D: 0.7785 Loss_G: 0.7801 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7616 Loss_G: 0.7871 acc: 62.5%\n",
      "[BATCH 17/149] Loss_D: 0.7595 Loss_G: 0.7806 acc: 79.7%\n",
      "[BATCH 18/149] Loss_D: 0.7509 Loss_G: 0.7868 acc: 56.2%\n",
      "[BATCH 19/149] Loss_D: 0.8271 Loss_G: 0.8100 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7594 Loss_G: 0.8144 acc: 48.4%\n",
      "[BATCH 21/149] Loss_D: 0.7902 Loss_G: 0.8098 acc: 57.8%\n",
      "[BATCH 22/149] Loss_D: 0.7919 Loss_G: 0.8179 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.7904 Loss_G: 0.8261 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.8108 Loss_G: 0.8196 acc: 64.1%\n",
      "[BATCH 25/149] Loss_D: 0.7946 Loss_G: 0.8065 acc: 56.2%\n",
      "[BATCH 26/149] Loss_D: 0.7630 Loss_G: 0.7941 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.7730 Loss_G: 0.7866 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.7785 Loss_G: 0.7958 acc: 59.4%\n",
      "[BATCH 29/149] Loss_D: 0.7660 Loss_G: 0.7975 acc: 62.5%\n",
      "[BATCH 30/149] Loss_D: 0.7928 Loss_G: 0.8014 acc: 76.6%\n",
      "[BATCH 31/149] Loss_D: 0.8072 Loss_G: 0.8013 acc: 57.8%\n",
      "[BATCH 32/149] Loss_D: 0.7321 Loss_G: 0.7874 acc: 54.7%\n",
      "[BATCH 33/149] Loss_D: 0.8084 Loss_G: 0.7925 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7568 Loss_G: 0.7919 acc: 53.1%\n",
      "[BATCH 35/149] Loss_D: 0.7776 Loss_G: 0.7988 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.8040 Loss_G: 0.8124 acc: 53.1%\n",
      "[BATCH 37/149] Loss_D: 0.7890 Loss_G: 0.8196 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.7737 Loss_G: 0.8129 acc: 73.4%\n",
      "[BATCH 39/149] Loss_D: 0.7403 Loss_G: 0.7994 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7548 Loss_G: 0.7936 acc: 65.6%\n",
      "[BATCH 41/149] Loss_D: 0.8270 Loss_G: 0.8160 acc: 45.3%\n",
      "[BATCH 42/149] Loss_D: 0.8041 Loss_G: 0.8132 acc: 65.6%\n",
      "[BATCH 43/149] Loss_D: 0.7646 Loss_G: 0.7927 acc: 65.6%\n",
      "[BATCH 44/149] Loss_D: 0.7780 Loss_G: 0.7877 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7280 Loss_G: 0.7774 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.8280 Loss_G: 0.7804 acc: 67.2%\n",
      "[BATCH 47/149] Loss_D: 0.8024 Loss_G: 0.7983 acc: 79.7%\n",
      "[BATCH 48/149] Loss_D: 0.8010 Loss_G: 0.8040 acc: 59.4%\n",
      "[BATCH 49/149] Loss_D: 0.7890 Loss_G: 0.8049 acc: 51.6%\n",
      "[BATCH 50/149] Loss_D: 0.7564 Loss_G: 0.7964 acc: 76.6%\n",
      "[BATCH 51/149] Loss_D: 0.7983 Loss_G: 0.7917 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.7556 Loss_G: 0.7932 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.7853 Loss_G: 0.8063 acc: 60.9%\n",
      "[BATCH 54/149] Loss_D: 0.7850 Loss_G: 0.8032 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7763 Loss_G: 0.7996 acc: 56.2%\n",
      "[BATCH 56/149] Loss_D: 0.7742 Loss_G: 0.8004 acc: 67.2%\n",
      "[BATCH 57/149] Loss_D: 0.7680 Loss_G: 0.7930 acc: 56.2%\n",
      "[BATCH 58/149] Loss_D: 0.7865 Loss_G: 0.7978 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7631 Loss_G: 0.7897 acc: 68.8%\n",
      "[EPOCH 1400] TEST ACC is : 66.8%\n",
      "[BATCH 60/149] Loss_D: 0.7493 Loss_G: 0.7898 acc: 54.7%\n",
      "[BATCH 61/149] Loss_D: 0.8002 Loss_G: 0.8010 acc: 62.5%\n",
      "[BATCH 62/149] Loss_D: 0.7971 Loss_G: 0.8156 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7663 Loss_G: 0.8066 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.7464 Loss_G: 0.8017 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7422 Loss_G: 0.7928 acc: 59.4%\n",
      "[BATCH 66/149] Loss_D: 0.8635 Loss_G: 0.8151 acc: 59.4%\n",
      "[BATCH 67/149] Loss_D: 0.7628 Loss_G: 0.8128 acc: 54.7%\n",
      "[BATCH 68/149] Loss_D: 0.7519 Loss_G: 0.7868 acc: 59.4%\n",
      "[BATCH 69/149] Loss_D: 0.7738 Loss_G: 0.7835 acc: 59.4%\n",
      "[BATCH 70/149] Loss_D: 0.8079 Loss_G: 0.7865 acc: 73.4%\n",
      "[BATCH 71/149] Loss_D: 0.7639 Loss_G: 0.7946 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.7592 Loss_G: 0.7901 acc: 59.4%\n",
      "[BATCH 73/149] Loss_D: 0.7785 Loss_G: 0.7901 acc: 67.2%\n",
      "[BATCH 74/149] Loss_D: 0.7919 Loss_G: 0.8034 acc: 56.2%\n",
      "[BATCH 75/149] Loss_D: 0.7780 Loss_G: 0.7938 acc: 64.1%\n",
      "[BATCH 76/149] Loss_D: 0.7378 Loss_G: 0.7889 acc: 71.9%\n",
      "[BATCH 77/149] Loss_D: 0.7883 Loss_G: 0.7890 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.8135 Loss_G: 0.7963 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.8508 Loss_G: 0.8224 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7584 Loss_G: 0.8014 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.7520 Loss_G: 0.7925 acc: 67.2%\n",
      "[BATCH 82/149] Loss_D: 0.7260 Loss_G: 0.7844 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.7409 Loss_G: 0.7802 acc: 60.9%\n",
      "[BATCH 84/149] Loss_D: 0.7612 Loss_G: 0.7786 acc: 59.4%\n",
      "[BATCH 85/149] Loss_D: 0.7527 Loss_G: 0.7770 acc: 51.6%\n",
      "[BATCH 86/149] Loss_D: 0.8370 Loss_G: 0.8057 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.7650 Loss_G: 0.8109 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7348 Loss_G: 0.7839 acc: 64.1%\n",
      "[BATCH 89/149] Loss_D: 0.7438 Loss_G: 0.7751 acc: 70.3%\n",
      "[BATCH 90/149] Loss_D: 0.7552 Loss_G: 0.7730 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7539 Loss_G: 0.7731 acc: 54.7%\n",
      "[BATCH 92/149] Loss_D: 0.7760 Loss_G: 0.7854 acc: 51.6%\n",
      "[BATCH 93/149] Loss_D: 0.7626 Loss_G: 0.7872 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.7580 Loss_G: 0.7835 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.8018 Loss_G: 0.7992 acc: 57.8%\n",
      "[BATCH 96/149] Loss_D: 0.7550 Loss_G: 0.7991 acc: 70.3%\n",
      "[BATCH 97/149] Loss_D: 0.7885 Loss_G: 0.7979 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.8147 Loss_G: 0.8024 acc: 73.4%\n",
      "[BATCH 99/149] Loss_D: 0.7904 Loss_G: 0.8132 acc: 56.2%\n",
      "[BATCH 100/149] Loss_D: 0.7442 Loss_G: 0.7965 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.8366 Loss_G: 0.8221 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.7891 Loss_G: 0.8205 acc: 54.7%\n",
      "[BATCH 103/149] Loss_D: 0.7561 Loss_G: 0.7989 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.8207 Loss_G: 0.8103 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.7980 Loss_G: 0.8168 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7943 Loss_G: 0.8052 acc: 62.5%\n",
      "[BATCH 107/149] Loss_D: 0.7609 Loss_G: 0.7981 acc: 62.5%\n",
      "[BATCH 108/149] Loss_D: 0.7631 Loss_G: 0.7934 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.7653 Loss_G: 0.7804 acc: 56.2%\n",
      "[EPOCH 1450] TEST ACC is : 68.6%\n",
      "[BATCH 110/149] Loss_D: 0.7400 Loss_G: 0.7741 acc: 53.1%\n",
      "[BATCH 111/149] Loss_D: 0.7533 Loss_G: 0.7708 acc: 75.0%\n",
      "[BATCH 112/149] Loss_D: 0.7673 Loss_G: 0.7720 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.8189 Loss_G: 0.7792 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.7581 Loss_G: 0.7817 acc: 53.1%\n",
      "[BATCH 115/149] Loss_D: 0.7543 Loss_G: 0.7807 acc: 57.8%\n",
      "[BATCH 116/149] Loss_D: 0.8132 Loss_G: 0.7933 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.7603 Loss_G: 0.7910 acc: 60.9%\n",
      "[BATCH 118/149] Loss_D: 0.7895 Loss_G: 0.7929 acc: 67.2%\n",
      "[BATCH 119/149] Loss_D: 0.7545 Loss_G: 0.7914 acc: 53.1%\n",
      "[BATCH 120/149] Loss_D: 0.8124 Loss_G: 0.7963 acc: 67.2%\n",
      "[BATCH 121/149] Loss_D: 0.7592 Loss_G: 0.7881 acc: 64.1%\n",
      "[BATCH 122/149] Loss_D: 0.7778 Loss_G: 0.7906 acc: 57.8%\n",
      "[BATCH 123/149] Loss_D: 0.7638 Loss_G: 0.7849 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7732 Loss_G: 0.7845 acc: 59.4%\n",
      "[BATCH 125/149] Loss_D: 0.7350 Loss_G: 0.7816 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.8015 Loss_G: 0.8033 acc: 59.4%\n",
      "[BATCH 127/149] Loss_D: 0.7504 Loss_G: 0.7784 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.8158 Loss_G: 0.7910 acc: 56.2%\n",
      "[BATCH 129/149] Loss_D: 0.7815 Loss_G: 0.7908 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7722 Loss_G: 0.7892 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.7655 Loss_G: 0.7981 acc: 79.7%\n",
      "[BATCH 132/149] Loss_D: 0.7408 Loss_G: 0.7904 acc: 70.3%\n",
      "[BATCH 133/149] Loss_D: 0.7774 Loss_G: 0.7910 acc: 62.5%\n",
      "[BATCH 134/149] Loss_D: 0.7772 Loss_G: 0.7961 acc: 53.1%\n",
      "[BATCH 135/149] Loss_D: 0.7789 Loss_G: 0.7983 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7765 Loss_G: 0.7981 acc: 70.3%\n",
      "[BATCH 137/149] Loss_D: 0.8076 Loss_G: 0.7983 acc: 62.5%\n",
      "[BATCH 138/149] Loss_D: 0.7959 Loss_G: 0.8060 acc: 51.6%\n",
      "[BATCH 139/149] Loss_D: 0.7642 Loss_G: 0.7956 acc: 60.9%\n",
      "[BATCH 140/149] Loss_D: 0.8001 Loss_G: 0.7969 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.7507 Loss_G: 0.7921 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7585 Loss_G: 0.7916 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.7891 Loss_G: 0.7848 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7807 Loss_G: 0.7880 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.8095 Loss_G: 0.8024 acc: 60.9%\n",
      "[BATCH 146/149] Loss_D: 0.7640 Loss_G: 0.7960 acc: 65.6%\n",
      "[BATCH 147/149] Loss_D: 0.7481 Loss_G: 0.7954 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.7504 Loss_G: 0.7863 acc: 64.1%\n",
      "[BATCH 149/149] Loss_D: 0.7925 Loss_G: 0.7908 acc: 62.5%\n",
      "-----THE [10/50] epoch end-----\n",
      "-----THE [11/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7552 Loss_G: 0.7853 acc: 71.9%\n",
      "[BATCH 2/149] Loss_D: 0.7964 Loss_G: 0.7938 acc: 53.1%\n",
      "[BATCH 3/149] Loss_D: 0.7684 Loss_G: 0.7938 acc: 62.5%\n",
      "[BATCH 4/149] Loss_D: 0.7343 Loss_G: 0.7865 acc: 71.9%\n",
      "[BATCH 5/149] Loss_D: 0.7448 Loss_G: 0.7806 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.7438 Loss_G: 0.7766 acc: 68.8%\n",
      "[BATCH 7/149] Loss_D: 0.7690 Loss_G: 0.7911 acc: 67.2%\n",
      "[BATCH 8/149] Loss_D: 0.7575 Loss_G: 0.7892 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7541 Loss_G: 0.7750 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.7678 Loss_G: 0.7806 acc: 57.8%\n",
      "[EPOCH 1500] TEST ACC is : 68.9%\n",
      "[BATCH 11/149] Loss_D: 0.7343 Loss_G: 0.7899 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7369 Loss_G: 0.7740 acc: 54.7%\n",
      "[BATCH 13/149] Loss_D: 0.7688 Loss_G: 0.7909 acc: 53.1%\n",
      "[BATCH 14/149] Loss_D: 0.7801 Loss_G: 0.7935 acc: 56.2%\n",
      "[BATCH 15/149] Loss_D: 0.7900 Loss_G: 0.7985 acc: 60.9%\n",
      "[BATCH 16/149] Loss_D: 0.7661 Loss_G: 0.8011 acc: 75.0%\n",
      "[BATCH 17/149] Loss_D: 0.8504 Loss_G: 0.8089 acc: 71.9%\n",
      "[BATCH 18/149] Loss_D: 0.7928 Loss_G: 0.8082 acc: 59.4%\n",
      "[BATCH 19/149] Loss_D: 0.7534 Loss_G: 0.7980 acc: 54.7%\n",
      "[BATCH 20/149] Loss_D: 0.7455 Loss_G: 0.7887 acc: 64.1%\n",
      "[BATCH 21/149] Loss_D: 0.7749 Loss_G: 0.7890 acc: 57.8%\n",
      "[BATCH 22/149] Loss_D: 0.8118 Loss_G: 0.8080 acc: 51.6%\n",
      "[BATCH 23/149] Loss_D: 0.7358 Loss_G: 0.7855 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.7930 Loss_G: 0.7974 acc: 67.2%\n",
      "[BATCH 25/149] Loss_D: 0.8096 Loss_G: 0.8132 acc: 60.9%\n",
      "[BATCH 26/149] Loss_D: 0.7858 Loss_G: 0.8210 acc: 76.6%\n",
      "[BATCH 27/149] Loss_D: 0.7575 Loss_G: 0.8055 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.7696 Loss_G: 0.7949 acc: 53.1%\n",
      "[BATCH 29/149] Loss_D: 0.7944 Loss_G: 0.8001 acc: 73.4%\n",
      "[BATCH 30/149] Loss_D: 0.7452 Loss_G: 0.7923 acc: 57.8%\n",
      "[BATCH 31/149] Loss_D: 0.7930 Loss_G: 0.8000 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.7849 Loss_G: 0.8056 acc: 57.8%\n",
      "[BATCH 33/149] Loss_D: 0.7805 Loss_G: 0.7993 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7366 Loss_G: 0.7805 acc: 65.6%\n",
      "[BATCH 35/149] Loss_D: 0.7825 Loss_G: 0.7882 acc: 62.5%\n",
      "[BATCH 36/149] Loss_D: 0.7605 Loss_G: 0.7809 acc: 51.6%\n",
      "[BATCH 37/149] Loss_D: 0.7559 Loss_G: 0.7774 acc: 65.6%\n",
      "[BATCH 38/149] Loss_D: 0.7599 Loss_G: 0.7916 acc: 70.3%\n",
      "[BATCH 39/149] Loss_D: 0.7780 Loss_G: 0.7914 acc: 60.9%\n",
      "[BATCH 40/149] Loss_D: 0.7989 Loss_G: 0.8061 acc: 59.4%\n",
      "[BATCH 41/149] Loss_D: 0.7868 Loss_G: 0.7992 acc: 54.7%\n",
      "[BATCH 42/149] Loss_D: 0.7931 Loss_G: 0.8049 acc: 53.1%\n",
      "[BATCH 43/149] Loss_D: 0.7771 Loss_G: 0.8030 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7887 Loss_G: 0.8004 acc: 64.1%\n",
      "[BATCH 45/149] Loss_D: 0.7408 Loss_G: 0.7931 acc: 53.1%\n",
      "[BATCH 46/149] Loss_D: 0.8245 Loss_G: 0.8122 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.7832 Loss_G: 0.8085 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.7673 Loss_G: 0.7993 acc: 59.4%\n",
      "[BATCH 49/149] Loss_D: 0.7507 Loss_G: 0.7925 acc: 54.7%\n",
      "[BATCH 50/149] Loss_D: 0.7613 Loss_G: 0.7830 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7512 Loss_G: 0.7815 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7451 Loss_G: 0.7781 acc: 70.3%\n",
      "[BATCH 53/149] Loss_D: 0.7398 Loss_G: 0.7760 acc: 73.4%\n",
      "[BATCH 54/149] Loss_D: 0.7922 Loss_G: 0.7887 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.7757 Loss_G: 0.7902 acc: 57.8%\n",
      "[BATCH 56/149] Loss_D: 0.7617 Loss_G: 0.7891 acc: 59.4%\n",
      "[BATCH 57/149] Loss_D: 0.7881 Loss_G: 0.7959 acc: 64.1%\n",
      "[BATCH 58/149] Loss_D: 0.7662 Loss_G: 0.8034 acc: 70.3%\n",
      "[BATCH 59/149] Loss_D: 0.7402 Loss_G: 0.8016 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.7765 Loss_G: 0.7974 acc: 60.9%\n",
      "[EPOCH 1550] TEST ACC is : 67.8%\n",
      "[BATCH 61/149] Loss_D: 0.7403 Loss_G: 0.7832 acc: 53.1%\n",
      "[BATCH 62/149] Loss_D: 0.7493 Loss_G: 0.7777 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.7299 Loss_G: 0.7671 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7458 Loss_G: 0.7721 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7702 Loss_G: 0.7806 acc: 57.8%\n",
      "[BATCH 66/149] Loss_D: 0.7827 Loss_G: 0.7949 acc: 68.8%\n",
      "[BATCH 67/149] Loss_D: 0.7564 Loss_G: 0.7877 acc: 64.1%\n",
      "[BATCH 68/149] Loss_D: 0.7582 Loss_G: 0.7846 acc: 73.4%\n",
      "[BATCH 69/149] Loss_D: 0.7794 Loss_G: 0.7874 acc: 68.8%\n",
      "[BATCH 70/149] Loss_D: 0.7877 Loss_G: 0.7996 acc: 54.7%\n",
      "[BATCH 71/149] Loss_D: 0.7473 Loss_G: 0.7824 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7756 Loss_G: 0.7843 acc: 62.5%\n",
      "[BATCH 73/149] Loss_D: 0.8057 Loss_G: 0.7935 acc: 57.8%\n",
      "[BATCH 74/149] Loss_D: 0.7917 Loss_G: 0.7995 acc: 56.2%\n",
      "[BATCH 75/149] Loss_D: 0.7905 Loss_G: 0.7993 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.7491 Loss_G: 0.7904 acc: 65.6%\n",
      "[BATCH 77/149] Loss_D: 0.7316 Loss_G: 0.7786 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.7651 Loss_G: 0.7802 acc: 56.2%\n",
      "[BATCH 79/149] Loss_D: 0.7504 Loss_G: 0.7797 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.7581 Loss_G: 0.7859 acc: 70.3%\n",
      "[BATCH 81/149] Loss_D: 0.7504 Loss_G: 0.7927 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.7755 Loss_G: 0.7913 acc: 56.2%\n",
      "[BATCH 83/149] Loss_D: 0.7728 Loss_G: 0.7859 acc: 68.8%\n",
      "[BATCH 84/149] Loss_D: 0.7627 Loss_G: 0.7901 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.7345 Loss_G: 0.7864 acc: 59.4%\n",
      "[BATCH 86/149] Loss_D: 0.8232 Loss_G: 0.8161 acc: 53.1%\n",
      "[BATCH 87/149] Loss_D: 0.7772 Loss_G: 0.8097 acc: 54.7%\n",
      "[BATCH 88/149] Loss_D: 0.7603 Loss_G: 0.8068 acc: 57.8%\n",
      "[BATCH 89/149] Loss_D: 0.7992 Loss_G: 0.8284 acc: 73.4%\n",
      "[BATCH 90/149] Loss_D: 0.7778 Loss_G: 0.8162 acc: 56.2%\n",
      "[BATCH 91/149] Loss_D: 0.7949 Loss_G: 0.8120 acc: 59.4%\n",
      "[BATCH 92/149] Loss_D: 0.8035 Loss_G: 0.8107 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.8254 Loss_G: 0.8115 acc: 64.1%\n",
      "[BATCH 94/149] Loss_D: 0.7967 Loss_G: 0.8091 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7551 Loss_G: 0.7897 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.7784 Loss_G: 0.7913 acc: 57.8%\n",
      "[BATCH 97/149] Loss_D: 0.7453 Loss_G: 0.7774 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7921 Loss_G: 0.7936 acc: 48.4%\n",
      "[BATCH 99/149] Loss_D: 0.8008 Loss_G: 0.7978 acc: 73.4%\n",
      "[BATCH 100/149] Loss_D: 0.7481 Loss_G: 0.7938 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.7885 Loss_G: 0.7923 acc: 70.3%\n",
      "[BATCH 102/149] Loss_D: 0.7636 Loss_G: 0.7826 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.7619 Loss_G: 0.7875 acc: 59.4%\n",
      "[BATCH 104/149] Loss_D: 0.7796 Loss_G: 0.7954 acc: 59.4%\n",
      "[BATCH 105/149] Loss_D: 0.8020 Loss_G: 0.7929 acc: 62.5%\n",
      "[BATCH 106/149] Loss_D: 0.7818 Loss_G: 0.7868 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.7711 Loss_G: 0.7816 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.8166 Loss_G: 0.7940 acc: 59.4%\n",
      "[BATCH 109/149] Loss_D: 0.7653 Loss_G: 0.8013 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.7574 Loss_G: 0.8016 acc: 60.9%\n",
      "[EPOCH 1600] TEST ACC is : 68.0%\n",
      "[BATCH 111/149] Loss_D: 0.7667 Loss_G: 0.8073 acc: 60.9%\n",
      "[BATCH 112/149] Loss_D: 0.7736 Loss_G: 0.7992 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7842 Loss_G: 0.8121 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.7901 Loss_G: 0.8056 acc: 54.7%\n",
      "[BATCH 115/149] Loss_D: 0.7725 Loss_G: 0.8085 acc: 79.7%\n",
      "[BATCH 116/149] Loss_D: 0.7565 Loss_G: 0.7932 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.7567 Loss_G: 0.7890 acc: 59.4%\n",
      "[BATCH 118/149] Loss_D: 0.8297 Loss_G: 0.8054 acc: 62.5%\n",
      "[BATCH 119/149] Loss_D: 0.8654 Loss_G: 0.8439 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7754 Loss_G: 0.8182 acc: 75.0%\n",
      "[BATCH 121/149] Loss_D: 0.7873 Loss_G: 0.8008 acc: 56.2%\n",
      "[BATCH 122/149] Loss_D: 0.7647 Loss_G: 0.7967 acc: 60.9%\n",
      "[BATCH 123/149] Loss_D: 0.8137 Loss_G: 0.8078 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.7761 Loss_G: 0.7896 acc: 62.5%\n",
      "[BATCH 125/149] Loss_D: 0.7603 Loss_G: 0.7834 acc: 57.8%\n",
      "[BATCH 126/149] Loss_D: 0.7697 Loss_G: 0.7940 acc: 56.2%\n",
      "[BATCH 127/149] Loss_D: 0.7668 Loss_G: 0.7923 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.8061 Loss_G: 0.7967 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7523 Loss_G: 0.7947 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7644 Loss_G: 0.7861 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.7381 Loss_G: 0.7764 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7833 Loss_G: 0.7845 acc: 67.2%\n",
      "[BATCH 133/149] Loss_D: 0.7713 Loss_G: 0.7909 acc: 53.1%\n",
      "[BATCH 134/149] Loss_D: 0.7674 Loss_G: 0.7785 acc: 57.8%\n",
      "[BATCH 135/149] Loss_D: 0.7682 Loss_G: 0.7774 acc: 57.8%\n",
      "[BATCH 136/149] Loss_D: 0.7848 Loss_G: 0.7781 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7402 Loss_G: 0.7720 acc: 59.4%\n",
      "[BATCH 138/149] Loss_D: 0.7960 Loss_G: 0.7835 acc: 59.4%\n",
      "[BATCH 139/149] Loss_D: 0.7494 Loss_G: 0.7872 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.7906 Loss_G: 0.7861 acc: 60.9%\n",
      "[BATCH 141/149] Loss_D: 0.8136 Loss_G: 0.7931 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.7813 Loss_G: 0.7884 acc: 62.5%\n",
      "[BATCH 143/149] Loss_D: 0.8167 Loss_G: 0.7899 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.8020 Loss_G: 0.7955 acc: 68.8%\n",
      "[BATCH 145/149] Loss_D: 0.7822 Loss_G: 0.8020 acc: 51.6%\n",
      "[BATCH 146/149] Loss_D: 0.7894 Loss_G: 0.8016 acc: 57.8%\n",
      "[BATCH 147/149] Loss_D: 0.7793 Loss_G: 0.7977 acc: 60.9%\n",
      "[BATCH 148/149] Loss_D: 0.7822 Loss_G: 0.7916 acc: 59.4%\n",
      "[BATCH 149/149] Loss_D: 0.7561 Loss_G: 0.7921 acc: 56.2%\n",
      "-----THE [11/50] epoch end-----\n",
      "-----THE [12/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8359 Loss_G: 0.8071 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.7517 Loss_G: 0.7907 acc: 76.6%\n",
      "[BATCH 3/149] Loss_D: 0.8069 Loss_G: 0.8143 acc: 59.4%\n",
      "[BATCH 4/149] Loss_D: 0.7720 Loss_G: 0.8052 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.7942 Loss_G: 0.7906 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7815 Loss_G: 0.7892 acc: 75.0%\n",
      "[BATCH 7/149] Loss_D: 0.7390 Loss_G: 0.7803 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7664 Loss_G: 0.7819 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.7669 Loss_G: 0.7843 acc: 53.1%\n",
      "[BATCH 10/149] Loss_D: 0.7976 Loss_G: 0.8006 acc: 53.1%\n",
      "[BATCH 11/149] Loss_D: 0.7732 Loss_G: 0.7930 acc: 60.9%\n",
      "[EPOCH 1650] TEST ACC is : 67.6%\n",
      "[BATCH 12/149] Loss_D: 0.7823 Loss_G: 0.7972 acc: 59.4%\n",
      "[BATCH 13/149] Loss_D: 0.7476 Loss_G: 0.7837 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.7630 Loss_G: 0.7922 acc: 56.2%\n",
      "[BATCH 15/149] Loss_D: 0.7852 Loss_G: 0.7925 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.7626 Loss_G: 0.7923 acc: 59.4%\n",
      "[BATCH 17/149] Loss_D: 0.7744 Loss_G: 0.7900 acc: 53.1%\n",
      "[BATCH 18/149] Loss_D: 0.7549 Loss_G: 0.7919 acc: 64.1%\n",
      "[BATCH 19/149] Loss_D: 0.7756 Loss_G: 0.7901 acc: 64.1%\n",
      "[BATCH 20/149] Loss_D: 0.7486 Loss_G: 0.7840 acc: 56.2%\n",
      "[BATCH 21/149] Loss_D: 0.7996 Loss_G: 0.7937 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.7493 Loss_G: 0.7914 acc: 67.2%\n",
      "[BATCH 23/149] Loss_D: 0.8323 Loss_G: 0.8153 acc: 59.4%\n",
      "[BATCH 24/149] Loss_D: 0.7737 Loss_G: 0.8233 acc: 60.9%\n",
      "[BATCH 25/149] Loss_D: 0.7759 Loss_G: 0.8083 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.8325 Loss_G: 0.8175 acc: 64.1%\n",
      "[BATCH 27/149] Loss_D: 0.8228 Loss_G: 0.8281 acc: 60.9%\n",
      "[BATCH 28/149] Loss_D: 0.7599 Loss_G: 0.8086 acc: 65.6%\n",
      "[BATCH 29/149] Loss_D: 0.7439 Loss_G: 0.8053 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.7668 Loss_G: 0.7946 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.7962 Loss_G: 0.8087 acc: 60.9%\n",
      "[BATCH 32/149] Loss_D: 0.8207 Loss_G: 0.8115 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.7679 Loss_G: 0.8001 acc: 60.9%\n",
      "[BATCH 34/149] Loss_D: 0.7620 Loss_G: 0.7829 acc: 64.1%\n",
      "[BATCH 35/149] Loss_D: 0.7910 Loss_G: 0.7838 acc: 59.4%\n",
      "[BATCH 36/149] Loss_D: 0.7577 Loss_G: 0.7854 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7518 Loss_G: 0.7878 acc: 67.2%\n",
      "[BATCH 38/149] Loss_D: 0.7474 Loss_G: 0.7833 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.7387 Loss_G: 0.7716 acc: 68.8%\n",
      "[BATCH 40/149] Loss_D: 0.7540 Loss_G: 0.7747 acc: 57.8%\n",
      "[BATCH 41/149] Loss_D: 0.7316 Loss_G: 0.7705 acc: 56.2%\n",
      "[BATCH 42/149] Loss_D: 0.7926 Loss_G: 0.7797 acc: 53.1%\n",
      "[BATCH 43/149] Loss_D: 0.7608 Loss_G: 0.7768 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.7456 Loss_G: 0.7814 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7367 Loss_G: 0.7716 acc: 57.8%\n",
      "[BATCH 46/149] Loss_D: 0.7803 Loss_G: 0.7897 acc: 62.5%\n",
      "[BATCH 47/149] Loss_D: 0.7756 Loss_G: 0.7737 acc: 56.2%\n",
      "[BATCH 48/149] Loss_D: 0.7737 Loss_G: 0.7850 acc: 57.8%\n",
      "[BATCH 49/149] Loss_D: 0.7495 Loss_G: 0.7770 acc: 59.4%\n",
      "[BATCH 50/149] Loss_D: 0.7787 Loss_G: 0.7939 acc: 54.7%\n",
      "[BATCH 51/149] Loss_D: 0.7791 Loss_G: 0.7968 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7973 Loss_G: 0.7969 acc: 71.9%\n",
      "[BATCH 53/149] Loss_D: 0.7345 Loss_G: 0.7809 acc: 54.7%\n",
      "[BATCH 54/149] Loss_D: 0.7904 Loss_G: 0.7996 acc: 56.2%\n",
      "[BATCH 55/149] Loss_D: 0.7649 Loss_G: 0.7894 acc: 73.4%\n",
      "[BATCH 56/149] Loss_D: 0.7729 Loss_G: 0.7944 acc: 76.6%\n",
      "[BATCH 57/149] Loss_D: 0.7911 Loss_G: 0.8092 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.7491 Loss_G: 0.7897 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.7714 Loss_G: 0.7862 acc: 57.8%\n",
      "[BATCH 60/149] Loss_D: 0.7547 Loss_G: 0.7899 acc: 59.4%\n",
      "[BATCH 61/149] Loss_D: 0.7882 Loss_G: 0.7985 acc: 60.9%\n",
      "[EPOCH 1700] TEST ACC is : 66.8%\n",
      "[BATCH 62/149] Loss_D: 0.7521 Loss_G: 0.7950 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.8131 Loss_G: 0.8117 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.7464 Loss_G: 0.7940 acc: 70.3%\n",
      "[BATCH 65/149] Loss_D: 0.7228 Loss_G: 0.7895 acc: 76.6%\n",
      "[BATCH 66/149] Loss_D: 0.7621 Loss_G: 0.7929 acc: 56.2%\n",
      "[BATCH 67/149] Loss_D: 0.7440 Loss_G: 0.7941 acc: 57.8%\n",
      "[BATCH 68/149] Loss_D: 0.7632 Loss_G: 0.7740 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.8227 Loss_G: 0.7983 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.7947 Loss_G: 0.8113 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.7509 Loss_G: 0.8012 acc: 53.1%\n",
      "[BATCH 72/149] Loss_D: 0.7791 Loss_G: 0.7908 acc: 65.6%\n",
      "[BATCH 73/149] Loss_D: 0.7329 Loss_G: 0.7943 acc: 70.3%\n",
      "[BATCH 74/149] Loss_D: 0.7417 Loss_G: 0.7854 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.7502 Loss_G: 0.7803 acc: 56.2%\n",
      "[BATCH 76/149] Loss_D: 0.8362 Loss_G: 0.7980 acc: 67.2%\n",
      "[BATCH 77/149] Loss_D: 0.7310 Loss_G: 0.7854 acc: 62.5%\n",
      "[BATCH 78/149] Loss_D: 0.7476 Loss_G: 0.7900 acc: 56.2%\n",
      "[BATCH 79/149] Loss_D: 0.7540 Loss_G: 0.7877 acc: 57.8%\n",
      "[BATCH 80/149] Loss_D: 0.8106 Loss_G: 0.8046 acc: 59.4%\n",
      "[BATCH 81/149] Loss_D: 0.7557 Loss_G: 0.8078 acc: 76.6%\n",
      "[BATCH 82/149] Loss_D: 0.8132 Loss_G: 0.8250 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7549 Loss_G: 0.8121 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.7484 Loss_G: 0.7925 acc: 57.8%\n",
      "[BATCH 85/149] Loss_D: 0.7794 Loss_G: 0.8016 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.7280 Loss_G: 0.7781 acc: 64.1%\n",
      "[BATCH 87/149] Loss_D: 0.7433 Loss_G: 0.7825 acc: 65.6%\n",
      "[BATCH 88/149] Loss_D: 0.7841 Loss_G: 0.7863 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.7528 Loss_G: 0.7824 acc: 57.8%\n",
      "[BATCH 90/149] Loss_D: 0.7378 Loss_G: 0.7708 acc: 48.4%\n",
      "[BATCH 91/149] Loss_D: 0.7961 Loss_G: 0.7824 acc: 60.9%\n",
      "[BATCH 92/149] Loss_D: 0.7872 Loss_G: 0.7969 acc: 57.8%\n",
      "[BATCH 93/149] Loss_D: 0.7233 Loss_G: 0.7815 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.7503 Loss_G: 0.7880 acc: 71.9%\n",
      "[BATCH 95/149] Loss_D: 0.8064 Loss_G: 0.8173 acc: 59.4%\n",
      "[BATCH 96/149] Loss_D: 0.7564 Loss_G: 0.8032 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7586 Loss_G: 0.7961 acc: 56.2%\n",
      "[BATCH 98/149] Loss_D: 0.7604 Loss_G: 0.7997 acc: 50.0%\n",
      "[BATCH 99/149] Loss_D: 0.7887 Loss_G: 0.8067 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.7987 Loss_G: 0.8084 acc: 56.2%\n",
      "[BATCH 101/149] Loss_D: 0.7679 Loss_G: 0.8023 acc: 62.5%\n",
      "[BATCH 102/149] Loss_D: 0.8094 Loss_G: 0.8173 acc: 59.4%\n",
      "[BATCH 103/149] Loss_D: 0.7591 Loss_G: 0.7985 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7404 Loss_G: 0.7846 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7989 Loss_G: 0.8040 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.7490 Loss_G: 0.7962 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.7374 Loss_G: 0.7908 acc: 59.4%\n",
      "[BATCH 108/149] Loss_D: 0.7497 Loss_G: 0.7990 acc: 64.1%\n",
      "[BATCH 109/149] Loss_D: 0.7877 Loss_G: 0.8039 acc: 64.1%\n",
      "[BATCH 110/149] Loss_D: 0.7413 Loss_G: 0.7887 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7747 Loss_G: 0.7868 acc: 67.2%\n",
      "[EPOCH 1750] TEST ACC is : 66.2%\n",
      "[BATCH 112/149] Loss_D: 0.7788 Loss_G: 0.7848 acc: 59.4%\n",
      "[BATCH 113/149] Loss_D: 0.7913 Loss_G: 0.7959 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7437 Loss_G: 0.7941 acc: 56.2%\n",
      "[BATCH 115/149] Loss_D: 0.7727 Loss_G: 0.8011 acc: 67.2%\n",
      "[BATCH 116/149] Loss_D: 0.8245 Loss_G: 0.8278 acc: 57.8%\n",
      "[BATCH 117/149] Loss_D: 0.7495 Loss_G: 0.7979 acc: 64.1%\n",
      "[BATCH 118/149] Loss_D: 0.7595 Loss_G: 0.7905 acc: 57.8%\n",
      "[BATCH 119/149] Loss_D: 0.7566 Loss_G: 0.7844 acc: 71.9%\n",
      "[BATCH 120/149] Loss_D: 0.7607 Loss_G: 0.7937 acc: 60.9%\n",
      "[BATCH 121/149] Loss_D: 0.7961 Loss_G: 0.8067 acc: 59.4%\n",
      "[BATCH 122/149] Loss_D: 0.7678 Loss_G: 0.8057 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7943 Loss_G: 0.8173 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7735 Loss_G: 0.8170 acc: 68.8%\n",
      "[BATCH 131/149] Loss_D: 0.7723 Loss_G: 0.8072 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.7395 Loss_G: 0.7934 acc: 64.1%\n",
      "[BATCH 133/149] Loss_D: 0.7738 Loss_G: 0.7948 acc: 60.9%\n",
      "[BATCH 134/149] Loss_D: 0.7643 Loss_G: 0.7996 acc: 50.0%\n",
      "[BATCH 135/149] Loss_D: 0.7640 Loss_G: 0.8144 acc: 68.8%\n",
      "[BATCH 136/149] Loss_D: 0.7583 Loss_G: 0.7986 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.7360 Loss_G: 0.7850 acc: 70.3%\n",
      "[BATCH 138/149] Loss_D: 0.7693 Loss_G: 0.7986 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.7617 Loss_G: 0.7999 acc: 54.7%\n",
      "[BATCH 140/149] Loss_D: 0.8195 Loss_G: 0.8158 acc: 53.1%\n",
      "[BATCH 141/149] Loss_D: 0.8028 Loss_G: 0.8207 acc: 71.9%\n",
      "[BATCH 142/149] Loss_D: 0.7693 Loss_G: 0.8024 acc: 70.3%\n",
      "[BATCH 143/149] Loss_D: 0.7545 Loss_G: 0.7922 acc: 59.4%\n",
      "[BATCH 144/149] Loss_D: 0.7936 Loss_G: 0.7972 acc: 57.8%\n",
      "[BATCH 145/149] Loss_D: 0.7886 Loss_G: 0.8077 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.7324 Loss_G: 0.7999 acc: 67.2%\n",
      "[BATCH 147/149] Loss_D: 0.7510 Loss_G: 0.7806 acc: 73.4%\n",
      "[BATCH 148/149] Loss_D: 0.7650 Loss_G: 0.7836 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7465 Loss_G: 0.7859 acc: 51.6%\n",
      "-----THE [12/50] epoch end-----\n",
      "-----THE [13/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7944 Loss_G: 0.7962 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7473 Loss_G: 0.7956 acc: 65.6%\n",
      "[BATCH 3/149] Loss_D: 0.7966 Loss_G: 0.8079 acc: 70.3%\n",
      "[BATCH 4/149] Loss_D: 0.7398 Loss_G: 0.7959 acc: 75.0%\n",
      "[BATCH 5/149] Loss_D: 0.8290 Loss_G: 0.8206 acc: 54.7%\n",
      "[BATCH 6/149] Loss_D: 0.7684 Loss_G: 0.8195 acc: 62.5%\n",
      "[BATCH 7/149] Loss_D: 0.7582 Loss_G: 0.7946 acc: 60.9%\n",
      "[BATCH 8/149] Loss_D: 0.7447 Loss_G: 0.7807 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.8218 Loss_G: 0.8050 acc: 57.8%\n",
      "[BATCH 10/149] Loss_D: 0.7443 Loss_G: 0.7949 acc: 71.9%\n",
      "[BATCH 11/149] Loss_D: 0.7539 Loss_G: 0.7887 acc: 53.1%\n",
      "[BATCH 12/149] Loss_D: 0.7284 Loss_G: 0.7769 acc: 67.2%\n",
      "[EPOCH 1800] TEST ACC is : 65.0%\n",
      "[BATCH 13/149] Loss_D: 0.7594 Loss_G: 0.7843 acc: 60.9%\n",
      "[BATCH 14/149] Loss_D: 0.7302 Loss_G: 0.7832 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.7294 Loss_G: 0.7859 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.8305 Loss_G: 0.8156 acc: 60.9%\n",
      "[BATCH 17/149] Loss_D: 0.7829 Loss_G: 0.8147 acc: 54.7%\n",
      "[BATCH 18/149] Loss_D: 0.7484 Loss_G: 0.8072 acc: 64.1%\n",
      "[BATCH 19/149] Loss_D: 0.7621 Loss_G: 0.8051 acc: 73.4%\n",
      "[BATCH 20/149] Loss_D: 0.7626 Loss_G: 0.7980 acc: 59.4%\n",
      "[BATCH 21/149] Loss_D: 0.7755 Loss_G: 0.7923 acc: 73.4%\n",
      "[BATCH 22/149] Loss_D: 0.7500 Loss_G: 0.7890 acc: 68.8%\n",
      "[BATCH 23/149] Loss_D: 0.7456 Loss_G: 0.7884 acc: 54.7%\n",
      "[BATCH 24/149] Loss_D: 0.7536 Loss_G: 0.7889 acc: 70.3%\n",
      "[BATCH 25/149] Loss_D: 0.7597 Loss_G: 0.8007 acc: 65.6%\n",
      "[BATCH 26/149] Loss_D: 0.7727 Loss_G: 0.7981 acc: 56.2%\n",
      "[BATCH 27/149] Loss_D: 0.7636 Loss_G: 0.7973 acc: 60.9%\n",
      "[BATCH 28/149] Loss_D: 0.7348 Loss_G: 0.7832 acc: 62.5%\n",
      "[BATCH 29/149] Loss_D: 0.7543 Loss_G: 0.7869 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.7894 Loss_G: 0.8024 acc: 60.9%\n",
      "[BATCH 31/149] Loss_D: 0.7670 Loss_G: 0.7987 acc: 62.5%\n",
      "[BATCH 32/149] Loss_D: 0.8006 Loss_G: 0.8076 acc: 53.1%\n",
      "[BATCH 33/149] Loss_D: 0.7957 Loss_G: 0.8085 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.7367 Loss_G: 0.7926 acc: 50.0%\n",
      "[BATCH 35/149] Loss_D: 0.7737 Loss_G: 0.7983 acc: 65.6%\n",
      "[BATCH 36/149] Loss_D: 0.7401 Loss_G: 0.7922 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.8064 Loss_G: 0.8159 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7718 Loss_G: 0.8047 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.7963 Loss_G: 0.8113 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7751 Loss_G: 0.8097 acc: 64.1%\n",
      "[BATCH 41/149] Loss_D: 0.7733 Loss_G: 0.8318 acc: 57.8%\n",
      "[BATCH 42/149] Loss_D: 0.8034 Loss_G: 0.8189 acc: 60.9%\n",
      "[BATCH 43/149] Loss_D: 0.7674 Loss_G: 0.7987 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7443 Loss_G: 0.7821 acc: 71.9%\n",
      "[BATCH 45/149] Loss_D: 0.7529 Loss_G: 0.7803 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7393 Loss_G: 0.7813 acc: 56.2%\n",
      "[BATCH 47/149] Loss_D: 0.7963 Loss_G: 0.7864 acc: 60.9%\n",
      "[BATCH 48/149] Loss_D: 0.7507 Loss_G: 0.7778 acc: 76.6%\n",
      "[BATCH 49/149] Loss_D: 0.8101 Loss_G: 0.7987 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7646 Loss_G: 0.8168 acc: 65.6%\n",
      "[BATCH 51/149] Loss_D: 0.7367 Loss_G: 0.7917 acc: 62.5%\n",
      "[BATCH 52/149] Loss_D: 0.7339 Loss_G: 0.7846 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.7316 Loss_G: 0.7819 acc: 54.7%\n",
      "[BATCH 54/149] Loss_D: 0.7367 Loss_G: 0.7792 acc: 65.6%\n",
      "[BATCH 55/149] Loss_D: 0.7844 Loss_G: 0.8007 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.7702 Loss_G: 0.7896 acc: 70.3%\n",
      "[BATCH 57/149] Loss_D: 0.7773 Loss_G: 0.7941 acc: 59.4%\n",
      "[BATCH 58/149] Loss_D: 0.7574 Loss_G: 0.7911 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.7586 Loss_G: 0.7929 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7511 Loss_G: 0.7820 acc: 75.0%\n",
      "[BATCH 61/149] Loss_D: 0.7640 Loss_G: 0.7932 acc: 54.7%\n",
      "[BATCH 62/149] Loss_D: 0.7821 Loss_G: 0.8007 acc: 64.1%\n",
      "[EPOCH 1850] TEST ACC is : 66.8%\n",
      "[BATCH 63/149] Loss_D: 0.7991 Loss_G: 0.8173 acc: 64.1%\n",
      "[BATCH 64/149] Loss_D: 0.7631 Loss_G: 0.8044 acc: 60.9%\n",
      "[BATCH 65/149] Loss_D: 0.7471 Loss_G: 0.7854 acc: 73.4%\n",
      "[BATCH 66/149] Loss_D: 0.7387 Loss_G: 0.7916 acc: 64.1%\n",
      "[BATCH 67/149] Loss_D: 0.7657 Loss_G: 0.7899 acc: 57.8%\n",
      "[BATCH 68/149] Loss_D: 0.7449 Loss_G: 0.7855 acc: 75.0%\n",
      "[BATCH 69/149] Loss_D: 0.7928 Loss_G: 0.8033 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.7496 Loss_G: 0.7973 acc: 76.6%\n",
      "[BATCH 71/149] Loss_D: 0.7705 Loss_G: 0.8033 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7821 Loss_G: 0.8024 acc: 57.8%\n",
      "[BATCH 73/149] Loss_D: 0.7580 Loss_G: 0.7878 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.7484 Loss_G: 0.7832 acc: 59.4%\n",
      "[BATCH 75/149] Loss_D: 0.7861 Loss_G: 0.7950 acc: 59.4%\n",
      "[BATCH 76/149] Loss_D: 0.7504 Loss_G: 0.7910 acc: 45.3%\n",
      "[BATCH 77/149] Loss_D: 0.7806 Loss_G: 0.7825 acc: 64.1%\n",
      "[BATCH 78/149] Loss_D: 0.7454 Loss_G: 0.7751 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.7972 Loss_G: 0.7979 acc: 62.5%\n",
      "[BATCH 80/149] Loss_D: 0.7564 Loss_G: 0.7887 acc: 67.2%\n",
      "[BATCH 81/149] Loss_D: 0.7596 Loss_G: 0.7823 acc: 60.9%\n",
      "[BATCH 82/149] Loss_D: 0.7651 Loss_G: 0.7982 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.7636 Loss_G: 0.7860 acc: 56.2%\n",
      "[BATCH 84/149] Loss_D: 0.7896 Loss_G: 0.7928 acc: 67.2%\n",
      "[BATCH 85/149] Loss_D: 0.7999 Loss_G: 0.8152 acc: 54.7%\n",
      "[BATCH 86/149] Loss_D: 0.7472 Loss_G: 0.7895 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.7650 Loss_G: 0.7953 acc: 50.0%\n",
      "[BATCH 88/149] Loss_D: 0.7622 Loss_G: 0.7930 acc: 59.4%\n",
      "[BATCH 89/149] Loss_D: 0.8000 Loss_G: 0.8097 acc: 62.5%\n",
      "[BATCH 90/149] Loss_D: 0.7622 Loss_G: 0.8087 acc: 54.7%\n",
      "[BATCH 91/149] Loss_D: 0.7872 Loss_G: 0.7991 acc: 64.1%\n",
      "[BATCH 92/149] Loss_D: 0.7914 Loss_G: 0.8157 acc: 65.6%\n",
      "[BATCH 93/149] Loss_D: 0.8311 Loss_G: 0.8194 acc: 65.6%\n",
      "[BATCH 94/149] Loss_D: 0.7323 Loss_G: 0.7854 acc: 64.1%\n",
      "[BATCH 95/149] Loss_D: 0.7592 Loss_G: 0.7804 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7406 Loss_G: 0.7799 acc: 62.5%\n",
      "[BATCH 97/149] Loss_D: 0.7385 Loss_G: 0.7778 acc: 51.6%\n",
      "[BATCH 98/149] Loss_D: 0.7847 Loss_G: 0.7824 acc: 70.3%\n",
      "[BATCH 99/149] Loss_D: 0.7267 Loss_G: 0.7817 acc: 67.2%\n",
      "[BATCH 100/149] Loss_D: 0.7387 Loss_G: 0.7865 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.7896 Loss_G: 0.8042 acc: 67.2%\n",
      "[BATCH 102/149] Loss_D: 0.7394 Loss_G: 0.7998 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.7454 Loss_G: 0.7937 acc: 67.2%\n",
      "[BATCH 104/149] Loss_D: 0.7426 Loss_G: 0.7893 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7981 Loss_G: 0.8104 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.7914 Loss_G: 0.8142 acc: 76.6%\n",
      "[BATCH 107/149] Loss_D: 0.7615 Loss_G: 0.8090 acc: 62.5%\n",
      "[BATCH 108/149] Loss_D: 0.7656 Loss_G: 0.8049 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.7530 Loss_G: 0.7954 acc: 57.8%\n",
      "[BATCH 110/149] Loss_D: 0.7850 Loss_G: 0.7964 acc: 57.8%\n",
      "[BATCH 111/149] Loss_D: 0.7938 Loss_G: 0.8022 acc: 64.1%\n",
      "[BATCH 112/149] Loss_D: 0.8111 Loss_G: 0.8023 acc: 67.2%\n",
      "[EPOCH 1900] TEST ACC is : 64.6%\n",
      "[BATCH 113/149] Loss_D: 0.7645 Loss_G: 0.7996 acc: 54.7%\n",
      "[BATCH 114/149] Loss_D: 0.7306 Loss_G: 0.7845 acc: 56.2%\n",
      "[BATCH 115/149] Loss_D: 0.7319 Loss_G: 0.7981 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.8036 Loss_G: 0.8066 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7240 Loss_G: 0.8027 acc: 76.6%\n",
      "[BATCH 118/149] Loss_D: 0.7533 Loss_G: 0.8008 acc: 51.6%\n",
      "[BATCH 119/149] Loss_D: 0.7493 Loss_G: 0.7830 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.7484 Loss_G: 0.7820 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.7419 Loss_G: 0.7849 acc: 64.1%\n",
      "[BATCH 122/149] Loss_D: 0.7454 Loss_G: 0.7807 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7555 Loss_G: 0.7731 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.7487 Loss_G: 0.7821 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.7832 Loss_G: 0.7879 acc: 57.8%\n",
      "[BATCH 126/149] Loss_D: 0.7828 Loss_G: 0.8031 acc: 64.1%\n",
      "[BATCH 127/149] Loss_D: 0.7264 Loss_G: 0.7891 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.7572 Loss_G: 0.7906 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7445 Loss_G: 0.8029 acc: 50.0%\n",
      "[BATCH 130/149] Loss_D: 0.7651 Loss_G: 0.8009 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.7806 Loss_G: 0.7983 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.7146 Loss_G: 0.7820 acc: 54.7%\n",
      "[BATCH 133/149] Loss_D: 0.8067 Loss_G: 0.8089 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.7247 Loss_G: 0.7862 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.7758 Loss_G: 0.7985 acc: 59.4%\n",
      "[BATCH 136/149] Loss_D: 0.7941 Loss_G: 0.8261 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7954 Loss_G: 0.8335 acc: 53.1%\n",
      "[BATCH 138/149] Loss_D: 0.7791 Loss_G: 0.8349 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.7447 Loss_G: 0.8058 acc: 64.1%\n",
      "[BATCH 140/149] Loss_D: 0.7522 Loss_G: 0.7943 acc: 59.4%\n",
      "[BATCH 141/149] Loss_D: 0.7777 Loss_G: 0.7877 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.7706 Loss_G: 0.7883 acc: 64.1%\n",
      "[BATCH 143/149] Loss_D: 0.7881 Loss_G: 0.7905 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7904 Loss_G: 0.8005 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.7551 Loss_G: 0.8013 acc: 73.4%\n",
      "[BATCH 146/149] Loss_D: 0.7884 Loss_G: 0.7982 acc: 73.4%\n",
      "[BATCH 147/149] Loss_D: 0.7880 Loss_G: 0.8159 acc: 65.6%\n",
      "[BATCH 148/149] Loss_D: 0.7757 Loss_G: 0.8026 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.7467 Loss_G: 0.8049 acc: 65.6%\n",
      "-----THE [13/50] epoch end-----\n",
      "-----THE [14/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7893 Loss_G: 0.8064 acc: 62.5%\n",
      "[BATCH 2/149] Loss_D: 0.7501 Loss_G: 0.8023 acc: 59.4%\n",
      "[BATCH 3/149] Loss_D: 0.7737 Loss_G: 0.7969 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.7249 Loss_G: 0.7903 acc: 68.8%\n",
      "[BATCH 5/149] Loss_D: 0.7358 Loss_G: 0.7759 acc: 71.9%\n",
      "[BATCH 6/149] Loss_D: 0.7940 Loss_G: 0.7971 acc: 45.3%\n",
      "[BATCH 7/149] Loss_D: 0.7940 Loss_G: 0.7845 acc: 67.2%\n",
      "[BATCH 8/149] Loss_D: 0.7365 Loss_G: 0.7868 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7486 Loss_G: 0.7966 acc: 56.2%\n",
      "[BATCH 10/149] Loss_D: 0.7705 Loss_G: 0.7923 acc: 59.4%\n",
      "[BATCH 11/149] Loss_D: 0.7701 Loss_G: 0.7871 acc: 64.1%\n",
      "[BATCH 12/149] Loss_D: 0.7849 Loss_G: 0.7786 acc: 68.8%\n",
      "[BATCH 13/149] Loss_D: 0.7514 Loss_G: 0.7907 acc: 73.4%\n",
      "[EPOCH 1950] TEST ACC is : 67.4%\n",
      "[BATCH 14/149] Loss_D: 0.7632 Loss_G: 0.7892 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.7559 Loss_G: 0.7919 acc: 51.6%\n",
      "[BATCH 16/149] Loss_D: 0.7636 Loss_G: 0.7954 acc: 54.7%\n",
      "[BATCH 17/149] Loss_D: 0.7533 Loss_G: 0.8011 acc: 54.7%\n",
      "[BATCH 18/149] Loss_D: 0.7273 Loss_G: 0.7855 acc: 68.8%\n",
      "[BATCH 19/149] Loss_D: 0.7550 Loss_G: 0.7781 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7633 Loss_G: 0.8001 acc: 62.5%\n",
      "[BATCH 21/149] Loss_D: 0.7944 Loss_G: 0.8063 acc: 60.9%\n",
      "[BATCH 22/149] Loss_D: 0.7902 Loss_G: 0.8078 acc: 57.8%\n",
      "[BATCH 23/149] Loss_D: 0.7727 Loss_G: 0.8000 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7578 Loss_G: 0.7904 acc: 56.2%\n",
      "[BATCH 25/149] Loss_D: 0.7453 Loss_G: 0.7731 acc: 73.4%\n",
      "[BATCH 26/149] Loss_D: 0.7877 Loss_G: 0.7953 acc: 60.9%\n",
      "[BATCH 27/149] Loss_D: 0.7644 Loss_G: 0.7966 acc: 73.4%\n",
      "[BATCH 28/149] Loss_D: 0.8164 Loss_G: 0.8028 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.7484 Loss_G: 0.7966 acc: 40.6%\n",
      "[BATCH 30/149] Loss_D: 0.7223 Loss_G: 0.7727 acc: 59.4%\n",
      "[BATCH 31/149] Loss_D: 0.8012 Loss_G: 0.7810 acc: 71.9%\n",
      "[BATCH 32/149] Loss_D: 0.7271 Loss_G: 0.7770 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.8054 Loss_G: 0.7867 acc: 62.5%\n",
      "[BATCH 34/149] Loss_D: 0.7716 Loss_G: 0.8120 acc: 57.8%\n",
      "[BATCH 35/149] Loss_D: 0.7694 Loss_G: 0.7953 acc: 68.8%\n",
      "[BATCH 36/149] Loss_D: 0.7499 Loss_G: 0.8029 acc: 54.7%\n",
      "[BATCH 37/149] Loss_D: 0.7684 Loss_G: 0.7928 acc: 73.4%\n",
      "[BATCH 38/149] Loss_D: 0.7220 Loss_G: 0.7901 acc: 59.4%\n",
      "[BATCH 39/149] Loss_D: 0.7541 Loss_G: 0.7963 acc: 57.8%\n",
      "[BATCH 40/149] Loss_D: 0.7156 Loss_G: 0.7873 acc: 73.4%\n",
      "[BATCH 41/149] Loss_D: 0.7321 Loss_G: 0.7851 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.7609 Loss_G: 0.7903 acc: 62.5%\n",
      "[BATCH 43/149] Loss_D: 0.7394 Loss_G: 0.7909 acc: 64.1%\n",
      "[BATCH 44/149] Loss_D: 0.7758 Loss_G: 0.8006 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7578 Loss_G: 0.8111 acc: 68.8%\n",
      "[BATCH 46/149] Loss_D: 0.7122 Loss_G: 0.7764 acc: 57.8%\n",
      "[BATCH 47/149] Loss_D: 0.7405 Loss_G: 0.7731 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.8058 Loss_G: 0.7945 acc: 64.1%\n",
      "[BATCH 49/149] Loss_D: 0.7841 Loss_G: 0.7993 acc: 65.6%\n",
      "[BATCH 50/149] Loss_D: 0.7779 Loss_G: 0.8109 acc: 48.4%\n",
      "[BATCH 51/149] Loss_D: 0.7510 Loss_G: 0.8091 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.7807 Loss_G: 0.8076 acc: 64.1%\n",
      "[BATCH 53/149] Loss_D: 0.7593 Loss_G: 0.8086 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.7407 Loss_G: 0.8037 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7591 Loss_G: 0.7876 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.8097 Loss_G: 0.8151 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.7561 Loss_G: 0.7847 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.7689 Loss_G: 0.7883 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.7274 Loss_G: 0.7860 acc: 59.4%\n",
      "[BATCH 60/149] Loss_D: 0.7817 Loss_G: 0.7883 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7255 Loss_G: 0.7873 acc: 57.8%\n",
      "[BATCH 62/149] Loss_D: 0.7584 Loss_G: 0.7761 acc: 71.9%\n",
      "[BATCH 63/149] Loss_D: 0.7629 Loss_G: 0.7834 acc: 64.1%\n",
      "[EPOCH 2000] TEST ACC is : 63.5%\n",
      "[BATCH 64/149] Loss_D: 0.7476 Loss_G: 0.8009 acc: 70.3%\n",
      "[BATCH 65/149] Loss_D: 0.7956 Loss_G: 0.8065 acc: 65.6%\n",
      "[BATCH 66/149] Loss_D: 0.7610 Loss_G: 0.8130 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7535 Loss_G: 0.7978 acc: 54.7%\n",
      "[BATCH 68/149] Loss_D: 0.7303 Loss_G: 0.7930 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.7300 Loss_G: 0.7824 acc: 54.7%\n",
      "[BATCH 70/149] Loss_D: 0.7856 Loss_G: 0.7978 acc: 56.2%\n",
      "[BATCH 71/149] Loss_D: 0.7729 Loss_G: 0.8126 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7877 Loss_G: 0.8532 acc: 50.0%\n",
      "[BATCH 73/149] Loss_D: 0.7764 Loss_G: 0.8224 acc: 59.4%\n",
      "[BATCH 74/149] Loss_D: 0.7489 Loss_G: 0.7892 acc: 64.1%\n",
      "[BATCH 75/149] Loss_D: 0.7431 Loss_G: 0.7792 acc: 53.1%\n",
      "[BATCH 76/149] Loss_D: 0.7456 Loss_G: 0.7821 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.7445 Loss_G: 0.7851 acc: 67.2%\n",
      "[BATCH 78/149] Loss_D: 0.7644 Loss_G: 0.7804 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.7917 Loss_G: 0.8101 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7611 Loss_G: 0.7860 acc: 54.7%\n",
      "[BATCH 81/149] Loss_D: 0.7928 Loss_G: 0.8167 acc: 67.2%\n",
      "[BATCH 82/149] Loss_D: 0.7289 Loss_G: 0.7993 acc: 57.8%\n",
      "[BATCH 83/149] Loss_D: 0.7708 Loss_G: 0.7928 acc: 75.0%\n",
      "[BATCH 84/149] Loss_D: 0.7462 Loss_G: 0.7881 acc: 75.0%\n",
      "[BATCH 85/149] Loss_D: 0.7486 Loss_G: 0.7824 acc: 67.2%\n",
      "[BATCH 86/149] Loss_D: 0.7685 Loss_G: 0.7956 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.7289 Loss_G: 0.7903 acc: 59.4%\n",
      "[BATCH 88/149] Loss_D: 0.7672 Loss_G: 0.8048 acc: 70.3%\n",
      "[BATCH 89/149] Loss_D: 0.7861 Loss_G: 0.8215 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.7502 Loss_G: 0.8038 acc: 50.0%\n",
      "[BATCH 91/149] Loss_D: 0.7256 Loss_G: 0.7836 acc: 75.0%\n",
      "[BATCH 92/149] Loss_D: 0.7916 Loss_G: 0.8116 acc: 60.9%\n",
      "[BATCH 93/149] Loss_D: 0.7932 Loss_G: 0.8119 acc: 67.2%\n",
      "[BATCH 94/149] Loss_D: 0.7272 Loss_G: 0.8002 acc: 57.8%\n",
      "[BATCH 95/149] Loss_D: 0.7736 Loss_G: 0.7963 acc: 76.6%\n",
      "[BATCH 96/149] Loss_D: 0.7651 Loss_G: 0.7974 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.7617 Loss_G: 0.8006 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7740 Loss_G: 0.8130 acc: 56.2%\n",
      "[BATCH 99/149] Loss_D: 0.8092 Loss_G: 0.8083 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.8028 Loss_G: 0.8098 acc: 71.9%\n",
      "[BATCH 101/149] Loss_D: 0.7365 Loss_G: 0.7816 acc: 70.3%\n",
      "[BATCH 102/149] Loss_D: 0.7878 Loss_G: 0.7955 acc: 62.5%\n",
      "[BATCH 103/149] Loss_D: 0.7653 Loss_G: 0.8000 acc: 64.1%\n",
      "[BATCH 104/149] Loss_D: 0.7430 Loss_G: 0.7856 acc: 60.9%\n",
      "[BATCH 105/149] Loss_D: 0.7161 Loss_G: 0.8001 acc: 78.1%\n",
      "[BATCH 106/149] Loss_D: 0.7413 Loss_G: 0.7905 acc: 70.3%\n",
      "[BATCH 107/149] Loss_D: 0.7981 Loss_G: 0.7957 acc: 65.6%\n",
      "[BATCH 108/149] Loss_D: 0.7924 Loss_G: 0.7930 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.8191 Loss_G: 0.8096 acc: 60.9%\n",
      "[BATCH 110/149] Loss_D: 0.7567 Loss_G: 0.8057 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7429 Loss_G: 0.8066 acc: 76.6%\n",
      "[BATCH 112/149] Loss_D: 0.7754 Loss_G: 0.8086 acc: 71.9%\n",
      "[BATCH 113/149] Loss_D: 0.7778 Loss_G: 0.8012 acc: 68.8%\n",
      "[EPOCH 2050] TEST ACC is : 65.2%\n",
      "[BATCH 114/149] Loss_D: 0.7940 Loss_G: 0.8061 acc: 73.4%\n",
      "[BATCH 115/149] Loss_D: 0.7944 Loss_G: 0.8153 acc: 62.5%\n",
      "[BATCH 116/149] Loss_D: 0.7461 Loss_G: 0.8020 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7788 Loss_G: 0.8062 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.7273 Loss_G: 0.7883 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.7750 Loss_G: 0.7922 acc: 62.5%\n",
      "[BATCH 120/149] Loss_D: 0.7391 Loss_G: 0.7868 acc: 71.9%\n",
      "[BATCH 121/149] Loss_D: 0.7525 Loss_G: 0.7746 acc: 71.9%\n",
      "[BATCH 122/149] Loss_D: 0.7959 Loss_G: 0.7836 acc: 54.7%\n",
      "[BATCH 123/149] Loss_D: 0.7624 Loss_G: 0.7967 acc: 59.4%\n",
      "[BATCH 124/149] Loss_D: 0.7398 Loss_G: 0.7837 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.7466 Loss_G: 0.7681 acc: 81.2%\n",
      "[BATCH 126/149] Loss_D: 0.7401 Loss_G: 0.7728 acc: 75.0%\n",
      "[BATCH 127/149] Loss_D: 0.7804 Loss_G: 0.7893 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7932 Loss_G: 0.7906 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.7545 Loss_G: 0.7909 acc: 76.6%\n",
      "[BATCH 130/149] Loss_D: 0.7626 Loss_G: 0.7932 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.7056 Loss_G: 0.7764 acc: 71.9%\n",
      "[BATCH 132/149] Loss_D: 0.7611 Loss_G: 0.7933 acc: 57.8%\n",
      "[BATCH 133/149] Loss_D: 0.7611 Loss_G: 0.7822 acc: 64.1%\n",
      "[BATCH 134/149] Loss_D: 0.7471 Loss_G: 0.7753 acc: 54.7%\n",
      "[BATCH 135/149] Loss_D: 0.7352 Loss_G: 0.7688 acc: 73.4%\n",
      "[BATCH 136/149] Loss_D: 0.7579 Loss_G: 0.7922 acc: 62.5%\n",
      "[BATCH 137/149] Loss_D: 0.7436 Loss_G: 0.7996 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7707 Loss_G: 0.8114 acc: 65.6%\n",
      "[BATCH 139/149] Loss_D: 0.7797 Loss_G: 0.8046 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.7306 Loss_G: 0.8132 acc: 71.9%\n",
      "[BATCH 141/149] Loss_D: 0.7622 Loss_G: 0.8072 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7815 Loss_G: 0.8131 acc: 57.8%\n",
      "[BATCH 143/149] Loss_D: 0.7584 Loss_G: 0.7919 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7444 Loss_G: 0.7781 acc: 64.1%\n",
      "[BATCH 145/149] Loss_D: 0.7804 Loss_G: 0.7911 acc: 50.0%\n",
      "[BATCH 146/149] Loss_D: 0.7539 Loss_G: 0.7944 acc: 73.4%\n",
      "[BATCH 147/149] Loss_D: 0.7280 Loss_G: 0.7859 acc: 64.1%\n",
      "[BATCH 148/149] Loss_D: 0.7507 Loss_G: 0.7810 acc: 62.5%\n",
      "[BATCH 149/149] Loss_D: 0.7586 Loss_G: 0.7849 acc: 68.8%\n",
      "-----THE [14/50] epoch end-----\n",
      "-----THE [15/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7591 Loss_G: 0.7849 acc: 71.9%\n",
      "[BATCH 2/149] Loss_D: 0.7679 Loss_G: 0.7909 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7961 Loss_G: 0.7994 acc: 60.9%\n",
      "[BATCH 4/149] Loss_D: 0.7827 Loss_G: 0.8184 acc: 57.8%\n",
      "[BATCH 5/149] Loss_D: 0.7942 Loss_G: 0.8182 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.7263 Loss_G: 0.7879 acc: 65.6%\n",
      "[BATCH 7/149] Loss_D: 0.7847 Loss_G: 0.7848 acc: 71.9%\n",
      "[BATCH 8/149] Loss_D: 0.7851 Loss_G: 0.7962 acc: 67.2%\n",
      "[BATCH 9/149] Loss_D: 0.7476 Loss_G: 0.7875 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7467 Loss_G: 0.7993 acc: 62.5%\n",
      "[BATCH 11/149] Loss_D: 0.7175 Loss_G: 0.7902 acc: 60.9%\n",
      "[BATCH 12/149] Loss_D: 0.7626 Loss_G: 0.7881 acc: 60.9%\n",
      "[BATCH 13/149] Loss_D: 0.7621 Loss_G: 0.7991 acc: 64.1%\n",
      "[BATCH 14/149] Loss_D: 0.7192 Loss_G: 0.7921 acc: 73.4%\n",
      "[EPOCH 2100] TEST ACC is : 64.3%\n",
      "[BATCH 15/149] Loss_D: 0.7566 Loss_G: 0.8033 acc: 60.9%\n",
      "[BATCH 16/149] Loss_D: 0.7661 Loss_G: 0.7884 acc: 70.3%\n",
      "[BATCH 17/149] Loss_D: 0.7726 Loss_G: 0.7796 acc: 76.6%\n",
      "[BATCH 18/149] Loss_D: 0.7474 Loss_G: 0.7951 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7571 Loss_G: 0.7902 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7728 Loss_G: 0.7871 acc: 70.3%\n",
      "[BATCH 21/149] Loss_D: 0.7888 Loss_G: 0.8052 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.7121 Loss_G: 0.7811 acc: 75.0%\n",
      "[BATCH 23/149] Loss_D: 0.7499 Loss_G: 0.7941 acc: 56.2%\n",
      "[BATCH 24/149] Loss_D: 0.7698 Loss_G: 0.7958 acc: 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.7772 Loss_G: 0.8157 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.8073 Loss_G: 0.8181 acc: 57.8%\n",
      "[BATCH 27/149] Loss_D: 0.7625 Loss_G: 0.8076 acc: 71.9%\n",
      "[BATCH 28/149] Loss_D: 0.7672 Loss_G: 0.7962 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7205 Loss_G: 0.7944 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7493 Loss_G: 0.7800 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7989 Loss_G: 0.8134 acc: 78.1%\n",
      "[BATCH 32/149] Loss_D: 0.7453 Loss_G: 0.7877 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.7558 Loss_G: 0.7882 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.7433 Loss_G: 0.7983 acc: 51.6%\n",
      "[BATCH 35/149] Loss_D: 0.8022 Loss_G: 0.8099 acc: 60.9%\n",
      "[BATCH 36/149] Loss_D: 0.7182 Loss_G: 0.7921 acc: 67.2%\n",
      "[BATCH 37/149] Loss_D: 0.8344 Loss_G: 0.8285 acc: 56.2%\n",
      "[BATCH 38/149] Loss_D: 0.7536 Loss_G: 0.7930 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7316 Loss_G: 0.7694 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.7279 Loss_G: 0.7755 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.7301 Loss_G: 0.7910 acc: 64.1%\n",
      "[BATCH 42/149] Loss_D: 0.7788 Loss_G: 0.8082 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7700 Loss_G: 0.8248 acc: 60.9%\n",
      "[BATCH 44/149] Loss_D: 0.7547 Loss_G: 0.8328 acc: 59.4%\n",
      "[BATCH 45/149] Loss_D: 0.7793 Loss_G: 0.8216 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.7238 Loss_G: 0.8025 acc: 64.1%\n",
      "[BATCH 47/149] Loss_D: 0.8127 Loss_G: 0.8087 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.7495 Loss_G: 0.7893 acc: 76.6%\n",
      "[BATCH 49/149] Loss_D: 0.7341 Loss_G: 0.7704 acc: 67.2%\n",
      "[BATCH 50/149] Loss_D: 0.7709 Loss_G: 0.7912 acc: 62.5%\n",
      "[BATCH 51/149] Loss_D: 0.7435 Loss_G: 0.7769 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.7241 Loss_G: 0.7761 acc: 59.4%\n",
      "[BATCH 53/149] Loss_D: 0.7295 Loss_G: 0.7668 acc: 67.2%\n",
      "[BATCH 54/149] Loss_D: 0.7296 Loss_G: 0.7778 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.7341 Loss_G: 0.7728 acc: 70.3%\n",
      "[BATCH 56/149] Loss_D: 0.6991 Loss_G: 0.7587 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.7492 Loss_G: 0.7776 acc: 78.1%\n",
      "[BATCH 58/149] Loss_D: 0.7721 Loss_G: 0.7846 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7450 Loss_G: 0.7884 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7606 Loss_G: 0.7842 acc: 70.3%\n",
      "[BATCH 61/149] Loss_D: 0.7239 Loss_G: 0.7946 acc: 59.4%\n",
      "[BATCH 62/149] Loss_D: 0.7557 Loss_G: 0.7944 acc: 78.1%\n",
      "[BATCH 63/149] Loss_D: 0.7905 Loss_G: 0.8113 acc: 64.1%\n",
      "[BATCH 64/149] Loss_D: 0.7220 Loss_G: 0.7963 acc: 57.8%\n",
      "[EPOCH 2150] TEST ACC is : 66.0%\n",
      "[BATCH 65/149] Loss_D: 0.7329 Loss_G: 0.7823 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.7542 Loss_G: 0.7858 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7619 Loss_G: 0.8050 acc: 73.4%\n",
      "[BATCH 68/149] Loss_D: 0.7580 Loss_G: 0.8193 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.7385 Loss_G: 0.7977 acc: 60.9%\n",
      "[BATCH 70/149] Loss_D: 0.8212 Loss_G: 0.8221 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.7501 Loss_G: 0.7923 acc: 70.3%\n",
      "[BATCH 72/149] Loss_D: 0.7108 Loss_G: 0.7796 acc: 56.2%\n",
      "[BATCH 73/149] Loss_D: 0.7479 Loss_G: 0.7770 acc: 81.2%\n",
      "[BATCH 74/149] Loss_D: 0.7581 Loss_G: 0.7730 acc: 57.8%\n",
      "[BATCH 75/149] Loss_D: 0.7640 Loss_G: 0.7857 acc: 62.5%\n",
      "[BATCH 76/149] Loss_D: 0.7235 Loss_G: 0.7709 acc: 59.4%\n",
      "[BATCH 77/149] Loss_D: 0.7316 Loss_G: 0.7664 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.8189 Loss_G: 0.8058 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.7525 Loss_G: 0.7948 acc: 64.1%\n",
      "[BATCH 80/149] Loss_D: 0.7652 Loss_G: 0.7957 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7279 Loss_G: 0.7796 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.7369 Loss_G: 0.7682 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7413 Loss_G: 0.7750 acc: 65.6%\n",
      "[BATCH 84/149] Loss_D: 0.7716 Loss_G: 0.7940 acc: 54.7%\n",
      "[BATCH 85/149] Loss_D: 0.7699 Loss_G: 0.8025 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.7412 Loss_G: 0.7991 acc: 70.3%\n",
      "[BATCH 87/149] Loss_D: 0.7399 Loss_G: 0.8033 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7598 Loss_G: 0.7993 acc: 67.2%\n",
      "[BATCH 89/149] Loss_D: 0.7146 Loss_G: 0.7866 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7243 Loss_G: 0.7773 acc: 56.2%\n",
      "[BATCH 91/149] Loss_D: 0.7548 Loss_G: 0.7780 acc: 73.4%\n",
      "[BATCH 92/149] Loss_D: 0.7304 Loss_G: 0.7842 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.8107 Loss_G: 0.8010 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7169 Loss_G: 0.7759 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.7477 Loss_G: 0.7887 acc: 62.5%\n",
      "[BATCH 96/149] Loss_D: 0.7222 Loss_G: 0.7723 acc: 73.4%\n",
      "[BATCH 97/149] Loss_D: 0.7840 Loss_G: 0.7880 acc: 71.9%\n",
      "[BATCH 98/149] Loss_D: 0.7296 Loss_G: 0.7971 acc: 60.9%\n",
      "[BATCH 99/149] Loss_D: 0.7726 Loss_G: 0.8019 acc: 68.8%\n",
      "[BATCH 100/149] Loss_D: 0.7415 Loss_G: 0.7889 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7611 Loss_G: 0.7867 acc: 73.4%\n",
      "[BATCH 102/149] Loss_D: 0.7206 Loss_G: 0.7749 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.7511 Loss_G: 0.7778 acc: 76.6%\n",
      "[BATCH 104/149] Loss_D: 0.7587 Loss_G: 0.7938 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7786 Loss_G: 0.7997 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7580 Loss_G: 0.8070 acc: 78.1%\n",
      "[BATCH 107/149] Loss_D: 0.7441 Loss_G: 0.8054 acc: 51.6%\n",
      "[BATCH 108/149] Loss_D: 0.7140 Loss_G: 0.7722 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.7881 Loss_G: 0.7859 acc: 68.8%\n",
      "[BATCH 110/149] Loss_D: 0.7579 Loss_G: 0.7942 acc: 59.4%\n",
      "[BATCH 111/149] Loss_D: 0.7882 Loss_G: 0.8133 acc: 59.4%\n",
      "[BATCH 112/149] Loss_D: 0.7230 Loss_G: 0.7940 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7775 Loss_G: 0.8123 acc: 57.8%\n",
      "[BATCH 114/149] Loss_D: 0.7880 Loss_G: 0.8023 acc: 56.2%\n",
      "[EPOCH 2200] TEST ACC is : 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.7891 Loss_G: 0.7953 acc: 71.9%\n",
      "[BATCH 116/149] Loss_D: 0.7697 Loss_G: 0.7935 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7876 Loss_G: 0.8125 acc: 70.3%\n",
      "[BATCH 118/149] Loss_D: 0.7450 Loss_G: 0.8032 acc: 68.8%\n",
      "[BATCH 119/149] Loss_D: 0.7625 Loss_G: 0.7829 acc: 73.4%\n",
      "[BATCH 120/149] Loss_D: 0.7691 Loss_G: 0.7960 acc: 59.4%\n",
      "[BATCH 121/149] Loss_D: 0.7444 Loss_G: 0.7923 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.7579 Loss_G: 0.7906 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7688 Loss_G: 0.7865 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.7393 Loss_G: 0.7855 acc: 53.1%\n",
      "[BATCH 125/149] Loss_D: 0.7510 Loss_G: 0.7821 acc: 67.2%\n",
      "[BATCH 126/149] Loss_D: 0.7319 Loss_G: 0.7713 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7417 Loss_G: 0.7767 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7565 Loss_G: 0.7849 acc: 62.5%\n",
      "[BATCH 129/149] Loss_D: 0.7469 Loss_G: 0.7833 acc: 75.0%\n",
      "[BATCH 130/149] Loss_D: 0.7747 Loss_G: 0.7965 acc: 64.1%\n",
      "[BATCH 131/149] Loss_D: 0.7855 Loss_G: 0.8004 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.7773 Loss_G: 0.8220 acc: 62.5%\n",
      "[BATCH 133/149] Loss_D: 0.7246 Loss_G: 0.8006 acc: 73.4%\n",
      "[BATCH 134/149] Loss_D: 0.7984 Loss_G: 0.8141 acc: 53.1%\n",
      "[BATCH 135/149] Loss_D: 0.7990 Loss_G: 0.8214 acc: 71.9%\n",
      "[BATCH 136/149] Loss_D: 0.7608 Loss_G: 0.8020 acc: 70.3%\n",
      "[BATCH 137/149] Loss_D: 0.7732 Loss_G: 0.8154 acc: 71.9%\n",
      "[BATCH 138/149] Loss_D: 0.7248 Loss_G: 0.7892 acc: 70.3%\n",
      "[BATCH 139/149] Loss_D: 0.7548 Loss_G: 0.7853 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.7964 Loss_G: 0.7850 acc: 76.6%\n",
      "[BATCH 141/149] Loss_D: 0.7700 Loss_G: 0.7969 acc: 65.6%\n",
      "[BATCH 142/149] Loss_D: 0.7493 Loss_G: 0.7842 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.7718 Loss_G: 0.7981 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7940 Loss_G: 0.8226 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.8096 Loss_G: 0.8182 acc: 70.3%\n",
      "[BATCH 146/149] Loss_D: 0.7670 Loss_G: 0.8123 acc: 56.2%\n",
      "[BATCH 147/149] Loss_D: 0.7324 Loss_G: 0.7903 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.7397 Loss_G: 0.8008 acc: 78.1%\n",
      "[BATCH 149/149] Loss_D: 0.7605 Loss_G: 0.8171 acc: 75.0%\n",
      "-----THE [15/50] epoch end-----\n",
      "-----THE [16/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7978 Loss_G: 0.8254 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.7633 Loss_G: 0.8036 acc: 70.3%\n",
      "[BATCH 3/149] Loss_D: 0.7637 Loss_G: 0.8044 acc: 64.1%\n",
      "[BATCH 4/149] Loss_D: 0.7639 Loss_G: 0.8009 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.7807 Loss_G: 0.7798 acc: 76.6%\n",
      "[BATCH 6/149] Loss_D: 0.7308 Loss_G: 0.7774 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.7636 Loss_G: 0.7874 acc: 68.8%\n",
      "[BATCH 8/149] Loss_D: 0.7361 Loss_G: 0.7925 acc: 70.3%\n",
      "[BATCH 9/149] Loss_D: 0.7676 Loss_G: 0.7834 acc: 62.5%\n",
      "[BATCH 10/149] Loss_D: 0.7468 Loss_G: 0.7876 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.7930 Loss_G: 0.7938 acc: 68.8%\n",
      "[BATCH 12/149] Loss_D: 0.7877 Loss_G: 0.7993 acc: 68.8%\n",
      "[BATCH 13/149] Loss_D: 0.7722 Loss_G: 0.7979 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7518 Loss_G: 0.8023 acc: 64.1%\n",
      "[BATCH 15/149] Loss_D: 0.7844 Loss_G: 0.7942 acc: 67.2%\n",
      "[EPOCH 2250] TEST ACC is : 65.8%\n",
      "[BATCH 16/149] Loss_D: 0.7854 Loss_G: 0.8192 acc: 56.2%\n",
      "[BATCH 17/149] Loss_D: 0.7614 Loss_G: 0.8061 acc: 60.9%\n",
      "[BATCH 18/149] Loss_D: 0.7334 Loss_G: 0.7817 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7670 Loss_G: 0.7869 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7561 Loss_G: 0.8064 acc: 65.6%\n",
      "[BATCH 21/149] Loss_D: 0.7619 Loss_G: 0.8061 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7831 Loss_G: 0.8190 acc: 73.4%\n",
      "[BATCH 23/149] Loss_D: 0.7764 Loss_G: 0.8264 acc: 65.6%\n",
      "[BATCH 24/149] Loss_D: 0.7367 Loss_G: 0.8005 acc: 68.8%\n",
      "[BATCH 25/149] Loss_D: 0.7303 Loss_G: 0.7872 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7070 Loss_G: 0.7777 acc: 65.6%\n",
      "[BATCH 27/149] Loss_D: 0.7396 Loss_G: 0.8000 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.7423 Loss_G: 0.7854 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.7248 Loss_G: 0.7847 acc: 73.4%\n",
      "[BATCH 30/149] Loss_D: 0.7700 Loss_G: 0.8118 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.8379 Loss_G: 0.8377 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.7674 Loss_G: 0.8131 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.8063 Loss_G: 0.8234 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.7726 Loss_G: 0.8119 acc: 70.3%\n",
      "[BATCH 35/149] Loss_D: 0.7238 Loss_G: 0.7912 acc: 76.6%\n",
      "[BATCH 36/149] Loss_D: 0.7341 Loss_G: 0.7856 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7245 Loss_G: 0.7700 acc: 70.3%\n",
      "[BATCH 38/149] Loss_D: 0.7153 Loss_G: 0.7672 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.7781 Loss_G: 0.7832 acc: 75.0%\n",
      "[BATCH 40/149] Loss_D: 0.7835 Loss_G: 0.8037 acc: 60.9%\n",
      "[BATCH 41/149] Loss_D: 0.7316 Loss_G: 0.7935 acc: 76.6%\n",
      "[BATCH 42/149] Loss_D: 0.7269 Loss_G: 0.7839 acc: 75.0%\n",
      "[BATCH 43/149] Loss_D: 0.7287 Loss_G: 0.7774 acc: 60.9%\n",
      "[BATCH 44/149] Loss_D: 0.7854 Loss_G: 0.7883 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7674 Loss_G: 0.7902 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.7229 Loss_G: 0.7770 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7890 Loss_G: 0.7979 acc: 60.9%\n",
      "[BATCH 48/149] Loss_D: 0.7780 Loss_G: 0.8080 acc: 59.4%\n",
      "[BATCH 49/149] Loss_D: 0.7432 Loss_G: 0.7871 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7510 Loss_G: 0.7896 acc: 59.4%\n",
      "[BATCH 51/149] Loss_D: 0.7898 Loss_G: 0.8063 acc: 60.9%\n",
      "[BATCH 52/149] Loss_D: 0.7320 Loss_G: 0.8059 acc: 67.2%\n",
      "[BATCH 53/149] Loss_D: 0.7169 Loss_G: 0.7944 acc: 71.9%\n",
      "[BATCH 54/149] Loss_D: 0.7846 Loss_G: 0.7961 acc: 62.5%\n",
      "[BATCH 55/149] Loss_D: 0.7564 Loss_G: 0.8025 acc: 78.1%\n",
      "[BATCH 56/149] Loss_D: 0.7376 Loss_G: 0.7901 acc: 78.1%\n",
      "[BATCH 57/149] Loss_D: 0.7843 Loss_G: 0.8160 acc: 62.5%\n",
      "[BATCH 58/149] Loss_D: 0.7542 Loss_G: 0.8031 acc: 64.1%\n",
      "[BATCH 59/149] Loss_D: 0.7601 Loss_G: 0.7859 acc: 82.8%\n",
      "[BATCH 60/149] Loss_D: 0.7437 Loss_G: 0.7967 acc: 71.9%\n",
      "[BATCH 61/149] Loss_D: 0.7056 Loss_G: 0.7865 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.7456 Loss_G: 0.7766 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7454 Loss_G: 0.7716 acc: 75.0%\n",
      "[BATCH 64/149] Loss_D: 0.7104 Loss_G: 0.7682 acc: 65.6%\n",
      "[BATCH 65/149] Loss_D: 0.7338 Loss_G: 0.7818 acc: 65.6%\n",
      "[EPOCH 2300] TEST ACC is : 64.3%\n",
      "[BATCH 66/149] Loss_D: 0.7368 Loss_G: 0.7820 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7496 Loss_G: 0.7785 acc: 71.9%\n",
      "[BATCH 68/149] Loss_D: 0.7410 Loss_G: 0.7919 acc: 59.4%\n",
      "[BATCH 69/149] Loss_D: 0.7228 Loss_G: 0.7783 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.7860 Loss_G: 0.7917 acc: 70.3%\n",
      "[BATCH 71/149] Loss_D: 0.7373 Loss_G: 0.7807 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7546 Loss_G: 0.7774 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7922 Loss_G: 0.7889 acc: 57.8%\n",
      "[BATCH 74/149] Loss_D: 0.8213 Loss_G: 0.8126 acc: 60.9%\n",
      "[BATCH 75/149] Loss_D: 0.7529 Loss_G: 0.7967 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.7464 Loss_G: 0.7857 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.7804 Loss_G: 0.8021 acc: 50.0%\n",
      "[BATCH 78/149] Loss_D: 0.7514 Loss_G: 0.7697 acc: 76.6%\n",
      "[BATCH 79/149] Loss_D: 0.7608 Loss_G: 0.7828 acc: 78.1%\n",
      "[BATCH 80/149] Loss_D: 0.7494 Loss_G: 0.8064 acc: 57.8%\n",
      "[BATCH 81/149] Loss_D: 0.7775 Loss_G: 0.8164 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.7316 Loss_G: 0.8028 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.7402 Loss_G: 0.7906 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.7651 Loss_G: 0.7993 acc: 67.2%\n",
      "[BATCH 85/149] Loss_D: 0.7290 Loss_G: 0.7930 acc: 64.1%\n",
      "[BATCH 86/149] Loss_D: 0.7008 Loss_G: 0.7825 acc: 75.0%\n",
      "[BATCH 87/149] Loss_D: 0.7473 Loss_G: 0.7877 acc: 68.8%\n",
      "[BATCH 88/149] Loss_D: 0.7385 Loss_G: 0.7906 acc: 73.4%\n",
      "[BATCH 89/149] Loss_D: 0.7647 Loss_G: 0.7962 acc: 65.6%\n",
      "[BATCH 90/149] Loss_D: 0.7614 Loss_G: 0.8014 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7719 Loss_G: 0.7907 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.7729 Loss_G: 0.7974 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.7541 Loss_G: 0.7861 acc: 76.6%\n",
      "[BATCH 94/149] Loss_D: 0.7751 Loss_G: 0.7916 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7121 Loss_G: 0.7808 acc: 60.9%\n",
      "[BATCH 96/149] Loss_D: 0.7705 Loss_G: 0.7819 acc: 67.2%\n",
      "[BATCH 97/149] Loss_D: 0.8019 Loss_G: 0.7844 acc: 70.3%\n",
      "[BATCH 98/149] Loss_D: 0.7415 Loss_G: 0.7834 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7763 Loss_G: 0.7819 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.7477 Loss_G: 0.7694 acc: 76.6%\n",
      "[BATCH 101/149] Loss_D: 0.7344 Loss_G: 0.7783 acc: 76.6%\n",
      "[BATCH 102/149] Loss_D: 0.7387 Loss_G: 0.7762 acc: 62.5%\n",
      "[BATCH 103/149] Loss_D: 0.7565 Loss_G: 0.7887 acc: 62.5%\n",
      "[BATCH 104/149] Loss_D: 0.7607 Loss_G: 0.7869 acc: 68.8%\n",
      "[BATCH 105/149] Loss_D: 0.7273 Loss_G: 0.7706 acc: 70.3%\n",
      "[BATCH 106/149] Loss_D: 0.7531 Loss_G: 0.7771 acc: 65.6%\n",
      "[BATCH 107/149] Loss_D: 0.7098 Loss_G: 0.7578 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.7382 Loss_G: 0.7591 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.7497 Loss_G: 0.7756 acc: 62.5%\n",
      "[BATCH 110/149] Loss_D: 0.7670 Loss_G: 0.7990 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7744 Loss_G: 0.8028 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7574 Loss_G: 0.7973 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.7788 Loss_G: 0.8140 acc: 64.1%\n",
      "[BATCH 114/149] Loss_D: 0.7259 Loss_G: 0.7851 acc: 67.2%\n",
      "[BATCH 115/149] Loss_D: 0.7409 Loss_G: 0.7836 acc: 75.0%\n",
      "[EPOCH 2350] TEST ACC is : 64.8%\n",
      "[BATCH 116/149] Loss_D: 0.7318 Loss_G: 0.7876 acc: 65.6%\n",
      "[BATCH 117/149] Loss_D: 0.7583 Loss_G: 0.7793 acc: 73.4%\n",
      "[BATCH 118/149] Loss_D: 0.7589 Loss_G: 0.7943 acc: 53.1%\n",
      "[BATCH 119/149] Loss_D: 0.7641 Loss_G: 0.7995 acc: 76.6%\n",
      "[BATCH 120/149] Loss_D: 0.7517 Loss_G: 0.7947 acc: 79.7%\n",
      "[BATCH 121/149] Loss_D: 0.7343 Loss_G: 0.7876 acc: 70.3%\n",
      "[BATCH 122/149] Loss_D: 0.7884 Loss_G: 0.8039 acc: 62.5%\n",
      "[BATCH 123/149] Loss_D: 0.7459 Loss_G: 0.8141 acc: 60.9%\n",
      "[BATCH 124/149] Loss_D: 0.7338 Loss_G: 0.8055 acc: 68.8%\n",
      "[BATCH 125/149] Loss_D: 0.7765 Loss_G: 0.8106 acc: 64.1%\n",
      "[BATCH 126/149] Loss_D: 0.7799 Loss_G: 0.7970 acc: 68.8%\n",
      "[BATCH 127/149] Loss_D: 0.7563 Loss_G: 0.7878 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.7413 Loss_G: 0.7800 acc: 71.9%\n",
      "[BATCH 129/149] Loss_D: 0.7695 Loss_G: 0.7968 acc: 64.1%\n",
      "[BATCH 130/149] Loss_D: 0.7892 Loss_G: 0.8065 acc: 68.8%\n",
      "[BATCH 131/149] Loss_D: 0.7464 Loss_G: 0.7897 acc: 62.5%\n",
      "[BATCH 132/149] Loss_D: 0.7665 Loss_G: 0.8042 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7519 Loss_G: 0.7960 acc: 64.1%\n",
      "[BATCH 134/149] Loss_D: 0.7272 Loss_G: 0.7937 acc: 76.6%\n",
      "[BATCH 135/149] Loss_D: 0.6995 Loss_G: 0.7770 acc: 68.8%\n",
      "[BATCH 136/149] Loss_D: 0.7493 Loss_G: 0.7934 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.7559 Loss_G: 0.7995 acc: 75.0%\n",
      "[BATCH 138/149] Loss_D: 0.7133 Loss_G: 0.7781 acc: 73.4%\n",
      "[BATCH 139/149] Loss_D: 0.8169 Loss_G: 0.8117 acc: 62.5%\n",
      "[BATCH 140/149] Loss_D: 0.8161 Loss_G: 0.8264 acc: 76.6%\n",
      "[BATCH 141/149] Loss_D: 0.7615 Loss_G: 0.8005 acc: 71.9%\n",
      "[BATCH 142/149] Loss_D: 0.7025 Loss_G: 0.7699 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7387 Loss_G: 0.7896 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7406 Loss_G: 0.7823 acc: 67.2%\n",
      "[BATCH 145/149] Loss_D: 0.7336 Loss_G: 0.7822 acc: 76.6%\n",
      "[BATCH 146/149] Loss_D: 0.7521 Loss_G: 0.7815 acc: 68.8%\n",
      "[BATCH 147/149] Loss_D: 0.7737 Loss_G: 0.7991 acc: 62.5%\n",
      "[BATCH 148/149] Loss_D: 0.7304 Loss_G: 0.7725 acc: 64.1%\n",
      "[BATCH 149/149] Loss_D: 0.7351 Loss_G: 0.7885 acc: 64.1%\n",
      "-----THE [16/50] epoch end-----\n",
      "-----THE [17/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7124 Loss_G: 0.7693 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.6977 Loss_G: 0.7639 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7509 Loss_G: 0.7774 acc: 78.1%\n",
      "[BATCH 4/149] Loss_D: 0.7425 Loss_G: 0.7871 acc: 62.5%\n",
      "[BATCH 5/149] Loss_D: 0.7312 Loss_G: 0.7613 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7458 Loss_G: 0.7555 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.7345 Loss_G: 0.7715 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.8051 Loss_G: 0.7970 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.7631 Loss_G: 0.7820 acc: 76.6%\n",
      "[BATCH 10/149] Loss_D: 0.7273 Loss_G: 0.7789 acc: 68.8%\n",
      "[BATCH 11/149] Loss_D: 0.7878 Loss_G: 0.7948 acc: 59.4%\n",
      "[BATCH 12/149] Loss_D: 0.7985 Loss_G: 0.7936 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7489 Loss_G: 0.7870 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7253 Loss_G: 0.7746 acc: 71.9%\n",
      "[BATCH 15/149] Loss_D: 0.7091 Loss_G: 0.7549 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.7329 Loss_G: 0.7708 acc: 67.2%\n",
      "[EPOCH 2400] TEST ACC is : 64.3%\n",
      "[BATCH 17/149] Loss_D: 0.7769 Loss_G: 0.7900 acc: 64.1%\n",
      "[BATCH 18/149] Loss_D: 0.7180 Loss_G: 0.7769 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.7478 Loss_G: 0.7963 acc: 59.4%\n",
      "[BATCH 20/149] Loss_D: 0.7480 Loss_G: 0.7968 acc: 65.6%\n",
      "[BATCH 21/149] Loss_D: 0.7424 Loss_G: 0.7941 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7982 Loss_G: 0.8062 acc: 64.1%\n",
      "[BATCH 23/149] Loss_D: 0.7638 Loss_G: 0.8020 acc: 57.8%\n",
      "[BATCH 24/149] Loss_D: 0.7140 Loss_G: 0.7854 acc: 70.3%\n",
      "[BATCH 25/149] Loss_D: 0.8183 Loss_G: 0.8095 acc: 57.8%\n",
      "[BATCH 26/149] Loss_D: 0.7234 Loss_G: 0.7769 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.7596 Loss_G: 0.7957 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7673 Loss_G: 0.7972 acc: 68.8%\n",
      "[BATCH 29/149] Loss_D: 0.7257 Loss_G: 0.7869 acc: 59.4%\n",
      "[BATCH 30/149] Loss_D: 0.7831 Loss_G: 0.7747 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.7429 Loss_G: 0.7846 acc: 76.6%\n",
      "[BATCH 32/149] Loss_D: 0.7343 Loss_G: 0.8002 acc: 59.4%\n",
      "[BATCH 33/149] Loss_D: 0.7400 Loss_G: 0.7997 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.7638 Loss_G: 0.7913 acc: 73.4%\n",
      "[BATCH 35/149] Loss_D: 0.7559 Loss_G: 0.8022 acc: 76.6%\n",
      "[BATCH 36/149] Loss_D: 0.7672 Loss_G: 0.8038 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.7412 Loss_G: 0.7832 acc: 68.8%\n",
      "[BATCH 38/149] Loss_D: 0.7419 Loss_G: 0.7915 acc: 76.6%\n",
      "[BATCH 39/149] Loss_D: 0.7555 Loss_G: 0.7993 acc: 70.3%\n",
      "[BATCH 40/149] Loss_D: 0.7288 Loss_G: 0.7851 acc: 76.6%\n",
      "[BATCH 41/149] Loss_D: 0.7506 Loss_G: 0.7972 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7424 Loss_G: 0.7940 acc: 73.4%\n",
      "[BATCH 43/149] Loss_D: 0.7049 Loss_G: 0.7705 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.7497 Loss_G: 0.7835 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7303 Loss_G: 0.7704 acc: 73.4%\n",
      "[BATCH 46/149] Loss_D: 0.7502 Loss_G: 0.7823 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7464 Loss_G: 0.7810 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.7755 Loss_G: 0.7888 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.7591 Loss_G: 0.7913 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7392 Loss_G: 0.7979 acc: 70.3%\n",
      "[BATCH 51/149] Loss_D: 0.7945 Loss_G: 0.8273 acc: 54.7%\n",
      "[BATCH 52/149] Loss_D: 0.7676 Loss_G: 0.7966 acc: 62.5%\n",
      "[BATCH 53/149] Loss_D: 0.7163 Loss_G: 0.7868 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.7280 Loss_G: 0.7702 acc: 75.0%\n",
      "[BATCH 55/149] Loss_D: 0.7312 Loss_G: 0.7774 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.7702 Loss_G: 0.8025 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7260 Loss_G: 0.7901 acc: 62.5%\n",
      "[BATCH 58/149] Loss_D: 0.7387 Loss_G: 0.7847 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7392 Loss_G: 0.7816 acc: 62.5%\n",
      "[BATCH 60/149] Loss_D: 0.7770 Loss_G: 0.7849 acc: 75.0%\n",
      "[BATCH 61/149] Loss_D: 0.7611 Loss_G: 0.8023 acc: 76.6%\n",
      "[BATCH 62/149] Loss_D: 0.7498 Loss_G: 0.7811 acc: 75.0%\n",
      "[BATCH 63/149] Loss_D: 0.7255 Loss_G: 0.7802 acc: 79.7%\n",
      "[BATCH 64/149] Loss_D: 0.7582 Loss_G: 0.7935 acc: 64.1%\n",
      "[BATCH 65/149] Loss_D: 0.7858 Loss_G: 0.8098 acc: 60.9%\n",
      "[BATCH 66/149] Loss_D: 0.7614 Loss_G: 0.7943 acc: 68.8%\n",
      "[EPOCH 2450] TEST ACC is : 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7996 Loss_G: 0.7942 acc: 73.4%\n",
      "[BATCH 68/149] Loss_D: 0.7696 Loss_G: 0.8119 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.7519 Loss_G: 0.8018 acc: 67.2%\n",
      "[BATCH 70/149] Loss_D: 0.7642 Loss_G: 0.7974 acc: 76.6%\n",
      "[BATCH 71/149] Loss_D: 0.7532 Loss_G: 0.7962 acc: 67.2%\n",
      "[BATCH 72/149] Loss_D: 0.7606 Loss_G: 0.7853 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.7803 Loss_G: 0.7973 acc: 73.4%\n",
      "[BATCH 74/149] Loss_D: 0.7193 Loss_G: 0.7884 acc: 67.2%\n",
      "[BATCH 75/149] Loss_D: 0.7994 Loss_G: 0.8129 acc: 68.8%\n",
      "[BATCH 76/149] Loss_D: 0.7102 Loss_G: 0.7925 acc: 78.1%\n",
      "[BATCH 77/149] Loss_D: 0.7707 Loss_G: 0.7864 acc: 68.8%\n",
      "[BATCH 78/149] Loss_D: 0.7250 Loss_G: 0.7738 acc: 60.9%\n",
      "[BATCH 79/149] Loss_D: 0.8108 Loss_G: 0.7976 acc: 79.7%\n",
      "[BATCH 80/149] Loss_D: 0.7248 Loss_G: 0.7875 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.7294 Loss_G: 0.7865 acc: 59.4%\n",
      "[BATCH 82/149] Loss_D: 0.7768 Loss_G: 0.7860 acc: 70.3%\n",
      "[BATCH 83/149] Loss_D: 0.8097 Loss_G: 0.8047 acc: 62.5%\n",
      "[BATCH 84/149] Loss_D: 0.7286 Loss_G: 0.7855 acc: 79.7%\n",
      "[BATCH 85/149] Loss_D: 0.7605 Loss_G: 0.7887 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7453 Loss_G: 0.7858 acc: 65.6%\n",
      "[BATCH 87/149] Loss_D: 0.7347 Loss_G: 0.7905 acc: 78.1%\n",
      "[BATCH 88/149] Loss_D: 0.7843 Loss_G: 0.8045 acc: 65.6%\n",
      "[BATCH 89/149] Loss_D: 0.7674 Loss_G: 0.7971 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7010 Loss_G: 0.7766 acc: 60.9%\n",
      "[BATCH 91/149] Loss_D: 0.7404 Loss_G: 0.7859 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7110 Loss_G: 0.7831 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.8007 Loss_G: 0.8009 acc: 65.6%\n",
      "[BATCH 94/149] Loss_D: 0.7427 Loss_G: 0.7897 acc: 62.5%\n",
      "[BATCH 95/149] Loss_D: 0.7915 Loss_G: 0.7857 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.7545 Loss_G: 0.7918 acc: 75.0%\n",
      "[BATCH 97/149] Loss_D: 0.7945 Loss_G: 0.8141 acc: 71.9%\n",
      "[BATCH 98/149] Loss_D: 0.7874 Loss_G: 0.8153 acc: 73.4%\n",
      "[BATCH 99/149] Loss_D: 0.7434 Loss_G: 0.7998 acc: 68.8%\n",
      "[BATCH 100/149] Loss_D: 0.7131 Loss_G: 0.7732 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7661 Loss_G: 0.7884 acc: 75.0%\n",
      "[BATCH 102/149] Loss_D: 0.7523 Loss_G: 0.7895 acc: 71.9%\n",
      "[BATCH 103/149] Loss_D: 0.7215 Loss_G: 0.7800 acc: 65.6%\n",
      "[BATCH 104/149] Loss_D: 0.7692 Loss_G: 0.7756 acc: 70.3%\n",
      "[BATCH 105/149] Loss_D: 0.7549 Loss_G: 0.8025 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7310 Loss_G: 0.7750 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.7772 Loss_G: 0.7852 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.7837 Loss_G: 0.7904 acc: 76.6%\n",
      "[BATCH 109/149] Loss_D: 0.7016 Loss_G: 0.7911 acc: 60.9%\n",
      "[BATCH 110/149] Loss_D: 0.7181 Loss_G: 0.7959 acc: 62.5%\n",
      "[BATCH 111/149] Loss_D: 0.7736 Loss_G: 0.7761 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7554 Loss_G: 0.7820 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7519 Loss_G: 0.7724 acc: 73.4%\n",
      "[BATCH 114/149] Loss_D: 0.7458 Loss_G: 0.7717 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7898 Loss_G: 0.7973 acc: 62.5%\n",
      "[BATCH 116/149] Loss_D: 0.7720 Loss_G: 0.8032 acc: 65.6%\n",
      "[EPOCH 2500] TEST ACC is : 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7367 Loss_G: 0.7974 acc: 62.5%\n",
      "[BATCH 118/149] Loss_D: 0.7184 Loss_G: 0.7685 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7134 Loss_G: 0.7862 acc: 73.4%\n",
      "[BATCH 120/149] Loss_D: 0.7560 Loss_G: 0.7895 acc: 65.6%\n",
      "[BATCH 121/149] Loss_D: 0.7770 Loss_G: 0.7923 acc: 67.2%\n",
      "[BATCH 122/149] Loss_D: 0.7397 Loss_G: 0.7954 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.7794 Loss_G: 0.8079 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.7893 Loss_G: 0.8099 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.7913 Loss_G: 0.8008 acc: 71.9%\n",
      "[BATCH 126/149] Loss_D: 0.7277 Loss_G: 0.7790 acc: 81.2%\n",
      "[BATCH 127/149] Loss_D: 0.7705 Loss_G: 0.7915 acc: 76.6%\n",
      "[BATCH 128/149] Loss_D: 0.7515 Loss_G: 0.7858 acc: 65.6%\n",
      "[BATCH 129/149] Loss_D: 0.7861 Loss_G: 0.7938 acc: 68.8%\n",
      "[BATCH 130/149] Loss_D: 0.7604 Loss_G: 0.7883 acc: 62.5%\n",
      "[BATCH 131/149] Loss_D: 0.7244 Loss_G: 0.7715 acc: 75.0%\n",
      "[BATCH 132/149] Loss_D: 0.7121 Loss_G: 0.7639 acc: 81.2%\n",
      "[BATCH 133/149] Loss_D: 0.7414 Loss_G: 0.7704 acc: 70.3%\n",
      "[BATCH 134/149] Loss_D: 0.7172 Loss_G: 0.7691 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.7983 Loss_G: 0.8076 acc: 78.1%\n",
      "[BATCH 136/149] Loss_D: 0.7644 Loss_G: 0.7986 acc: 64.1%\n",
      "[BATCH 137/149] Loss_D: 0.7184 Loss_G: 0.7790 acc: 73.4%\n",
      "[BATCH 138/149] Loss_D: 0.7195 Loss_G: 0.7637 acc: 70.3%\n",
      "[BATCH 139/149] Loss_D: 0.7658 Loss_G: 0.7794 acc: 67.2%\n",
      "[BATCH 140/149] Loss_D: 0.7231 Loss_G: 0.7816 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.6969 Loss_G: 0.7678 acc: 73.4%\n",
      "[BATCH 142/149] Loss_D: 0.7225 Loss_G: 0.7760 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7453 Loss_G: 0.7939 acc: 68.8%\n",
      "[BATCH 144/149] Loss_D: 0.7442 Loss_G: 0.8046 acc: 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.7790 Loss_G: 0.8157 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7116 Loss_G: 0.7933 acc: 71.9%\n",
      "[BATCH 147/149] Loss_D: 0.7472 Loss_G: 0.7931 acc: 71.9%\n",
      "[BATCH 148/149] Loss_D: 0.7845 Loss_G: 0.7998 acc: 57.8%\n",
      "[BATCH 149/149] Loss_D: 0.7697 Loss_G: 0.7936 acc: 78.1%\n",
      "-----THE [17/50] epoch end-----\n",
      "-----THE [18/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7869 Loss_G: 0.7967 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.7553 Loss_G: 0.7834 acc: 78.1%\n",
      "[BATCH 3/149] Loss_D: 0.7254 Loss_G: 0.7725 acc: 70.3%\n",
      "[BATCH 4/149] Loss_D: 0.7222 Loss_G: 0.7609 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.8160 Loss_G: 0.7878 acc: 67.2%\n",
      "[BATCH 6/149] Loss_D: 0.7457 Loss_G: 0.7827 acc: 76.6%\n",
      "[BATCH 7/149] Loss_D: 0.7446 Loss_G: 0.7827 acc: 75.0%\n",
      "[BATCH 8/149] Loss_D: 0.7670 Loss_G: 0.8087 acc: 57.8%\n",
      "[BATCH 9/149] Loss_D: 0.7491 Loss_G: 0.8017 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7500 Loss_G: 0.7899 acc: 51.6%\n",
      "[BATCH 11/149] Loss_D: 0.6988 Loss_G: 0.7734 acc: 67.2%\n",
      "[BATCH 12/149] Loss_D: 0.7500 Loss_G: 0.7774 acc: 65.6%\n",
      "[BATCH 13/149] Loss_D: 0.7560 Loss_G: 0.7813 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7746 Loss_G: 0.7951 acc: 70.3%\n",
      "[BATCH 15/149] Loss_D: 0.7110 Loss_G: 0.7871 acc: 79.7%\n",
      "[BATCH 16/149] Loss_D: 0.7321 Loss_G: 0.8005 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7589 Loss_G: 0.7980 acc: 71.9%\n",
      "[EPOCH 2550] TEST ACC is : 67.6%\n",
      "[BATCH 18/149] Loss_D: 0.7308 Loss_G: 0.7801 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.7748 Loss_G: 0.7865 acc: 71.9%\n",
      "[BATCH 20/149] Loss_D: 0.7782 Loss_G: 0.8009 acc: 67.2%\n",
      "[BATCH 21/149] Loss_D: 0.7188 Loss_G: 0.7822 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7273 Loss_G: 0.7739 acc: 68.8%\n",
      "[BATCH 23/149] Loss_D: 0.7220 Loss_G: 0.7807 acc: 64.1%\n",
      "[BATCH 24/149] Loss_D: 0.7868 Loss_G: 0.7854 acc: 76.6%\n",
      "[BATCH 25/149] Loss_D: 0.7499 Loss_G: 0.8076 acc: 67.2%\n",
      "[BATCH 26/149] Loss_D: 0.7890 Loss_G: 0.8180 acc: 59.4%\n",
      "[BATCH 27/149] Loss_D: 0.7708 Loss_G: 0.8150 acc: 79.7%\n",
      "[BATCH 28/149] Loss_D: 0.7775 Loss_G: 0.7966 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.8100 Loss_G: 0.7950 acc: 75.0%\n",
      "[BATCH 30/149] Loss_D: 0.7698 Loss_G: 0.8024 acc: 75.0%\n",
      "[BATCH 31/149] Loss_D: 0.7327 Loss_G: 0.7872 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.7537 Loss_G: 0.7919 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.7332 Loss_G: 0.7886 acc: 65.6%\n",
      "[BATCH 34/149] Loss_D: 0.7368 Loss_G: 0.7798 acc: 73.4%\n",
      "[BATCH 35/149] Loss_D: 0.7575 Loss_G: 0.7790 acc: 70.3%\n",
      "[BATCH 36/149] Loss_D: 0.7893 Loss_G: 0.7922 acc: 70.3%\n",
      "[BATCH 37/149] Loss_D: 0.7492 Loss_G: 0.7945 acc: 59.4%\n",
      "[BATCH 38/149] Loss_D: 0.7858 Loss_G: 0.8017 acc: 73.4%\n",
      "[BATCH 39/149] Loss_D: 0.6947 Loss_G: 0.7992 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.7227 Loss_G: 0.7877 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.7399 Loss_G: 0.7715 acc: 79.7%\n",
      "[BATCH 42/149] Loss_D: 0.7419 Loss_G: 0.7783 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7966 Loss_G: 0.8133 acc: 57.8%\n",
      "[BATCH 44/149] Loss_D: 0.7654 Loss_G: 0.8106 acc: 75.0%\n",
      "[BATCH 45/149] Loss_D: 0.7384 Loss_G: 0.8128 acc: 59.4%\n",
      "[BATCH 46/149] Loss_D: 0.7359 Loss_G: 0.7961 acc: 64.1%\n",
      "[BATCH 47/149] Loss_D: 0.7624 Loss_G: 0.7835 acc: 78.1%\n",
      "[BATCH 48/149] Loss_D: 0.7452 Loss_G: 0.7832 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.7686 Loss_G: 0.7977 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.7660 Loss_G: 0.7883 acc: 75.0%\n",
      "[BATCH 51/149] Loss_D: 0.7569 Loss_G: 0.8052 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.7210 Loss_G: 0.7896 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.7662 Loss_G: 0.8111 acc: 64.1%\n",
      "[BATCH 54/149] Loss_D: 0.7722 Loss_G: 0.7959 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7527 Loss_G: 0.7901 acc: 73.4%\n",
      "[BATCH 56/149] Loss_D: 0.7135 Loss_G: 0.7861 acc: 79.7%\n",
      "[BATCH 57/149] Loss_D: 0.7122 Loss_G: 0.7725 acc: 73.4%\n",
      "[BATCH 58/149] Loss_D: 0.6985 Loss_G: 0.7715 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.7768 Loss_G: 0.7954 acc: 65.6%\n",
      "[BATCH 60/149] Loss_D: 0.6987 Loss_G: 0.7722 acc: 65.6%\n",
      "[BATCH 61/149] Loss_D: 0.7357 Loss_G: 0.7766 acc: 71.9%\n",
      "[BATCH 62/149] Loss_D: 0.7597 Loss_G: 0.7775 acc: 65.6%\n",
      "[BATCH 63/149] Loss_D: 0.7367 Loss_G: 0.7738 acc: 73.4%\n",
      "[BATCH 64/149] Loss_D: 0.7206 Loss_G: 0.7820 acc: 68.8%\n",
      "[BATCH 65/149] Loss_D: 0.7831 Loss_G: 0.7947 acc: 73.4%\n",
      "[BATCH 66/149] Loss_D: 0.7454 Loss_G: 0.7912 acc: 60.9%\n",
      "[BATCH 67/149] Loss_D: 0.7193 Loss_G: 0.7928 acc: 75.0%\n",
      "[EPOCH 2600] TEST ACC is : 65.4%\n",
      "[BATCH 68/149] Loss_D: 0.7720 Loss_G: 0.7962 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.7922 Loss_G: 0.8065 acc: 76.6%\n",
      "[BATCH 70/149] Loss_D: 0.7258 Loss_G: 0.7962 acc: 65.6%\n",
      "[BATCH 71/149] Loss_D: 0.8533 Loss_G: 0.8133 acc: 68.8%\n",
      "[BATCH 72/149] Loss_D: 0.7586 Loss_G: 0.8006 acc: 68.8%\n",
      "[BATCH 73/149] Loss_D: 0.7735 Loss_G: 0.7795 acc: 71.9%\n",
      "[BATCH 74/149] Loss_D: 0.7446 Loss_G: 0.7845 acc: 79.7%\n",
      "[BATCH 75/149] Loss_D: 0.7692 Loss_G: 0.7892 acc: 76.6%\n",
      "[BATCH 76/149] Loss_D: 0.7968 Loss_G: 0.8129 acc: 68.8%\n",
      "[BATCH 77/149] Loss_D: 0.7463 Loss_G: 0.7850 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7411 Loss_G: 0.7912 acc: 75.0%\n",
      "[BATCH 79/149] Loss_D: 0.7439 Loss_G: 0.7885 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7350 Loss_G: 0.7673 acc: 62.5%\n",
      "[BATCH 81/149] Loss_D: 0.7567 Loss_G: 0.7823 acc: 70.3%\n",
      "[BATCH 82/149] Loss_D: 0.7301 Loss_G: 0.7755 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.7264 Loss_G: 0.7705 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.7396 Loss_G: 0.7737 acc: 70.3%\n",
      "[BATCH 85/149] Loss_D: 0.7320 Loss_G: 0.7669 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7924 Loss_G: 0.7897 acc: 59.4%\n",
      "[BATCH 87/149] Loss_D: 0.7359 Loss_G: 0.7775 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.7593 Loss_G: 0.7805 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.7668 Loss_G: 0.8032 acc: 56.2%\n",
      "[BATCH 90/149] Loss_D: 0.7596 Loss_G: 0.8002 acc: 73.4%\n",
      "[BATCH 91/149] Loss_D: 0.7233 Loss_G: 0.7872 acc: 67.2%\n",
      "[BATCH 92/149] Loss_D: 0.7685 Loss_G: 0.7930 acc: 73.4%\n",
      "[BATCH 93/149] Loss_D: 0.7545 Loss_G: 0.8000 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7177 Loss_G: 0.7769 acc: 81.2%\n",
      "[BATCH 95/149] Loss_D: 0.7327 Loss_G: 0.7572 acc: 76.6%\n",
      "[BATCH 96/149] Loss_D: 0.7745 Loss_G: 0.7769 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7420 Loss_G: 0.7857 acc: 75.0%\n",
      "[BATCH 98/149] Loss_D: 0.7344 Loss_G: 0.7937 acc: 71.9%\n",
      "[BATCH 99/149] Loss_D: 0.7411 Loss_G: 0.7866 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7440 Loss_G: 0.7937 acc: 68.8%\n",
      "[BATCH 101/149] Loss_D: 0.7427 Loss_G: 0.7921 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.7268 Loss_G: 0.7775 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.7527 Loss_G: 0.7755 acc: 75.0%\n",
      "[BATCH 104/149] Loss_D: 0.7108 Loss_G: 0.7720 acc: 68.8%\n",
      "[BATCH 105/149] Loss_D: 0.7271 Loss_G: 0.7765 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.7071 Loss_G: 0.7665 acc: 57.8%\n",
      "[BATCH 107/149] Loss_D: 0.7450 Loss_G: 0.7805 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.7044 Loss_G: 0.7667 acc: 71.9%\n",
      "[BATCH 109/149] Loss_D: 0.7527 Loss_G: 0.7588 acc: 78.1%\n",
      "[BATCH 110/149] Loss_D: 0.7249 Loss_G: 0.7622 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.7500 Loss_G: 0.7732 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.7405 Loss_G: 0.8003 acc: 65.6%\n",
      "[BATCH 113/149] Loss_D: 0.7202 Loss_G: 0.7953 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.7337 Loss_G: 0.7978 acc: 64.1%\n",
      "[BATCH 115/149] Loss_D: 0.7644 Loss_G: 0.8076 acc: 62.5%\n",
      "[BATCH 116/149] Loss_D: 0.7011 Loss_G: 0.7826 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7277 Loss_G: 0.7703 acc: 71.9%\n",
      "[EPOCH 2650] TEST ACC is : 66.2%\n",
      "[BATCH 118/149] Loss_D: 0.7384 Loss_G: 0.7720 acc: 76.6%\n",
      "[BATCH 119/149] Loss_D: 0.7518 Loss_G: 0.7780 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.7409 Loss_G: 0.7684 acc: 76.6%\n",
      "[BATCH 121/149] Loss_D: 0.7604 Loss_G: 0.7692 acc: 68.8%\n",
      "[BATCH 122/149] Loss_D: 0.7532 Loss_G: 0.7841 acc: 67.2%\n",
      "[BATCH 123/149] Loss_D: 0.7589 Loss_G: 0.7760 acc: 73.4%\n",
      "[BATCH 124/149] Loss_D: 0.7633 Loss_G: 0.7986 acc: 64.1%\n",
      "[BATCH 125/149] Loss_D: 0.7749 Loss_G: 0.7936 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.7438 Loss_G: 0.8097 acc: 76.6%\n",
      "[BATCH 127/149] Loss_D: 0.7695 Loss_G: 0.8054 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.7239 Loss_G: 0.7847 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.7230 Loss_G: 0.7894 acc: 65.6%\n",
      "[BATCH 130/149] Loss_D: 0.7449 Loss_G: 0.8032 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.8270 Loss_G: 0.8171 acc: 71.9%\n",
      "[BATCH 132/149] Loss_D: 0.7409 Loss_G: 0.8020 acc: 68.8%\n",
      "[BATCH 133/149] Loss_D: 0.7780 Loss_G: 0.7909 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7276 Loss_G: 0.7820 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.7295 Loss_G: 0.7697 acc: 65.6%\n",
      "[BATCH 136/149] Loss_D: 0.7266 Loss_G: 0.7655 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7189 Loss_G: 0.7612 acc: 79.7%\n",
      "[BATCH 138/149] Loss_D: 0.7282 Loss_G: 0.7810 acc: 79.7%\n",
      "[BATCH 139/149] Loss_D: 0.7912 Loss_G: 0.8003 acc: 65.6%\n",
      "[BATCH 140/149] Loss_D: 0.7291 Loss_G: 0.7858 acc: 75.0%\n",
      "[BATCH 141/149] Loss_D: 0.7612 Loss_G: 0.7940 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7230 Loss_G: 0.7728 acc: 73.4%\n",
      "[BATCH 143/149] Loss_D: 0.7638 Loss_G: 0.7845 acc: 70.3%\n",
      "[BATCH 144/149] Loss_D: 0.7675 Loss_G: 0.7818 acc: 82.8%\n",
      "[BATCH 145/149] Loss_D: 0.7469 Loss_G: 0.7872 acc: 67.2%\n",
      "[BATCH 146/149] Loss_D: 0.7360 Loss_G: 0.7844 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.7923 Loss_G: 0.8029 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7308 Loss_G: 0.7750 acc: 68.8%\n",
      "[BATCH 149/149] Loss_D: 0.7365 Loss_G: 0.7728 acc: 67.2%\n",
      "-----THE [18/50] epoch end-----\n",
      "-----THE [19/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7162 Loss_G: 0.7688 acc: 73.4%\n",
      "[BATCH 2/149] Loss_D: 0.7520 Loss_G: 0.7742 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.7163 Loss_G: 0.7689 acc: 67.2%\n",
      "[BATCH 4/149] Loss_D: 0.7358 Loss_G: 0.7693 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.6967 Loss_G: 0.7649 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.7313 Loss_G: 0.7692 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.7503 Loss_G: 0.7862 acc: 64.1%\n",
      "[BATCH 8/149] Loss_D: 0.7799 Loss_G: 0.7913 acc: 64.1%\n",
      "[BATCH 9/149] Loss_D: 0.7636 Loss_G: 0.7917 acc: 64.1%\n",
      "[BATCH 10/149] Loss_D: 0.7539 Loss_G: 0.7929 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.7361 Loss_G: 0.7794 acc: 73.4%\n",
      "[BATCH 12/149] Loss_D: 0.7348 Loss_G: 0.7874 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7075 Loss_G: 0.7851 acc: 62.5%\n",
      "[BATCH 14/149] Loss_D: 0.7174 Loss_G: 0.7698 acc: 76.6%\n",
      "[BATCH 15/149] Loss_D: 0.7725 Loss_G: 0.8070 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.7280 Loss_G: 0.8012 acc: 76.6%\n",
      "[BATCH 17/149] Loss_D: 0.7656 Loss_G: 0.8071 acc: 73.4%\n",
      "[BATCH 18/149] Loss_D: 0.7650 Loss_G: 0.8050 acc: 62.5%\n",
      "[EPOCH 2700] TEST ACC is : 68.4%\n",
      "[BATCH 19/149] Loss_D: 0.7692 Loss_G: 0.7905 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.7140 Loss_G: 0.7743 acc: 78.1%\n",
      "[BATCH 21/149] Loss_D: 0.7543 Loss_G: 0.7838 acc: 76.6%\n",
      "[BATCH 22/149] Loss_D: 0.7770 Loss_G: 0.7995 acc: 57.8%\n",
      "[BATCH 23/149] Loss_D: 0.7797 Loss_G: 0.7889 acc: 75.0%\n",
      "[BATCH 24/149] Loss_D: 0.7398 Loss_G: 0.7794 acc: 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.7396 Loss_G: 0.7809 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7537 Loss_G: 0.7968 acc: 70.3%\n",
      "[BATCH 27/149] Loss_D: 0.7352 Loss_G: 0.8054 acc: 62.5%\n",
      "[BATCH 28/149] Loss_D: 0.7367 Loss_G: 0.7707 acc: 64.1%\n",
      "[BATCH 29/149] Loss_D: 0.7522 Loss_G: 0.7783 acc: 71.9%\n",
      "[BATCH 30/149] Loss_D: 0.7571 Loss_G: 0.7890 acc: 81.2%\n",
      "[BATCH 31/149] Loss_D: 0.7194 Loss_G: 0.7774 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.7641 Loss_G: 0.7818 acc: 76.6%\n",
      "[BATCH 33/149] Loss_D: 0.7623 Loss_G: 0.8079 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.7673 Loss_G: 0.8155 acc: 67.2%\n",
      "[BATCH 35/149] Loss_D: 0.7357 Loss_G: 0.7914 acc: 73.4%\n",
      "[BATCH 36/149] Loss_D: 0.7146 Loss_G: 0.7729 acc: 75.0%\n",
      "[BATCH 37/149] Loss_D: 0.7231 Loss_G: 0.7646 acc: 81.2%\n",
      "[BATCH 38/149] Loss_D: 0.7650 Loss_G: 0.7957 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.7230 Loss_G: 0.7886 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.7152 Loss_G: 0.7771 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.7843 Loss_G: 0.8090 acc: 76.6%\n",
      "[BATCH 42/149] Loss_D: 0.7830 Loss_G: 0.8092 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.7025 Loss_G: 0.7793 acc: 73.4%\n",
      "[BATCH 44/149] Loss_D: 0.7572 Loss_G: 0.7762 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7127 Loss_G: 0.7623 acc: 75.0%\n",
      "[BATCH 46/149] Loss_D: 0.7350 Loss_G: 0.7671 acc: 70.3%\n",
      "[BATCH 47/149] Loss_D: 0.7269 Loss_G: 0.7624 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.7758 Loss_G: 0.7865 acc: 79.7%\n",
      "[BATCH 49/149] Loss_D: 0.7330 Loss_G: 0.7836 acc: 78.1%\n",
      "[BATCH 50/149] Loss_D: 0.8008 Loss_G: 0.8016 acc: 75.0%\n",
      "[BATCH 51/149] Loss_D: 0.7471 Loss_G: 0.7897 acc: 78.1%\n",
      "[BATCH 52/149] Loss_D: 0.7759 Loss_G: 0.8063 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.7635 Loss_G: 0.8059 acc: 73.4%\n",
      "[BATCH 54/149] Loss_D: 0.7907 Loss_G: 0.7999 acc: 76.6%\n",
      "[BATCH 55/149] Loss_D: 0.7569 Loss_G: 0.8006 acc: 70.3%\n",
      "[BATCH 56/149] Loss_D: 0.7296 Loss_G: 0.7838 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7209 Loss_G: 0.7815 acc: 73.4%\n",
      "[BATCH 58/149] Loss_D: 0.7312 Loss_G: 0.7837 acc: 75.0%\n",
      "[BATCH 59/149] Loss_D: 0.8002 Loss_G: 0.7981 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7293 Loss_G: 0.7823 acc: 68.8%\n",
      "[BATCH 61/149] Loss_D: 0.7654 Loss_G: 0.7951 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.7502 Loss_G: 0.7905 acc: 73.4%\n",
      "[BATCH 63/149] Loss_D: 0.7321 Loss_G: 0.7820 acc: 71.9%\n",
      "[BATCH 64/149] Loss_D: 0.7758 Loss_G: 0.8207 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.7197 Loss_G: 0.7896 acc: 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.7576 Loss_G: 0.8051 acc: 68.8%\n",
      "[BATCH 67/149] Loss_D: 0.7289 Loss_G: 0.7890 acc: 70.3%\n",
      "[BATCH 68/149] Loss_D: 0.7845 Loss_G: 0.8142 acc: 73.4%\n",
      "[EPOCH 2750] TEST ACC is : 66.2%\n",
      "[BATCH 69/149] Loss_D: 0.7235 Loss_G: 0.8047 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.7406 Loss_G: 0.7910 acc: 79.7%\n",
      "[BATCH 71/149] Loss_D: 0.7229 Loss_G: 0.7890 acc: 70.3%\n",
      "[BATCH 72/149] Loss_D: 0.7518 Loss_G: 0.7949 acc: 65.6%\n",
      "[BATCH 73/149] Loss_D: 0.7351 Loss_G: 0.7850 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7398 Loss_G: 0.7630 acc: 82.8%\n",
      "[BATCH 75/149] Loss_D: 0.6968 Loss_G: 0.7649 acc: 71.9%\n",
      "[BATCH 76/149] Loss_D: 0.7658 Loss_G: 0.8041 acc: 60.9%\n",
      "[BATCH 77/149] Loss_D: 0.7504 Loss_G: 0.7847 acc: 73.4%\n",
      "[BATCH 78/149] Loss_D: 0.7651 Loss_G: 0.7700 acc: 70.3%\n",
      "[BATCH 79/149] Loss_D: 0.7486 Loss_G: 0.7718 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.7100 Loss_G: 0.7777 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.7273 Loss_G: 0.7687 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.7608 Loss_G: 0.7704 acc: 65.6%\n",
      "[BATCH 83/149] Loss_D: 0.7188 Loss_G: 0.7665 acc: 71.9%\n",
      "[BATCH 84/149] Loss_D: 0.7082 Loss_G: 0.7629 acc: 73.4%\n",
      "[BATCH 85/149] Loss_D: 0.7348 Loss_G: 0.7760 acc: 64.1%\n",
      "[BATCH 86/149] Loss_D: 0.7188 Loss_G: 0.7629 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.7517 Loss_G: 0.7731 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.7648 Loss_G: 0.7865 acc: 79.7%\n",
      "[BATCH 89/149] Loss_D: 0.7119 Loss_G: 0.7803 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.7406 Loss_G: 0.7760 acc: 73.4%\n",
      "[BATCH 91/149] Loss_D: 0.6840 Loss_G: 0.7601 acc: 73.4%\n",
      "[BATCH 92/149] Loss_D: 0.7655 Loss_G: 0.7756 acc: 56.2%\n",
      "[BATCH 93/149] Loss_D: 0.7631 Loss_G: 0.7885 acc: 79.7%\n",
      "[BATCH 94/149] Loss_D: 0.7548 Loss_G: 0.7842 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.7409 Loss_G: 0.7680 acc: 78.1%\n",
      "[BATCH 96/149] Loss_D: 0.7625 Loss_G: 0.7732 acc: 64.1%\n",
      "[BATCH 97/149] Loss_D: 0.7595 Loss_G: 0.7797 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7642 Loss_G: 0.7731 acc: 75.0%\n",
      "[BATCH 99/149] Loss_D: 0.7370 Loss_G: 0.7797 acc: 65.6%\n",
      "[BATCH 100/149] Loss_D: 0.7644 Loss_G: 0.7902 acc: 60.9%\n",
      "[BATCH 101/149] Loss_D: 0.7312 Loss_G: 0.7738 acc: 78.1%\n",
      "[BATCH 102/149] Loss_D: 0.7574 Loss_G: 0.7743 acc: 60.9%\n",
      "[BATCH 103/149] Loss_D: 0.7220 Loss_G: 0.7712 acc: 79.7%\n",
      "[BATCH 104/149] Loss_D: 0.7062 Loss_G: 0.7753 acc: 68.8%\n",
      "[BATCH 105/149] Loss_D: 0.7478 Loss_G: 0.7638 acc: 64.1%\n",
      "[BATCH 106/149] Loss_D: 0.7791 Loss_G: 0.7920 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.7128 Loss_G: 0.7573 acc: 73.4%\n",
      "[BATCH 108/149] Loss_D: 0.7628 Loss_G: 0.7786 acc: 71.9%\n",
      "[BATCH 109/149] Loss_D: 0.7298 Loss_G: 0.7664 acc: 75.0%\n",
      "[BATCH 110/149] Loss_D: 0.7501 Loss_G: 0.7928 acc: 67.2%\n",
      "[BATCH 111/149] Loss_D: 0.7628 Loss_G: 0.7841 acc: 60.9%\n",
      "[BATCH 112/149] Loss_D: 0.7321 Loss_G: 0.7783 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.7169 Loss_G: 0.7836 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.7216 Loss_G: 0.7804 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7426 Loss_G: 0.7739 acc: 64.1%\n",
      "[BATCH 116/149] Loss_D: 0.7134 Loss_G: 0.7690 acc: 73.4%\n",
      "[BATCH 117/149] Loss_D: 0.7729 Loss_G: 0.7792 acc: 78.1%\n",
      "[BATCH 118/149] Loss_D: 0.7850 Loss_G: 0.7878 acc: 62.5%\n",
      "[EPOCH 2800] TEST ACC is : 67.8%\n",
      "[BATCH 119/149] Loss_D: 0.7210 Loss_G: 0.7832 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7728 Loss_G: 0.7949 acc: 81.2%\n",
      "[BATCH 121/149] Loss_D: 0.7974 Loss_G: 0.8021 acc: 70.3%\n",
      "[BATCH 122/149] Loss_D: 0.7550 Loss_G: 0.7924 acc: 79.7%\n",
      "[BATCH 123/149] Loss_D: 0.7416 Loss_G: 0.7864 acc: 70.3%\n",
      "[BATCH 124/149] Loss_D: 0.7712 Loss_G: 0.7956 acc: 65.6%\n",
      "[BATCH 125/149] Loss_D: 0.7965 Loss_G: 0.8084 acc: 70.3%\n",
      "[BATCH 126/149] Loss_D: 0.7675 Loss_G: 0.7906 acc: 60.9%\n",
      "[BATCH 127/149] Loss_D: 0.8041 Loss_G: 0.7761 acc: 68.8%\n",
      "[BATCH 128/149] Loss_D: 0.7180 Loss_G: 0.7629 acc: 76.6%\n",
      "[BATCH 129/149] Loss_D: 0.7525 Loss_G: 0.7688 acc: 62.5%\n",
      "[BATCH 130/149] Loss_D: 0.7594 Loss_G: 0.7815 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.7192 Loss_G: 0.7599 acc: 68.8%\n",
      "[BATCH 132/149] Loss_D: 0.7124 Loss_G: 0.7540 acc: 76.6%\n",
      "[BATCH 133/149] Loss_D: 0.7641 Loss_G: 0.7814 acc: 71.9%\n",
      "[BATCH 134/149] Loss_D: 0.7499 Loss_G: 0.7818 acc: 75.0%\n",
      "[BATCH 135/149] Loss_D: 0.7812 Loss_G: 0.7973 acc: 60.9%\n",
      "[BATCH 136/149] Loss_D: 0.7481 Loss_G: 0.7828 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7738 Loss_G: 0.7984 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7892 Loss_G: 0.7879 acc: 75.0%\n",
      "[BATCH 139/149] Loss_D: 0.7637 Loss_G: 0.7735 acc: 78.1%\n",
      "[BATCH 140/149] Loss_D: 0.7311 Loss_G: 0.7717 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7476 Loss_G: 0.7783 acc: 71.9%\n",
      "[BATCH 142/149] Loss_D: 0.7480 Loss_G: 0.7772 acc: 73.4%\n",
      "[BATCH 143/149] Loss_D: 0.7159 Loss_G: 0.7685 acc: 73.4%\n",
      "[BATCH 144/149] Loss_D: 0.7609 Loss_G: 0.7731 acc: 81.2%\n",
      "[BATCH 145/149] Loss_D: 0.7272 Loss_G: 0.7676 acc: 76.6%\n",
      "[BATCH 146/149] Loss_D: 0.7473 Loss_G: 0.7728 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.7540 Loss_G: 0.7780 acc: 73.4%\n",
      "[BATCH 148/149] Loss_D: 0.7686 Loss_G: 0.8006 acc: 73.4%\n",
      "[BATCH 149/149] Loss_D: 0.7295 Loss_G: 0.7870 acc: 68.8%\n",
      "-----THE [19/50] epoch end-----\n",
      "-----THE [20/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7817 Loss_G: 0.8036 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.7206 Loss_G: 0.7846 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7287 Loss_G: 0.7793 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.7421 Loss_G: 0.7814 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.7318 Loss_G: 0.7684 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7607 Loss_G: 0.7772 acc: 68.8%\n",
      "[BATCH 7/149] Loss_D: 0.7512 Loss_G: 0.7839 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.7313 Loss_G: 0.7684 acc: 76.6%\n",
      "[BATCH 9/149] Loss_D: 0.7050 Loss_G: 0.7639 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7958 Loss_G: 0.7918 acc: 64.1%\n",
      "[BATCH 11/149] Loss_D: 0.7161 Loss_G: 0.7763 acc: 79.7%\n",
      "[BATCH 12/149] Loss_D: 0.7348 Loss_G: 0.7739 acc: 79.7%\n",
      "[BATCH 13/149] Loss_D: 0.7608 Loss_G: 0.7687 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.7123 Loss_G: 0.7796 acc: 60.9%\n",
      "[BATCH 15/149] Loss_D: 0.7115 Loss_G: 0.7598 acc: 76.6%\n",
      "[BATCH 16/149] Loss_D: 0.7982 Loss_G: 0.7867 acc: 75.0%\n",
      "[BATCH 17/149] Loss_D: 0.7363 Loss_G: 0.7815 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.7213 Loss_G: 0.7869 acc: 78.1%\n",
      "[BATCH 19/149] Loss_D: 0.7263 Loss_G: 0.7762 acc: 70.3%\n",
      "[EPOCH 2850] TEST ACC is : 67.0%\n",
      "[BATCH 20/149] Loss_D: 0.7295 Loss_G: 0.7922 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.7424 Loss_G: 0.7748 acc: 59.4%\n",
      "[BATCH 22/149] Loss_D: 0.7515 Loss_G: 0.7900 acc: 81.2%\n",
      "[BATCH 23/149] Loss_D: 0.7055 Loss_G: 0.7643 acc: 71.9%\n",
      "[BATCH 24/149] Loss_D: 0.7298 Loss_G: 0.7516 acc: 65.6%\n",
      "[BATCH 25/149] Loss_D: 0.7421 Loss_G: 0.7471 acc: 78.1%\n",
      "[BATCH 26/149] Loss_D: 0.7396 Loss_G: 0.7639 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.7305 Loss_G: 0.7805 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7526 Loss_G: 0.7743 acc: 78.1%\n",
      "[BATCH 29/149] Loss_D: 0.7334 Loss_G: 0.7726 acc: 64.1%\n",
      "[BATCH 30/149] Loss_D: 0.7331 Loss_G: 0.7577 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.7473 Loss_G: 0.7634 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.7303 Loss_G: 0.7628 acc: 70.3%\n",
      "[BATCH 33/149] Loss_D: 0.7152 Loss_G: 0.7662 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.8264 Loss_G: 0.8113 acc: 73.4%\n",
      "[BATCH 35/149] Loss_D: 0.7466 Loss_G: 0.7973 acc: 75.0%\n",
      "[BATCH 36/149] Loss_D: 0.7473 Loss_G: 0.7821 acc: 65.6%\n",
      "[BATCH 37/149] Loss_D: 0.7490 Loss_G: 0.7825 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7649 Loss_G: 0.7924 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.7230 Loss_G: 0.7797 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7752 Loss_G: 0.7774 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.7615 Loss_G: 0.7772 acc: 79.7%\n",
      "[BATCH 42/149] Loss_D: 0.7019 Loss_G: 0.7709 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7958 Loss_G: 0.7793 acc: 62.5%\n",
      "[BATCH 44/149] Loss_D: 0.7648 Loss_G: 0.7801 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7761 Loss_G: 0.8048 acc: 60.9%\n",
      "[BATCH 46/149] Loss_D: 0.7473 Loss_G: 0.7883 acc: 76.6%\n",
      "[BATCH 47/149] Loss_D: 0.7582 Loss_G: 0.7867 acc: 68.8%\n",
      "[BATCH 48/149] Loss_D: 0.7664 Loss_G: 0.8161 acc: 59.4%\n",
      "[BATCH 49/149] Loss_D: 0.7320 Loss_G: 0.7874 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.7440 Loss_G: 0.7815 acc: 76.6%\n",
      "[BATCH 51/149] Loss_D: 0.7138 Loss_G: 0.7771 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7920 Loss_G: 0.7932 acc: 71.9%\n",
      "[BATCH 53/149] Loss_D: 0.7034 Loss_G: 0.7840 acc: 71.9%\n",
      "[BATCH 54/149] Loss_D: 0.7648 Loss_G: 0.7850 acc: 53.1%\n",
      "[BATCH 55/149] Loss_D: 0.7357 Loss_G: 0.7679 acc: 64.1%\n",
      "[BATCH 56/149] Loss_D: 0.7523 Loss_G: 0.7654 acc: 75.0%\n",
      "[BATCH 57/149] Loss_D: 0.6912 Loss_G: 0.7570 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7137 Loss_G: 0.7625 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.7172 Loss_G: 0.7745 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7119 Loss_G: 0.7749 acc: 70.3%\n",
      "[BATCH 61/149] Loss_D: 0.7696 Loss_G: 0.7841 acc: 71.9%\n",
      "[BATCH 62/149] Loss_D: 0.7700 Loss_G: 0.7888 acc: 71.9%\n",
      "[BATCH 63/149] Loss_D: 0.7501 Loss_G: 0.7970 acc: 60.9%\n",
      "[BATCH 64/149] Loss_D: 0.7365 Loss_G: 0.7918 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7255 Loss_G: 0.7831 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7179 Loss_G: 0.7730 acc: 76.6%\n",
      "[BATCH 67/149] Loss_D: 0.7642 Loss_G: 0.7869 acc: 65.6%\n",
      "[BATCH 68/149] Loss_D: 0.7466 Loss_G: 0.7805 acc: 79.7%\n",
      "[BATCH 69/149] Loss_D: 0.7703 Loss_G: 0.8022 acc: 64.1%\n",
      "[EPOCH 2900] TEST ACC is : 66.0%\n",
      "[BATCH 70/149] Loss_D: 0.7662 Loss_G: 0.7889 acc: 62.5%\n",
      "[BATCH 71/149] Loss_D: 0.7396 Loss_G: 0.7706 acc: 78.1%\n",
      "[BATCH 72/149] Loss_D: 0.7502 Loss_G: 0.7803 acc: 60.9%\n",
      "[BATCH 73/149] Loss_D: 0.7673 Loss_G: 0.7803 acc: 75.0%\n",
      "[BATCH 74/149] Loss_D: 0.7440 Loss_G: 0.7793 acc: 75.0%\n",
      "[BATCH 75/149] Loss_D: 0.7458 Loss_G: 0.7748 acc: 75.0%\n",
      "[BATCH 76/149] Loss_D: 0.7713 Loss_G: 0.7736 acc: 62.5%\n",
      "[BATCH 77/149] Loss_D: 0.7003 Loss_G: 0.7625 acc: 73.4%\n",
      "[BATCH 78/149] Loss_D: 0.7187 Loss_G: 0.7754 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.7381 Loss_G: 0.7820 acc: 79.7%\n",
      "[BATCH 80/149] Loss_D: 0.7828 Loss_G: 0.8062 acc: 64.1%\n",
      "[BATCH 81/149] Loss_D: 0.7492 Loss_G: 0.7836 acc: 68.8%\n",
      "[BATCH 82/149] Loss_D: 0.7192 Loss_G: 0.7774 acc: 75.0%\n",
      "[BATCH 83/149] Loss_D: 0.7418 Loss_G: 0.7793 acc: 59.4%\n",
      "[BATCH 84/149] Loss_D: 0.7953 Loss_G: 0.7864 acc: 60.9%\n",
      "[BATCH 85/149] Loss_D: 0.7384 Loss_G: 0.7752 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.7603 Loss_G: 0.7740 acc: 68.8%\n",
      "[BATCH 87/149] Loss_D: 0.7422 Loss_G: 0.7841 acc: 78.1%\n",
      "[BATCH 88/149] Loss_D: 0.6977 Loss_G: 0.7604 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7413 Loss_G: 0.7791 acc: 70.3%\n",
      "[BATCH 90/149] Loss_D: 0.7714 Loss_G: 0.7995 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7200 Loss_G: 0.7819 acc: 71.9%\n",
      "[BATCH 92/149] Loss_D: 0.7184 Loss_G: 0.7870 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.7454 Loss_G: 0.7834 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.7855 Loss_G: 0.7999 acc: 71.9%\n",
      "[BATCH 95/149] Loss_D: 0.7024 Loss_G: 0.7882 acc: 75.0%\n",
      "[BATCH 96/149] Loss_D: 0.7865 Loss_G: 0.8140 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.7539 Loss_G: 0.8154 acc: 76.6%\n",
      "[BATCH 98/149] Loss_D: 0.7738 Loss_G: 0.7957 acc: 64.1%\n",
      "[BATCH 99/149] Loss_D: 0.7508 Loss_G: 0.7825 acc: 75.0%\n",
      "[BATCH 100/149] Loss_D: 0.7699 Loss_G: 0.7822 acc: 78.1%\n",
      "[BATCH 101/149] Loss_D: 0.7289 Loss_G: 0.7982 acc: 71.9%\n",
      "[BATCH 102/149] Loss_D: 0.7665 Loss_G: 0.7834 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.6984 Loss_G: 0.7527 acc: 70.3%\n",
      "[BATCH 104/149] Loss_D: 0.7469 Loss_G: 0.7711 acc: 68.8%\n",
      "[BATCH 105/149] Loss_D: 0.7374 Loss_G: 0.7710 acc: 68.8%\n",
      "[BATCH 106/149] Loss_D: 0.7129 Loss_G: 0.7600 acc: 64.1%\n",
      "[BATCH 107/149] Loss_D: 0.7399 Loss_G: 0.7698 acc: 60.9%\n",
      "[BATCH 108/149] Loss_D: 0.7533 Loss_G: 0.7599 acc: 76.6%\n",
      "[BATCH 109/149] Loss_D: 0.7533 Loss_G: 0.7577 acc: 75.0%\n",
      "[BATCH 110/149] Loss_D: 0.7525 Loss_G: 0.7687 acc: 76.6%\n",
      "[BATCH 111/149] Loss_D: 0.7237 Loss_G: 0.7734 acc: 70.3%\n",
      "[BATCH 112/149] Loss_D: 0.8292 Loss_G: 0.8039 acc: 64.1%\n",
      "[BATCH 113/149] Loss_D: 0.7146 Loss_G: 0.7823 acc: 62.5%\n",
      "[BATCH 114/149] Loss_D: 0.7740 Loss_G: 0.7851 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7786 Loss_G: 0.8221 acc: 75.0%\n",
      "[BATCH 116/149] Loss_D: 0.7171 Loss_G: 0.8225 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7448 Loss_G: 0.7902 acc: 78.1%\n",
      "[BATCH 118/149] Loss_D: 0.7831 Loss_G: 0.7920 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.7256 Loss_G: 0.7783 acc: 78.1%\n",
      "[EPOCH 2950] TEST ACC is : 68.0%\n",
      "[BATCH 120/149] Loss_D: 0.7371 Loss_G: 0.7760 acc: 67.2%\n",
      "[BATCH 121/149] Loss_D: 0.7085 Loss_G: 0.7706 acc: 65.6%\n",
      "[BATCH 122/149] Loss_D: 0.7866 Loss_G: 0.7910 acc: 75.0%\n",
      "[BATCH 123/149] Loss_D: 0.7200 Loss_G: 0.7894 acc: 70.3%\n",
      "[BATCH 124/149] Loss_D: 0.8102 Loss_G: 0.7841 acc: 73.4%\n",
      "[BATCH 125/149] Loss_D: 0.7354 Loss_G: 0.7688 acc: 76.6%\n",
      "[BATCH 126/149] Loss_D: 0.7113 Loss_G: 0.7694 acc: 67.2%\n",
      "[BATCH 127/149] Loss_D: 0.7087 Loss_G: 0.7573 acc: 76.6%\n",
      "[BATCH 128/149] Loss_D: 0.7115 Loss_G: 0.7734 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7634 Loss_G: 0.7751 acc: 76.6%\n",
      "[BATCH 130/149] Loss_D: 0.7766 Loss_G: 0.8038 acc: 67.2%\n",
      "[BATCH 131/149] Loss_D: 0.7240 Loss_G: 0.7551 acc: 79.7%\n",
      "[BATCH 132/149] Loss_D: 0.7275 Loss_G: 0.7642 acc: 70.3%\n",
      "[BATCH 133/149] Loss_D: 0.7484 Loss_G: 0.7793 acc: 67.2%\n",
      "[BATCH 134/149] Loss_D: 0.7501 Loss_G: 0.7707 acc: 78.1%\n",
      "[BATCH 135/149] Loss_D: 0.7540 Loss_G: 0.7828 acc: 75.0%\n",
      "[BATCH 136/149] Loss_D: 0.7533 Loss_G: 0.8004 acc: 60.9%\n",
      "[BATCH 137/149] Loss_D: 0.7702 Loss_G: 0.7882 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.7467 Loss_G: 0.8025 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7431 Loss_G: 0.7880 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.7443 Loss_G: 0.7812 acc: 79.7%\n",
      "[BATCH 141/149] Loss_D: 0.7700 Loss_G: 0.7917 acc: 64.1%\n",
      "[BATCH 142/149] Loss_D: 0.7509 Loss_G: 0.7803 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.7258 Loss_G: 0.7762 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7890 Loss_G: 0.8000 acc: 78.1%\n",
      "[BATCH 145/149] Loss_D: 0.7674 Loss_G: 0.7822 acc: 75.0%\n",
      "[BATCH 146/149] Loss_D: 0.7856 Loss_G: 0.7961 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.7346 Loss_G: 0.7866 acc: 75.0%\n",
      "[BATCH 148/149] Loss_D: 0.7214 Loss_G: 0.7698 acc: 76.6%\n",
      "[BATCH 149/149] Loss_D: 0.7189 Loss_G: 0.7698 acc: 70.3%\n",
      "-----THE [20/50] epoch end-----\n",
      "-----THE [21/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7096 Loss_G: 0.7629 acc: 82.8%\n",
      "[BATCH 2/149] Loss_D: 0.7288 Loss_G: 0.7586 acc: 75.0%\n",
      "[BATCH 3/149] Loss_D: 0.7514 Loss_G: 0.7589 acc: 78.1%\n",
      "[BATCH 4/149] Loss_D: 0.7636 Loss_G: 0.7789 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.7315 Loss_G: 0.7785 acc: 60.9%\n",
      "[BATCH 6/149] Loss_D: 0.7672 Loss_G: 0.7945 acc: 57.8%\n",
      "[BATCH 7/149] Loss_D: 0.7503 Loss_G: 0.7829 acc: 76.6%\n",
      "[BATCH 8/149] Loss_D: 0.7083 Loss_G: 0.7762 acc: 68.8%\n",
      "[BATCH 9/149] Loss_D: 0.7644 Loss_G: 0.7920 acc: 78.1%\n",
      "[BATCH 10/149] Loss_D: 0.7444 Loss_G: 0.7775 acc: 73.4%\n",
      "[BATCH 11/149] Loss_D: 0.7333 Loss_G: 0.7823 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7451 Loss_G: 0.7654 acc: 76.6%\n",
      "[BATCH 13/149] Loss_D: 0.7224 Loss_G: 0.7702 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7511 Loss_G: 0.7710 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.8056 Loss_G: 0.7898 acc: 78.1%\n",
      "[BATCH 16/149] Loss_D: 0.7346 Loss_G: 0.7968 acc: 67.2%\n",
      "[BATCH 17/149] Loss_D: 0.7262 Loss_G: 0.7789 acc: 78.1%\n",
      "[BATCH 18/149] Loss_D: 0.7621 Loss_G: 0.7743 acc: 75.0%\n",
      "[BATCH 19/149] Loss_D: 0.7835 Loss_G: 0.8079 acc: 57.8%\n",
      "[BATCH 20/149] Loss_D: 0.7588 Loss_G: 0.7958 acc: 56.2%\n",
      "[EPOCH 3000] TEST ACC is : 68.6%\n",
      "[BATCH 21/149] Loss_D: 0.7623 Loss_G: 0.8072 acc: 81.2%\n",
      "[BATCH 22/149] Loss_D: 0.7383 Loss_G: 0.7894 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7329 Loss_G: 0.7802 acc: 75.0%\n",
      "[BATCH 24/149] Loss_D: 0.7349 Loss_G: 0.7836 acc: 68.8%\n",
      "[BATCH 25/149] Loss_D: 0.7653 Loss_G: 0.7754 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.7297 Loss_G: 0.7725 acc: 73.4%\n",
      "[BATCH 27/149] Loss_D: 0.7465 Loss_G: 0.7871 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7235 Loss_G: 0.7748 acc: 73.4%\n",
      "[BATCH 29/149] Loss_D: 0.8083 Loss_G: 0.7982 acc: 73.4%\n",
      "[BATCH 30/149] Loss_D: 0.7464 Loss_G: 0.7982 acc: 68.8%\n",
      "[BATCH 31/149] Loss_D: 0.7617 Loss_G: 0.7845 acc: 68.8%\n",
      "[BATCH 32/149] Loss_D: 0.7323 Loss_G: 0.7816 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.7516 Loss_G: 0.7776 acc: 79.7%\n",
      "[BATCH 34/149] Loss_D: 0.7651 Loss_G: 0.7818 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.7018 Loss_G: 0.7707 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7481 Loss_G: 0.7751 acc: 78.1%\n",
      "[BATCH 37/149] Loss_D: 0.7518 Loss_G: 0.7844 acc: 76.6%\n",
      "[BATCH 38/149] Loss_D: 0.7478 Loss_G: 0.7710 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7643 Loss_G: 0.7812 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7338 Loss_G: 0.7737 acc: 79.7%\n",
      "[BATCH 41/149] Loss_D: 0.7185 Loss_G: 0.7730 acc: 73.4%\n",
      "[BATCH 42/149] Loss_D: 0.7343 Loss_G: 0.7913 acc: 64.1%\n",
      "[BATCH 43/149] Loss_D: 0.7470 Loss_G: 0.7812 acc: 68.8%\n",
      "[BATCH 44/149] Loss_D: 0.7900 Loss_G: 0.7850 acc: 76.6%\n",
      "[BATCH 45/149] Loss_D: 0.7355 Loss_G: 0.7904 acc: 71.9%\n",
      "[BATCH 46/149] Loss_D: 0.7540 Loss_G: 0.7884 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.6937 Loss_G: 0.7640 acc: 73.4%\n",
      "[BATCH 48/149] Loss_D: 0.7197 Loss_G: 0.7494 acc: 70.3%\n",
      "[BATCH 49/149] Loss_D: 0.7434 Loss_G: 0.7713 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.7659 Loss_G: 0.7961 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7372 Loss_G: 0.7716 acc: 75.0%\n",
      "[BATCH 52/149] Loss_D: 0.6830 Loss_G: 0.7517 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.7163 Loss_G: 0.7588 acc: 75.0%\n",
      "[BATCH 54/149] Loss_D: 0.7489 Loss_G: 0.7836 acc: 67.2%\n",
      "[BATCH 55/149] Loss_D: 0.7112 Loss_G: 0.7679 acc: 68.8%\n",
      "[BATCH 56/149] Loss_D: 0.7720 Loss_G: 0.7614 acc: 79.7%\n",
      "[BATCH 57/149] Loss_D: 0.7488 Loss_G: 0.7741 acc: 75.0%\n",
      "[BATCH 58/149] Loss_D: 0.7578 Loss_G: 0.7900 acc: 65.6%\n",
      "[BATCH 59/149] Loss_D: 0.7461 Loss_G: 0.7887 acc: 76.6%\n",
      "[BATCH 60/149] Loss_D: 0.7185 Loss_G: 0.7752 acc: 62.5%\n",
      "[BATCH 61/149] Loss_D: 0.7829 Loss_G: 0.7889 acc: 78.1%\n",
      "[BATCH 62/149] Loss_D: 0.7489 Loss_G: 0.7889 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7052 Loss_G: 0.7678 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7679 Loss_G: 0.7755 acc: 67.2%\n",
      "[BATCH 65/149] Loss_D: 0.7628 Loss_G: 0.7906 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.7165 Loss_G: 0.7767 acc: 71.9%\n",
      "[BATCH 67/149] Loss_D: 0.7624 Loss_G: 0.7778 acc: 73.4%\n",
      "[BATCH 68/149] Loss_D: 0.7495 Loss_G: 0.7770 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.7516 Loss_G: 0.7671 acc: 64.1%\n",
      "[BATCH 70/149] Loss_D: 0.7223 Loss_G: 0.7560 acc: 76.6%\n",
      "[EPOCH 3050] TEST ACC is : 66.6%\n",
      "[BATCH 71/149] Loss_D: 0.7024 Loss_G: 0.7586 acc: 76.6%\n",
      "[BATCH 72/149] Loss_D: 0.7122 Loss_G: 0.7695 acc: 78.1%\n",
      "[BATCH 73/149] Loss_D: 0.7375 Loss_G: 0.7692 acc: 73.4%\n",
      "[BATCH 74/149] Loss_D: 0.7634 Loss_G: 0.7727 acc: 68.8%\n",
      "[BATCH 75/149] Loss_D: 0.7257 Loss_G: 0.7689 acc: 59.4%\n",
      "[BATCH 76/149] Loss_D: 0.7427 Loss_G: 0.7732 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.7885 Loss_G: 0.8080 acc: 76.6%\n",
      "[BATCH 78/149] Loss_D: 0.7159 Loss_G: 0.7887 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.7724 Loss_G: 0.7979 acc: 70.3%\n",
      "[BATCH 80/149] Loss_D: 0.7456 Loss_G: 0.7887 acc: 70.3%\n",
      "[BATCH 81/149] Loss_D: 0.7800 Loss_G: 0.7886 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.7653 Loss_G: 0.8179 acc: 60.9%\n",
      "[BATCH 83/149] Loss_D: 0.7456 Loss_G: 0.8047 acc: 71.9%\n",
      "[BATCH 84/149] Loss_D: 0.7488 Loss_G: 0.7863 acc: 70.3%\n",
      "[BATCH 85/149] Loss_D: 0.7364 Loss_G: 0.7675 acc: 64.1%\n",
      "[BATCH 86/149] Loss_D: 0.7606 Loss_G: 0.7861 acc: 76.6%\n",
      "[BATCH 87/149] Loss_D: 0.7069 Loss_G: 0.7703 acc: 71.9%\n",
      "[BATCH 88/149] Loss_D: 0.7714 Loss_G: 0.7815 acc: 70.3%\n",
      "[BATCH 89/149] Loss_D: 0.7956 Loss_G: 0.8053 acc: 71.9%\n",
      "[BATCH 90/149] Loss_D: 0.7121 Loss_G: 0.7809 acc: 81.2%\n",
      "[BATCH 91/149] Loss_D: 0.7502 Loss_G: 0.7720 acc: 75.0%\n",
      "[BATCH 92/149] Loss_D: 0.7430 Loss_G: 0.7624 acc: 68.8%\n",
      "[BATCH 93/149] Loss_D: 0.7731 Loss_G: 0.7667 acc: 73.4%\n",
      "[BATCH 94/149] Loss_D: 0.7284 Loss_G: 0.7720 acc: 68.8%\n",
      "[BATCH 95/149] Loss_D: 0.7076 Loss_G: 0.7585 acc: 67.2%\n",
      "[BATCH 96/149] Loss_D: 0.7905 Loss_G: 0.7771 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7260 Loss_G: 0.7648 acc: 65.6%\n",
      "[BATCH 98/149] Loss_D: 0.7615 Loss_G: 0.7783 acc: 73.4%\n",
      "[BATCH 99/149] Loss_D: 0.7273 Loss_G: 0.7855 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.7685 Loss_G: 0.7901 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7585 Loss_G: 0.7885 acc: 71.9%\n",
      "[BATCH 102/149] Loss_D: 0.7559 Loss_G: 0.7779 acc: 73.4%\n",
      "[BATCH 103/149] Loss_D: 0.7611 Loss_G: 0.7798 acc: 79.7%\n",
      "[BATCH 104/149] Loss_D: 0.7575 Loss_G: 0.7792 acc: 76.6%\n",
      "[BATCH 105/149] Loss_D: 0.7233 Loss_G: 0.7654 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7228 Loss_G: 0.7521 acc: 76.6%\n",
      "[BATCH 107/149] Loss_D: 0.7312 Loss_G: 0.7535 acc: 73.4%\n",
      "[BATCH 108/149] Loss_D: 0.7248 Loss_G: 0.7677 acc: 71.9%\n",
      "[BATCH 109/149] Loss_D: 0.7299 Loss_G: 0.7730 acc: 70.3%\n",
      "[BATCH 110/149] Loss_D: 0.7474 Loss_G: 0.7804 acc: 65.6%\n",
      "[BATCH 111/149] Loss_D: 0.7266 Loss_G: 0.7751 acc: 73.4%\n",
      "[BATCH 112/149] Loss_D: 0.7237 Loss_G: 0.7929 acc: 71.9%\n",
      "[BATCH 113/149] Loss_D: 0.7822 Loss_G: 0.8036 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.7703 Loss_G: 0.8027 acc: 68.8%\n",
      "[BATCH 115/149] Loss_D: 0.7647 Loss_G: 0.7776 acc: 71.9%\n",
      "[BATCH 116/149] Loss_D: 0.7513 Loss_G: 0.7834 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7114 Loss_G: 0.7684 acc: 78.1%\n",
      "[BATCH 118/149] Loss_D: 0.7120 Loss_G: 0.7601 acc: 76.6%\n",
      "[BATCH 119/149] Loss_D: 0.7448 Loss_G: 0.7738 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7734 Loss_G: 0.7874 acc: 65.6%\n",
      "[EPOCH 3100] TEST ACC is : 68.2%\n",
      "[BATCH 121/149] Loss_D: 0.7655 Loss_G: 0.7798 acc: 70.3%\n",
      "[BATCH 122/149] Loss_D: 0.6984 Loss_G: 0.7625 acc: 81.2%\n",
      "[BATCH 123/149] Loss_D: 0.7817 Loss_G: 0.7837 acc: 64.1%\n",
      "[BATCH 124/149] Loss_D: 0.7576 Loss_G: 0.7827 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.7444 Loss_G: 0.7802 acc: 73.4%\n",
      "[BATCH 126/149] Loss_D: 0.7352 Loss_G: 0.7724 acc: 76.6%\n",
      "[BATCH 127/149] Loss_D: 0.7758 Loss_G: 0.7868 acc: 67.2%\n",
      "[BATCH 128/149] Loss_D: 0.7366 Loss_G: 0.7761 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.7668 Loss_G: 0.7842 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.7259 Loss_G: 0.7942 acc: 56.2%\n",
      "[BATCH 131/149] Loss_D: 0.7273 Loss_G: 0.7762 acc: 60.9%\n",
      "[BATCH 132/149] Loss_D: 0.7451 Loss_G: 0.7705 acc: 73.4%\n",
      "[BATCH 133/149] Loss_D: 0.7023 Loss_G: 0.7542 acc: 71.9%\n",
      "[BATCH 134/149] Loss_D: 0.7441 Loss_G: 0.7653 acc: 81.2%\n",
      "[BATCH 135/149] Loss_D: 0.7347 Loss_G: 0.7750 acc: 76.6%\n",
      "[BATCH 136/149] Loss_D: 0.7186 Loss_G: 0.7642 acc: 79.7%\n",
      "[BATCH 137/149] Loss_D: 0.7312 Loss_G: 0.7674 acc: 75.0%\n",
      "[BATCH 138/149] Loss_D: 0.7122 Loss_G: 0.7593 acc: 71.9%\n",
      "[BATCH 139/149] Loss_D: 0.7405 Loss_G: 0.7557 acc: 75.0%\n",
      "[BATCH 140/149] Loss_D: 0.7492 Loss_G: 0.7634 acc: 65.6%\n",
      "[BATCH 141/149] Loss_D: 0.7420 Loss_G: 0.7636 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.8382 Loss_G: 0.8140 acc: 71.9%\n",
      "[BATCH 143/149] Loss_D: 0.7209 Loss_G: 0.8045 acc: 73.4%\n",
      "[BATCH 144/149] Loss_D: 0.6968 Loss_G: 0.7885 acc: 73.4%\n",
      "[BATCH 145/149] Loss_D: 0.7445 Loss_G: 0.7763 acc: 73.4%\n",
      "[BATCH 146/149] Loss_D: 0.7362 Loss_G: 0.7804 acc: 71.9%\n",
      "[BATCH 147/149] Loss_D: 0.7232 Loss_G: 0.7925 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7071 Loss_G: 0.7647 acc: 81.2%\n",
      "[BATCH 149/149] Loss_D: 0.7007 Loss_G: 0.7602 acc: 71.9%\n",
      "-----THE [21/50] epoch end-----\n",
      "-----THE [22/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7314 Loss_G: 0.7676 acc: 78.1%\n",
      "[BATCH 2/149] Loss_D: 0.7963 Loss_G: 0.7707 acc: 76.6%\n",
      "[BATCH 3/149] Loss_D: 0.6962 Loss_G: 0.7625 acc: 81.2%\n",
      "[BATCH 4/149] Loss_D: 0.7480 Loss_G: 0.7755 acc: 71.9%\n",
      "[BATCH 5/149] Loss_D: 0.7528 Loss_G: 0.7834 acc: 70.3%\n",
      "[BATCH 6/149] Loss_D: 0.8176 Loss_G: 0.7937 acc: 76.6%\n",
      "[BATCH 7/149] Loss_D: 0.7356 Loss_G: 0.8035 acc: 65.6%\n",
      "[BATCH 8/149] Loss_D: 0.7281 Loss_G: 0.7873 acc: 54.7%\n",
      "[BATCH 9/149] Loss_D: 0.7500 Loss_G: 0.7767 acc: 70.3%\n",
      "[BATCH 10/149] Loss_D: 0.7901 Loss_G: 0.7845 acc: 76.6%\n",
      "[BATCH 11/149] Loss_D: 0.7370 Loss_G: 0.7769 acc: 78.1%\n",
      "[BATCH 12/149] Loss_D: 0.7401 Loss_G: 0.7825 acc: 79.7%\n",
      "[BATCH 13/149] Loss_D: 0.7807 Loss_G: 0.7764 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.7358 Loss_G: 0.7728 acc: 70.3%\n",
      "[BATCH 15/149] Loss_D: 0.7456 Loss_G: 0.7832 acc: 70.3%\n",
      "[BATCH 16/149] Loss_D: 0.7096 Loss_G: 0.7739 acc: 75.0%\n",
      "[BATCH 17/149] Loss_D: 0.7457 Loss_G: 0.7759 acc: 78.1%\n",
      "[BATCH 18/149] Loss_D: 0.7504 Loss_G: 0.7789 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.7429 Loss_G: 0.7736 acc: 65.6%\n",
      "[BATCH 20/149] Loss_D: 0.7814 Loss_G: 0.7744 acc: 73.4%\n",
      "[BATCH 21/149] Loss_D: 0.7936 Loss_G: 0.8042 acc: 84.4%\n",
      "[EPOCH 3150] TEST ACC is : 66.0%\n",
      "[BATCH 22/149] Loss_D: 0.7376 Loss_G: 0.7990 acc: 65.6%\n",
      "[BATCH 23/149] Loss_D: 0.7989 Loss_G: 0.8024 acc: 78.1%\n",
      "[BATCH 24/149] Loss_D: 0.7176 Loss_G: 0.7679 acc: 76.6%\n",
      "[BATCH 25/149] Loss_D: 0.7279 Loss_G: 0.7731 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7390 Loss_G: 0.7894 acc: 57.8%\n",
      "[BATCH 27/149] Loss_D: 0.7712 Loss_G: 0.7933 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.7713 Loss_G: 0.7802 acc: 60.9%\n",
      "[BATCH 29/149] Loss_D: 0.7349 Loss_G: 0.7855 acc: 76.6%\n",
      "[BATCH 30/149] Loss_D: 0.7787 Loss_G: 0.8136 acc: 56.2%\n",
      "[BATCH 31/149] Loss_D: 0.7774 Loss_G: 0.7830 acc: 73.4%\n",
      "[BATCH 32/149] Loss_D: 0.7318 Loss_G: 0.7887 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7226 Loss_G: 0.7834 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.7263 Loss_G: 0.7964 acc: 73.4%\n",
      "[BATCH 35/149] Loss_D: 0.7369 Loss_G: 0.7462 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7250 Loss_G: 0.7651 acc: 82.8%\n",
      "[BATCH 37/149] Loss_D: 0.7024 Loss_G: 0.7551 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7355 Loss_G: 0.7416 acc: 73.4%\n",
      "[BATCH 39/149] Loss_D: 0.7298 Loss_G: 0.7547 acc: 65.6%\n",
      "[BATCH 40/149] Loss_D: 0.7542 Loss_G: 0.7620 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.7162 Loss_G: 0.7497 acc: 71.9%\n",
      "[BATCH 42/149] Loss_D: 0.7269 Loss_G: 0.7600 acc: 68.8%\n",
      "[BATCH 43/149] Loss_D: 0.7207 Loss_G: 0.7583 acc: 75.0%\n",
      "[BATCH 44/149] Loss_D: 0.6996 Loss_G: 0.7692 acc: 75.0%\n",
      "[BATCH 45/149] Loss_D: 0.7545 Loss_G: 0.7867 acc: 68.8%\n",
      "[BATCH 46/149] Loss_D: 0.7390 Loss_G: 0.7634 acc: 76.6%\n",
      "[BATCH 47/149] Loss_D: 0.8137 Loss_G: 0.7652 acc: 71.9%\n",
      "[BATCH 48/149] Loss_D: 0.7286 Loss_G: 0.7701 acc: 60.9%\n",
      "[BATCH 49/149] Loss_D: 0.7266 Loss_G: 0.7799 acc: 70.3%\n",
      "[BATCH 50/149] Loss_D: 0.7383 Loss_G: 0.7791 acc: 75.0%\n",
      "[BATCH 51/149] Loss_D: 0.7550 Loss_G: 0.7708 acc: 64.1%\n",
      "[BATCH 52/149] Loss_D: 0.7237 Loss_G: 0.7677 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.7505 Loss_G: 0.7708 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.7360 Loss_G: 0.7674 acc: 64.1%\n",
      "[BATCH 55/149] Loss_D: 0.7365 Loss_G: 0.7621 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.7393 Loss_G: 0.7779 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7436 Loss_G: 0.7858 acc: 79.7%\n",
      "[BATCH 58/149] Loss_D: 0.7350 Loss_G: 0.7787 acc: 73.4%\n",
      "[BATCH 59/149] Loss_D: 0.7213 Loss_G: 0.7688 acc: 68.8%\n",
      "[BATCH 60/149] Loss_D: 0.7452 Loss_G: 0.7686 acc: 76.6%\n",
      "[BATCH 61/149] Loss_D: 0.7286 Loss_G: 0.7727 acc: 79.7%\n",
      "[BATCH 62/149] Loss_D: 0.7358 Loss_G: 0.7737 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.7697 Loss_G: 0.7963 acc: 65.6%\n",
      "[BATCH 64/149] Loss_D: 0.7646 Loss_G: 0.7968 acc: 78.1%\n",
      "[BATCH 65/149] Loss_D: 0.7770 Loss_G: 0.8059 acc: 73.4%\n",
      "[BATCH 66/149] Loss_D: 0.7526 Loss_G: 0.8023 acc: 67.2%\n",
      "[BATCH 67/149] Loss_D: 0.7317 Loss_G: 0.7585 acc: 70.3%\n",
      "[BATCH 68/149] Loss_D: 0.7583 Loss_G: 0.7720 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.7644 Loss_G: 0.7829 acc: 67.2%\n",
      "[BATCH 70/149] Loss_D: 0.7400 Loss_G: 0.7774 acc: 60.9%\n",
      "[BATCH 71/149] Loss_D: 0.7769 Loss_G: 0.7687 acc: 70.3%\n",
      "[EPOCH 3200] TEST ACC is : 67.8%\n",
      "[BATCH 72/149] Loss_D: 0.7292 Loss_G: 0.7724 acc: 70.3%\n",
      "[BATCH 73/149] Loss_D: 0.7108 Loss_G: 0.7701 acc: 73.4%\n",
      "[BATCH 74/149] Loss_D: 0.7074 Loss_G: 0.7613 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7462 Loss_G: 0.7755 acc: 67.2%\n",
      "[BATCH 76/149] Loss_D: 0.7635 Loss_G: 0.7821 acc: 75.0%\n",
      "[BATCH 77/149] Loss_D: 0.7716 Loss_G: 0.7744 acc: 79.7%\n",
      "[BATCH 78/149] Loss_D: 0.7840 Loss_G: 0.7905 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.7545 Loss_G: 0.7831 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.7130 Loss_G: 0.7704 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.6843 Loss_G: 0.7561 acc: 79.7%\n",
      "[BATCH 82/149] Loss_D: 0.7453 Loss_G: 0.7593 acc: 70.3%\n",
      "[BATCH 83/149] Loss_D: 0.7784 Loss_G: 0.7551 acc: 71.9%\n",
      "[BATCH 84/149] Loss_D: 0.7909 Loss_G: 0.7754 acc: 75.0%\n",
      "[BATCH 85/149] Loss_D: 0.7568 Loss_G: 0.7813 acc: 62.5%\n",
      "[BATCH 86/149] Loss_D: 0.7439 Loss_G: 0.7806 acc: 67.2%\n",
      "[BATCH 87/149] Loss_D: 0.7109 Loss_G: 0.7755 acc: 81.2%\n",
      "[BATCH 88/149] Loss_D: 0.7462 Loss_G: 0.7753 acc: 70.3%\n",
      "[BATCH 89/149] Loss_D: 0.7205 Loss_G: 0.7830 acc: 78.1%\n",
      "[BATCH 90/149] Loss_D: 0.7690 Loss_G: 0.7813 acc: 81.2%\n",
      "[BATCH 91/149] Loss_D: 0.8056 Loss_G: 0.8134 acc: 65.6%\n",
      "[BATCH 92/149] Loss_D: 0.7449 Loss_G: 0.7844 acc: 67.2%\n",
      "[BATCH 93/149] Loss_D: 0.7506 Loss_G: 0.7903 acc: 60.9%\n",
      "[BATCH 94/149] Loss_D: 0.7566 Loss_G: 0.7745 acc: 78.1%\n",
      "[BATCH 95/149] Loss_D: 0.7388 Loss_G: 0.7917 acc: 79.7%\n",
      "[BATCH 96/149] Loss_D: 0.7322 Loss_G: 0.7754 acc: 78.1%\n",
      "[BATCH 97/149] Loss_D: 0.7195 Loss_G: 0.7634 acc: 75.0%\n",
      "[BATCH 98/149] Loss_D: 0.7293 Loss_G: 0.7566 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7049 Loss_G: 0.7658 acc: 75.0%\n",
      "[BATCH 100/149] Loss_D: 0.7261 Loss_G: 0.7545 acc: 75.0%\n",
      "[BATCH 101/149] Loss_D: 0.7767 Loss_G: 0.7933 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.7481 Loss_G: 0.7750 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7423 Loss_G: 0.7608 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.7387 Loss_G: 0.7803 acc: 70.3%\n",
      "[BATCH 105/149] Loss_D: 0.7380 Loss_G: 0.7756 acc: 70.3%\n",
      "[BATCH 106/149] Loss_D: 0.7333 Loss_G: 0.7762 acc: 73.4%\n",
      "[BATCH 107/149] Loss_D: 0.7332 Loss_G: 0.7639 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.7363 Loss_G: 0.7630 acc: 76.6%\n",
      "[BATCH 109/149] Loss_D: 0.7810 Loss_G: 0.7759 acc: 60.9%\n",
      "[BATCH 110/149] Loss_D: 0.7185 Loss_G: 0.7705 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7202 Loss_G: 0.7769 acc: 68.8%\n",
      "[BATCH 112/149] Loss_D: 0.7209 Loss_G: 0.7772 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7622 Loss_G: 0.7749 acc: 73.4%\n",
      "[BATCH 114/149] Loss_D: 0.7213 Loss_G: 0.7685 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7426 Loss_G: 0.7705 acc: 75.0%\n",
      "[BATCH 116/149] Loss_D: 0.6956 Loss_G: 0.7673 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7462 Loss_G: 0.7722 acc: 70.3%\n",
      "[BATCH 118/149] Loss_D: 0.7209 Loss_G: 0.7715 acc: 68.8%\n",
      "[BATCH 119/149] Loss_D: 0.7067 Loss_G: 0.7670 acc: 67.2%\n",
      "[BATCH 120/149] Loss_D: 0.7301 Loss_G: 0.7755 acc: 68.8%\n",
      "[BATCH 121/149] Loss_D: 0.7590 Loss_G: 0.7814 acc: 64.1%\n",
      "[EPOCH 3250] TEST ACC is : 66.6%\n",
      "[BATCH 122/149] Loss_D: 0.7060 Loss_G: 0.7688 acc: 75.0%\n",
      "[BATCH 123/149] Loss_D: 0.7349 Loss_G: 0.7587 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.7409 Loss_G: 0.7630 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.7246 Loss_G: 0.7614 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.7465 Loss_G: 0.7687 acc: 62.5%\n",
      "[BATCH 127/149] Loss_D: 0.7194 Loss_G: 0.7587 acc: 82.8%\n",
      "[BATCH 128/149] Loss_D: 0.7089 Loss_G: 0.7631 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.6971 Loss_G: 0.7570 acc: 75.0%\n",
      "[BATCH 130/149] Loss_D: 0.7057 Loss_G: 0.7566 acc: 76.6%\n",
      "[BATCH 131/149] Loss_D: 0.8260 Loss_G: 0.7947 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.7134 Loss_G: 0.7712 acc: 78.1%\n",
      "[BATCH 133/149] Loss_D: 0.7583 Loss_G: 0.7547 acc: 78.1%\n",
      "[BATCH 134/149] Loss_D: 0.7042 Loss_G: 0.7550 acc: 65.6%\n",
      "[BATCH 135/149] Loss_D: 0.7460 Loss_G: 0.7665 acc: 68.8%\n",
      "[BATCH 136/149] Loss_D: 0.7240 Loss_G: 0.7803 acc: 68.8%\n",
      "[BATCH 137/149] Loss_D: 0.7859 Loss_G: 0.7885 acc: 75.0%\n",
      "[BATCH 138/149] Loss_D: 0.7617 Loss_G: 0.7884 acc: 68.8%\n",
      "[BATCH 139/149] Loss_D: 0.7150 Loss_G: 0.7700 acc: 70.3%\n",
      "[BATCH 140/149] Loss_D: 0.7733 Loss_G: 0.7782 acc: 67.2%\n",
      "[BATCH 141/149] Loss_D: 0.7600 Loss_G: 0.7761 acc: 60.9%\n",
      "[BATCH 142/149] Loss_D: 0.7309 Loss_G: 0.7587 acc: 78.1%\n",
      "[BATCH 143/149] Loss_D: 0.7089 Loss_G: 0.7517 acc: 75.0%\n",
      "[BATCH 144/149] Loss_D: 0.7455 Loss_G: 0.7575 acc: 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.7446 Loss_G: 0.7720 acc: 73.4%\n",
      "[BATCH 146/149] Loss_D: 0.6937 Loss_G: 0.7583 acc: 70.3%\n",
      "[BATCH 147/149] Loss_D: 0.7332 Loss_G: 0.7750 acc: 59.4%\n",
      "[BATCH 148/149] Loss_D: 0.7503 Loss_G: 0.7816 acc: 81.2%\n",
      "[BATCH 149/149] Loss_D: 0.7112 Loss_G: 0.7780 acc: 70.3%\n",
      "-----THE [22/50] epoch end-----\n",
      "-----THE [23/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7523 Loss_G: 0.7660 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7032 Loss_G: 0.7574 acc: 76.6%\n",
      "[BATCH 3/149] Loss_D: 0.7218 Loss_G: 0.7813 acc: 75.0%\n",
      "[BATCH 4/149] Loss_D: 0.7559 Loss_G: 0.7831 acc: 71.9%\n",
      "[BATCH 5/149] Loss_D: 0.7629 Loss_G: 0.7867 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7097 Loss_G: 0.7705 acc: 78.1%\n",
      "[BATCH 7/149] Loss_D: 0.7210 Loss_G: 0.7633 acc: 71.9%\n",
      "[BATCH 8/149] Loss_D: 0.7544 Loss_G: 0.7661 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.7214 Loss_G: 0.7555 acc: 73.4%\n",
      "[BATCH 10/149] Loss_D: 0.7587 Loss_G: 0.7681 acc: 70.3%\n",
      "[BATCH 11/149] Loss_D: 0.6986 Loss_G: 0.7432 acc: 78.1%\n",
      "[BATCH 12/149] Loss_D: 0.7513 Loss_G: 0.7724 acc: 75.0%\n",
      "[BATCH 13/149] Loss_D: 0.7150 Loss_G: 0.7636 acc: 68.8%\n",
      "[BATCH 14/149] Loss_D: 0.7429 Loss_G: 0.7636 acc: 73.4%\n",
      "[BATCH 15/149] Loss_D: 0.7593 Loss_G: 0.7723 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7257 Loss_G: 0.7857 acc: 81.2%\n",
      "[BATCH 17/149] Loss_D: 0.7540 Loss_G: 0.8001 acc: 62.5%\n",
      "[BATCH 18/149] Loss_D: 0.7023 Loss_G: 0.7727 acc: 75.0%\n",
      "[BATCH 19/149] Loss_D: 0.7995 Loss_G: 0.7889 acc: 59.4%\n",
      "[BATCH 20/149] Loss_D: 0.7728 Loss_G: 0.7968 acc: 76.6%\n",
      "[BATCH 21/149] Loss_D: 0.7246 Loss_G: 0.7946 acc: 67.2%\n",
      "[BATCH 22/149] Loss_D: 0.7482 Loss_G: 0.7758 acc: 64.1%\n",
      "[EPOCH 3300] TEST ACC is : 68.4%\n",
      "[BATCH 23/149] Loss_D: 0.7622 Loss_G: 0.7635 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7473 Loss_G: 0.7843 acc: 73.4%\n",
      "[BATCH 25/149] Loss_D: 0.7574 Loss_G: 0.7751 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.7239 Loss_G: 0.7604 acc: 78.1%\n",
      "[BATCH 27/149] Loss_D: 0.7472 Loss_G: 0.7757 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7712 Loss_G: 0.7978 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.7333 Loss_G: 0.7752 acc: 68.8%\n",
      "[BATCH 30/149] Loss_D: 0.7579 Loss_G: 0.7758 acc: 70.3%\n",
      "[BATCH 31/149] Loss_D: 0.7397 Loss_G: 0.7825 acc: 67.2%\n",
      "[BATCH 32/149] Loss_D: 0.8189 Loss_G: 0.7927 acc: 68.8%\n",
      "[BATCH 33/149] Loss_D: 0.7477 Loss_G: 0.8002 acc: 73.4%\n",
      "[BATCH 34/149] Loss_D: 0.7986 Loss_G: 0.8068 acc: 70.3%\n",
      "[BATCH 35/149] Loss_D: 0.7295 Loss_G: 0.7810 acc: 75.0%\n",
      "[BATCH 36/149] Loss_D: 0.7491 Loss_G: 0.7685 acc: 75.0%\n",
      "[BATCH 37/149] Loss_D: 0.7046 Loss_G: 0.7514 acc: 79.7%\n",
      "[BATCH 38/149] Loss_D: 0.7230 Loss_G: 0.7557 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7302 Loss_G: 0.7381 acc: 81.2%\n",
      "[BATCH 40/149] Loss_D: 0.7528 Loss_G: 0.7664 acc: 79.7%\n",
      "[BATCH 41/149] Loss_D: 0.7072 Loss_G: 0.7576 acc: 79.7%\n",
      "[BATCH 42/149] Loss_D: 0.7024 Loss_G: 0.7536 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.7254 Loss_G: 0.7481 acc: 73.4%\n",
      "[BATCH 44/149] Loss_D: 0.7373 Loss_G: 0.7506 acc: 70.3%\n",
      "[BATCH 45/149] Loss_D: 0.7127 Loss_G: 0.7606 acc: 75.0%\n",
      "[BATCH 46/149] Loss_D: 0.7235 Loss_G: 0.7661 acc: 71.9%\n",
      "[BATCH 47/149] Loss_D: 0.7584 Loss_G: 0.7878 acc: 59.4%\n",
      "[BATCH 48/149] Loss_D: 0.7109 Loss_G: 0.7810 acc: 78.1%\n",
      "[BATCH 49/149] Loss_D: 0.7581 Loss_G: 0.7860 acc: 76.6%\n",
      "[BATCH 50/149] Loss_D: 0.7240 Loss_G: 0.7832 acc: 68.8%\n",
      "[BATCH 51/149] Loss_D: 0.7452 Loss_G: 0.7950 acc: 68.8%\n",
      "[BATCH 52/149] Loss_D: 0.7388 Loss_G: 0.7929 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.7347 Loss_G: 0.7839 acc: 59.4%\n",
      "[BATCH 54/149] Loss_D: 0.7794 Loss_G: 0.7855 acc: 71.9%\n",
      "[BATCH 55/149] Loss_D: 0.7544 Loss_G: 0.7760 acc: 75.0%\n",
      "[BATCH 56/149] Loss_D: 0.7537 Loss_G: 0.7678 acc: 68.8%\n",
      "[BATCH 57/149] Loss_D: 0.7714 Loss_G: 0.7874 acc: 76.6%\n",
      "[BATCH 58/149] Loss_D: 0.7575 Loss_G: 0.7796 acc: 70.3%\n",
      "[BATCH 59/149] Loss_D: 0.7425 Loss_G: 0.7792 acc: 73.4%\n",
      "[BATCH 60/149] Loss_D: 0.7159 Loss_G: 0.7572 acc: 60.9%\n",
      "[BATCH 61/149] Loss_D: 0.7508 Loss_G: 0.7521 acc: 78.1%\n",
      "[BATCH 62/149] Loss_D: 0.7683 Loss_G: 0.7748 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.7409 Loss_G: 0.7659 acc: 67.2%\n",
      "[BATCH 64/149] Loss_D: 0.7160 Loss_G: 0.7616 acc: 75.0%\n",
      "[BATCH 65/149] Loss_D: 0.7244 Loss_G: 0.7501 acc: 70.3%\n",
      "[BATCH 66/149] Loss_D: 0.7645 Loss_G: 0.7718 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7473 Loss_G: 0.7802 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.7563 Loss_G: 0.7829 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.7080 Loss_G: 0.7897 acc: 79.7%\n",
      "[BATCH 70/149] Loss_D: 0.7359 Loss_G: 0.7989 acc: 64.1%\n",
      "[BATCH 71/149] Loss_D: 0.7166 Loss_G: 0.7834 acc: 71.9%\n",
      "[BATCH 72/149] Loss_D: 0.8030 Loss_G: 0.7935 acc: 73.4%\n",
      "[EPOCH 3350] TEST ACC is : 67.2%\n",
      "[BATCH 73/149] Loss_D: 0.6839 Loss_G: 0.7558 acc: 71.9%\n",
      "[BATCH 74/149] Loss_D: 0.7422 Loss_G: 0.7588 acc: 82.8%\n",
      "[BATCH 75/149] Loss_D: 0.7153 Loss_G: 0.7625 acc: 79.7%\n",
      "[BATCH 76/149] Loss_D: 0.7339 Loss_G: 0.7527 acc: 78.1%\n",
      "[BATCH 77/149] Loss_D: 0.7347 Loss_G: 0.7572 acc: 76.6%\n",
      "[BATCH 78/149] Loss_D: 0.7398 Loss_G: 0.7610 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.7467 Loss_G: 0.7715 acc: 75.0%\n",
      "[BATCH 80/149] Loss_D: 0.7700 Loss_G: 0.7909 acc: 60.9%\n",
      "[BATCH 81/149] Loss_D: 0.6791 Loss_G: 0.7538 acc: 73.4%\n",
      "[BATCH 82/149] Loss_D: 0.7522 Loss_G: 0.7689 acc: 62.5%\n",
      "[BATCH 83/149] Loss_D: 0.7319 Loss_G: 0.7531 acc: 73.4%\n",
      "[BATCH 84/149] Loss_D: 0.7165 Loss_G: 0.7585 acc: 76.6%\n",
      "[BATCH 85/149] Loss_D: 0.7411 Loss_G: 0.7464 acc: 59.4%\n",
      "[BATCH 86/149] Loss_D: 0.7389 Loss_G: 0.7564 acc: 70.3%\n",
      "[BATCH 87/149] Loss_D: 0.7189 Loss_G: 0.7447 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.7733 Loss_G: 0.7622 acc: 73.4%\n",
      "[BATCH 89/149] Loss_D: 0.6958 Loss_G: 0.7407 acc: 67.2%\n",
      "[BATCH 90/149] Loss_D: 0.7634 Loss_G: 0.7571 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.7620 Loss_G: 0.7601 acc: 73.4%\n",
      "[BATCH 92/149] Loss_D: 0.7147 Loss_G: 0.7632 acc: 70.3%\n",
      "[BATCH 93/149] Loss_D: 0.7985 Loss_G: 0.7857 acc: 70.3%\n",
      "[BATCH 94/149] Loss_D: 0.7601 Loss_G: 0.7707 acc: 78.1%\n",
      "[BATCH 95/149] Loss_D: 0.7215 Loss_G: 0.7972 acc: 54.7%\n",
      "[BATCH 96/149] Loss_D: 0.7325 Loss_G: 0.7716 acc: 70.3%\n",
      "[BATCH 97/149] Loss_D: 0.7317 Loss_G: 0.7682 acc: 64.1%\n",
      "[BATCH 98/149] Loss_D: 0.7568 Loss_G: 0.7880 acc: 70.3%\n",
      "[BATCH 99/149] Loss_D: 0.8167 Loss_G: 0.7934 acc: 64.1%\n",
      "[BATCH 100/149] Loss_D: 0.7254 Loss_G: 0.7732 acc: 78.1%\n",
      "[BATCH 101/149] Loss_D: 0.7526 Loss_G: 0.8031 acc: 59.4%\n",
      "[BATCH 102/149] Loss_D: 0.7893 Loss_G: 0.7993 acc: 64.1%\n",
      "[BATCH 103/149] Loss_D: 0.7656 Loss_G: 0.7943 acc: 71.9%\n",
      "[BATCH 104/149] Loss_D: 0.7636 Loss_G: 0.7879 acc: 64.1%\n",
      "[BATCH 105/149] Loss_D: 0.7600 Loss_G: 0.7700 acc: 65.6%\n",
      "[BATCH 106/149] Loss_D: 0.7678 Loss_G: 0.7810 acc: 68.8%\n",
      "[BATCH 107/149] Loss_D: 0.7027 Loss_G: 0.7553 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7252 Loss_G: 0.7568 acc: 68.8%\n",
      "[BATCH 109/149] Loss_D: 0.6984 Loss_G: 0.7622 acc: 68.8%\n",
      "[BATCH 110/149] Loss_D: 0.7440 Loss_G: 0.7729 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.7824 Loss_G: 0.7763 acc: 73.4%\n",
      "[BATCH 112/149] Loss_D: 0.7378 Loss_G: 0.7615 acc: 75.0%\n",
      "[BATCH 113/149] Loss_D: 0.7344 Loss_G: 0.7706 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.7457 Loss_G: 0.7636 acc: 65.6%\n",
      "[BATCH 115/149] Loss_D: 0.7667 Loss_G: 0.7727 acc: 68.8%\n",
      "[BATCH 116/149] Loss_D: 0.7676 Loss_G: 0.7600 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.7011 Loss_G: 0.7730 acc: 73.4%\n",
      "[BATCH 118/149] Loss_D: 0.7489 Loss_G: 0.7625 acc: 76.6%\n",
      "[BATCH 119/149] Loss_D: 0.7713 Loss_G: 0.7691 acc: 68.8%\n",
      "[BATCH 120/149] Loss_D: 0.7441 Loss_G: 0.7663 acc: 62.5%\n",
      "[BATCH 121/149] Loss_D: 0.7234 Loss_G: 0.7641 acc: 76.6%\n",
      "[BATCH 122/149] Loss_D: 0.7735 Loss_G: 0.7817 acc: 67.2%\n",
      "[EPOCH 3400] TEST ACC is : 68.0%\n",
      "[BATCH 123/149] Loss_D: 0.7279 Loss_G: 0.7644 acc: 70.3%\n",
      "[BATCH 124/149] Loss_D: 0.7476 Loss_G: 0.7670 acc: 62.5%\n",
      "[BATCH 125/149] Loss_D: 0.7234 Loss_G: 0.7463 acc: 81.2%\n",
      "[BATCH 126/149] Loss_D: 0.6876 Loss_G: 0.7477 acc: 81.2%\n",
      "[BATCH 127/149] Loss_D: 0.7275 Loss_G: 0.7710 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7217 Loss_G: 0.7483 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.7337 Loss_G: 0.7636 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.7462 Loss_G: 0.7726 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.7456 Loss_G: 0.7662 acc: 64.1%\n",
      "[BATCH 132/149] Loss_D: 0.7496 Loss_G: 0.7740 acc: 76.6%\n",
      "[BATCH 133/149] Loss_D: 0.7260 Loss_G: 0.7670 acc: 65.6%\n",
      "[BATCH 134/149] Loss_D: 0.7510 Loss_G: 0.7644 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.7402 Loss_G: 0.7661 acc: 70.3%\n",
      "[BATCH 136/149] Loss_D: 0.7190 Loss_G: 0.7590 acc: 78.1%\n",
      "[BATCH 137/149] Loss_D: 0.7379 Loss_G: 0.7613 acc: 68.8%\n",
      "[BATCH 138/149] Loss_D: 0.7597 Loss_G: 0.7639 acc: 73.4%\n",
      "[BATCH 139/149] Loss_D: 0.8003 Loss_G: 0.7712 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.7400 Loss_G: 0.7725 acc: 76.6%\n",
      "[BATCH 141/149] Loss_D: 0.7557 Loss_G: 0.7660 acc: 67.2%\n",
      "[BATCH 142/149] Loss_D: 0.7531 Loss_G: 0.7733 acc: 73.4%\n",
      "[BATCH 143/149] Loss_D: 0.7336 Loss_G: 0.7667 acc: 75.0%\n",
      "[BATCH 144/149] Loss_D: 0.7500 Loss_G: 0.7848 acc: 60.9%\n",
      "[BATCH 145/149] Loss_D: 0.7825 Loss_G: 0.7764 acc: 75.0%\n",
      "[BATCH 146/149] Loss_D: 0.7405 Loss_G: 0.7787 acc: 68.8%\n",
      "[BATCH 147/149] Loss_D: 0.7222 Loss_G: 0.7618 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7453 Loss_G: 0.7701 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7335 Loss_G: 0.7637 acc: 71.9%\n",
      "-----THE [23/50] epoch end-----\n",
      "-----THE [24/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7154 Loss_G: 0.7600 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7743 Loss_G: 0.7664 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.6874 Loss_G: 0.7569 acc: 76.6%\n",
      "[BATCH 4/149] Loss_D: 0.7352 Loss_G: 0.7572 acc: 75.0%\n",
      "[BATCH 5/149] Loss_D: 0.7526 Loss_G: 0.7696 acc: 65.6%\n",
      "[BATCH 6/149] Loss_D: 0.7118 Loss_G: 0.7461 acc: 79.7%\n",
      "[BATCH 7/149] Loss_D: 0.7016 Loss_G: 0.7610 acc: 75.0%\n",
      "[BATCH 8/149] Loss_D: 0.7073 Loss_G: 0.7566 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.7576 Loss_G: 0.7473 acc: 70.3%\n",
      "[BATCH 10/149] Loss_D: 0.6801 Loss_G: 0.7414 acc: 75.0%\n",
      "[BATCH 11/149] Loss_D: 0.6975 Loss_G: 0.7304 acc: 73.4%\n",
      "[BATCH 12/149] Loss_D: 0.7683 Loss_G: 0.7546 acc: 71.9%\n",
      "[BATCH 13/149] Loss_D: 0.7025 Loss_G: 0.7640 acc: 57.8%\n",
      "[BATCH 14/149] Loss_D: 0.6937 Loss_G: 0.7418 acc: 73.4%\n",
      "[BATCH 15/149] Loss_D: 0.7792 Loss_G: 0.7518 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7145 Loss_G: 0.7494 acc: 79.7%\n",
      "[BATCH 17/149] Loss_D: 0.8255 Loss_G: 0.7825 acc: 65.6%\n",
      "[BATCH 18/149] Loss_D: 0.7002 Loss_G: 0.7425 acc: 78.1%\n",
      "[BATCH 19/149] Loss_D: 0.7890 Loss_G: 0.7691 acc: 67.2%\n",
      "[BATCH 20/149] Loss_D: 0.7077 Loss_G: 0.7559 acc: 62.5%\n",
      "[BATCH 21/149] Loss_D: 0.7318 Loss_G: 0.7463 acc: 70.3%\n",
      "[BATCH 22/149] Loss_D: 0.7391 Loss_G: 0.7579 acc: 73.4%\n",
      "[BATCH 23/149] Loss_D: 0.7407 Loss_G: 0.7799 acc: 71.9%\n",
      "[EPOCH 3450] TEST ACC is : 67.4%\n",
      "[BATCH 24/149] Loss_D: 0.7387 Loss_G: 0.7738 acc: 71.9%\n",
      "[BATCH 25/149] Loss_D: 0.7214 Loss_G: 0.7728 acc: 75.0%\n",
      "[BATCH 26/149] Loss_D: 0.7740 Loss_G: 0.8031 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.7337 Loss_G: 0.7968 acc: 59.4%\n",
      "[BATCH 28/149] Loss_D: 0.7044 Loss_G: 0.7681 acc: 73.4%\n",
      "[BATCH 29/149] Loss_D: 0.7224 Loss_G: 0.7635 acc: 56.2%\n",
      "[BATCH 30/149] Loss_D: 0.7423 Loss_G: 0.7758 acc: 71.9%\n",
      "[BATCH 31/149] Loss_D: 0.7229 Loss_G: 0.7576 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.7721 Loss_G: 0.7592 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7610 Loss_G: 0.7804 acc: 67.2%\n",
      "[BATCH 34/149] Loss_D: 0.7108 Loss_G: 0.7608 acc: 79.7%\n",
      "[BATCH 35/149] Loss_D: 0.7392 Loss_G: 0.7657 acc: 68.8%\n",
      "[BATCH 36/149] Loss_D: 0.7389 Loss_G: 0.7694 acc: 76.6%\n",
      "[BATCH 37/149] Loss_D: 0.7365 Loss_G: 0.7539 acc: 75.0%\n",
      "[BATCH 38/149] Loss_D: 0.7126 Loss_G: 0.7382 acc: 67.2%\n",
      "[BATCH 39/149] Loss_D: 0.7330 Loss_G: 0.7439 acc: 59.4%\n",
      "[BATCH 40/149] Loss_D: 0.7445 Loss_G: 0.7526 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7414 Loss_G: 0.7694 acc: 71.9%\n",
      "[BATCH 42/149] Loss_D: 0.7181 Loss_G: 0.7596 acc: 78.1%\n",
      "[BATCH 43/149] Loss_D: 0.7651 Loss_G: 0.7633 acc: 76.6%\n",
      "[BATCH 44/149] Loss_D: 0.7494 Loss_G: 0.7704 acc: 67.2%\n",
      "[BATCH 45/149] Loss_D: 0.7273 Loss_G: 0.7436 acc: 70.3%\n",
      "[BATCH 46/149] Loss_D: 0.7381 Loss_G: 0.7539 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.7660 Loss_G: 0.7671 acc: 71.9%\n",
      "[BATCH 48/149] Loss_D: 0.7620 Loss_G: 0.7605 acc: 73.4%\n",
      "[BATCH 49/149] Loss_D: 0.7325 Loss_G: 0.7564 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.7350 Loss_G: 0.7645 acc: 64.1%\n",
      "[BATCH 51/149] Loss_D: 0.7644 Loss_G: 0.7706 acc: 57.8%\n",
      "[BATCH 52/149] Loss_D: 0.7605 Loss_G: 0.7690 acc: 76.6%\n",
      "[BATCH 53/149] Loss_D: 0.7526 Loss_G: 0.7775 acc: 71.9%\n",
      "[BATCH 54/149] Loss_D: 0.7295 Loss_G: 0.7865 acc: 71.9%\n",
      "[BATCH 55/149] Loss_D: 0.7307 Loss_G: 0.7692 acc: 62.5%\n",
      "[BATCH 56/149] Loss_D: 0.7625 Loss_G: 0.7594 acc: 73.4%\n",
      "[BATCH 57/149] Loss_D: 0.7261 Loss_G: 0.7553 acc: 78.1%\n",
      "[BATCH 58/149] Loss_D: 0.7412 Loss_G: 0.7588 acc: 68.8%\n",
      "[BATCH 59/149] Loss_D: 0.7457 Loss_G: 0.7651 acc: 67.2%\n",
      "[BATCH 60/149] Loss_D: 0.7350 Loss_G: 0.7575 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.7188 Loss_G: 0.7610 acc: 76.6%\n",
      "[BATCH 62/149] Loss_D: 0.7580 Loss_G: 0.7574 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.7289 Loss_G: 0.7583 acc: 68.8%\n",
      "[BATCH 64/149] Loss_D: 0.8185 Loss_G: 0.7835 acc: 76.6%\n",
      "[BATCH 65/149] Loss_D: 0.7548 Loss_G: 0.7848 acc: 67.2%\n",
      "[BATCH 66/149] Loss_D: 0.7562 Loss_G: 0.7803 acc: 76.6%\n",
      "[BATCH 67/149] Loss_D: 0.7331 Loss_G: 0.7655 acc: 75.0%\n",
      "[BATCH 68/149] Loss_D: 0.7435 Loss_G: 0.7737 acc: 76.6%\n",
      "[BATCH 69/149] Loss_D: 0.7727 Loss_G: 0.7854 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.7201 Loss_G: 0.7737 acc: 70.3%\n",
      "[BATCH 71/149] Loss_D: 0.7182 Loss_G: 0.7598 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6910 Loss_G: 0.7388 acc: 73.4%\n",
      "[BATCH 73/149] Loss_D: 0.7193 Loss_G: 0.7425 acc: 71.9%\n",
      "[EPOCH 3500] TEST ACC is : 67.6%\n",
      "[BATCH 74/149] Loss_D: 0.7601 Loss_G: 0.7634 acc: 56.2%\n",
      "[BATCH 75/149] Loss_D: 0.7222 Loss_G: 0.7502 acc: 81.2%\n",
      "[BATCH 76/149] Loss_D: 0.6793 Loss_G: 0.7377 acc: 70.3%\n",
      "[BATCH 77/149] Loss_D: 0.7314 Loss_G: 0.7535 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.7942 Loss_G: 0.7794 acc: 57.8%\n",
      "[BATCH 79/149] Loss_D: 0.8182 Loss_G: 0.7838 acc: 71.9%\n",
      "[BATCH 80/149] Loss_D: 0.7495 Loss_G: 0.7763 acc: 75.0%\n",
      "[BATCH 81/149] Loss_D: 0.7604 Loss_G: 0.7701 acc: 75.0%\n",
      "[BATCH 82/149] Loss_D: 0.7117 Loss_G: 0.7633 acc: 79.7%\n",
      "[BATCH 83/149] Loss_D: 0.7439 Loss_G: 0.7525 acc: 78.1%\n",
      "[BATCH 84/149] Loss_D: 0.7431 Loss_G: 0.7561 acc: 68.8%\n",
      "[BATCH 85/149] Loss_D: 0.7402 Loss_G: 0.7467 acc: 79.7%\n",
      "[BATCH 86/149] Loss_D: 0.7949 Loss_G: 0.7943 acc: 70.3%\n",
      "[BATCH 87/149] Loss_D: 0.7563 Loss_G: 0.7926 acc: 67.2%\n",
      "[BATCH 88/149] Loss_D: 0.8286 Loss_G: 0.7869 acc: 73.4%\n",
      "[BATCH 89/149] Loss_D: 0.7452 Loss_G: 0.7714 acc: 75.0%\n",
      "[BATCH 90/149] Loss_D: 0.7568 Loss_G: 0.7638 acc: 71.9%\n",
      "[BATCH 91/149] Loss_D: 0.7898 Loss_G: 0.7774 acc: 73.4%\n",
      "[BATCH 92/149] Loss_D: 0.7723 Loss_G: 0.7713 acc: 76.6%\n",
      "[BATCH 93/149] Loss_D: 0.7251 Loss_G: 0.7752 acc: 62.5%\n",
      "[BATCH 94/149] Loss_D: 0.7089 Loss_G: 0.7502 acc: 82.8%\n",
      "[BATCH 95/149] Loss_D: 0.7507 Loss_G: 0.7563 acc: 60.9%\n",
      "[BATCH 96/149] Loss_D: 0.7061 Loss_G: 0.7620 acc: 75.0%\n",
      "[BATCH 97/149] Loss_D: 0.7505 Loss_G: 0.7699 acc: 67.2%\n",
      "[BATCH 98/149] Loss_D: 0.7427 Loss_G: 0.7772 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7476 Loss_G: 0.7688 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.7814 Loss_G: 0.7768 acc: 70.3%\n",
      "[BATCH 101/149] Loss_D: 0.7626 Loss_G: 0.7896 acc: 65.6%\n",
      "[BATCH 102/149] Loss_D: 0.7192 Loss_G: 0.7663 acc: 78.1%\n",
      "[BATCH 103/149] Loss_D: 0.7338 Loss_G: 0.7608 acc: 71.9%\n",
      "[BATCH 104/149] Loss_D: 0.7144 Loss_G: 0.7675 acc: 76.6%\n",
      "[BATCH 105/149] Loss_D: 0.7747 Loss_G: 0.7829 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7328 Loss_G: 0.7746 acc: 78.1%\n",
      "[BATCH 107/149] Loss_D: 0.7615 Loss_G: 0.7825 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7583 Loss_G: 0.7738 acc: 76.6%\n",
      "[BATCH 109/149] Loss_D: 0.7391 Loss_G: 0.7682 acc: 75.0%\n",
      "[BATCH 110/149] Loss_D: 0.7427 Loss_G: 0.7778 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7126 Loss_G: 0.7536 acc: 65.6%\n",
      "[BATCH 112/149] Loss_D: 0.7250 Loss_G: 0.7550 acc: 78.1%\n",
      "[BATCH 113/149] Loss_D: 0.7495 Loss_G: 0.7572 acc: 65.6%\n",
      "[BATCH 114/149] Loss_D: 0.7254 Loss_G: 0.7595 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7132 Loss_G: 0.7428 acc: 71.9%\n",
      "[BATCH 116/149] Loss_D: 0.7301 Loss_G: 0.7484 acc: 81.2%\n",
      "[BATCH 117/149] Loss_D: 0.7176 Loss_G: 0.7581 acc: 71.9%\n",
      "[BATCH 118/149] Loss_D: 0.7707 Loss_G: 0.7736 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.7305 Loss_G: 0.7632 acc: 73.4%\n",
      "[BATCH 120/149] Loss_D: 0.7703 Loss_G: 0.7787 acc: 57.8%\n",
      "[BATCH 121/149] Loss_D: 0.7330 Loss_G: 0.7641 acc: 64.1%\n",
      "[BATCH 122/149] Loss_D: 0.7023 Loss_G: 0.7718 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7463 Loss_G: 0.7627 acc: 71.9%\n",
      "[EPOCH 3550] TEST ACC is : 68.2%\n",
      "[BATCH 124/149] Loss_D: 0.7490 Loss_G: 0.7546 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.7420 Loss_G: 0.7680 acc: 68.8%\n",
      "[BATCH 126/149] Loss_D: 0.7064 Loss_G: 0.7450 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.7499 Loss_G: 0.7540 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.7245 Loss_G: 0.7528 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.7052 Loss_G: 0.7625 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.7487 Loss_G: 0.7565 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7319 Loss_G: 0.7717 acc: 70.3%\n",
      "[BATCH 132/149] Loss_D: 0.7470 Loss_G: 0.7579 acc: 71.9%\n",
      "[BATCH 133/149] Loss_D: 0.7378 Loss_G: 0.7629 acc: 76.6%\n",
      "[BATCH 134/149] Loss_D: 0.7138 Loss_G: 0.7579 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7690 Loss_G: 0.7686 acc: 78.1%\n",
      "[BATCH 136/149] Loss_D: 0.7598 Loss_G: 0.7600 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.7377 Loss_G: 0.7609 acc: 79.7%\n",
      "[BATCH 138/149] Loss_D: 0.7806 Loss_G: 0.7630 acc: 67.2%\n",
      "[BATCH 139/149] Loss_D: 0.7735 Loss_G: 0.7662 acc: 79.7%\n",
      "[BATCH 140/149] Loss_D: 0.7281 Loss_G: 0.7696 acc: 75.0%\n",
      "[BATCH 141/149] Loss_D: 0.7948 Loss_G: 0.7787 acc: 59.4%\n",
      "[BATCH 142/149] Loss_D: 0.7402 Loss_G: 0.7737 acc: 73.4%\n",
      "[BATCH 143/149] Loss_D: 0.7267 Loss_G: 0.7670 acc: 67.2%\n",
      "[BATCH 144/149] Loss_D: 0.7565 Loss_G: 0.7800 acc: 73.4%\n",
      "[BATCH 145/149] Loss_D: 0.7433 Loss_G: 0.7786 acc: 68.8%\n",
      "[BATCH 146/149] Loss_D: 0.7437 Loss_G: 0.7734 acc: 60.9%\n",
      "[BATCH 147/149] Loss_D: 0.7990 Loss_G: 0.7822 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.7311 Loss_G: 0.7855 acc: 70.3%\n",
      "[BATCH 149/149] Loss_D: 0.7036 Loss_G: 0.7598 acc: 75.0%\n",
      "-----THE [24/50] epoch end-----\n",
      "-----THE [25/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7482 Loss_G: 0.7550 acc: 78.1%\n",
      "[BATCH 2/149] Loss_D: 0.7567 Loss_G: 0.7564 acc: 78.1%\n",
      "[BATCH 3/149] Loss_D: 0.7171 Loss_G: 0.7519 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.7234 Loss_G: 0.7428 acc: 65.6%\n",
      "[BATCH 5/149] Loss_D: 0.7652 Loss_G: 0.7619 acc: 75.0%\n",
      "[BATCH 6/149] Loss_D: 0.7532 Loss_G: 0.7685 acc: 67.2%\n",
      "[BATCH 7/149] Loss_D: 0.7002 Loss_G: 0.7581 acc: 73.4%\n",
      "[BATCH 8/149] Loss_D: 0.7206 Loss_G: 0.7586 acc: 60.9%\n",
      "[BATCH 9/149] Loss_D: 0.7351 Loss_G: 0.7562 acc: 76.6%\n",
      "[BATCH 10/149] Loss_D: 0.7198 Loss_G: 0.7615 acc: 75.0%\n",
      "[BATCH 11/149] Loss_D: 0.7088 Loss_G: 0.7585 acc: 65.6%\n",
      "[BATCH 12/149] Loss_D: 0.7251 Loss_G: 0.7552 acc: 75.0%\n",
      "[BATCH 13/149] Loss_D: 0.7416 Loss_G: 0.7610 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.7001 Loss_G: 0.7578 acc: 71.9%\n",
      "[BATCH 15/149] Loss_D: 0.7952 Loss_G: 0.7733 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.7827 Loss_G: 0.7733 acc: 73.4%\n",
      "[BATCH 17/149] Loss_D: 0.7379 Loss_G: 0.7683 acc: 70.3%\n",
      "[BATCH 18/149] Loss_D: 0.7282 Loss_G: 0.7712 acc: 67.2%\n",
      "[BATCH 19/149] Loss_D: 0.7859 Loss_G: 0.7887 acc: 75.0%\n",
      "[BATCH 20/149] Loss_D: 0.6845 Loss_G: 0.7522 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.7078 Loss_G: 0.7376 acc: 68.8%\n",
      "[BATCH 22/149] Loss_D: 0.7279 Loss_G: 0.7484 acc: 73.4%\n",
      "[BATCH 23/149] Loss_D: 0.7446 Loss_G: 0.7595 acc: 62.5%\n",
      "[BATCH 24/149] Loss_D: 0.7050 Loss_G: 0.7528 acc: 82.8%\n",
      "[EPOCH 3600] TEST ACC is : 66.4%\n",
      "[BATCH 25/149] Loss_D: 0.7078 Loss_G: 0.7478 acc: 82.8%\n",
      "[BATCH 26/149] Loss_D: 0.6984 Loss_G: 0.7382 acc: 73.4%\n",
      "[BATCH 27/149] Loss_D: 0.7391 Loss_G: 0.7566 acc: 71.9%\n",
      "[BATCH 28/149] Loss_D: 0.7355 Loss_G: 0.7489 acc: 78.1%\n",
      "[BATCH 29/149] Loss_D: 0.7677 Loss_G: 0.7808 acc: 75.0%\n",
      "[BATCH 30/149] Loss_D: 0.7579 Loss_G: 0.7771 acc: 70.3%\n",
      "[BATCH 31/149] Loss_D: 0.7612 Loss_G: 0.7728 acc: 78.1%\n",
      "[BATCH 32/149] Loss_D: 0.7155 Loss_G: 0.7668 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.7219 Loss_G: 0.7522 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.7113 Loss_G: 0.7409 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7630 Loss_G: 0.7629 acc: 76.6%\n",
      "[BATCH 36/149] Loss_D: 0.7528 Loss_G: 0.7748 acc: 68.8%\n",
      "[BATCH 37/149] Loss_D: 0.7674 Loss_G: 0.7622 acc: 70.3%\n",
      "[BATCH 38/149] Loss_D: 0.7560 Loss_G: 0.7680 acc: 73.4%\n",
      "[BATCH 39/149] Loss_D: 0.7539 Loss_G: 0.7777 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.7170 Loss_G: 0.7646 acc: 76.6%\n",
      "[BATCH 41/149] Loss_D: 0.7876 Loss_G: 0.7949 acc: 71.9%\n",
      "[BATCH 42/149] Loss_D: 0.7002 Loss_G: 0.7836 acc: 75.0%\n",
      "[BATCH 43/149] Loss_D: 0.7087 Loss_G: 0.7601 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.7435 Loss_G: 0.7638 acc: 65.6%\n",
      "[BATCH 45/149] Loss_D: 0.7414 Loss_G: 0.7574 acc: 75.0%\n",
      "[BATCH 46/149] Loss_D: 0.7501 Loss_G: 0.7696 acc: 73.4%\n",
      "[BATCH 47/149] Loss_D: 0.7849 Loss_G: 0.7865 acc: 65.6%\n",
      "[BATCH 48/149] Loss_D: 0.7554 Loss_G: 0.7802 acc: 73.4%\n",
      "[BATCH 49/149] Loss_D: 0.7173 Loss_G: 0.7729 acc: 78.1%\n",
      "[BATCH 50/149] Loss_D: 0.7529 Loss_G: 0.7645 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.7692 Loss_G: 0.7696 acc: 76.6%\n",
      "[BATCH 52/149] Loss_D: 0.7319 Loss_G: 0.7748 acc: 79.7%\n",
      "[BATCH 53/149] Loss_D: 0.7375 Loss_G: 0.7721 acc: 65.6%\n",
      "[BATCH 54/149] Loss_D: 0.7375 Loss_G: 0.7696 acc: 73.4%\n",
      "[BATCH 55/149] Loss_D: 0.7128 Loss_G: 0.7522 acc: 68.8%\n",
      "[BATCH 56/149] Loss_D: 0.7533 Loss_G: 0.7647 acc: 81.2%\n",
      "[BATCH 57/149] Loss_D: 0.7352 Loss_G: 0.7662 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7141 Loss_G: 0.7502 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.7275 Loss_G: 0.7478 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7392 Loss_G: 0.7785 acc: 67.2%\n",
      "[BATCH 61/149] Loss_D: 0.7105 Loss_G: 0.7412 acc: 75.0%\n",
      "[BATCH 62/149] Loss_D: 0.7653 Loss_G: 0.7577 acc: 67.2%\n",
      "[BATCH 63/149] Loss_D: 0.7175 Loss_G: 0.7541 acc: 67.2%\n",
      "[BATCH 64/149] Loss_D: 0.7039 Loss_G: 0.7587 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.7344 Loss_G: 0.7620 acc: 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.7390 Loss_G: 0.7702 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7364 Loss_G: 0.7684 acc: 68.8%\n",
      "[BATCH 68/149] Loss_D: 0.6806 Loss_G: 0.7538 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7120 Loss_G: 0.7562 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.7445 Loss_G: 0.7755 acc: 78.1%\n",
      "[BATCH 71/149] Loss_D: 0.7292 Loss_G: 0.7633 acc: 71.9%\n",
      "[BATCH 72/149] Loss_D: 0.7078 Loss_G: 0.7603 acc: 75.0%\n",
      "[BATCH 73/149] Loss_D: 0.7509 Loss_G: 0.7785 acc: 68.8%\n",
      "[BATCH 74/149] Loss_D: 0.7347 Loss_G: 0.7621 acc: 75.0%\n",
      "[EPOCH 3650] TEST ACC is : 68.0%\n",
      "[BATCH 75/149] Loss_D: 0.7759 Loss_G: 0.7648 acc: 65.6%\n",
      "[BATCH 76/149] Loss_D: 0.7610 Loss_G: 0.7700 acc: 78.1%\n",
      "[BATCH 77/149] Loss_D: 0.7302 Loss_G: 0.7614 acc: 78.1%\n",
      "[BATCH 78/149] Loss_D: 0.8169 Loss_G: 0.7895 acc: 70.3%\n",
      "[BATCH 79/149] Loss_D: 0.7411 Loss_G: 0.7704 acc: 79.7%\n",
      "[BATCH 80/149] Loss_D: 0.7477 Loss_G: 0.7695 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7041 Loss_G: 0.7597 acc: 64.1%\n",
      "[BATCH 82/149] Loss_D: 0.7165 Loss_G: 0.7530 acc: 68.8%\n",
      "[BATCH 83/149] Loss_D: 0.7581 Loss_G: 0.7645 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.7134 Loss_G: 0.7530 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.7451 Loss_G: 0.7609 acc: 73.4%\n",
      "[BATCH 86/149] Loss_D: 0.7353 Loss_G: 0.7610 acc: 79.7%\n",
      "[BATCH 87/149] Loss_D: 0.7093 Loss_G: 0.7503 acc: 76.6%\n",
      "[BATCH 88/149] Loss_D: 0.7038 Loss_G: 0.7533 acc: 76.6%\n",
      "[BATCH 89/149] Loss_D: 0.7337 Loss_G: 0.7636 acc: 71.9%\n",
      "[BATCH 90/149] Loss_D: 0.7690 Loss_G: 0.7590 acc: 62.5%\n",
      "[BATCH 91/149] Loss_D: 0.7376 Loss_G: 0.7593 acc: 75.0%\n",
      "[BATCH 92/149] Loss_D: 0.7494 Loss_G: 0.7659 acc: 75.0%\n",
      "[BATCH 93/149] Loss_D: 0.7662 Loss_G: 0.7772 acc: 78.1%\n",
      "[BATCH 94/149] Loss_D: 0.7611 Loss_G: 0.7688 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7253 Loss_G: 0.7607 acc: 64.1%\n",
      "[BATCH 96/149] Loss_D: 0.7852 Loss_G: 0.7699 acc: 73.4%\n",
      "[BATCH 97/149] Loss_D: 0.7220 Loss_G: 0.7529 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7190 Loss_G: 0.7603 acc: 73.4%\n",
      "[BATCH 99/149] Loss_D: 0.7352 Loss_G: 0.7562 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.7489 Loss_G: 0.7704 acc: 62.5%\n",
      "[BATCH 101/149] Loss_D: 0.7499 Loss_G: 0.7620 acc: 68.8%\n",
      "[BATCH 102/149] Loss_D: 0.7501 Loss_G: 0.7678 acc: 67.2%\n",
      "[BATCH 103/149] Loss_D: 0.7612 Loss_G: 0.7634 acc: 73.4%\n",
      "[BATCH 104/149] Loss_D: 0.7290 Loss_G: 0.7704 acc: 75.0%\n",
      "[BATCH 105/149] Loss_D: 0.7413 Loss_G: 0.7719 acc: 71.9%\n",
      "[BATCH 106/149] Loss_D: 0.7553 Loss_G: 0.7729 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.7686 Loss_G: 0.7704 acc: 75.0%\n",
      "[BATCH 108/149] Loss_D: 0.6968 Loss_G: 0.7566 acc: 78.1%\n",
      "[BATCH 109/149] Loss_D: 0.7223 Loss_G: 0.7562 acc: 76.6%\n",
      "[BATCH 110/149] Loss_D: 0.7399 Loss_G: 0.7667 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.7361 Loss_G: 0.7652 acc: 70.3%\n",
      "[BATCH 112/149] Loss_D: 0.7218 Loss_G: 0.7627 acc: 71.9%\n",
      "[BATCH 113/149] Loss_D: 0.7269 Loss_G: 0.7668 acc: 73.4%\n",
      "[BATCH 114/149] Loss_D: 0.7599 Loss_G: 0.7822 acc: 70.3%\n",
      "[BATCH 115/149] Loss_D: 0.7310 Loss_G: 0.7740 acc: 78.1%\n",
      "[BATCH 116/149] Loss_D: 0.7611 Loss_G: 0.7648 acc: 60.9%\n",
      "[BATCH 117/149] Loss_D: 0.7092 Loss_G: 0.7563 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7328 Loss_G: 0.7579 acc: 78.1%\n",
      "[BATCH 119/149] Loss_D: 0.7375 Loss_G: 0.7583 acc: 70.3%\n",
      "[BATCH 120/149] Loss_D: 0.7967 Loss_G: 0.7745 acc: 78.1%\n",
      "[BATCH 121/149] Loss_D: 0.7782 Loss_G: 0.7713 acc: 75.0%\n",
      "[BATCH 122/149] Loss_D: 0.7577 Loss_G: 0.7650 acc: 73.4%\n",
      "[BATCH 123/149] Loss_D: 0.7196 Loss_G: 0.7565 acc: 75.0%\n",
      "[BATCH 124/149] Loss_D: 0.7259 Loss_G: 0.7577 acc: 70.3%\n",
      "[EPOCH 3700] TEST ACC is : 68.0%\n",
      "[BATCH 125/149] Loss_D: 0.7744 Loss_G: 0.7600 acc: 71.9%\n",
      "[BATCH 126/149] Loss_D: 0.7310 Loss_G: 0.7642 acc: 71.9%\n",
      "[BATCH 127/149] Loss_D: 0.7707 Loss_G: 0.7721 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.7236 Loss_G: 0.7553 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.7034 Loss_G: 0.7595 acc: 73.4%\n",
      "[BATCH 130/149] Loss_D: 0.7254 Loss_G: 0.7617 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7372 Loss_G: 0.7710 acc: 73.4%\n",
      "[BATCH 132/149] Loss_D: 0.7528 Loss_G: 0.7719 acc: 71.9%\n",
      "[BATCH 133/149] Loss_D: 0.7372 Loss_G: 0.7669 acc: 73.4%\n",
      "[BATCH 134/149] Loss_D: 0.7661 Loss_G: 0.7903 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.7844 Loss_G: 0.7879 acc: 75.0%\n",
      "[BATCH 136/149] Loss_D: 0.7266 Loss_G: 0.7590 acc: 78.1%\n",
      "[BATCH 137/149] Loss_D: 0.7564 Loss_G: 0.7679 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.7426 Loss_G: 0.7701 acc: 64.1%\n",
      "[BATCH 139/149] Loss_D: 0.7329 Loss_G: 0.7657 acc: 78.1%\n",
      "[BATCH 140/149] Loss_D: 0.8192 Loss_G: 0.7895 acc: 65.6%\n",
      "[BATCH 141/149] Loss_D: 0.7714 Loss_G: 0.7821 acc: 78.1%\n",
      "[BATCH 142/149] Loss_D: 0.7390 Loss_G: 0.7643 acc: 53.1%\n",
      "[BATCH 143/149] Loss_D: 0.7868 Loss_G: 0.7698 acc: 62.5%\n",
      "[BATCH 144/149] Loss_D: 0.7669 Loss_G: 0.7749 acc: 73.4%\n",
      "[BATCH 145/149] Loss_D: 0.7299 Loss_G: 0.7570 acc: 62.5%\n",
      "[BATCH 146/149] Loss_D: 0.7138 Loss_G: 0.7614 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.7836 Loss_G: 0.7728 acc: 75.0%\n",
      "[BATCH 148/149] Loss_D: 0.7458 Loss_G: 0.7753 acc: 56.2%\n",
      "[BATCH 149/149] Loss_D: 0.7623 Loss_G: 0.7748 acc: 56.2%\n",
      "-----THE [25/50] epoch end-----\n",
      "-----THE [26/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7343 Loss_G: 0.7614 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.7004 Loss_G: 0.7483 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7698 Loss_G: 0.7668 acc: 68.8%\n",
      "[BATCH 4/149] Loss_D: 0.7261 Loss_G: 0.7655 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.7362 Loss_G: 0.7567 acc: 78.1%\n",
      "[BATCH 6/149] Loss_D: 0.7470 Loss_G: 0.7573 acc: 73.4%\n",
      "[BATCH 7/149] Loss_D: 0.7491 Loss_G: 0.7671 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.7663 Loss_G: 0.7833 acc: 79.7%\n",
      "[BATCH 9/149] Loss_D: 0.7055 Loss_G: 0.7695 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.7564 Loss_G: 0.7738 acc: 68.8%\n",
      "[BATCH 11/149] Loss_D: 0.7264 Loss_G: 0.7728 acc: 71.9%\n",
      "[BATCH 12/149] Loss_D: 0.7113 Loss_G: 0.7733 acc: 71.9%\n",
      "[BATCH 13/149] Loss_D: 0.7518 Loss_G: 0.7518 acc: 81.2%\n",
      "[BATCH 14/149] Loss_D: 0.7226 Loss_G: 0.7549 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.7108 Loss_G: 0.7417 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.7345 Loss_G: 0.7431 acc: 68.8%\n",
      "[BATCH 17/149] Loss_D: 0.7092 Loss_G: 0.7538 acc: 67.2%\n",
      "[BATCH 18/149] Loss_D: 0.7361 Loss_G: 0.7535 acc: 76.6%\n",
      "[BATCH 19/149] Loss_D: 0.7661 Loss_G: 0.7625 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7755 Loss_G: 0.7634 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.7012 Loss_G: 0.7692 acc: 79.7%\n",
      "[BATCH 22/149] Loss_D: 0.7490 Loss_G: 0.7562 acc: 75.0%\n",
      "[BATCH 23/149] Loss_D: 0.7085 Loss_G: 0.7565 acc: 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.7449 Loss_G: 0.7858 acc: 73.4%\n",
      "[BATCH 25/149] Loss_D: 0.7297 Loss_G: 0.7775 acc: 73.4%\n",
      "[EPOCH 3750] TEST ACC is : 67.0%\n",
      "[BATCH 26/149] Loss_D: 0.7591 Loss_G: 0.7698 acc: 73.4%\n",
      "[BATCH 27/149] Loss_D: 0.7380 Loss_G: 0.7586 acc: 75.0%\n",
      "[BATCH 28/149] Loss_D: 0.7243 Loss_G: 0.7604 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.7782 Loss_G: 0.7837 acc: 75.0%\n",
      "[BATCH 30/149] Loss_D: 0.7950 Loss_G: 0.8099 acc: 79.7%\n",
      "[BATCH 31/149] Loss_D: 0.7301 Loss_G: 0.7829 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.7448 Loss_G: 0.7653 acc: 81.2%\n",
      "[BATCH 33/149] Loss_D: 0.7664 Loss_G: 0.7659 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.7581 Loss_G: 0.7792 acc: 76.6%\n",
      "[BATCH 35/149] Loss_D: 0.7383 Loss_G: 0.7484 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7223 Loss_G: 0.7475 acc: 71.9%\n",
      "[BATCH 37/149] Loss_D: 0.6937 Loss_G: 0.7436 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7503 Loss_G: 0.7572 acc: 64.1%\n",
      "[BATCH 39/149] Loss_D: 0.7095 Loss_G: 0.7448 acc: 59.4%\n",
      "[BATCH 40/149] Loss_D: 0.7177 Loss_G: 0.7362 acc: 71.9%\n",
      "[BATCH 41/149] Loss_D: 0.7578 Loss_G: 0.7672 acc: 67.2%\n",
      "[BATCH 42/149] Loss_D: 0.7624 Loss_G: 0.7658 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.7324 Loss_G: 0.7526 acc: 73.4%\n",
      "[BATCH 44/149] Loss_D: 0.7956 Loss_G: 0.7732 acc: 73.4%\n",
      "[BATCH 45/149] Loss_D: 0.7235 Loss_G: 0.7654 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.7067 Loss_G: 0.7640 acc: 68.8%\n",
      "[BATCH 47/149] Loss_D: 0.7886 Loss_G: 0.7630 acc: 62.5%\n",
      "[BATCH 48/149] Loss_D: 0.7566 Loss_G: 0.7680 acc: 73.4%\n",
      "[BATCH 49/149] Loss_D: 0.7106 Loss_G: 0.7522 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.7079 Loss_G: 0.7567 acc: 70.3%\n",
      "[BATCH 51/149] Loss_D: 0.7434 Loss_G: 0.7590 acc: 75.0%\n",
      "[BATCH 52/149] Loss_D: 0.7517 Loss_G: 0.7619 acc: 71.9%\n",
      "[BATCH 53/149] Loss_D: 0.7403 Loss_G: 0.7659 acc: 78.1%\n",
      "[BATCH 54/149] Loss_D: 0.7239 Loss_G: 0.7488 acc: 75.0%\n",
      "[BATCH 55/149] Loss_D: 0.7312 Loss_G: 0.7621 acc: 71.9%\n",
      "[BATCH 56/149] Loss_D: 0.7265 Loss_G: 0.7622 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.7393 Loss_G: 0.7659 acc: 62.5%\n",
      "[BATCH 58/149] Loss_D: 0.7294 Loss_G: 0.7561 acc: 70.3%\n",
      "[BATCH 59/149] Loss_D: 0.7403 Loss_G: 0.7543 acc: 75.0%\n",
      "[BATCH 60/149] Loss_D: 0.7305 Loss_G: 0.7469 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.8153 Loss_G: 0.7831 acc: 75.0%\n",
      "[BATCH 62/149] Loss_D: 0.7519 Loss_G: 0.7845 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.7243 Loss_G: 0.7643 acc: 71.9%\n",
      "[BATCH 64/149] Loss_D: 0.7295 Loss_G: 0.7476 acc: 57.8%\n",
      "[BATCH 65/149] Loss_D: 0.7651 Loss_G: 0.7726 acc: 67.2%\n",
      "[BATCH 66/149] Loss_D: 0.7647 Loss_G: 0.7741 acc: 68.8%\n",
      "[BATCH 67/149] Loss_D: 0.7598 Loss_G: 0.7591 acc: 76.6%\n",
      "[BATCH 68/149] Loss_D: 0.7046 Loss_G: 0.7527 acc: 75.0%\n",
      "[BATCH 69/149] Loss_D: 0.7654 Loss_G: 0.7641 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.7485 Loss_G: 0.7828 acc: 75.0%\n",
      "[BATCH 71/149] Loss_D: 0.7341 Loss_G: 0.7803 acc: 75.0%\n",
      "[BATCH 72/149] Loss_D: 0.7484 Loss_G: 0.7739 acc: 76.6%\n",
      "[BATCH 73/149] Loss_D: 0.7355 Loss_G: 0.7650 acc: 62.5%\n",
      "[BATCH 74/149] Loss_D: 0.7387 Loss_G: 0.7643 acc: 68.8%\n",
      "[BATCH 75/149] Loss_D: 0.7575 Loss_G: 0.7615 acc: 70.3%\n",
      "[EPOCH 3800] TEST ACC is : 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7211 Loss_G: 0.7609 acc: 71.9%\n",
      "[BATCH 77/149] Loss_D: 0.7471 Loss_G: 0.7635 acc: 70.3%\n",
      "[BATCH 78/149] Loss_D: 0.6899 Loss_G: 0.7462 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.7199 Loss_G: 0.7537 acc: 78.1%\n",
      "[BATCH 80/149] Loss_D: 0.7906 Loss_G: 0.7703 acc: 65.6%\n",
      "[BATCH 81/149] Loss_D: 0.7098 Loss_G: 0.7689 acc: 67.2%\n",
      "[BATCH 82/149] Loss_D: 0.7528 Loss_G: 0.7574 acc: 73.4%\n",
      "[BATCH 83/149] Loss_D: 0.7217 Loss_G: 0.7555 acc: 78.1%\n",
      "[BATCH 84/149] Loss_D: 0.7073 Loss_G: 0.7586 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.7293 Loss_G: 0.7633 acc: 71.9%\n",
      "[BATCH 86/149] Loss_D: 0.7317 Loss_G: 0.7534 acc: 60.9%\n",
      "[BATCH 87/149] Loss_D: 0.7661 Loss_G: 0.7570 acc: 71.9%\n",
      "[BATCH 88/149] Loss_D: 0.7609 Loss_G: 0.7604 acc: 71.9%\n",
      "[BATCH 89/149] Loss_D: 0.7288 Loss_G: 0.7553 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7365 Loss_G: 0.7635 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.7630 Loss_G: 0.7714 acc: 76.6%\n",
      "[BATCH 92/149] Loss_D: 0.7267 Loss_G: 0.7609 acc: 71.9%\n",
      "[BATCH 93/149] Loss_D: 0.7739 Loss_G: 0.7539 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.7435 Loss_G: 0.7717 acc: 67.2%\n",
      "[BATCH 95/149] Loss_D: 0.7444 Loss_G: 0.7583 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.7343 Loss_G: 0.7756 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7220 Loss_G: 0.7517 acc: 76.6%\n",
      "[BATCH 98/149] Loss_D: 0.7426 Loss_G: 0.7622 acc: 68.8%\n",
      "[BATCH 99/149] Loss_D: 0.7069 Loss_G: 0.7487 acc: 73.4%\n",
      "[BATCH 100/149] Loss_D: 0.7201 Loss_G: 0.7459 acc: 76.6%\n",
      "[BATCH 101/149] Loss_D: 0.7143 Loss_G: 0.7567 acc: 67.2%\n",
      "[BATCH 102/149] Loss_D: 0.7393 Loss_G: 0.7626 acc: 81.2%\n",
      "[BATCH 103/149] Loss_D: 0.7212 Loss_G: 0.7716 acc: 75.0%\n",
      "[BATCH 104/149] Loss_D: 0.6992 Loss_G: 0.7562 acc: 78.1%\n",
      "[BATCH 105/149] Loss_D: 0.7235 Loss_G: 0.7480 acc: 82.8%\n",
      "[BATCH 106/149] Loss_D: 0.7611 Loss_G: 0.7612 acc: 67.2%\n",
      "[BATCH 107/149] Loss_D: 0.7360 Loss_G: 0.7628 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.7790 Loss_G: 0.7773 acc: 62.5%\n",
      "[BATCH 109/149] Loss_D: 0.7343 Loss_G: 0.7618 acc: 73.4%\n",
      "[BATCH 110/149] Loss_D: 0.7438 Loss_G: 0.7605 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7194 Loss_G: 0.7651 acc: 79.7%\n",
      "[BATCH 112/149] Loss_D: 0.7160 Loss_G: 0.7587 acc: 79.7%\n",
      "[BATCH 113/149] Loss_D: 0.6929 Loss_G: 0.7500 acc: 76.6%\n",
      "[BATCH 114/149] Loss_D: 0.6996 Loss_G: 0.7556 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7353 Loss_G: 0.7584 acc: 68.8%\n",
      "[BATCH 116/149] Loss_D: 0.7776 Loss_G: 0.7758 acc: 67.2%\n",
      "[BATCH 117/149] Loss_D: 0.7226 Loss_G: 0.7646 acc: 59.4%\n",
      "[BATCH 118/149] Loss_D: 0.7545 Loss_G: 0.7701 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.7440 Loss_G: 0.7692 acc: 76.6%\n",
      "[BATCH 120/149] Loss_D: 0.7436 Loss_G: 0.7614 acc: 75.0%\n",
      "[BATCH 121/149] Loss_D: 0.7700 Loss_G: 0.7624 acc: 73.4%\n",
      "[BATCH 122/149] Loss_D: 0.7234 Loss_G: 0.7700 acc: 79.7%\n",
      "[BATCH 123/149] Loss_D: 0.7558 Loss_G: 0.7639 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.7537 Loss_G: 0.7715 acc: 78.1%\n",
      "[BATCH 125/149] Loss_D: 0.7749 Loss_G: 0.7792 acc: 67.2%\n",
      "[EPOCH 3850] TEST ACC is : 68.6%\n",
      "[BATCH 126/149] Loss_D: 0.7474 Loss_G: 0.7794 acc: 79.7%\n",
      "[BATCH 127/149] Loss_D: 0.7689 Loss_G: 0.7824 acc: 62.5%\n",
      "[BATCH 128/149] Loss_D: 0.7448 Loss_G: 0.7634 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.7621 Loss_G: 0.7552 acc: 76.6%\n",
      "[BATCH 130/149] Loss_D: 0.7758 Loss_G: 0.7714 acc: 68.8%\n",
      "[BATCH 131/149] Loss_D: 0.7147 Loss_G: 0.7574 acc: 73.4%\n",
      "[BATCH 132/149] Loss_D: 0.7630 Loss_G: 0.7687 acc: 78.1%\n",
      "[BATCH 133/149] Loss_D: 0.7477 Loss_G: 0.7701 acc: 73.4%\n",
      "[BATCH 134/149] Loss_D: 0.7676 Loss_G: 0.7587 acc: 79.7%\n",
      "[BATCH 135/149] Loss_D: 0.7431 Loss_G: 0.7674 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.7157 Loss_G: 0.7786 acc: 65.6%\n",
      "[BATCH 137/149] Loss_D: 0.7163 Loss_G: 0.7458 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.7268 Loss_G: 0.7453 acc: 79.7%\n",
      "[BATCH 139/149] Loss_D: 0.7479 Loss_G: 0.7424 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.7239 Loss_G: 0.7515 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7293 Loss_G: 0.7563 acc: 76.6%\n",
      "[BATCH 142/149] Loss_D: 0.7074 Loss_G: 0.7538 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7350 Loss_G: 0.7843 acc: 64.1%\n",
      "[BATCH 144/149] Loss_D: 0.7357 Loss_G: 0.7639 acc: 78.1%\n",
      "[BATCH 145/149] Loss_D: 0.7668 Loss_G: 0.7724 acc: 65.6%\n",
      "[BATCH 146/149] Loss_D: 0.7538 Loss_G: 0.7662 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.7351 Loss_G: 0.7615 acc: 75.0%\n",
      "[BATCH 148/149] Loss_D: 0.7622 Loss_G: 0.7836 acc: 76.6%\n",
      "[BATCH 149/149] Loss_D: 0.7168 Loss_G: 0.7675 acc: 75.0%\n",
      "-----THE [26/50] epoch end-----\n",
      "-----THE [27/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7144 Loss_G: 0.7616 acc: 70.3%\n",
      "[BATCH 2/149] Loss_D: 0.8121 Loss_G: 0.7862 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7658 Loss_G: 0.7868 acc: 73.4%\n",
      "[BATCH 4/149] Loss_D: 0.7481 Loss_G: 0.7711 acc: 81.2%\n",
      "[BATCH 5/149] Loss_D: 0.7509 Loss_G: 0.7939 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.7331 Loss_G: 0.7812 acc: 76.6%\n",
      "[BATCH 7/149] Loss_D: 0.7406 Loss_G: 0.7761 acc: 79.7%\n",
      "[BATCH 8/149] Loss_D: 0.7409 Loss_G: 0.7709 acc: 73.4%\n",
      "[BATCH 9/149] Loss_D: 0.7434 Loss_G: 0.7701 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7121 Loss_G: 0.7518 acc: 79.7%\n",
      "[BATCH 11/149] Loss_D: 0.7491 Loss_G: 0.7617 acc: 68.8%\n",
      "[BATCH 12/149] Loss_D: 0.7413 Loss_G: 0.7657 acc: 81.2%\n",
      "[BATCH 13/149] Loss_D: 0.7719 Loss_G: 0.7589 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7763 Loss_G: 0.7648 acc: 65.6%\n",
      "[BATCH 15/149] Loss_D: 0.7456 Loss_G: 0.7609 acc: 73.4%\n",
      "[BATCH 16/149] Loss_D: 0.7446 Loss_G: 0.7605 acc: 75.0%\n",
      "[BATCH 17/149] Loss_D: 0.6882 Loss_G: 0.7779 acc: 76.6%\n",
      "[BATCH 18/149] Loss_D: 0.7657 Loss_G: 0.7470 acc: 70.3%\n",
      "[BATCH 19/149] Loss_D: 0.8172 Loss_G: 0.7603 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.7503 Loss_G: 0.7884 acc: 68.8%\n",
      "[BATCH 21/149] Loss_D: 0.7592 Loss_G: 0.7618 acc: 62.5%\n",
      "[BATCH 22/149] Loss_D: 0.7649 Loss_G: 0.7676 acc: 70.3%\n",
      "[BATCH 23/149] Loss_D: 0.7168 Loss_G: 0.7643 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7152 Loss_G: 0.7543 acc: 79.7%\n",
      "[BATCH 25/149] Loss_D: 0.7275 Loss_G: 0.7564 acc: 62.5%\n",
      "[BATCH 26/149] Loss_D: 0.7302 Loss_G: 0.7404 acc: 76.6%\n",
      "[EPOCH 3900] TEST ACC is : 67.8%\n",
      "[BATCH 27/149] Loss_D: 0.7362 Loss_G: 0.7786 acc: 67.2%\n",
      "[BATCH 28/149] Loss_D: 0.7370 Loss_G: 0.7746 acc: 70.3%\n",
      "[BATCH 29/149] Loss_D: 0.7029 Loss_G: 0.7472 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.7162 Loss_G: 0.7599 acc: 73.4%\n",
      "[BATCH 31/149] Loss_D: 0.7462 Loss_G: 0.7673 acc: 59.4%\n",
      "[BATCH 32/149] Loss_D: 0.7317 Loss_G: 0.7678 acc: 54.7%\n",
      "[BATCH 33/149] Loss_D: 0.7371 Loss_G: 0.7577 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.7192 Loss_G: 0.7652 acc: 71.9%\n",
      "[BATCH 35/149] Loss_D: 0.7796 Loss_G: 0.7825 acc: 78.1%\n",
      "[BATCH 36/149] Loss_D: 0.8095 Loss_G: 0.8184 acc: 75.0%\n",
      "[BATCH 37/149] Loss_D: 0.7794 Loss_G: 0.7956 acc: 79.7%\n",
      "[BATCH 38/149] Loss_D: 0.7180 Loss_G: 0.7775 acc: 78.1%\n",
      "[BATCH 39/149] Loss_D: 0.7268 Loss_G: 0.7472 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.7436 Loss_G: 0.7610 acc: 73.4%\n",
      "[BATCH 41/149] Loss_D: 0.7121 Loss_G: 0.7532 acc: 75.0%\n",
      "[BATCH 42/149] Loss_D: 0.8114 Loss_G: 0.7721 acc: 73.4%\n",
      "[BATCH 43/149] Loss_D: 0.7753 Loss_G: 0.7868 acc: 71.9%\n",
      "[BATCH 44/149] Loss_D: 0.7442 Loss_G: 0.7666 acc: 75.0%\n",
      "[BATCH 45/149] Loss_D: 0.7250 Loss_G: 0.7527 acc: 73.4%\n",
      "[BATCH 46/149] Loss_D: 0.7420 Loss_G: 0.7733 acc: 75.0%\n",
      "[BATCH 47/149] Loss_D: 0.7511 Loss_G: 0.7697 acc: 81.2%\n",
      "[BATCH 48/149] Loss_D: 0.7233 Loss_G: 0.7598 acc: 75.0%\n",
      "[BATCH 49/149] Loss_D: 0.8166 Loss_G: 0.7915 acc: 62.5%\n",
      "[BATCH 50/149] Loss_D: 0.7510 Loss_G: 0.7748 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.7430 Loss_G: 0.7497 acc: 79.7%\n",
      "[BATCH 52/149] Loss_D: 0.7732 Loss_G: 0.7662 acc: 68.8%\n",
      "[BATCH 53/149] Loss_D: 0.7993 Loss_G: 0.7839 acc: 60.9%\n",
      "[BATCH 54/149] Loss_D: 0.7358 Loss_G: 0.7707 acc: 71.9%\n",
      "[BATCH 55/149] Loss_D: 0.7152 Loss_G: 0.7561 acc: 75.0%\n",
      "[BATCH 56/149] Loss_D: 0.7062 Loss_G: 0.7501 acc: 73.4%\n",
      "[BATCH 57/149] Loss_D: 0.7260 Loss_G: 0.7556 acc: 70.3%\n",
      "[BATCH 58/149] Loss_D: 0.7258 Loss_G: 0.7670 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.7224 Loss_G: 0.7507 acc: 71.9%\n",
      "[BATCH 60/149] Loss_D: 0.7308 Loss_G: 0.7548 acc: 81.2%\n",
      "[BATCH 61/149] Loss_D: 0.7241 Loss_G: 0.7367 acc: 71.9%\n",
      "[BATCH 62/149] Loss_D: 0.7110 Loss_G: 0.7550 acc: 64.1%\n",
      "[BATCH 63/149] Loss_D: 0.7248 Loss_G: 0.7476 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7291 Loss_G: 0.7455 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7486 Loss_G: 0.7528 acc: 73.4%\n",
      "[BATCH 66/149] Loss_D: 0.7418 Loss_G: 0.7570 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7448 Loss_G: 0.7633 acc: 76.6%\n",
      "[BATCH 68/149] Loss_D: 0.7829 Loss_G: 0.7712 acc: 65.6%\n",
      "[BATCH 69/149] Loss_D: 0.7595 Loss_G: 0.7865 acc: 70.3%\n",
      "[BATCH 70/149] Loss_D: 0.7802 Loss_G: 0.7788 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.6775 Loss_G: 0.7616 acc: 75.0%\n",
      "[BATCH 72/149] Loss_D: 0.7045 Loss_G: 0.7691 acc: 73.4%\n",
      "[BATCH 73/149] Loss_D: 0.7022 Loss_G: 0.7468 acc: 71.9%\n",
      "[BATCH 74/149] Loss_D: 0.7238 Loss_G: 0.7600 acc: 73.4%\n",
      "[BATCH 75/149] Loss_D: 0.7550 Loss_G: 0.7669 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7565 Loss_G: 0.7662 acc: 79.7%\n",
      "[EPOCH 3950] TEST ACC is : 67.6%\n",
      "[BATCH 77/149] Loss_D: 0.7639 Loss_G: 0.7772 acc: 59.4%\n",
      "[BATCH 78/149] Loss_D: 0.7292 Loss_G: 0.7584 acc: 68.8%\n",
      "[BATCH 79/149] Loss_D: 0.7971 Loss_G: 0.7900 acc: 68.8%\n",
      "[BATCH 80/149] Loss_D: 0.7420 Loss_G: 0.7777 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.7074 Loss_G: 0.7587 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7231 Loss_G: 0.7540 acc: 79.7%\n",
      "[BATCH 83/149] Loss_D: 0.7569 Loss_G: 0.7494 acc: 71.9%\n",
      "[BATCH 84/149] Loss_D: 0.7369 Loss_G: 0.7492 acc: 67.2%\n",
      "[BATCH 85/149] Loss_D: 0.7687 Loss_G: 0.7623 acc: 78.1%\n",
      "[BATCH 86/149] Loss_D: 0.7014 Loss_G: 0.7547 acc: 81.2%\n",
      "[BATCH 87/149] Loss_D: 0.7080 Loss_G: 0.7580 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.7534 Loss_G: 0.7600 acc: 70.3%\n",
      "[BATCH 89/149] Loss_D: 0.7243 Loss_G: 0.7537 acc: 76.6%\n",
      "[BATCH 90/149] Loss_D: 0.7242 Loss_G: 0.7572 acc: 65.6%\n",
      "[BATCH 91/149] Loss_D: 0.7383 Loss_G: 0.7522 acc: 68.8%\n",
      "[BATCH 92/149] Loss_D: 0.7133 Loss_G: 0.7528 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.7384 Loss_G: 0.7688 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.7478 Loss_G: 0.7836 acc: 79.7%\n",
      "[BATCH 95/149] Loss_D: 0.7414 Loss_G: 0.7876 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.7419 Loss_G: 0.7767 acc: 71.9%\n",
      "[BATCH 97/149] Loss_D: 0.7539 Loss_G: 0.7695 acc: 75.0%\n",
      "[BATCH 98/149] Loss_D: 0.6899 Loss_G: 0.7599 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7181 Loss_G: 0.7648 acc: 68.8%\n",
      "[BATCH 100/149] Loss_D: 0.7414 Loss_G: 0.7599 acc: 64.1%\n",
      "[BATCH 101/149] Loss_D: 0.7674 Loss_G: 0.7656 acc: 64.1%\n",
      "[BATCH 102/149] Loss_D: 0.7308 Loss_G: 0.7663 acc: 75.0%\n",
      "[BATCH 103/149] Loss_D: 0.7137 Loss_G: 0.7647 acc: 73.4%\n",
      "[BATCH 104/149] Loss_D: 0.7607 Loss_G: 0.8031 acc: 67.2%\n",
      "[BATCH 105/149] Loss_D: 0.7270 Loss_G: 0.7711 acc: 82.8%\n",
      "[BATCH 106/149] Loss_D: 0.7257 Loss_G: 0.7698 acc: 70.3%\n",
      "[BATCH 107/149] Loss_D: 0.7391 Loss_G: 0.7681 acc: 70.3%\n",
      "[BATCH 108/149] Loss_D: 0.7000 Loss_G: 0.7358 acc: 79.7%\n",
      "[BATCH 109/149] Loss_D: 0.7427 Loss_G: 0.7544 acc: 75.0%\n",
      "[BATCH 110/149] Loss_D: 0.7390 Loss_G: 0.7466 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6940 Loss_G: 0.7361 acc: 78.1%\n",
      "[BATCH 112/149] Loss_D: 0.7089 Loss_G: 0.7425 acc: 82.8%\n",
      "[BATCH 113/149] Loss_D: 0.7937 Loss_G: 0.7689 acc: 71.9%\n",
      "[BATCH 114/149] Loss_D: 0.7383 Loss_G: 0.7506 acc: 78.1%\n",
      "[BATCH 115/149] Loss_D: 0.7283 Loss_G: 0.7509 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7675 Loss_G: 0.7723 acc: 71.9%\n",
      "[BATCH 117/149] Loss_D: 0.6821 Loss_G: 0.7353 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.7336 Loss_G: 0.7553 acc: 62.5%\n",
      "[BATCH 119/149] Loss_D: 0.7063 Loss_G: 0.7517 acc: 79.7%\n",
      "[BATCH 120/149] Loss_D: 0.7320 Loss_G: 0.7606 acc: 70.3%\n",
      "[BATCH 121/149] Loss_D: 0.7189 Loss_G: 0.7582 acc: 78.1%\n",
      "[BATCH 122/149] Loss_D: 0.7144 Loss_G: 0.7734 acc: 76.6%\n",
      "[BATCH 123/149] Loss_D: 0.7471 Loss_G: 0.7783 acc: 73.4%\n",
      "[BATCH 124/149] Loss_D: 0.6916 Loss_G: 0.7511 acc: 67.2%\n",
      "[BATCH 125/149] Loss_D: 0.7772 Loss_G: 0.7793 acc: 78.1%\n",
      "[BATCH 126/149] Loss_D: 0.7409 Loss_G: 0.7703 acc: 70.3%\n",
      "[EPOCH 4000] TEST ACC is : 68.2%\n",
      "[BATCH 127/149] Loss_D: 0.7021 Loss_G: 0.7625 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.7533 Loss_G: 0.7685 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7454 Loss_G: 0.7565 acc: 73.4%\n",
      "[BATCH 130/149] Loss_D: 0.7248 Loss_G: 0.7509 acc: 76.6%\n",
      "[BATCH 131/149] Loss_D: 0.7435 Loss_G: 0.7616 acc: 75.0%\n",
      "[BATCH 132/149] Loss_D: 0.7081 Loss_G: 0.7450 acc: 79.7%\n",
      "[BATCH 133/149] Loss_D: 0.7399 Loss_G: 0.7457 acc: 70.3%\n",
      "[BATCH 134/149] Loss_D: 0.7383 Loss_G: 0.7684 acc: 68.8%\n",
      "[BATCH 135/149] Loss_D: 0.7113 Loss_G: 0.7439 acc: 71.9%\n",
      "[BATCH 136/149] Loss_D: 0.7626 Loss_G: 0.7512 acc: 70.3%\n",
      "[BATCH 137/149] Loss_D: 0.7348 Loss_G: 0.7665 acc: 64.1%\n",
      "[BATCH 138/149] Loss_D: 0.7401 Loss_G: 0.7585 acc: 79.7%\n",
      "[BATCH 139/149] Loss_D: 0.7267 Loss_G: 0.7543 acc: 76.6%\n",
      "[BATCH 140/149] Loss_D: 0.7227 Loss_G: 0.7431 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.7593 Loss_G: 0.7674 acc: 62.5%\n",
      "[BATCH 142/149] Loss_D: 0.7289 Loss_G: 0.7561 acc: 68.8%\n",
      "[BATCH 143/149] Loss_D: 0.7271 Loss_G: 0.7562 acc: 71.9%\n",
      "[BATCH 144/149] Loss_D: 0.6964 Loss_G: 0.7418 acc: 76.6%\n",
      "[BATCH 145/149] Loss_D: 0.7269 Loss_G: 0.7699 acc: 68.8%\n",
      "[BATCH 146/149] Loss_D: 0.7735 Loss_G: 0.7701 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.6938 Loss_G: 0.7696 acc: 75.0%\n",
      "[BATCH 148/149] Loss_D: 0.7607 Loss_G: 0.7554 acc: 78.1%\n",
      "[BATCH 149/149] Loss_D: 0.7230 Loss_G: 0.7590 acc: 73.4%\n",
      "-----THE [27/50] epoch end-----\n",
      "-----THE [28/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7544 Loss_G: 0.7837 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7780 Loss_G: 0.7810 acc: 67.2%\n",
      "[BATCH 3/149] Loss_D: 0.7471 Loss_G: 0.7713 acc: 76.6%\n",
      "[BATCH 4/149] Loss_D: 0.7462 Loss_G: 0.7655 acc: 75.0%\n",
      "[BATCH 5/149] Loss_D: 0.7317 Loss_G: 0.7551 acc: 79.7%\n",
      "[BATCH 6/149] Loss_D: 0.7640 Loss_G: 0.7661 acc: 78.1%\n",
      "[BATCH 7/149] Loss_D: 0.7373 Loss_G: 0.7650 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.7016 Loss_G: 0.7500 acc: 78.1%\n",
      "[BATCH 9/149] Loss_D: 0.6865 Loss_G: 0.7398 acc: 79.7%\n",
      "[BATCH 10/149] Loss_D: 0.7622 Loss_G: 0.7646 acc: 78.1%\n",
      "[BATCH 11/149] Loss_D: 0.7647 Loss_G: 0.7704 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.7372 Loss_G: 0.7610 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6917 Loss_G: 0.7607 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7389 Loss_G: 0.7678 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.7561 Loss_G: 0.7713 acc: 68.8%\n",
      "[BATCH 16/149] Loss_D: 0.7582 Loss_G: 0.7693 acc: 73.4%\n",
      "[BATCH 17/149] Loss_D: 0.7125 Loss_G: 0.7380 acc: 70.3%\n",
      "[BATCH 18/149] Loss_D: 0.7513 Loss_G: 0.7608 acc: 81.2%\n",
      "[BATCH 19/149] Loss_D: 0.7333 Loss_G: 0.7589 acc: 73.4%\n",
      "[BATCH 20/149] Loss_D: 0.6944 Loss_G: 0.7592 acc: 70.3%\n",
      "[BATCH 21/149] Loss_D: 0.7365 Loss_G: 0.7821 acc: 75.0%\n",
      "[BATCH 22/149] Loss_D: 0.7751 Loss_G: 0.7759 acc: 73.4%\n",
      "[BATCH 23/149] Loss_D: 0.7271 Loss_G: 0.7635 acc: 68.8%\n",
      "[BATCH 24/149] Loss_D: 0.7660 Loss_G: 0.7845 acc: 76.6%\n",
      "[BATCH 25/149] Loss_D: 0.7607 Loss_G: 0.8008 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.6992 Loss_G: 0.7816 acc: 82.8%\n",
      "[BATCH 27/149] Loss_D: 0.7538 Loss_G: 0.7685 acc: 78.1%\n",
      "[EPOCH 4050] TEST ACC is : 69.3%\n",
      "[BATCH 28/149] Loss_D: 0.7638 Loss_G: 0.7706 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.7275 Loss_G: 0.7635 acc: 78.1%\n",
      "[BATCH 30/149] Loss_D: 0.7271 Loss_G: 0.7603 acc: 71.9%\n",
      "[BATCH 31/149] Loss_D: 0.6992 Loss_G: 0.7494 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.7732 Loss_G: 0.7598 acc: 64.1%\n",
      "[BATCH 33/149] Loss_D: 0.6898 Loss_G: 0.7559 acc: 73.4%\n",
      "[BATCH 34/149] Loss_D: 0.7236 Loss_G: 0.7399 acc: 75.0%\n",
      "[BATCH 35/149] Loss_D: 0.6902 Loss_G: 0.7411 acc: 79.7%\n",
      "[BATCH 36/149] Loss_D: 0.7437 Loss_G: 0.7552 acc: 75.0%\n",
      "[BATCH 37/149] Loss_D: 0.7599 Loss_G: 0.7783 acc: 64.1%\n",
      "[BATCH 38/149] Loss_D: 0.7054 Loss_G: 0.7615 acc: 75.0%\n",
      "[BATCH 39/149] Loss_D: 0.7202 Loss_G: 0.7487 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.7311 Loss_G: 0.7532 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.7385 Loss_G: 0.7404 acc: 73.4%\n",
      "[BATCH 42/149] Loss_D: 0.6966 Loss_G: 0.7474 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7022 Loss_G: 0.7410 acc: 79.7%\n",
      "[BATCH 44/149] Loss_D: 0.7373 Loss_G: 0.7766 acc: 76.6%\n",
      "[BATCH 45/149] Loss_D: 0.7690 Loss_G: 0.7865 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.7507 Loss_G: 0.7522 acc: 71.9%\n",
      "[BATCH 47/149] Loss_D: 0.7224 Loss_G: 0.7585 acc: 75.0%\n",
      "[BATCH 48/149] Loss_D: 0.7217 Loss_G: 0.7652 acc: 65.6%\n",
      "[BATCH 49/149] Loss_D: 0.7562 Loss_G: 0.7599 acc: 78.1%\n",
      "[BATCH 50/149] Loss_D: 0.7144 Loss_G: 0.7935 acc: 71.9%\n",
      "[BATCH 51/149] Loss_D: 0.7236 Loss_G: 0.7701 acc: 73.4%\n",
      "[BATCH 52/149] Loss_D: 0.7504 Loss_G: 0.7802 acc: 75.0%\n",
      "[BATCH 53/149] Loss_D: 0.7631 Loss_G: 0.7718 acc: 73.4%\n",
      "[BATCH 54/149] Loss_D: 0.7498 Loss_G: 0.7652 acc: 76.6%\n",
      "[BATCH 55/149] Loss_D: 0.7785 Loss_G: 0.7749 acc: 81.2%\n",
      "[BATCH 56/149] Loss_D: 0.7802 Loss_G: 0.7959 acc: 64.1%\n",
      "[BATCH 57/149] Loss_D: 0.6821 Loss_G: 0.7497 acc: 75.0%\n",
      "[BATCH 58/149] Loss_D: 0.7178 Loss_G: 0.7466 acc: 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.7215 Loss_G: 0.7419 acc: 76.6%\n",
      "[BATCH 60/149] Loss_D: 0.7318 Loss_G: 0.7578 acc: 81.2%\n",
      "[BATCH 61/149] Loss_D: 0.7276 Loss_G: 0.7602 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.7600 Loss_G: 0.7578 acc: 79.7%\n",
      "[BATCH 63/149] Loss_D: 0.7667 Loss_G: 0.7705 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.7283 Loss_G: 0.7778 acc: 79.7%\n",
      "[BATCH 65/149] Loss_D: 0.7633 Loss_G: 0.7767 acc: 67.2%\n",
      "[BATCH 66/149] Loss_D: 0.7122 Loss_G: 0.7512 acc: 79.7%\n",
      "[BATCH 67/149] Loss_D: 0.7159 Loss_G: 0.7711 acc: 73.4%\n",
      "[BATCH 68/149] Loss_D: 0.7454 Loss_G: 0.7602 acc: 73.4%\n",
      "[BATCH 69/149] Loss_D: 0.7176 Loss_G: 0.7499 acc: 75.0%\n",
      "[BATCH 70/149] Loss_D: 0.7152 Loss_G: 0.7404 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.7186 Loss_G: 0.7537 acc: 53.1%\n",
      "[BATCH 72/149] Loss_D: 0.7172 Loss_G: 0.7418 acc: 71.9%\n",
      "[BATCH 73/149] Loss_D: 0.7010 Loss_G: 0.7512 acc: 75.0%\n",
      "[BATCH 74/149] Loss_D: 0.7557 Loss_G: 0.7580 acc: 78.1%\n",
      "[BATCH 75/149] Loss_D: 0.8198 Loss_G: 0.7943 acc: 70.3%\n",
      "[BATCH 76/149] Loss_D: 0.7363 Loss_G: 0.7619 acc: 73.4%\n",
      "[BATCH 77/149] Loss_D: 0.7287 Loss_G: 0.7591 acc: 59.4%\n",
      "[EPOCH 4100] TEST ACC is : 69.5%\n",
      "[BATCH 78/149] Loss_D: 0.7651 Loss_G: 0.7608 acc: 78.1%\n",
      "[BATCH 79/149] Loss_D: 0.7287 Loss_G: 0.7557 acc: 78.1%\n",
      "[BATCH 80/149] Loss_D: 0.7601 Loss_G: 0.7585 acc: 68.8%\n",
      "[BATCH 81/149] Loss_D: 0.7485 Loss_G: 0.7756 acc: 76.6%\n",
      "[BATCH 82/149] Loss_D: 0.7215 Loss_G: 0.7557 acc: 81.2%\n",
      "[BATCH 83/149] Loss_D: 0.7253 Loss_G: 0.7450 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.7104 Loss_G: 0.7357 acc: 71.9%\n",
      "[BATCH 85/149] Loss_D: 0.8272 Loss_G: 0.7756 acc: 78.1%\n",
      "[BATCH 86/149] Loss_D: 0.7229 Loss_G: 0.7886 acc: 75.0%\n",
      "[BATCH 87/149] Loss_D: 0.7117 Loss_G: 0.7747 acc: 70.3%\n",
      "[BATCH 88/149] Loss_D: 0.7045 Loss_G: 0.7527 acc: 75.0%\n",
      "[BATCH 89/149] Loss_D: 0.7027 Loss_G: 0.7464 acc: 70.3%\n",
      "[BATCH 90/149] Loss_D: 0.7874 Loss_G: 0.7676 acc: 64.1%\n",
      "[BATCH 91/149] Loss_D: 0.7216 Loss_G: 0.7520 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7390 Loss_G: 0.7525 acc: 71.9%\n",
      "[BATCH 93/149] Loss_D: 0.7374 Loss_G: 0.7587 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.7373 Loss_G: 0.7568 acc: 79.7%\n",
      "[BATCH 95/149] Loss_D: 0.7450 Loss_G: 0.7619 acc: 68.8%\n",
      "[BATCH 96/149] Loss_D: 0.7374 Loss_G: 0.7682 acc: 75.0%\n",
      "[BATCH 97/149] Loss_D: 0.7548 Loss_G: 0.7682 acc: 78.1%\n",
      "[BATCH 98/149] Loss_D: 0.7157 Loss_G: 0.7579 acc: 82.8%\n",
      "[BATCH 99/149] Loss_D: 0.7768 Loss_G: 0.7728 acc: 57.8%\n",
      "[BATCH 100/149] Loss_D: 0.7155 Loss_G: 0.7544 acc: 65.6%\n",
      "[BATCH 101/149] Loss_D: 0.7217 Loss_G: 0.7573 acc: 76.6%\n",
      "[BATCH 102/149] Loss_D: 0.7085 Loss_G: 0.7476 acc: 78.1%\n",
      "[BATCH 103/149] Loss_D: 0.7442 Loss_G: 0.7705 acc: 73.4%\n",
      "[BATCH 104/149] Loss_D: 0.7126 Loss_G: 0.7586 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.8390 Loss_G: 0.7972 acc: 71.9%\n",
      "[BATCH 106/149] Loss_D: 0.7191 Loss_G: 0.7640 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.7073 Loss_G: 0.7635 acc: 67.2%\n",
      "[BATCH 108/149] Loss_D: 0.7697 Loss_G: 0.7750 acc: 65.6%\n",
      "[BATCH 109/149] Loss_D: 0.7609 Loss_G: 0.7823 acc: 67.2%\n",
      "[BATCH 110/149] Loss_D: 0.7725 Loss_G: 0.7686 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.7469 Loss_G: 0.7517 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7656 Loss_G: 0.7616 acc: 68.8%\n",
      "[BATCH 113/149] Loss_D: 0.7565 Loss_G: 0.7681 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.7792 Loss_G: 0.7871 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7605 Loss_G: 0.7735 acc: 73.4%\n",
      "[BATCH 116/149] Loss_D: 0.7234 Loss_G: 0.7546 acc: 76.6%\n",
      "[BATCH 117/149] Loss_D: 0.7228 Loss_G: 0.7455 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7333 Loss_G: 0.7495 acc: 70.3%\n",
      "[BATCH 119/149] Loss_D: 0.7553 Loss_G: 0.7645 acc: 73.4%\n",
      "[BATCH 120/149] Loss_D: 0.7536 Loss_G: 0.7652 acc: 64.1%\n",
      "[BATCH 121/149] Loss_D: 0.7560 Loss_G: 0.7728 acc: 73.4%\n",
      "[BATCH 122/149] Loss_D: 0.7648 Loss_G: 0.7569 acc: 79.7%\n",
      "[BATCH 123/149] Loss_D: 0.7243 Loss_G: 0.7588 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.7300 Loss_G: 0.7544 acc: 81.2%\n",
      "[BATCH 125/149] Loss_D: 0.7347 Loss_G: 0.7573 acc: 71.9%\n",
      "[BATCH 126/149] Loss_D: 0.7141 Loss_G: 0.7591 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.7223 Loss_G: 0.7658 acc: 71.9%\n",
      "[EPOCH 4150] TEST ACC is : 68.9%\n",
      "[BATCH 128/149] Loss_D: 0.7360 Loss_G: 0.7775 acc: 70.3%\n",
      "[BATCH 129/149] Loss_D: 0.7504 Loss_G: 0.7792 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7459 Loss_G: 0.7808 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7272 Loss_G: 0.7738 acc: 76.6%\n",
      "[BATCH 132/149] Loss_D: 0.7365 Loss_G: 0.7716 acc: 73.4%\n",
      "[BATCH 133/149] Loss_D: 0.7255 Loss_G: 0.7692 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.7563 Loss_G: 0.7706 acc: 75.0%\n",
      "[BATCH 135/149] Loss_D: 0.7383 Loss_G: 0.7690 acc: 76.6%\n",
      "[BATCH 136/149] Loss_D: 0.7253 Loss_G: 0.7553 acc: 68.8%\n",
      "[BATCH 137/149] Loss_D: 0.7162 Loss_G: 0.7590 acc: 71.9%\n",
      "[BATCH 138/149] Loss_D: 0.7357 Loss_G: 0.7521 acc: 78.1%\n",
      "[BATCH 139/149] Loss_D: 0.7563 Loss_G: 0.7591 acc: 71.9%\n",
      "[BATCH 140/149] Loss_D: 0.7464 Loss_G: 0.7494 acc: 76.6%\n",
      "[BATCH 141/149] Loss_D: 0.7198 Loss_G: 0.7434 acc: 68.8%\n",
      "[BATCH 142/149] Loss_D: 0.7126 Loss_G: 0.7453 acc: 76.6%\n",
      "[BATCH 143/149] Loss_D: 0.7196 Loss_G: 0.7540 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.7332 Loss_G: 0.7598 acc: 81.2%\n",
      "[BATCH 145/149] Loss_D: 0.7438 Loss_G: 0.7724 acc: 68.8%\n",
      "[BATCH 146/149] Loss_D: 0.7656 Loss_G: 0.7674 acc: 68.8%\n",
      "[BATCH 147/149] Loss_D: 0.7290 Loss_G: 0.7628 acc: 78.1%\n",
      "[BATCH 148/149] Loss_D: 0.7154 Loss_G: 0.7375 acc: 78.1%\n",
      "[BATCH 149/149] Loss_D: 0.7720 Loss_G: 0.7607 acc: 75.0%\n",
      "-----THE [28/50] epoch end-----\n",
      "-----THE [29/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6919 Loss_G: 0.7424 acc: 79.7%\n",
      "[BATCH 2/149] Loss_D: 0.7647 Loss_G: 0.7615 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7359 Loss_G: 0.7624 acc: 73.4%\n",
      "[BATCH 4/149] Loss_D: 0.7462 Loss_G: 0.7664 acc: 70.3%\n",
      "[BATCH 5/149] Loss_D: 0.7609 Loss_G: 0.7645 acc: 71.9%\n",
      "[BATCH 6/149] Loss_D: 0.7275 Loss_G: 0.7508 acc: 76.6%\n",
      "[BATCH 7/149] Loss_D: 0.7180 Loss_G: 0.7503 acc: 78.1%\n",
      "[BATCH 8/149] Loss_D: 0.7776 Loss_G: 0.7776 acc: 76.6%\n",
      "[BATCH 9/149] Loss_D: 0.7137 Loss_G: 0.7623 acc: 78.1%\n",
      "[BATCH 10/149] Loss_D: 0.7469 Loss_G: 0.7647 acc: 60.9%\n",
      "[BATCH 11/149] Loss_D: 0.7355 Loss_G: 0.7611 acc: 71.9%\n",
      "[BATCH 12/149] Loss_D: 0.7256 Loss_G: 0.7676 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.7224 Loss_G: 0.7559 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.6971 Loss_G: 0.7585 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.7329 Loss_G: 0.7666 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7716 Loss_G: 0.7760 acc: 70.3%\n",
      "[BATCH 17/149] Loss_D: 0.7285 Loss_G: 0.7688 acc: 68.8%\n",
      "[BATCH 18/149] Loss_D: 0.7379 Loss_G: 0.7546 acc: 75.0%\n",
      "[BATCH 19/149] Loss_D: 0.7265 Loss_G: 0.7540 acc: 82.8%\n",
      "[BATCH 20/149] Loss_D: 0.7835 Loss_G: 0.7730 acc: 78.1%\n",
      "[BATCH 21/149] Loss_D: 0.7610 Loss_G: 0.7726 acc: 75.0%\n",
      "[BATCH 22/149] Loss_D: 0.7408 Loss_G: 0.7679 acc: 79.7%\n",
      "[BATCH 23/149] Loss_D: 0.7058 Loss_G: 0.7395 acc: 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.7237 Loss_G: 0.7431 acc: 62.5%\n",
      "[BATCH 25/149] Loss_D: 0.7244 Loss_G: 0.7451 acc: 76.6%\n",
      "[BATCH 26/149] Loss_D: 0.6782 Loss_G: 0.7412 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7425 Loss_G: 0.7458 acc: 71.9%\n",
      "[BATCH 28/149] Loss_D: 0.7112 Loss_G: 0.7466 acc: 79.7%\n",
      "[EPOCH 4200] TEST ACC is : 70.1%\n",
      "[BATCH 29/149] Loss_D: 0.7377 Loss_G: 0.7578 acc: 70.3%\n",
      "[BATCH 30/149] Loss_D: 0.7106 Loss_G: 0.7532 acc: 67.2%\n",
      "[BATCH 31/149] Loss_D: 0.7147 Loss_G: 0.7489 acc: 76.6%\n",
      "[BATCH 32/149] Loss_D: 0.7430 Loss_G: 0.7553 acc: 67.2%\n",
      "[BATCH 33/149] Loss_D: 0.7224 Loss_G: 0.7310 acc: 81.2%\n",
      "[BATCH 34/149] Loss_D: 0.8250 Loss_G: 0.7725 acc: 71.9%\n",
      "[BATCH 35/149] Loss_D: 0.6963 Loss_G: 0.7357 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7744 Loss_G: 0.7580 acc: 73.4%\n",
      "[BATCH 37/149] Loss_D: 0.7121 Loss_G: 0.7400 acc: 71.9%\n",
      "[BATCH 38/149] Loss_D: 0.7327 Loss_G: 0.7585 acc: 68.8%\n",
      "[BATCH 39/149] Loss_D: 0.7064 Loss_G: 0.7401 acc: 71.9%\n",
      "[BATCH 40/149] Loss_D: 0.7696 Loss_G: 0.7637 acc: 67.2%\n",
      "[BATCH 41/149] Loss_D: 0.7698 Loss_G: 0.7706 acc: 68.8%\n",
      "[BATCH 42/149] Loss_D: 0.7318 Loss_G: 0.7540 acc: 67.2%\n",
      "[BATCH 43/149] Loss_D: 0.7651 Loss_G: 0.7643 acc: 75.0%\n",
      "[BATCH 44/149] Loss_D: 0.7493 Loss_G: 0.7639 acc: 71.9%\n",
      "[BATCH 45/149] Loss_D: 0.7430 Loss_G: 0.7771 acc: 78.1%\n",
      "[BATCH 46/149] Loss_D: 0.6831 Loss_G: 0.7470 acc: 78.1%\n",
      "[BATCH 47/149] Loss_D: 0.7309 Loss_G: 0.7614 acc: 75.0%\n",
      "[BATCH 48/149] Loss_D: 0.7051 Loss_G: 0.7413 acc: 67.2%\n",
      "[BATCH 49/149] Loss_D: 0.7687 Loss_G: 0.7672 acc: 64.1%\n",
      "[BATCH 50/149] Loss_D: 0.7489 Loss_G: 0.7735 acc: 67.2%\n",
      "[BATCH 51/149] Loss_D: 0.7282 Loss_G: 0.7562 acc: 73.4%\n",
      "[BATCH 52/149] Loss_D: 0.7149 Loss_G: 0.7484 acc: 73.4%\n",
      "[BATCH 53/149] Loss_D: 0.7584 Loss_G: 0.7623 acc: 62.5%\n",
      "[BATCH 54/149] Loss_D: 0.7091 Loss_G: 0.7587 acc: 79.7%\n",
      "[BATCH 55/149] Loss_D: 0.7373 Loss_G: 0.7586 acc: 76.6%\n",
      "[BATCH 56/149] Loss_D: 0.6888 Loss_G: 0.7467 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7260 Loss_G: 0.7464 acc: 79.7%\n",
      "[BATCH 58/149] Loss_D: 0.7502 Loss_G: 0.7654 acc: 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.7174 Loss_G: 0.7514 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7386 Loss_G: 0.7650 acc: 71.9%\n",
      "[BATCH 61/149] Loss_D: 0.7393 Loss_G: 0.7699 acc: 76.6%\n",
      "[BATCH 62/149] Loss_D: 0.7650 Loss_G: 0.7813 acc: 59.4%\n",
      "[BATCH 63/149] Loss_D: 0.7388 Loss_G: 0.7664 acc: 73.4%\n",
      "[BATCH 64/149] Loss_D: 0.7635 Loss_G: 0.7594 acc: 78.1%\n",
      "[BATCH 65/149] Loss_D: 0.7529 Loss_G: 0.7513 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7318 Loss_G: 0.7486 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.7640 Loss_G: 0.7563 acc: 73.4%\n",
      "[BATCH 68/149] Loss_D: 0.7090 Loss_G: 0.7484 acc: 67.2%\n",
      "[BATCH 69/149] Loss_D: 0.7179 Loss_G: 0.7522 acc: 78.1%\n",
      "[BATCH 70/149] Loss_D: 0.6732 Loss_G: 0.7376 acc: 70.3%\n",
      "[BATCH 71/149] Loss_D: 0.7764 Loss_G: 0.7559 acc: 73.4%\n",
      "[BATCH 72/149] Loss_D: 0.7415 Loss_G: 0.7717 acc: 73.4%\n",
      "[BATCH 73/149] Loss_D: 0.7174 Loss_G: 0.7538 acc: 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.7177 Loss_G: 0.7545 acc: 78.1%\n",
      "[BATCH 75/149] Loss_D: 0.7389 Loss_G: 0.7454 acc: 75.0%\n",
      "[BATCH 76/149] Loss_D: 0.7199 Loss_G: 0.7482 acc: 71.9%\n",
      "[BATCH 77/149] Loss_D: 0.7221 Loss_G: 0.7558 acc: 76.6%\n",
      "[BATCH 78/149] Loss_D: 0.7476 Loss_G: 0.7682 acc: 68.8%\n",
      "[EPOCH 4250] TEST ACC is : 67.8%\n",
      "[BATCH 79/149] Loss_D: 0.7243 Loss_G: 0.7818 acc: 78.1%\n",
      "[BATCH 80/149] Loss_D: 0.7663 Loss_G: 0.7793 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.7530 Loss_G: 0.7599 acc: 81.2%\n",
      "[BATCH 82/149] Loss_D: 0.7267 Loss_G: 0.7623 acc: 78.1%\n",
      "[BATCH 83/149] Loss_D: 0.7100 Loss_G: 0.7514 acc: 64.1%\n",
      "[BATCH 84/149] Loss_D: 0.7098 Loss_G: 0.7613 acc: 79.7%\n",
      "[BATCH 85/149] Loss_D: 0.6780 Loss_G: 0.7639 acc: 76.6%\n",
      "[BATCH 86/149] Loss_D: 0.7500 Loss_G: 0.7684 acc: 73.4%\n",
      "[BATCH 87/149] Loss_D: 0.7170 Loss_G: 0.7562 acc: 65.6%\n",
      "[BATCH 88/149] Loss_D: 0.7572 Loss_G: 0.7709 acc: 71.9%\n",
      "[BATCH 89/149] Loss_D: 0.7940 Loss_G: 0.8039 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.7322 Loss_G: 0.7725 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.7740 Loss_G: 0.7896 acc: 79.7%\n",
      "[BATCH 92/149] Loss_D: 0.7825 Loss_G: 0.7973 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.7635 Loss_G: 0.7735 acc: 71.9%\n",
      "[BATCH 94/149] Loss_D: 0.7290 Loss_G: 0.7756 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.7207 Loss_G: 0.7503 acc: 70.3%\n",
      "[BATCH 96/149] Loss_D: 0.7336 Loss_G: 0.7544 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.7391 Loss_G: 0.7711 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7422 Loss_G: 0.7739 acc: 75.0%\n",
      "[BATCH 99/149] Loss_D: 0.7208 Loss_G: 0.7660 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.7118 Loss_G: 0.7583 acc: 76.6%\n",
      "[BATCH 101/149] Loss_D: 0.7310 Loss_G: 0.7666 acc: 79.7%\n",
      "[BATCH 102/149] Loss_D: 0.7518 Loss_G: 0.7771 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.7427 Loss_G: 0.7736 acc: 75.0%\n",
      "[BATCH 104/149] Loss_D: 0.7910 Loss_G: 0.7808 acc: 75.0%\n",
      "[BATCH 105/149] Loss_D: 0.7186 Loss_G: 0.7608 acc: 73.4%\n",
      "[BATCH 106/149] Loss_D: 0.7529 Loss_G: 0.7619 acc: 78.1%\n",
      "[BATCH 107/149] Loss_D: 0.7385 Loss_G: 0.7630 acc: 82.8%\n",
      "[BATCH 108/149] Loss_D: 0.7291 Loss_G: 0.7583 acc: 67.2%\n",
      "[BATCH 109/149] Loss_D: 0.7553 Loss_G: 0.7643 acc: 75.0%\n",
      "[BATCH 110/149] Loss_D: 0.7483 Loss_G: 0.7604 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.6943 Loss_G: 0.7429 acc: 70.3%\n",
      "[BATCH 112/149] Loss_D: 0.7859 Loss_G: 0.7636 acc: 73.4%\n",
      "[BATCH 113/149] Loss_D: 0.7673 Loss_G: 0.7767 acc: 67.2%\n",
      "[BATCH 114/149] Loss_D: 0.7444 Loss_G: 0.7711 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7426 Loss_G: 0.7804 acc: 82.8%\n",
      "[BATCH 116/149] Loss_D: 0.7768 Loss_G: 0.7873 acc: 79.7%\n",
      "[BATCH 117/149] Loss_D: 0.7332 Loss_G: 0.7738 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.7467 Loss_G: 0.7569 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.7303 Loss_G: 0.7842 acc: 65.6%\n",
      "[BATCH 120/149] Loss_D: 0.7477 Loss_G: 0.7760 acc: 73.4%\n",
      "[BATCH 121/149] Loss_D: 0.7072 Loss_G: 0.7512 acc: 81.2%\n",
      "[BATCH 122/149] Loss_D: 0.7422 Loss_G: 0.7577 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7419 Loss_G: 0.7554 acc: 67.2%\n",
      "[BATCH 124/149] Loss_D: 0.7211 Loss_G: 0.7379 acc: 78.1%\n",
      "[BATCH 125/149] Loss_D: 0.7373 Loss_G: 0.7775 acc: 73.4%\n",
      "[BATCH 126/149] Loss_D: 0.7207 Loss_G: 0.7701 acc: 65.6%\n",
      "[BATCH 127/149] Loss_D: 0.7144 Loss_G: 0.7528 acc: 71.9%\n",
      "[BATCH 128/149] Loss_D: 0.7751 Loss_G: 0.7652 acc: 70.3%\n",
      "[EPOCH 4300] TEST ACC is : 69.7%\n",
      "[BATCH 129/149] Loss_D: 0.7494 Loss_G: 0.7727 acc: 73.4%\n",
      "[BATCH 130/149] Loss_D: 0.7179 Loss_G: 0.7529 acc: 75.0%\n",
      "[BATCH 131/149] Loss_D: 0.7264 Loss_G: 0.7502 acc: 71.9%\n",
      "[BATCH 132/149] Loss_D: 0.7534 Loss_G: 0.7621 acc: 78.1%\n",
      "[BATCH 133/149] Loss_D: 0.7677 Loss_G: 0.7583 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7332 Loss_G: 0.7684 acc: 75.0%\n",
      "[BATCH 135/149] Loss_D: 0.7675 Loss_G: 0.7576 acc: 75.0%\n",
      "[BATCH 136/149] Loss_D: 0.7242 Loss_G: 0.7560 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7355 Loss_G: 0.7602 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.7868 Loss_G: 0.7711 acc: 73.4%\n",
      "[BATCH 139/149] Loss_D: 0.7185 Loss_G: 0.7606 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.7463 Loss_G: 0.7770 acc: 75.0%\n",
      "[BATCH 141/149] Loss_D: 0.7254 Loss_G: 0.7728 acc: 70.3%\n",
      "[BATCH 142/149] Loss_D: 0.7135 Loss_G: 0.7568 acc: 81.2%\n",
      "[BATCH 143/149] Loss_D: 0.7105 Loss_G: 0.7597 acc: 79.7%\n",
      "[BATCH 144/149] Loss_D: 0.7015 Loss_G: 0.7457 acc: 76.6%\n",
      "[BATCH 145/149] Loss_D: 0.7317 Loss_G: 0.7511 acc: 73.4%\n",
      "[BATCH 146/149] Loss_D: 0.7276 Loss_G: 0.7562 acc: 73.4%\n",
      "[BATCH 147/149] Loss_D: 0.7693 Loss_G: 0.7763 acc: 71.9%\n",
      "[BATCH 148/149] Loss_D: 0.7510 Loss_G: 0.7771 acc: 76.6%\n",
      "[BATCH 149/149] Loss_D: 0.7452 Loss_G: 0.7833 acc: 76.6%\n",
      "-----THE [29/50] epoch end-----\n",
      "-----THE [30/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7387 Loss_G: 0.7586 acc: 73.4%\n",
      "[BATCH 2/149] Loss_D: 0.7101 Loss_G: 0.7595 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.7315 Loss_G: 0.7678 acc: 73.4%\n",
      "[BATCH 4/149] Loss_D: 0.7515 Loss_G: 0.7638 acc: 79.7%\n",
      "[BATCH 5/149] Loss_D: 0.7375 Loss_G: 0.7509 acc: 76.6%\n",
      "[BATCH 6/149] Loss_D: 0.7096 Loss_G: 0.7286 acc: 78.1%\n",
      "[BATCH 7/149] Loss_D: 0.7299 Loss_G: 0.7582 acc: 73.4%\n",
      "[BATCH 8/149] Loss_D: 0.7280 Loss_G: 0.7457 acc: 76.6%\n",
      "[BATCH 9/149] Loss_D: 0.7744 Loss_G: 0.7631 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.7636 Loss_G: 0.7513 acc: 79.7%\n",
      "[BATCH 11/149] Loss_D: 0.7451 Loss_G: 0.7629 acc: 78.1%\n",
      "[BATCH 12/149] Loss_D: 0.7201 Loss_G: 0.7632 acc: 75.0%\n",
      "[BATCH 13/149] Loss_D: 0.7505 Loss_G: 0.7558 acc: 71.9%\n",
      "[BATCH 14/149] Loss_D: 0.7248 Loss_G: 0.7683 acc: 81.2%\n",
      "[BATCH 15/149] Loss_D: 0.7693 Loss_G: 0.7751 acc: 79.7%\n",
      "[BATCH 16/149] Loss_D: 0.7014 Loss_G: 0.7640 acc: 73.4%\n",
      "[BATCH 17/149] Loss_D: 0.6827 Loss_G: 0.7599 acc: 81.2%\n",
      "[BATCH 18/149] Loss_D: 0.6961 Loss_G: 0.7634 acc: 76.6%\n",
      "[BATCH 19/149] Loss_D: 0.6958 Loss_G: 0.7586 acc: 75.0%\n",
      "[BATCH 20/149] Loss_D: 0.7401 Loss_G: 0.7694 acc: 73.4%\n",
      "[BATCH 21/149] Loss_D: 0.7218 Loss_G: 0.7588 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7091 Loss_G: 0.7444 acc: 76.6%\n",
      "[BATCH 23/149] Loss_D: 0.7348 Loss_G: 0.7465 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7724 Loss_G: 0.7702 acc: 73.4%\n",
      "[BATCH 25/149] Loss_D: 0.7430 Loss_G: 0.7807 acc: 67.2%\n",
      "[BATCH 26/149] Loss_D: 0.7342 Loss_G: 0.7729 acc: 78.1%\n",
      "[BATCH 27/149] Loss_D: 0.6961 Loss_G: 0.7474 acc: 78.1%\n",
      "[BATCH 28/149] Loss_D: 0.6827 Loss_G: 0.7372 acc: 78.1%\n",
      "[BATCH 29/149] Loss_D: 0.7658 Loss_G: 0.7543 acc: 78.1%\n",
      "[EPOCH 4350] TEST ACC is : 68.0%\n",
      "[BATCH 30/149] Loss_D: 0.7409 Loss_G: 0.7520 acc: 79.7%\n",
      "[BATCH 31/149] Loss_D: 0.7249 Loss_G: 0.7516 acc: 75.0%\n",
      "[BATCH 32/149] Loss_D: 0.7287 Loss_G: 0.7539 acc: 78.1%\n",
      "[BATCH 33/149] Loss_D: 0.7157 Loss_G: 0.7669 acc: 64.1%\n",
      "[BATCH 34/149] Loss_D: 0.6959 Loss_G: 0.7558 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.7388 Loss_G: 0.7613 acc: 71.9%\n",
      "[BATCH 36/149] Loss_D: 0.7729 Loss_G: 0.7619 acc: 73.4%\n",
      "[BATCH 37/149] Loss_D: 0.7144 Loss_G: 0.7574 acc: 79.7%\n",
      "[BATCH 38/149] Loss_D: 0.7360 Loss_G: 0.7647 acc: 76.6%\n",
      "[BATCH 39/149] Loss_D: 0.7490 Loss_G: 0.7833 acc: 67.2%\n",
      "[BATCH 40/149] Loss_D: 0.7744 Loss_G: 0.7785 acc: 59.4%\n",
      "[BATCH 41/149] Loss_D: 0.7529 Loss_G: 0.7569 acc: 75.0%\n",
      "[BATCH 42/149] Loss_D: 0.7367 Loss_G: 0.7696 acc: 70.3%\n",
      "[BATCH 43/149] Loss_D: 0.7394 Loss_G: 0.7568 acc: 75.0%\n",
      "[BATCH 44/149] Loss_D: 0.7640 Loss_G: 0.7540 acc: 76.6%\n",
      "[BATCH 45/149] Loss_D: 0.7679 Loss_G: 0.7726 acc: 78.1%\n",
      "[BATCH 46/149] Loss_D: 0.7369 Loss_G: 0.7579 acc: 71.9%\n",
      "[BATCH 47/149] Loss_D: 0.7535 Loss_G: 0.7677 acc: 76.6%\n",
      "[BATCH 48/149] Loss_D: 0.7617 Loss_G: 0.7593 acc: 68.8%\n",
      "[BATCH 49/149] Loss_D: 0.6967 Loss_G: 0.7449 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.7813 Loss_G: 0.7453 acc: 78.1%\n",
      "[BATCH 51/149] Loss_D: 0.7320 Loss_G: 0.7680 acc: 67.2%\n",
      "[BATCH 52/149] Loss_D: 0.7536 Loss_G: 0.7682 acc: 65.6%\n",
      "[BATCH 53/149] Loss_D: 0.6999 Loss_G: 0.7455 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.6751 Loss_G: 0.7292 acc: 76.6%\n",
      "[BATCH 55/149] Loss_D: 0.7323 Loss_G: 0.7392 acc: 71.9%\n",
      "[BATCH 56/149] Loss_D: 0.7262 Loss_G: 0.7507 acc: 75.0%\n",
      "[BATCH 57/149] Loss_D: 0.7288 Loss_G: 0.7624 acc: 73.4%\n",
      "[BATCH 58/149] Loss_D: 0.7160 Loss_G: 0.7592 acc: 70.3%\n",
      "[BATCH 59/149] Loss_D: 0.7496 Loss_G: 0.7703 acc: 76.6%\n",
      "[BATCH 60/149] Loss_D: 0.7795 Loss_G: 0.7845 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.7671 Loss_G: 0.7777 acc: 70.3%\n",
      "[BATCH 62/149] Loss_D: 0.7475 Loss_G: 0.7645 acc: 71.9%\n",
      "[BATCH 63/149] Loss_D: 0.7777 Loss_G: 0.7792 acc: 75.0%\n",
      "[BATCH 64/149] Loss_D: 0.7814 Loss_G: 0.7768 acc: 70.3%\n",
      "[BATCH 65/149] Loss_D: 0.7673 Loss_G: 0.7699 acc: 76.6%\n",
      "[BATCH 66/149] Loss_D: 0.7714 Loss_G: 0.7784 acc: 78.1%\n",
      "[BATCH 67/149] Loss_D: 0.7342 Loss_G: 0.7520 acc: 75.0%\n",
      "[BATCH 68/149] Loss_D: 0.7541 Loss_G: 0.7566 acc: 68.8%\n",
      "[BATCH 69/149] Loss_D: 0.7122 Loss_G: 0.7671 acc: 79.7%\n",
      "[BATCH 70/149] Loss_D: 0.7271 Loss_G: 0.7632 acc: 75.0%\n",
      "[BATCH 71/149] Loss_D: 0.7384 Loss_G: 0.7601 acc: 78.1%\n",
      "[BATCH 72/149] Loss_D: 0.7318 Loss_G: 0.7664 acc: 78.1%\n",
      "[BATCH 73/149] Loss_D: 0.7136 Loss_G: 0.7544 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7163 Loss_G: 0.7599 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7655 Loss_G: 0.7702 acc: 62.5%\n",
      "[BATCH 76/149] Loss_D: 0.7499 Loss_G: 0.7595 acc: 79.7%\n",
      "[BATCH 77/149] Loss_D: 0.7301 Loss_G: 0.7604 acc: 65.6%\n",
      "[BATCH 78/149] Loss_D: 0.7464 Loss_G: 0.7552 acc: 65.6%\n",
      "[BATCH 79/149] Loss_D: 0.7537 Loss_G: 0.7529 acc: 79.7%\n",
      "[EPOCH 4400] TEST ACC is : 68.2%\n",
      "[BATCH 80/149] Loss_D: 0.7082 Loss_G: 0.7525 acc: 81.2%\n",
      "[BATCH 81/149] Loss_D: 0.7301 Loss_G: 0.7491 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.8130 Loss_G: 0.8052 acc: 67.2%\n",
      "[BATCH 83/149] Loss_D: 0.7081 Loss_G: 0.7636 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7378 Loss_G: 0.7544 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7766 Loss_G: 0.7818 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7055 Loss_G: 0.7803 acc: 71.9%\n",
      "[BATCH 87/149] Loss_D: 0.7224 Loss_G: 0.7560 acc: 76.6%\n",
      "[BATCH 88/149] Loss_D: 0.7178 Loss_G: 0.7561 acc: 79.7%\n",
      "[BATCH 89/149] Loss_D: 0.7547 Loss_G: 0.7769 acc: 78.1%\n",
      "[BATCH 90/149] Loss_D: 0.7309 Loss_G: 0.7540 acc: 68.8%\n",
      "[BATCH 91/149] Loss_D: 0.7010 Loss_G: 0.7372 acc: 79.7%\n",
      "[BATCH 92/149] Loss_D: 0.7047 Loss_G: 0.7470 acc: 71.9%\n",
      "[BATCH 93/149] Loss_D: 0.7221 Loss_G: 0.7467 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.7750 Loss_G: 0.7640 acc: 78.1%\n",
      "[BATCH 95/149] Loss_D: 0.7527 Loss_G: 0.7574 acc: 79.7%\n",
      "[BATCH 96/149] Loss_D: 0.7448 Loss_G: 0.7479 acc: 73.4%\n",
      "[BATCH 97/149] Loss_D: 0.7444 Loss_G: 0.7560 acc: 73.4%\n",
      "[BATCH 98/149] Loss_D: 0.7389 Loss_G: 0.7630 acc: 82.8%\n",
      "[BATCH 99/149] Loss_D: 0.7538 Loss_G: 0.7597 acc: 78.1%\n",
      "[BATCH 100/149] Loss_D: 0.7279 Loss_G: 0.7705 acc: 79.7%\n",
      "[BATCH 101/149] Loss_D: 0.7283 Loss_G: 0.7469 acc: 67.2%\n",
      "[BATCH 102/149] Loss_D: 0.7068 Loss_G: 0.7375 acc: 76.6%\n",
      "[BATCH 103/149] Loss_D: 0.7044 Loss_G: 0.7429 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7270 Loss_G: 0.7532 acc: 76.6%\n",
      "[BATCH 105/149] Loss_D: 0.7214 Loss_G: 0.7493 acc: 67.2%\n",
      "[BATCH 106/149] Loss_D: 0.7957 Loss_G: 0.7764 acc: 75.0%\n",
      "[BATCH 107/149] Loss_D: 0.7243 Loss_G: 0.7648 acc: 79.7%\n",
      "[BATCH 108/149] Loss_D: 0.7396 Loss_G: 0.7591 acc: 78.1%\n",
      "[BATCH 109/149] Loss_D: 0.7069 Loss_G: 0.7581 acc: 70.3%\n",
      "[BATCH 110/149] Loss_D: 0.7575 Loss_G: 0.7741 acc: 79.7%\n",
      "[BATCH 111/149] Loss_D: 0.7532 Loss_G: 0.7706 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7297 Loss_G: 0.7590 acc: 75.0%\n",
      "[BATCH 113/149] Loss_D: 0.6811 Loss_G: 0.7449 acc: 73.4%\n",
      "[BATCH 114/149] Loss_D: 0.7318 Loss_G: 0.7623 acc: 78.1%\n",
      "[BATCH 115/149] Loss_D: 0.7598 Loss_G: 0.7514 acc: 75.0%\n",
      "[BATCH 116/149] Loss_D: 0.7640 Loss_G: 0.7670 acc: 68.8%\n",
      "[BATCH 117/149] Loss_D: 0.7249 Loss_G: 0.7392 acc: 78.1%\n",
      "[BATCH 118/149] Loss_D: 0.7289 Loss_G: 0.7483 acc: 81.2%\n",
      "[BATCH 119/149] Loss_D: 0.7488 Loss_G: 0.7704 acc: 76.6%\n",
      "[BATCH 120/149] Loss_D: 0.7351 Loss_G: 0.7636 acc: 75.0%\n",
      "[BATCH 121/149] Loss_D: 0.7000 Loss_G: 0.7496 acc: 78.1%\n",
      "[BATCH 122/149] Loss_D: 0.6998 Loss_G: 0.7407 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7344 Loss_G: 0.7617 acc: 75.0%\n",
      "[BATCH 124/149] Loss_D: 0.6856 Loss_G: 0.7389 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7543 Loss_G: 0.7518 acc: 70.3%\n",
      "[BATCH 126/149] Loss_D: 0.7166 Loss_G: 0.7375 acc: 78.1%\n",
      "[BATCH 127/149] Loss_D: 0.7475 Loss_G: 0.7568 acc: 65.6%\n",
      "[BATCH 128/149] Loss_D: 0.7357 Loss_G: 0.7601 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7643 Loss_G: 0.7611 acc: 75.0%\n",
      "[EPOCH 4450] TEST ACC is : 68.9%\n",
      "[BATCH 130/149] Loss_D: 0.7093 Loss_G: 0.7408 acc: 73.4%\n",
      "[BATCH 131/149] Loss_D: 0.7594 Loss_G: 0.7524 acc: 76.6%\n",
      "[BATCH 132/149] Loss_D: 0.7373 Loss_G: 0.7485 acc: 75.0%\n",
      "[BATCH 133/149] Loss_D: 0.7303 Loss_G: 0.7494 acc: 68.8%\n",
      "[BATCH 134/149] Loss_D: 0.7222 Loss_G: 0.7608 acc: 71.9%\n",
      "[BATCH 135/149] Loss_D: 0.7587 Loss_G: 0.7447 acc: 76.6%\n",
      "[BATCH 136/149] Loss_D: 0.7401 Loss_G: 0.7613 acc: 67.2%\n",
      "[BATCH 137/149] Loss_D: 0.7287 Loss_G: 0.7575 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.7677 Loss_G: 0.7699 acc: 79.7%\n",
      "[BATCH 139/149] Loss_D: 0.7561 Loss_G: 0.7701 acc: 81.2%\n",
      "[BATCH 140/149] Loss_D: 0.7481 Loss_G: 0.7724 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7657 Loss_G: 0.7895 acc: 70.3%\n",
      "[BATCH 142/149] Loss_D: 0.7326 Loss_G: 0.7547 acc: 76.6%\n",
      "[BATCH 143/149] Loss_D: 0.7937 Loss_G: 0.7767 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.7346 Loss_G: 0.7636 acc: 78.1%\n",
      "[BATCH 145/149] Loss_D: 0.7337 Loss_G: 0.7490 acc: 76.6%\n",
      "[BATCH 146/149] Loss_D: 0.7272 Loss_G: 0.7396 acc: 79.7%\n",
      "[BATCH 147/149] Loss_D: 0.6972 Loss_G: 0.7465 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.7367 Loss_G: 0.7549 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.7534 Loss_G: 0.7594 acc: 73.4%\n",
      "-----THE [30/50] epoch end-----\n",
      "-----THE [31/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7014 Loss_G: 0.7459 acc: 76.6%\n",
      "[BATCH 2/149] Loss_D: 0.7323 Loss_G: 0.7692 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7504 Loss_G: 0.7636 acc: 71.9%\n",
      "[BATCH 4/149] Loss_D: 0.7857 Loss_G: 0.7746 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.7536 Loss_G: 0.7696 acc: 79.7%\n",
      "[BATCH 6/149] Loss_D: 0.7884 Loss_G: 0.7821 acc: 71.9%\n",
      "[BATCH 7/149] Loss_D: 0.7493 Loss_G: 0.7866 acc: 78.1%\n",
      "[BATCH 8/149] Loss_D: 0.7189 Loss_G: 0.7761 acc: 68.8%\n",
      "[BATCH 9/149] Loss_D: 0.7612 Loss_G: 0.7637 acc: 68.8%\n",
      "[BATCH 10/149] Loss_D: 0.7501 Loss_G: 0.7672 acc: 73.4%\n",
      "[BATCH 11/149] Loss_D: 0.7410 Loss_G: 0.7617 acc: 78.1%\n",
      "[BATCH 12/149] Loss_D: 0.6875 Loss_G: 0.7386 acc: 73.4%\n",
      "[BATCH 13/149] Loss_D: 0.7371 Loss_G: 0.7539 acc: 79.7%\n",
      "[BATCH 14/149] Loss_D: 0.7374 Loss_G: 0.7636 acc: 79.7%\n",
      "[BATCH 15/149] Loss_D: 0.7607 Loss_G: 0.7649 acc: 79.7%\n",
      "[BATCH 16/149] Loss_D: 0.7696 Loss_G: 0.7710 acc: 76.6%\n",
      "[BATCH 17/149] Loss_D: 0.7308 Loss_G: 0.7643 acc: 71.9%\n",
      "[BATCH 18/149] Loss_D: 0.7368 Loss_G: 0.7647 acc: 81.2%\n",
      "[BATCH 19/149] Loss_D: 0.7425 Loss_G: 0.7602 acc: 81.2%\n",
      "[BATCH 20/149] Loss_D: 0.7844 Loss_G: 0.7713 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.7591 Loss_G: 0.7670 acc: 78.1%\n",
      "[BATCH 22/149] Loss_D: 0.7289 Loss_G: 0.7606 acc: 75.0%\n",
      "[BATCH 23/149] Loss_D: 0.7405 Loss_G: 0.7497 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7408 Loss_G: 0.7554 acc: 76.6%\n",
      "[BATCH 25/149] Loss_D: 0.7454 Loss_G: 0.7685 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7159 Loss_G: 0.7487 acc: 82.8%\n",
      "[BATCH 27/149] Loss_D: 0.7305 Loss_G: 0.7683 acc: 68.8%\n",
      "[BATCH 28/149] Loss_D: 0.7233 Loss_G: 0.7533 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.6870 Loss_G: 0.7449 acc: 82.8%\n",
      "[BATCH 30/149] Loss_D: 0.7172 Loss_G: 0.7589 acc: 73.4%\n",
      "[EPOCH 4500] TEST ACC is : 69.1%\n",
      "[BATCH 31/149] Loss_D: 0.6899 Loss_G: 0.7308 acc: 76.6%\n",
      "[BATCH 32/149] Loss_D: 0.7485 Loss_G: 0.7532 acc: 76.6%\n",
      "[BATCH 33/149] Loss_D: 0.7051 Loss_G: 0.7570 acc: 79.7%\n",
      "[BATCH 34/149] Loss_D: 0.7234 Loss_G: 0.7592 acc: 62.5%\n",
      "[BATCH 35/149] Loss_D: 0.7363 Loss_G: 0.7419 acc: 78.1%\n",
      "[BATCH 36/149] Loss_D: 0.7170 Loss_G: 0.7647 acc: 73.4%\n",
      "[BATCH 37/149] Loss_D: 0.6948 Loss_G: 0.7375 acc: 81.2%\n",
      "[BATCH 38/149] Loss_D: 0.7539 Loss_G: 0.7358 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.7340 Loss_G: 0.7502 acc: 78.1%\n",
      "[BATCH 40/149] Loss_D: 0.7447 Loss_G: 0.7716 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.7408 Loss_G: 0.7530 acc: 71.9%\n",
      "[BATCH 42/149] Loss_D: 0.7442 Loss_G: 0.7443 acc: 73.4%\n",
      "[BATCH 43/149] Loss_D: 0.7507 Loss_G: 0.7533 acc: 75.0%\n",
      "[BATCH 44/149] Loss_D: 0.7290 Loss_G: 0.7663 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7261 Loss_G: 0.7461 acc: 70.3%\n",
      "[BATCH 46/149] Loss_D: 0.7029 Loss_G: 0.7362 acc: 73.4%\n",
      "[BATCH 47/149] Loss_D: 0.7460 Loss_G: 0.7724 acc: 71.9%\n",
      "[BATCH 48/149] Loss_D: 0.7508 Loss_G: 0.7704 acc: 73.4%\n",
      "[BATCH 49/149] Loss_D: 0.7090 Loss_G: 0.7569 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.6789 Loss_G: 0.7296 acc: 82.8%\n",
      "[BATCH 51/149] Loss_D: 0.7380 Loss_G: 0.7428 acc: 81.2%\n",
      "[BATCH 52/149] Loss_D: 0.7559 Loss_G: 0.7613 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.6974 Loss_G: 0.7433 acc: 70.3%\n",
      "[BATCH 54/149] Loss_D: 0.7476 Loss_G: 0.7465 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7626 Loss_G: 0.7558 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7566 Loss_G: 0.7569 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7198 Loss_G: 0.7612 acc: 79.7%\n",
      "[BATCH 58/149] Loss_D: 0.7364 Loss_G: 0.7772 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.7129 Loss_G: 0.7669 acc: 81.2%\n",
      "[BATCH 60/149] Loss_D: 0.7348 Loss_G: 0.7821 acc: 68.8%\n",
      "[BATCH 61/149] Loss_D: 0.7489 Loss_G: 0.7699 acc: 68.8%\n",
      "[BATCH 62/149] Loss_D: 0.7292 Loss_G: 0.7541 acc: 78.1%\n",
      "[BATCH 63/149] Loss_D: 0.7129 Loss_G: 0.7509 acc: 78.1%\n",
      "[BATCH 64/149] Loss_D: 0.7585 Loss_G: 0.7580 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7562 Loss_G: 0.7633 acc: 76.6%\n",
      "[BATCH 66/149] Loss_D: 0.7384 Loss_G: 0.7704 acc: 70.3%\n",
      "[BATCH 67/149] Loss_D: 0.6997 Loss_G: 0.7598 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7049 Loss_G: 0.7285 acc: 81.2%\n",
      "[BATCH 69/149] Loss_D: 0.7382 Loss_G: 0.7494 acc: 65.6%\n",
      "[BATCH 70/149] Loss_D: 0.7334 Loss_G: 0.7472 acc: 73.4%\n",
      "[BATCH 71/149] Loss_D: 0.7314 Loss_G: 0.7518 acc: 70.3%\n",
      "[BATCH 72/149] Loss_D: 0.7000 Loss_G: 0.7404 acc: 76.6%\n",
      "[BATCH 73/149] Loss_D: 0.7736 Loss_G: 0.7665 acc: 64.1%\n",
      "[BATCH 74/149] Loss_D: 0.7245 Loss_G: 0.7540 acc: 73.4%\n",
      "[BATCH 75/149] Loss_D: 0.7068 Loss_G: 0.7441 acc: 76.6%\n",
      "[BATCH 76/149] Loss_D: 0.7611 Loss_G: 0.7471 acc: 75.0%\n",
      "[BATCH 77/149] Loss_D: 0.7488 Loss_G: 0.7640 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.7487 Loss_G: 0.7883 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.7652 Loss_G: 0.7730 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.7693 Loss_G: 0.7546 acc: 76.6%\n",
      "[EPOCH 4550] TEST ACC is : 68.4%\n",
      "[BATCH 81/149] Loss_D: 0.7135 Loss_G: 0.7415 acc: 76.6%\n",
      "[BATCH 82/149] Loss_D: 0.7686 Loss_G: 0.7718 acc: 59.4%\n",
      "[BATCH 83/149] Loss_D: 0.7442 Loss_G: 0.7524 acc: 75.0%\n",
      "[BATCH 84/149] Loss_D: 0.7320 Loss_G: 0.7389 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7535 Loss_G: 0.7513 acc: 70.3%\n",
      "[BATCH 86/149] Loss_D: 0.7294 Loss_G: 0.7464 acc: 76.6%\n",
      "[BATCH 87/149] Loss_D: 0.7450 Loss_G: 0.7637 acc: 64.1%\n",
      "[BATCH 88/149] Loss_D: 0.7100 Loss_G: 0.7494 acc: 71.9%\n",
      "[BATCH 89/149] Loss_D: 0.7302 Loss_G: 0.7603 acc: 78.1%\n",
      "[BATCH 90/149] Loss_D: 0.7374 Loss_G: 0.7580 acc: 81.2%\n",
      "[BATCH 91/149] Loss_D: 0.7816 Loss_G: 0.7998 acc: 78.1%\n",
      "[BATCH 92/149] Loss_D: 0.7148 Loss_G: 0.7648 acc: 75.0%\n",
      "[BATCH 93/149] Loss_D: 0.7315 Loss_G: 0.7630 acc: 71.9%\n",
      "[BATCH 94/149] Loss_D: 0.7076 Loss_G: 0.7338 acc: 79.7%\n",
      "[BATCH 95/149] Loss_D: 0.7396 Loss_G: 0.7388 acc: 76.6%\n",
      "[BATCH 96/149] Loss_D: 0.7249 Loss_G: 0.7329 acc: 65.6%\n",
      "[BATCH 97/149] Loss_D: 0.7474 Loss_G: 0.7429 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7446 Loss_G: 0.7608 acc: 73.4%\n",
      "[BATCH 99/149] Loss_D: 0.7141 Loss_G: 0.7508 acc: 71.9%\n",
      "[BATCH 100/149] Loss_D: 0.7000 Loss_G: 0.7504 acc: 81.2%\n",
      "[BATCH 101/149] Loss_D: 0.8204 Loss_G: 0.7716 acc: 79.7%\n",
      "[BATCH 102/149] Loss_D: 0.7233 Loss_G: 0.7659 acc: 68.8%\n",
      "[BATCH 103/149] Loss_D: 0.7130 Loss_G: 0.7512 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7262 Loss_G: 0.7486 acc: 79.7%\n",
      "[BATCH 105/149] Loss_D: 0.7168 Loss_G: 0.7483 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7621 Loss_G: 0.7677 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.7467 Loss_G: 0.7659 acc: 71.9%\n",
      "[BATCH 108/149] Loss_D: 0.7520 Loss_G: 0.7747 acc: 79.7%\n",
      "[BATCH 109/149] Loss_D: 0.7475 Loss_G: 0.7629 acc: 76.6%\n",
      "[BATCH 110/149] Loss_D: 0.7152 Loss_G: 0.7484 acc: 75.0%\n",
      "[BATCH 111/149] Loss_D: 0.7635 Loss_G: 0.7583 acc: 62.5%\n",
      "[BATCH 112/149] Loss_D: 0.7414 Loss_G: 0.7619 acc: 76.6%\n",
      "[BATCH 113/149] Loss_D: 0.7185 Loss_G: 0.7751 acc: 78.1%\n",
      "[BATCH 114/149] Loss_D: 0.7481 Loss_G: 0.7665 acc: 78.1%\n",
      "[BATCH 115/149] Loss_D: 0.7402 Loss_G: 0.7815 acc: 65.6%\n",
      "[BATCH 116/149] Loss_D: 0.7217 Loss_G: 0.7511 acc: 81.2%\n",
      "[BATCH 117/149] Loss_D: 0.7346 Loss_G: 0.7459 acc: 75.0%\n",
      "[BATCH 118/149] Loss_D: 0.7513 Loss_G: 0.7603 acc: 76.6%\n",
      "[BATCH 119/149] Loss_D: 0.7455 Loss_G: 0.7624 acc: 79.7%\n",
      "[BATCH 120/149] Loss_D: 0.7662 Loss_G: 0.7648 acc: 79.7%\n",
      "[BATCH 121/149] Loss_D: 0.7263 Loss_G: 0.7696 acc: 62.5%\n",
      "[BATCH 122/149] Loss_D: 0.7155 Loss_G: 0.7593 acc: 78.1%\n",
      "[BATCH 123/149] Loss_D: 0.7156 Loss_G: 0.7690 acc: 71.9%\n",
      "[BATCH 124/149] Loss_D: 0.7239 Loss_G: 0.7664 acc: 70.3%\n",
      "[BATCH 125/149] Loss_D: 0.7094 Loss_G: 0.7503 acc: 78.1%\n",
      "[BATCH 126/149] Loss_D: 0.6971 Loss_G: 0.7465 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.7141 Loss_G: 0.7302 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.7296 Loss_G: 0.7532 acc: 75.0%\n",
      "[BATCH 129/149] Loss_D: 0.7451 Loss_G: 0.7688 acc: 70.3%\n",
      "[BATCH 130/149] Loss_D: 0.7487 Loss_G: 0.7571 acc: 76.6%\n",
      "[EPOCH 4600] TEST ACC is : 69.7%\n",
      "[BATCH 131/149] Loss_D: 0.7721 Loss_G: 0.7653 acc: 73.4%\n",
      "[BATCH 132/149] Loss_D: 0.7408 Loss_G: 0.7789 acc: 78.1%\n",
      "[BATCH 133/149] Loss_D: 0.7764 Loss_G: 0.7743 acc: 79.7%\n",
      "[BATCH 134/149] Loss_D: 0.7638 Loss_G: 0.7753 acc: 76.6%\n",
      "[BATCH 135/149] Loss_D: 0.7461 Loss_G: 0.7720 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.7880 Loss_G: 0.7720 acc: 78.1%\n",
      "[BATCH 137/149] Loss_D: 0.7609 Loss_G: 0.7596 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7302 Loss_G: 0.7672 acc: 75.0%\n",
      "[BATCH 139/149] Loss_D: 0.7124 Loss_G: 0.7526 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.7382 Loss_G: 0.7557 acc: 73.4%\n",
      "[BATCH 141/149] Loss_D: 0.7030 Loss_G: 0.7747 acc: 73.4%\n",
      "[BATCH 142/149] Loss_D: 0.7327 Loss_G: 0.7744 acc: 76.6%\n",
      "[BATCH 143/149] Loss_D: 0.7438 Loss_G: 0.7660 acc: 76.6%\n",
      "[BATCH 144/149] Loss_D: 0.7472 Loss_G: 0.7661 acc: 70.3%\n",
      "[BATCH 145/149] Loss_D: 0.7398 Loss_G: 0.7536 acc: 79.7%\n",
      "[BATCH 146/149] Loss_D: 0.7160 Loss_G: 0.7561 acc: 78.1%\n",
      "[BATCH 147/149] Loss_D: 0.7079 Loss_G: 0.7512 acc: 75.0%\n",
      "[BATCH 148/149] Loss_D: 0.7387 Loss_G: 0.7578 acc: 73.4%\n",
      "[BATCH 149/149] Loss_D: 0.7565 Loss_G: 0.7524 acc: 68.8%\n",
      "-----THE [31/50] epoch end-----\n",
      "-----THE [32/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7333 Loss_G: 0.7728 acc: 71.9%\n",
      "[BATCH 2/149] Loss_D: 0.7515 Loss_G: 0.7597 acc: 75.0%\n",
      "[BATCH 3/149] Loss_D: 0.8009 Loss_G: 0.7709 acc: 75.0%\n",
      "[BATCH 4/149] Loss_D: 0.7212 Loss_G: 0.7502 acc: 79.7%\n",
      "[BATCH 5/149] Loss_D: 0.7291 Loss_G: 0.7499 acc: 76.6%\n",
      "[BATCH 6/149] Loss_D: 0.7517 Loss_G: 0.7557 acc: 76.6%\n",
      "[BATCH 7/149] Loss_D: 0.7017 Loss_G: 0.7447 acc: 76.6%\n",
      "[BATCH 8/149] Loss_D: 0.7076 Loss_G: 0.7484 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.7166 Loss_G: 0.7422 acc: 73.4%\n",
      "[BATCH 10/149] Loss_D: 0.7331 Loss_G: 0.7410 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.7085 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7274 Loss_G: 0.7519 acc: 75.0%\n",
      "[BATCH 13/149] Loss_D: 0.7282 Loss_G: 0.7644 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.7447 Loss_G: 0.7666 acc: 79.7%\n",
      "[BATCH 15/149] Loss_D: 0.7464 Loss_G: 0.7628 acc: 65.6%\n",
      "[BATCH 16/149] Loss_D: 0.7041 Loss_G: 0.7373 acc: 75.0%\n",
      "[BATCH 17/149] Loss_D: 0.7135 Loss_G: 0.7187 acc: 76.6%\n",
      "[BATCH 18/149] Loss_D: 0.7409 Loss_G: 0.7614 acc: 79.7%\n",
      "[BATCH 19/149] Loss_D: 0.7837 Loss_G: 0.7783 acc: 70.3%\n",
      "[BATCH 20/149] Loss_D: 0.7258 Loss_G: 0.7686 acc: 78.1%\n",
      "[BATCH 21/149] Loss_D: 0.7069 Loss_G: 0.7478 acc: 79.7%\n",
      "[BATCH 22/149] Loss_D: 0.7475 Loss_G: 0.7533 acc: 76.6%\n",
      "[BATCH 23/149] Loss_D: 0.7526 Loss_G: 0.7772 acc: 75.0%\n",
      "[BATCH 24/149] Loss_D: 0.7743 Loss_G: 0.7623 acc: 70.3%\n",
      "[BATCH 25/149] Loss_D: 0.7958 Loss_G: 0.7900 acc: 64.1%\n",
      "[BATCH 26/149] Loss_D: 0.7166 Loss_G: 0.7507 acc: 79.7%\n",
      "[BATCH 27/149] Loss_D: 0.7367 Loss_G: 0.7637 acc: 79.7%\n",
      "[BATCH 28/149] Loss_D: 0.8302 Loss_G: 0.7904 acc: 71.9%\n",
      "[BATCH 29/149] Loss_D: 0.7609 Loss_G: 0.7691 acc: 78.1%\n",
      "[BATCH 30/149] Loss_D: 0.7160 Loss_G: 0.7619 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.7708 Loss_G: 0.7907 acc: 70.3%\n",
      "[EPOCH 4650] TEST ACC is : 71.5%\n",
      "[BATCH 32/149] Loss_D: 0.7680 Loss_G: 0.7962 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.7015 Loss_G: 0.7664 acc: 81.2%\n",
      "[BATCH 34/149] Loss_D: 0.7299 Loss_G: 0.7657 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.7121 Loss_G: 0.7654 acc: 73.4%\n",
      "[BATCH 36/149] Loss_D: 0.7347 Loss_G: 0.7588 acc: 78.1%\n",
      "[BATCH 37/149] Loss_D: 0.7048 Loss_G: 0.7370 acc: 79.7%\n",
      "[BATCH 38/149] Loss_D: 0.7418 Loss_G: 0.7634 acc: 79.7%\n",
      "[BATCH 39/149] Loss_D: 0.7864 Loss_G: 0.7738 acc: 78.1%\n",
      "[BATCH 40/149] Loss_D: 0.7592 Loss_G: 0.7744 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.7186 Loss_G: 0.7670 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.7012 Loss_G: 0.7561 acc: 81.2%\n",
      "[BATCH 43/149] Loss_D: 0.7330 Loss_G: 0.7525 acc: 79.7%\n",
      "[BATCH 44/149] Loss_D: 0.7306 Loss_G: 0.7741 acc: 79.7%\n",
      "[BATCH 45/149] Loss_D: 0.7187 Loss_G: 0.7738 acc: 76.6%\n",
      "[BATCH 46/149] Loss_D: 0.7486 Loss_G: 0.7714 acc: 82.8%\n",
      "[BATCH 47/149] Loss_D: 0.7035 Loss_G: 0.7669 acc: 70.3%\n",
      "[BATCH 48/149] Loss_D: 0.7275 Loss_G: 0.7454 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7414 Loss_G: 0.7593 acc: 75.0%\n",
      "[BATCH 50/149] Loss_D: 0.7226 Loss_G: 0.7564 acc: 70.3%\n",
      "[BATCH 51/149] Loss_D: 0.7726 Loss_G: 0.7457 acc: 78.1%\n",
      "[BATCH 52/149] Loss_D: 0.7325 Loss_G: 0.7593 acc: 71.9%\n",
      "[BATCH 53/149] Loss_D: 0.7686 Loss_G: 0.7704 acc: 73.4%\n",
      "[BATCH 54/149] Loss_D: 0.7164 Loss_G: 0.7527 acc: 75.0%\n",
      "[BATCH 55/149] Loss_D: 0.7696 Loss_G: 0.7760 acc: 68.8%\n",
      "[BATCH 56/149] Loss_D: 0.7261 Loss_G: 0.7635 acc: 65.6%\n",
      "[BATCH 57/149] Loss_D: 0.7225 Loss_G: 0.7440 acc: 79.7%\n",
      "[BATCH 58/149] Loss_D: 0.7551 Loss_G: 0.7588 acc: 78.1%\n",
      "[BATCH 59/149] Loss_D: 0.7487 Loss_G: 0.7650 acc: 73.4%\n",
      "[BATCH 60/149] Loss_D: 0.7012 Loss_G: 0.7423 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.7542 Loss_G: 0.7503 acc: 81.2%\n",
      "[BATCH 62/149] Loss_D: 0.7251 Loss_G: 0.7492 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.7210 Loss_G: 0.7636 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.7552 Loss_G: 0.7620 acc: 78.1%\n",
      "[BATCH 65/149] Loss_D: 0.7065 Loss_G: 0.7605 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7069 Loss_G: 0.7416 acc: 81.2%\n",
      "[BATCH 67/149] Loss_D: 0.6678 Loss_G: 0.7330 acc: 76.6%\n",
      "[BATCH 68/149] Loss_D: 0.7445 Loss_G: 0.7550 acc: 75.0%\n",
      "[BATCH 69/149] Loss_D: 0.6923 Loss_G: 0.7450 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.7380 Loss_G: 0.7487 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.7426 Loss_G: 0.7348 acc: 75.0%\n",
      "[BATCH 72/149] Loss_D: 0.7003 Loss_G: 0.7470 acc: 76.6%\n",
      "[BATCH 73/149] Loss_D: 0.7345 Loss_G: 0.7740 acc: 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.7653 Loss_G: 0.7650 acc: 82.8%\n",
      "[BATCH 75/149] Loss_D: 0.7866 Loss_G: 0.7667 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7190 Loss_G: 0.7642 acc: 67.2%\n",
      "[BATCH 77/149] Loss_D: 0.7684 Loss_G: 0.7749 acc: 78.1%\n",
      "[BATCH 78/149] Loss_D: 0.7645 Loss_G: 0.7930 acc: 64.1%\n",
      "[BATCH 79/149] Loss_D: 0.7008 Loss_G: 0.7499 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7225 Loss_G: 0.7572 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.7465 Loss_G: 0.7602 acc: 84.4%\n",
      "[EPOCH 4700] TEST ACC is : 70.5%\n",
      "[BATCH 82/149] Loss_D: 0.7024 Loss_G: 0.7392 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7247 Loss_G: 0.7542 acc: 76.6%\n",
      "[BATCH 84/149] Loss_D: 0.7223 Loss_G: 0.7383 acc: 79.7%\n",
      "[BATCH 85/149] Loss_D: 0.7241 Loss_G: 0.7536 acc: 68.8%\n",
      "[BATCH 86/149] Loss_D: 0.7787 Loss_G: 0.7521 acc: 76.6%\n",
      "[BATCH 87/149] Loss_D: 0.7729 Loss_G: 0.7729 acc: 71.9%\n",
      "[BATCH 88/149] Loss_D: 0.7470 Loss_G: 0.7653 acc: 78.1%\n",
      "[BATCH 89/149] Loss_D: 0.7358 Loss_G: 0.7717 acc: 78.1%\n",
      "[BATCH 90/149] Loss_D: 0.7674 Loss_G: 0.7936 acc: 79.7%\n",
      "[BATCH 91/149] Loss_D: 0.7390 Loss_G: 0.7662 acc: 76.6%\n",
      "[BATCH 92/149] Loss_D: 0.6991 Loss_G: 0.7638 acc: 75.0%\n",
      "[BATCH 93/149] Loss_D: 0.7233 Loss_G: 0.7375 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.7551 Loss_G: 0.7513 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.7263 Loss_G: 0.7635 acc: 78.1%\n",
      "[BATCH 96/149] Loss_D: 0.7332 Loss_G: 0.7628 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.7202 Loss_G: 0.7681 acc: 73.4%\n",
      "[BATCH 98/149] Loss_D: 0.7248 Loss_G: 0.7538 acc: 73.4%\n",
      "[BATCH 99/149] Loss_D: 0.7294 Loss_G: 0.7466 acc: 75.0%\n",
      "[BATCH 100/149] Loss_D: 0.6857 Loss_G: 0.7292 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7248 Loss_G: 0.7548 acc: 71.9%\n",
      "[BATCH 102/149] Loss_D: 0.7770 Loss_G: 0.7703 acc: 65.6%\n",
      "[BATCH 103/149] Loss_D: 0.7313 Loss_G: 0.7456 acc: 79.7%\n",
      "[BATCH 104/149] Loss_D: 0.7453 Loss_G: 0.7497 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.7052 Loss_G: 0.7465 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7375 Loss_G: 0.7539 acc: 78.1%\n",
      "[BATCH 107/149] Loss_D: 0.7790 Loss_G: 0.7678 acc: 73.4%\n",
      "[BATCH 108/149] Loss_D: 0.7344 Loss_G: 0.7617 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7613 Loss_G: 0.7687 acc: 78.1%\n",
      "[BATCH 110/149] Loss_D: 0.7454 Loss_G: 0.7639 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7266 Loss_G: 0.7495 acc: 76.6%\n",
      "[BATCH 112/149] Loss_D: 0.6994 Loss_G: 0.7495 acc: 71.9%\n",
      "[BATCH 113/149] Loss_D: 0.7495 Loss_G: 0.7534 acc: 70.3%\n",
      "[BATCH 114/149] Loss_D: 0.7118 Loss_G: 0.7559 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7514 Loss_G: 0.7567 acc: 60.9%\n",
      "[BATCH 116/149] Loss_D: 0.7537 Loss_G: 0.7598 acc: 70.3%\n",
      "[BATCH 117/149] Loss_D: 0.7360 Loss_G: 0.7496 acc: 70.3%\n",
      "[BATCH 118/149] Loss_D: 0.7521 Loss_G: 0.7530 acc: 78.1%\n",
      "[BATCH 119/149] Loss_D: 0.7405 Loss_G: 0.7544 acc: 79.7%\n",
      "[BATCH 120/149] Loss_D: 0.7097 Loss_G: 0.7526 acc: 70.3%\n",
      "[BATCH 121/149] Loss_D: 0.7439 Loss_G: 0.7420 acc: 73.4%\n",
      "[BATCH 122/149] Loss_D: 0.7114 Loss_G: 0.7467 acc: 78.1%\n",
      "[BATCH 123/149] Loss_D: 0.7085 Loss_G: 0.7473 acc: 81.2%\n",
      "[BATCH 124/149] Loss_D: 0.7015 Loss_G: 0.7558 acc: 76.6%\n",
      "[BATCH 125/149] Loss_D: 0.7744 Loss_G: 0.7709 acc: 71.9%\n",
      "[BATCH 126/149] Loss_D: 0.6867 Loss_G: 0.7494 acc: 79.7%\n",
      "[BATCH 127/149] Loss_D: 0.6978 Loss_G: 0.7504 acc: 79.7%\n",
      "[BATCH 128/149] Loss_D: 0.7094 Loss_G: 0.7579 acc: 67.2%\n",
      "[BATCH 129/149] Loss_D: 0.7000 Loss_G: 0.7604 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7211 Loss_G: 0.7562 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7212 Loss_G: 0.7444 acc: 82.8%\n",
      "[EPOCH 4750] TEST ACC is : 70.5%\n",
      "[BATCH 132/149] Loss_D: 0.7342 Loss_G: 0.7420 acc: 64.1%\n",
      "[BATCH 133/149] Loss_D: 0.7729 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.7568 Loss_G: 0.7415 acc: 81.2%\n",
      "[BATCH 135/149] Loss_D: 0.7714 Loss_G: 0.7605 acc: 78.1%\n",
      "[BATCH 136/149] Loss_D: 0.7451 Loss_G: 0.7635 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7286 Loss_G: 0.7557 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.7386 Loss_G: 0.7645 acc: 79.7%\n",
      "[BATCH 139/149] Loss_D: 0.6917 Loss_G: 0.7539 acc: 78.1%\n",
      "[BATCH 140/149] Loss_D: 0.7659 Loss_G: 0.7635 acc: 79.7%\n",
      "[BATCH 141/149] Loss_D: 0.7524 Loss_G: 0.7639 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7449 Loss_G: 0.7685 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.7119 Loss_G: 0.7459 acc: 65.6%\n",
      "[BATCH 144/149] Loss_D: 0.7230 Loss_G: 0.7452 acc: 79.7%\n",
      "[BATCH 145/149] Loss_D: 0.7036 Loss_G: 0.7474 acc: 78.1%\n",
      "[BATCH 146/149] Loss_D: 0.7812 Loss_G: 0.7632 acc: 76.6%\n",
      "[BATCH 147/149] Loss_D: 0.7395 Loss_G: 0.7746 acc: 76.6%\n",
      "[BATCH 148/149] Loss_D: 0.7497 Loss_G: 0.7753 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7080 Loss_G: 0.7623 acc: 79.7%\n",
      "-----THE [32/50] epoch end-----\n",
      "-----THE [33/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.8039 Loss_G: 0.8048 acc: 68.8%\n",
      "[BATCH 2/149] Loss_D: 0.7455 Loss_G: 0.7992 acc: 71.9%\n",
      "[BATCH 3/149] Loss_D: 0.7540 Loss_G: 0.7675 acc: 79.7%\n",
      "[BATCH 4/149] Loss_D: 0.7143 Loss_G: 0.7585 acc: 73.4%\n",
      "[BATCH 5/149] Loss_D: 0.7072 Loss_G: 0.7510 acc: 79.7%\n",
      "[BATCH 6/149] Loss_D: 0.7437 Loss_G: 0.7603 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7392 Loss_G: 0.7627 acc: 73.4%\n",
      "[BATCH 8/149] Loss_D: 0.7589 Loss_G: 0.7584 acc: 79.7%\n",
      "[BATCH 9/149] Loss_D: 0.7402 Loss_G: 0.7534 acc: 65.6%\n",
      "[BATCH 10/149] Loss_D: 0.7320 Loss_G: 0.7579 acc: 67.2%\n",
      "[BATCH 11/149] Loss_D: 0.7550 Loss_G: 0.7646 acc: 75.0%\n",
      "[BATCH 12/149] Loss_D: 0.7152 Loss_G: 0.7495 acc: 73.4%\n",
      "[BATCH 13/149] Loss_D: 0.7127 Loss_G: 0.7366 acc: 78.1%\n",
      "[BATCH 14/149] Loss_D: 0.7350 Loss_G: 0.7288 acc: 79.7%\n",
      "[BATCH 15/149] Loss_D: 0.7808 Loss_G: 0.7693 acc: 79.7%\n",
      "[BATCH 16/149] Loss_D: 0.8009 Loss_G: 0.7841 acc: 79.7%\n",
      "[BATCH 17/149] Loss_D: 0.7690 Loss_G: 0.7756 acc: 78.1%\n",
      "[BATCH 18/149] Loss_D: 0.6946 Loss_G: 0.7604 acc: 71.9%\n",
      "[BATCH 19/149] Loss_D: 0.7727 Loss_G: 0.7583 acc: 76.6%\n",
      "[BATCH 20/149] Loss_D: 0.7482 Loss_G: 0.7572 acc: 71.9%\n",
      "[BATCH 21/149] Loss_D: 0.7690 Loss_G: 0.7675 acc: 76.6%\n",
      "[BATCH 22/149] Loss_D: 0.7359 Loss_G: 0.7908 acc: 76.6%\n",
      "[BATCH 23/149] Loss_D: 0.6976 Loss_G: 0.7499 acc: 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.7363 Loss_G: 0.7571 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7508 Loss_G: 0.7533 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.7176 Loss_G: 0.7431 acc: 78.1%\n",
      "[BATCH 27/149] Loss_D: 0.7029 Loss_G: 0.7416 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7775 Loss_G: 0.7647 acc: 79.7%\n",
      "[BATCH 29/149] Loss_D: 0.7381 Loss_G: 0.7615 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7316 Loss_G: 0.7628 acc: 76.6%\n",
      "[BATCH 31/149] Loss_D: 0.7635 Loss_G: 0.7737 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.7739 Loss_G: 0.7704 acc: 76.6%\n",
      "[EPOCH 4800] TEST ACC is : 69.3%\n",
      "[BATCH 33/149] Loss_D: 0.7523 Loss_G: 0.7734 acc: 71.9%\n",
      "[BATCH 34/149] Loss_D: 0.7033 Loss_G: 0.7491 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.7610 Loss_G: 0.7675 acc: 73.4%\n",
      "[BATCH 36/149] Loss_D: 0.7408 Loss_G: 0.7593 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.7489 Loss_G: 0.7564 acc: 81.2%\n",
      "[BATCH 38/149] Loss_D: 0.7205 Loss_G: 0.7527 acc: 76.6%\n",
      "[BATCH 39/149] Loss_D: 0.7491 Loss_G: 0.7486 acc: 78.1%\n",
      "[BATCH 40/149] Loss_D: 0.7206 Loss_G: 0.7559 acc: 79.7%\n",
      "[BATCH 41/149] Loss_D: 0.7324 Loss_G: 0.7678 acc: 73.4%\n",
      "[BATCH 42/149] Loss_D: 0.7430 Loss_G: 0.7486 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.7337 Loss_G: 0.7577 acc: 81.2%\n",
      "[BATCH 44/149] Loss_D: 0.7122 Loss_G: 0.7348 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.7393 Loss_G: 0.7532 acc: 76.6%\n",
      "[BATCH 46/149] Loss_D: 0.7454 Loss_G: 0.7584 acc: 76.6%\n",
      "[BATCH 47/149] Loss_D: 0.7424 Loss_G: 0.7598 acc: 73.4%\n",
      "[BATCH 48/149] Loss_D: 0.6974 Loss_G: 0.7499 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.7298 Loss_G: 0.7642 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.7100 Loss_G: 0.7506 acc: 73.4%\n",
      "[BATCH 51/149] Loss_D: 0.7793 Loss_G: 0.7861 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.7626 Loss_G: 0.7769 acc: 79.7%\n",
      "[BATCH 53/149] Loss_D: 0.7272 Loss_G: 0.7611 acc: 73.4%\n",
      "[BATCH 54/149] Loss_D: 0.7148 Loss_G: 0.7446 acc: 79.7%\n",
      "[BATCH 55/149] Loss_D: 0.7209 Loss_G: 0.7462 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.7116 Loss_G: 0.7320 acc: 75.0%\n",
      "[BATCH 57/149] Loss_D: 0.7515 Loss_G: 0.7403 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.7586 Loss_G: 0.7682 acc: 67.2%\n",
      "[BATCH 59/149] Loss_D: 0.7614 Loss_G: 0.7585 acc: 81.2%\n",
      "[BATCH 60/149] Loss_D: 0.6770 Loss_G: 0.7446 acc: 79.7%\n",
      "[BATCH 61/149] Loss_D: 0.7211 Loss_G: 0.7645 acc: 75.0%\n",
      "[BATCH 62/149] Loss_D: 0.7316 Loss_G: 0.7510 acc: 73.4%\n",
      "[BATCH 63/149] Loss_D: 0.7734 Loss_G: 0.7670 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7559 Loss_G: 0.7688 acc: 76.6%\n",
      "[BATCH 65/149] Loss_D: 0.7700 Loss_G: 0.7686 acc: 76.6%\n",
      "[BATCH 66/149] Loss_D: 0.6961 Loss_G: 0.7426 acc: 73.4%\n",
      "[BATCH 67/149] Loss_D: 0.7025 Loss_G: 0.7372 acc: 76.6%\n",
      "[BATCH 68/149] Loss_D: 0.6919 Loss_G: 0.7488 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7167 Loss_G: 0.7503 acc: 78.1%\n",
      "[BATCH 70/149] Loss_D: 0.7664 Loss_G: 0.7582 acc: 75.0%\n",
      "[BATCH 71/149] Loss_D: 0.7052 Loss_G: 0.7472 acc: 73.4%\n",
      "[BATCH 72/149] Loss_D: 0.7517 Loss_G: 0.7524 acc: 81.2%\n",
      "[BATCH 73/149] Loss_D: 0.7341 Loss_G: 0.7660 acc: 71.9%\n",
      "[BATCH 74/149] Loss_D: 0.7475 Loss_G: 0.7665 acc: 75.0%\n",
      "[BATCH 75/149] Loss_D: 0.7182 Loss_G: 0.7495 acc: 76.6%\n",
      "[BATCH 76/149] Loss_D: 0.7362 Loss_G: 0.7540 acc: 79.7%\n",
      "[BATCH 77/149] Loss_D: 0.7379 Loss_G: 0.7643 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.7364 Loss_G: 0.7518 acc: 76.6%\n",
      "[BATCH 79/149] Loss_D: 0.6986 Loss_G: 0.7433 acc: 71.9%\n",
      "[BATCH 80/149] Loss_D: 0.7535 Loss_G: 0.7604 acc: 71.9%\n",
      "[BATCH 81/149] Loss_D: 0.7522 Loss_G: 0.7589 acc: 75.0%\n",
      "[BATCH 82/149] Loss_D: 0.7139 Loss_G: 0.7573 acc: 87.5%\n",
      "[EPOCH 4850] TEST ACC is : 69.1%\n",
      "[BATCH 83/149] Loss_D: 0.6852 Loss_G: 0.7343 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6909 Loss_G: 0.7517 acc: 64.1%\n",
      "[BATCH 85/149] Loss_D: 0.7621 Loss_G: 0.7516 acc: 70.3%\n",
      "[BATCH 86/149] Loss_D: 0.7148 Loss_G: 0.7421 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6790 Loss_G: 0.7363 acc: 79.7%\n",
      "[BATCH 88/149] Loss_D: 0.7419 Loss_G: 0.7544 acc: 76.6%\n",
      "[BATCH 89/149] Loss_D: 0.7444 Loss_G: 0.7458 acc: 76.6%\n",
      "[BATCH 90/149] Loss_D: 0.7704 Loss_G: 0.7570 acc: 78.1%\n",
      "[BATCH 91/149] Loss_D: 0.7262 Loss_G: 0.7599 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7466 Loss_G: 0.7599 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7022 Loss_G: 0.7362 acc: 75.0%\n",
      "[BATCH 94/149] Loss_D: 0.7182 Loss_G: 0.7445 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.7179 Loss_G: 0.7632 acc: 73.4%\n",
      "[BATCH 96/149] Loss_D: 0.7140 Loss_G: 0.7710 acc: 68.8%\n",
      "[BATCH 97/149] Loss_D: 0.7239 Loss_G: 0.7544 acc: 75.0%\n",
      "[BATCH 98/149] Loss_D: 0.7400 Loss_G: 0.7532 acc: 67.2%\n",
      "[BATCH 99/149] Loss_D: 0.7144 Loss_G: 0.7554 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.7150 Loss_G: 0.7302 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.7526 Loss_G: 0.7453 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.7160 Loss_G: 0.7587 acc: 70.3%\n",
      "[BATCH 103/149] Loss_D: 0.7608 Loss_G: 0.7713 acc: 76.6%\n",
      "[BATCH 104/149] Loss_D: 0.7022 Loss_G: 0.7439 acc: 73.4%\n",
      "[BATCH 105/149] Loss_D: 0.7712 Loss_G: 0.7632 acc: 76.6%\n",
      "[BATCH 106/149] Loss_D: 0.7265 Loss_G: 0.7567 acc: 76.6%\n",
      "[BATCH 107/149] Loss_D: 0.7308 Loss_G: 0.7622 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7640 Loss_G: 0.7681 acc: 79.7%\n",
      "[BATCH 109/149] Loss_D: 0.7212 Loss_G: 0.7531 acc: 75.0%\n",
      "[BATCH 110/149] Loss_D: 0.7348 Loss_G: 0.7526 acc: 76.6%\n",
      "[BATCH 111/149] Loss_D: 0.7261 Loss_G: 0.7425 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.7504 Loss_G: 0.7562 acc: 70.3%\n",
      "[BATCH 113/149] Loss_D: 0.7297 Loss_G: 0.7594 acc: 78.1%\n",
      "[BATCH 114/149] Loss_D: 0.7538 Loss_G: 0.7619 acc: 81.2%\n",
      "[BATCH 115/149] Loss_D: 0.7392 Loss_G: 0.7712 acc: 75.0%\n",
      "[BATCH 116/149] Loss_D: 0.7228 Loss_G: 0.7627 acc: 75.0%\n",
      "[BATCH 117/149] Loss_D: 0.6930 Loss_G: 0.7453 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.7792 Loss_G: 0.7667 acc: 78.1%\n",
      "[BATCH 119/149] Loss_D: 0.7297 Loss_G: 0.7544 acc: 75.0%\n",
      "[BATCH 120/149] Loss_D: 0.7669 Loss_G: 0.7652 acc: 76.6%\n",
      "[BATCH 121/149] Loss_D: 0.7384 Loss_G: 0.7677 acc: 79.7%\n",
      "[BATCH 122/149] Loss_D: 0.7179 Loss_G: 0.7583 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.7504 Loss_G: 0.7552 acc: 78.1%\n",
      "[BATCH 124/149] Loss_D: 0.7322 Loss_G: 0.7618 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.7110 Loss_G: 0.7501 acc: 73.4%\n",
      "[BATCH 126/149] Loss_D: 0.7960 Loss_G: 0.7708 acc: 79.7%\n",
      "[BATCH 127/149] Loss_D: 0.7549 Loss_G: 0.7683 acc: 79.7%\n",
      "[BATCH 128/149] Loss_D: 0.6954 Loss_G: 0.7526 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.6980 Loss_G: 0.7622 acc: 78.1%\n",
      "[BATCH 130/149] Loss_D: 0.7241 Loss_G: 0.7555 acc: 71.9%\n",
      "[BATCH 131/149] Loss_D: 0.7435 Loss_G: 0.7489 acc: 73.4%\n",
      "[BATCH 132/149] Loss_D: 0.7377 Loss_G: 0.7505 acc: 75.0%\n",
      "[EPOCH 4900] TEST ACC is : 67.4%\n",
      "[BATCH 133/149] Loss_D: 0.7655 Loss_G: 0.7740 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7135 Loss_G: 0.7643 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.7507 Loss_G: 0.7769 acc: 75.0%\n",
      "[BATCH 136/149] Loss_D: 0.7428 Loss_G: 0.7677 acc: 78.1%\n",
      "[BATCH 137/149] Loss_D: 0.7413 Loss_G: 0.7472 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.7468 Loss_G: 0.7576 acc: 82.8%\n",
      "[BATCH 139/149] Loss_D: 0.7266 Loss_G: 0.7480 acc: 70.3%\n",
      "[BATCH 140/149] Loss_D: 0.7272 Loss_G: 0.7626 acc: 76.6%\n",
      "[BATCH 141/149] Loss_D: 0.7256 Loss_G: 0.7685 acc: 81.2%\n",
      "[BATCH 142/149] Loss_D: 0.7327 Loss_G: 0.7848 acc: 73.4%\n",
      "[BATCH 143/149] Loss_D: 0.6812 Loss_G: 0.7469 acc: 76.6%\n",
      "[BATCH 144/149] Loss_D: 0.7004 Loss_G: 0.7345 acc: 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.7136 Loss_G: 0.7488 acc: 75.0%\n",
      "[BATCH 146/149] Loss_D: 0.7262 Loss_G: 0.7360 acc: 71.9%\n",
      "[BATCH 147/149] Loss_D: 0.7293 Loss_G: 0.7422 acc: 81.2%\n",
      "[BATCH 148/149] Loss_D: 0.7002 Loss_G: 0.7360 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.7155 Loss_G: 0.7364 acc: 81.2%\n",
      "-----THE [33/50] epoch end-----\n",
      "-----THE [34/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7807 Loss_G: 0.7753 acc: 65.6%\n",
      "[BATCH 2/149] Loss_D: 0.7555 Loss_G: 0.7653 acc: 75.0%\n",
      "[BATCH 3/149] Loss_D: 0.7338 Loss_G: 0.7515 acc: 75.0%\n",
      "[BATCH 4/149] Loss_D: 0.7379 Loss_G: 0.7573 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.7400 Loss_G: 0.7700 acc: 78.1%\n",
      "[BATCH 6/149] Loss_D: 0.7530 Loss_G: 0.7768 acc: 79.7%\n",
      "[BATCH 7/149] Loss_D: 0.7674 Loss_G: 0.7869 acc: 75.0%\n",
      "[BATCH 8/149] Loss_D: 0.7364 Loss_G: 0.7607 acc: 78.1%\n",
      "[BATCH 9/149] Loss_D: 0.7849 Loss_G: 0.7675 acc: 73.4%\n",
      "[BATCH 10/149] Loss_D: 0.6968 Loss_G: 0.7385 acc: 79.7%\n",
      "[BATCH 11/149] Loss_D: 0.6990 Loss_G: 0.7333 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.7312 Loss_G: 0.7280 acc: 79.7%\n",
      "[BATCH 13/149] Loss_D: 0.7147 Loss_G: 0.7355 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.7128 Loss_G: 0.7507 acc: 79.7%\n",
      "[BATCH 15/149] Loss_D: 0.7190 Loss_G: 0.7330 acc: 79.7%\n",
      "[BATCH 16/149] Loss_D: 0.7592 Loss_G: 0.7592 acc: 78.1%\n",
      "[BATCH 17/149] Loss_D: 0.7059 Loss_G: 0.7388 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.7441 Loss_G: 0.7335 acc: 76.6%\n",
      "[BATCH 19/149] Loss_D: 0.7065 Loss_G: 0.7369 acc: 68.8%\n",
      "[BATCH 20/149] Loss_D: 0.7508 Loss_G: 0.7573 acc: 70.3%\n",
      "[BATCH 21/149] Loss_D: 0.7271 Loss_G: 0.7391 acc: 75.0%\n",
      "[BATCH 22/149] Loss_D: 0.7033 Loss_G: 0.7436 acc: 75.0%\n",
      "[BATCH 23/149] Loss_D: 0.7202 Loss_G: 0.7510 acc: 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.7540 Loss_G: 0.7649 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.7140 Loss_G: 0.7612 acc: 81.2%\n",
      "[BATCH 26/149] Loss_D: 0.7253 Loss_G: 0.7575 acc: 73.4%\n",
      "[BATCH 27/149] Loss_D: 0.7561 Loss_G: 0.7507 acc: 81.2%\n",
      "[BATCH 28/149] Loss_D: 0.7129 Loss_G: 0.7426 acc: 78.1%\n",
      "[BATCH 29/149] Loss_D: 0.7314 Loss_G: 0.7699 acc: 67.2%\n",
      "[BATCH 30/149] Loss_D: 0.7280 Loss_G: 0.7443 acc: 79.7%\n",
      "[BATCH 31/149] Loss_D: 0.7278 Loss_G: 0.7588 acc: 73.4%\n",
      "[BATCH 32/149] Loss_D: 0.7264 Loss_G: 0.7441 acc: 71.9%\n",
      "[BATCH 33/149] Loss_D: 0.7642 Loss_G: 0.7589 acc: 81.2%\n",
      "[EPOCH 4950] TEST ACC is : 70.1%\n",
      "[BATCH 34/149] Loss_D: 0.7309 Loss_G: 0.7569 acc: 75.0%\n",
      "[BATCH 35/149] Loss_D: 0.7438 Loss_G: 0.7325 acc: 81.2%\n",
      "[BATCH 36/149] Loss_D: 0.7488 Loss_G: 0.7434 acc: 76.6%\n",
      "[BATCH 37/149] Loss_D: 0.7027 Loss_G: 0.7433 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.7603 Loss_G: 0.7577 acc: 78.1%\n",
      "[BATCH 39/149] Loss_D: 0.7495 Loss_G: 0.7801 acc: 79.7%\n",
      "[BATCH 40/149] Loss_D: 0.7498 Loss_G: 0.7695 acc: 70.3%\n",
      "[BATCH 41/149] Loss_D: 0.7414 Loss_G: 0.7889 acc: 75.0%\n",
      "[BATCH 42/149] Loss_D: 0.7653 Loss_G: 0.7710 acc: 79.7%\n",
      "[BATCH 43/149] Loss_D: 0.7495 Loss_G: 0.7600 acc: 78.1%\n",
      "[BATCH 44/149] Loss_D: 0.7650 Loss_G: 0.7657 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.6959 Loss_G: 0.7489 acc: 78.1%\n",
      "[BATCH 46/149] Loss_D: 0.7404 Loss_G: 0.7513 acc: 81.2%\n",
      "[BATCH 47/149] Loss_D: 0.7014 Loss_G: 0.7424 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.7068 Loss_G: 0.7565 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7211 Loss_G: 0.7653 acc: 68.8%\n",
      "[BATCH 50/149] Loss_D: 0.8252 Loss_G: 0.8015 acc: 73.4%\n",
      "[BATCH 51/149] Loss_D: 0.7363 Loss_G: 0.7721 acc: 75.0%\n",
      "[BATCH 52/149] Loss_D: 0.7424 Loss_G: 0.7676 acc: 78.1%\n",
      "[BATCH 53/149] Loss_D: 0.7626 Loss_G: 0.7656 acc: 75.0%\n",
      "[BATCH 54/149] Loss_D: 0.7141 Loss_G: 0.7525 acc: 75.0%\n",
      "[BATCH 55/149] Loss_D: 0.7492 Loss_G: 0.7599 acc: 67.2%\n",
      "[BATCH 56/149] Loss_D: 0.7557 Loss_G: 0.7528 acc: 81.2%\n",
      "[BATCH 57/149] Loss_D: 0.7718 Loss_G: 0.7693 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7139 Loss_G: 0.7420 acc: 79.7%\n",
      "[BATCH 59/149] Loss_D: 0.7317 Loss_G: 0.7478 acc: 71.9%\n",
      "[BATCH 60/149] Loss_D: 0.7009 Loss_G: 0.7424 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7377 Loss_G: 0.7639 acc: 78.1%\n",
      "[BATCH 62/149] Loss_D: 0.7277 Loss_G: 0.7567 acc: 78.1%\n",
      "[BATCH 63/149] Loss_D: 0.7248 Loss_G: 0.7425 acc: 76.6%\n",
      "[BATCH 64/149] Loss_D: 0.7212 Loss_G: 0.7597 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.7192 Loss_G: 0.7284 acc: 78.1%\n",
      "[BATCH 66/149] Loss_D: 0.7089 Loss_G: 0.7330 acc: 75.0%\n",
      "[BATCH 67/149] Loss_D: 0.7261 Loss_G: 0.7524 acc: 71.9%\n",
      "[BATCH 68/149] Loss_D: 0.7687 Loss_G: 0.7697 acc: 70.3%\n",
      "[BATCH 69/149] Loss_D: 0.7459 Loss_G: 0.7521 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.7588 Loss_G: 0.7488 acc: 78.1%\n",
      "[BATCH 71/149] Loss_D: 0.7146 Loss_G: 0.7588 acc: 76.6%\n",
      "[BATCH 72/149] Loss_D: 0.7389 Loss_G: 0.7673 acc: 73.4%\n",
      "[BATCH 73/149] Loss_D: 0.7656 Loss_G: 0.7734 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.7239 Loss_G: 0.7568 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7014 Loss_G: 0.7743 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.7335 Loss_G: 0.7594 acc: 75.0%\n",
      "[BATCH 77/149] Loss_D: 0.7421 Loss_G: 0.7624 acc: 73.4%\n",
      "[BATCH 78/149] Loss_D: 0.7306 Loss_G: 0.7470 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.7360 Loss_G: 0.7452 acc: 71.9%\n",
      "[BATCH 80/149] Loss_D: 0.7237 Loss_G: 0.7525 acc: 75.0%\n",
      "[BATCH 81/149] Loss_D: 0.7540 Loss_G: 0.7475 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7324 Loss_G: 0.7501 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.7465 Loss_G: 0.7607 acc: 79.7%\n",
      "[EPOCH 5000] TEST ACC is : 69.1%\n",
      "[BATCH 84/149] Loss_D: 0.7404 Loss_G: 0.7510 acc: 75.0%\n",
      "[BATCH 85/149] Loss_D: 0.7302 Loss_G: 0.7430 acc: 81.2%\n",
      "[BATCH 86/149] Loss_D: 0.7029 Loss_G: 0.7365 acc: 76.6%\n",
      "[BATCH 87/149] Loss_D: 0.7644 Loss_G: 0.7646 acc: 78.1%\n",
      "[BATCH 88/149] Loss_D: 0.7184 Loss_G: 0.7561 acc: 81.2%\n",
      "[BATCH 89/149] Loss_D: 0.7519 Loss_G: 0.7596 acc: 68.8%\n",
      "[BATCH 90/149] Loss_D: 0.7507 Loss_G: 0.7620 acc: 81.2%\n",
      "[BATCH 91/149] Loss_D: 0.7399 Loss_G: 0.7747 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7386 Loss_G: 0.7626 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7728 Loss_G: 0.7671 acc: 79.7%\n",
      "[BATCH 94/149] Loss_D: 0.7647 Loss_G: 0.7752 acc: 82.8%\n",
      "[BATCH 95/149] Loss_D: 0.7442 Loss_G: 0.7721 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7093 Loss_G: 0.7591 acc: 76.6%\n",
      "[BATCH 97/149] Loss_D: 0.7365 Loss_G: 0.7487 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.7062 Loss_G: 0.7481 acc: 73.4%\n",
      "[BATCH 99/149] Loss_D: 0.7428 Loss_G: 0.7540 acc: 70.3%\n",
      "[BATCH 100/149] Loss_D: 0.7307 Loss_G: 0.7580 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7200 Loss_G: 0.7520 acc: 78.1%\n",
      "[BATCH 102/149] Loss_D: 0.6972 Loss_G: 0.7511 acc: 84.4%\n",
      "[BATCH 103/149] Loss_D: 0.7087 Loss_G: 0.7626 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7112 Loss_G: 0.7608 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.7159 Loss_G: 0.7597 acc: 79.7%\n",
      "[BATCH 106/149] Loss_D: 0.7645 Loss_G: 0.7627 acc: 75.0%\n",
      "[BATCH 107/149] Loss_D: 0.7220 Loss_G: 0.7513 acc: 82.8%\n",
      "[BATCH 108/149] Loss_D: 0.7382 Loss_G: 0.7499 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7065 Loss_G: 0.7518 acc: 78.1%\n",
      "[BATCH 110/149] Loss_D: 0.6809 Loss_G: 0.7447 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7570 Loss_G: 0.7650 acc: 73.4%\n",
      "[BATCH 112/149] Loss_D: 0.7246 Loss_G: 0.7330 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.7154 Loss_G: 0.7438 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.7167 Loss_G: 0.7469 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.7288 Loss_G: 0.7499 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7515 Loss_G: 0.7548 acc: 79.7%\n",
      "[BATCH 117/149] Loss_D: 0.7402 Loss_G: 0.7558 acc: 68.8%\n",
      "[BATCH 118/149] Loss_D: 0.7497 Loss_G: 0.7549 acc: 75.0%\n",
      "[BATCH 119/149] Loss_D: 0.7478 Loss_G: 0.7636 acc: 76.6%\n",
      "[BATCH 120/149] Loss_D: 0.7218 Loss_G: 0.7462 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6864 Loss_G: 0.7351 acc: 76.6%\n",
      "[BATCH 122/149] Loss_D: 0.7165 Loss_G: 0.7479 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7304 Loss_G: 0.7399 acc: 75.0%\n",
      "[BATCH 124/149] Loss_D: 0.7157 Loss_G: 0.7209 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.7501 Loss_G: 0.7582 acc: 78.1%\n",
      "[BATCH 126/149] Loss_D: 0.7242 Loss_G: 0.7463 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.6794 Loss_G: 0.7296 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.7280 Loss_G: 0.7494 acc: 75.0%\n",
      "[BATCH 129/149] Loss_D: 0.7469 Loss_G: 0.7583 acc: 76.6%\n",
      "[BATCH 130/149] Loss_D: 0.7349 Loss_G: 0.7489 acc: 78.1%\n",
      "[BATCH 131/149] Loss_D: 0.7009 Loss_G: 0.7456 acc: 75.0%\n",
      "[BATCH 132/149] Loss_D: 0.8247 Loss_G: 0.7698 acc: 78.1%\n",
      "[BATCH 133/149] Loss_D: 0.7507 Loss_G: 0.7644 acc: 73.4%\n",
      "[EPOCH 5050] TEST ACC is : 68.6%\n",
      "[BATCH 134/149] Loss_D: 0.7169 Loss_G: 0.7522 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7355 Loss_G: 0.7380 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7395 Loss_G: 0.7597 acc: 81.2%\n",
      "[BATCH 137/149] Loss_D: 0.6866 Loss_G: 0.7373 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.7758 Loss_G: 0.7672 acc: 79.7%\n",
      "[BATCH 139/149] Loss_D: 0.7445 Loss_G: 0.7629 acc: 68.8%\n",
      "[BATCH 140/149] Loss_D: 0.7305 Loss_G: 0.7472 acc: 70.3%\n",
      "[BATCH 141/149] Loss_D: 0.7540 Loss_G: 0.7540 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7723 Loss_G: 0.7557 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.7263 Loss_G: 0.7625 acc: 75.0%\n",
      "[BATCH 144/149] Loss_D: 0.6853 Loss_G: 0.7372 acc: 78.1%\n",
      "[BATCH 145/149] Loss_D: 0.7035 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7308 Loss_G: 0.7433 acc: 78.1%\n",
      "[BATCH 147/149] Loss_D: 0.7366 Loss_G: 0.7502 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.7106 Loss_G: 0.7432 acc: 75.0%\n",
      "[BATCH 149/149] Loss_D: 0.6939 Loss_G: 0.7283 acc: 75.0%\n",
      "-----THE [34/50] epoch end-----\n",
      "-----THE [35/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7265 Loss_G: 0.7506 acc: 73.4%\n",
      "[BATCH 2/149] Loss_D: 0.7315 Loss_G: 0.7531 acc: 76.6%\n",
      "[BATCH 3/149] Loss_D: 0.7234 Loss_G: 0.7631 acc: 79.7%\n",
      "[BATCH 4/149] Loss_D: 0.7203 Loss_G: 0.7732 acc: 78.1%\n",
      "[BATCH 5/149] Loss_D: 0.7739 Loss_G: 0.7751 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7280 Loss_G: 0.7645 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7340 Loss_G: 0.7544 acc: 76.6%\n",
      "[BATCH 8/149] Loss_D: 0.7159 Loss_G: 0.7556 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7315 Loss_G: 0.7565 acc: 79.7%\n",
      "[BATCH 10/149] Loss_D: 0.6961 Loss_G: 0.7305 acc: 81.2%\n",
      "[BATCH 11/149] Loss_D: 0.7166 Loss_G: 0.7160 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.7637 Loss_G: 0.7416 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.7261 Loss_G: 0.7579 acc: 76.6%\n",
      "[BATCH 14/149] Loss_D: 0.7055 Loss_G: 0.7414 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.7326 Loss_G: 0.7751 acc: 64.1%\n",
      "[BATCH 16/149] Loss_D: 0.7187 Loss_G: 0.7281 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.7525 Loss_G: 0.7397 acc: 79.7%\n",
      "[BATCH 18/149] Loss_D: 0.7922 Loss_G: 0.7722 acc: 71.9%\n",
      "[BATCH 19/149] Loss_D: 0.7414 Loss_G: 0.7630 acc: 81.2%\n",
      "[BATCH 20/149] Loss_D: 0.7122 Loss_G: 0.7520 acc: 75.0%\n",
      "[BATCH 21/149] Loss_D: 0.7178 Loss_G: 0.7516 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7407 Loss_G: 0.7682 acc: 76.6%\n",
      "[BATCH 23/149] Loss_D: 0.7539 Loss_G: 0.7739 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7623 Loss_G: 0.7815 acc: 73.4%\n",
      "[BATCH 25/149] Loss_D: 0.7223 Loss_G: 0.7656 acc: 78.1%\n",
      "[BATCH 26/149] Loss_D: 0.7423 Loss_G: 0.7546 acc: 75.0%\n",
      "[BATCH 27/149] Loss_D: 0.7134 Loss_G: 0.7401 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7593 Loss_G: 0.7534 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6956 Loss_G: 0.7364 acc: 76.6%\n",
      "[BATCH 30/149] Loss_D: 0.7283 Loss_G: 0.7508 acc: 78.1%\n",
      "[BATCH 31/149] Loss_D: 0.7464 Loss_G: 0.7607 acc: 76.6%\n",
      "[BATCH 32/149] Loss_D: 0.7297 Loss_G: 0.7442 acc: 75.0%\n",
      "[BATCH 33/149] Loss_D: 0.7000 Loss_G: 0.7338 acc: 68.8%\n",
      "[BATCH 34/149] Loss_D: 0.6904 Loss_G: 0.7259 acc: 82.8%\n",
      "[EPOCH 5100] TEST ACC is : 71.1%\n",
      "[BATCH 35/149] Loss_D: 0.7453 Loss_G: 0.7526 acc: 68.8%\n",
      "[BATCH 36/149] Loss_D: 0.7286 Loss_G: 0.7442 acc: 75.0%\n",
      "[BATCH 37/149] Loss_D: 0.7202 Loss_G: 0.7549 acc: 78.1%\n",
      "[BATCH 38/149] Loss_D: 0.7421 Loss_G: 0.7467 acc: 76.6%\n",
      "[BATCH 39/149] Loss_D: 0.7044 Loss_G: 0.7270 acc: 76.6%\n",
      "[BATCH 40/149] Loss_D: 0.7249 Loss_G: 0.7462 acc: 79.7%\n",
      "[BATCH 41/149] Loss_D: 0.7195 Loss_G: 0.7421 acc: 75.0%\n",
      "[BATCH 42/149] Loss_D: 0.7792 Loss_G: 0.7704 acc: 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.6938 Loss_G: 0.7374 acc: 73.4%\n",
      "[BATCH 44/149] Loss_D: 0.7584 Loss_G: 0.7474 acc: 76.6%\n",
      "[BATCH 45/149] Loss_D: 0.7913 Loss_G: 0.7657 acc: 76.6%\n",
      "[BATCH 46/149] Loss_D: 0.7248 Loss_G: 0.7613 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.7269 Loss_G: 0.7549 acc: 64.1%\n",
      "[BATCH 48/149] Loss_D: 0.7523 Loss_G: 0.7532 acc: 79.7%\n",
      "[BATCH 49/149] Loss_D: 0.6756 Loss_G: 0.7366 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.7532 Loss_G: 0.7618 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7116 Loss_G: 0.7561 acc: 79.7%\n",
      "[BATCH 52/149] Loss_D: 0.7280 Loss_G: 0.7511 acc: 67.2%\n",
      "[BATCH 53/149] Loss_D: 0.7373 Loss_G: 0.7486 acc: 81.2%\n",
      "[BATCH 54/149] Loss_D: 0.7433 Loss_G: 0.7479 acc: 68.8%\n",
      "[BATCH 55/149] Loss_D: 0.7360 Loss_G: 0.7508 acc: 76.6%\n",
      "[BATCH 56/149] Loss_D: 0.7270 Loss_G: 0.7536 acc: 76.6%\n",
      "[BATCH 57/149] Loss_D: 0.7376 Loss_G: 0.7365 acc: 73.4%\n",
      "[BATCH 58/149] Loss_D: 0.7322 Loss_G: 0.7582 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7216 Loss_G: 0.7739 acc: 78.1%\n",
      "[BATCH 60/149] Loss_D: 0.7573 Loss_G: 0.7776 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.7257 Loss_G: 0.7546 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7370 Loss_G: 0.7568 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7427 Loss_G: 0.7561 acc: 70.3%\n",
      "[BATCH 64/149] Loss_D: 0.7518 Loss_G: 0.7524 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7276 Loss_G: 0.7648 acc: 79.7%\n",
      "[BATCH 66/149] Loss_D: 0.7686 Loss_G: 0.7666 acc: 75.0%\n",
      "[BATCH 67/149] Loss_D: 0.7418 Loss_G: 0.7508 acc: 79.7%\n",
      "[BATCH 68/149] Loss_D: 0.7346 Loss_G: 0.7653 acc: 76.6%\n",
      "[BATCH 69/149] Loss_D: 0.7517 Loss_G: 0.7454 acc: 78.1%\n",
      "[BATCH 70/149] Loss_D: 0.7453 Loss_G: 0.7542 acc: 75.0%\n",
      "[BATCH 71/149] Loss_D: 0.7856 Loss_G: 0.7819 acc: 65.6%\n",
      "[BATCH 72/149] Loss_D: 0.7470 Loss_G: 0.7640 acc: 76.6%\n",
      "[BATCH 73/149] Loss_D: 0.6997 Loss_G: 0.7374 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.7351 Loss_G: 0.7322 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7156 Loss_G: 0.7425 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.7410 Loss_G: 0.7526 acc: 76.6%\n",
      "[BATCH 77/149] Loss_D: 0.7584 Loss_G: 0.7458 acc: 75.0%\n",
      "[BATCH 78/149] Loss_D: 0.7098 Loss_G: 0.7476 acc: 76.6%\n",
      "[BATCH 79/149] Loss_D: 0.7300 Loss_G: 0.7426 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.7259 Loss_G: 0.7384 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6979 Loss_G: 0.7410 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.7592 Loss_G: 0.7567 acc: 70.3%\n",
      "[BATCH 83/149] Loss_D: 0.7518 Loss_G: 0.7477 acc: 78.1%\n",
      "[BATCH 84/149] Loss_D: 0.7465 Loss_G: 0.7386 acc: 84.4%\n",
      "[EPOCH 5150] TEST ACC is : 69.9%\n",
      "[BATCH 85/149] Loss_D: 0.6803 Loss_G: 0.7435 acc: 76.6%\n",
      "[BATCH 86/149] Loss_D: 0.7434 Loss_G: 0.7479 acc: 73.4%\n",
      "[BATCH 87/149] Loss_D: 0.7184 Loss_G: 0.7360 acc: 78.1%\n",
      "[BATCH 88/149] Loss_D: 0.7216 Loss_G: 0.7642 acc: 68.8%\n",
      "[BATCH 89/149] Loss_D: 0.7333 Loss_G: 0.7280 acc: 75.0%\n",
      "[BATCH 90/149] Loss_D: 0.7333 Loss_G: 0.7509 acc: 82.8%\n",
      "[BATCH 91/149] Loss_D: 0.7073 Loss_G: 0.7496 acc: 75.0%\n",
      "[BATCH 92/149] Loss_D: 0.7443 Loss_G: 0.7437 acc: 78.1%\n",
      "[BATCH 93/149] Loss_D: 0.7118 Loss_G: 0.7395 acc: 76.6%\n",
      "[BATCH 94/149] Loss_D: 0.7133 Loss_G: 0.7412 acc: 70.3%\n",
      "[BATCH 95/149] Loss_D: 0.7390 Loss_G: 0.7388 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7513 Loss_G: 0.7397 acc: 67.2%\n",
      "[BATCH 97/149] Loss_D: 0.7668 Loss_G: 0.7631 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7071 Loss_G: 0.7552 acc: 70.3%\n",
      "[BATCH 99/149] Loss_D: 0.7285 Loss_G: 0.7423 acc: 73.4%\n",
      "[BATCH 100/149] Loss_D: 0.7347 Loss_G: 0.7553 acc: 78.1%\n",
      "[BATCH 101/149] Loss_D: 0.7080 Loss_G: 0.7560 acc: 76.6%\n",
      "[BATCH 102/149] Loss_D: 0.7548 Loss_G: 0.7697 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.7509 Loss_G: 0.7867 acc: 71.9%\n",
      "[BATCH 104/149] Loss_D: 0.7836 Loss_G: 0.7543 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.7328 Loss_G: 0.7485 acc: 79.7%\n",
      "[BATCH 106/149] Loss_D: 0.6997 Loss_G: 0.7498 acc: 75.0%\n",
      "[BATCH 107/149] Loss_D: 0.7705 Loss_G: 0.7621 acc: 68.8%\n",
      "[BATCH 108/149] Loss_D: 0.7460 Loss_G: 0.7524 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7344 Loss_G: 0.7460 acc: 79.7%\n",
      "[BATCH 110/149] Loss_D: 0.7451 Loss_G: 0.7483 acc: 71.9%\n",
      "[BATCH 111/149] Loss_D: 0.7057 Loss_G: 0.7512 acc: 71.9%\n",
      "[BATCH 112/149] Loss_D: 0.7942 Loss_G: 0.7480 acc: 67.2%\n",
      "[BATCH 113/149] Loss_D: 0.7600 Loss_G: 0.7644 acc: 68.8%\n",
      "[BATCH 114/149] Loss_D: 0.7838 Loss_G: 0.7705 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7138 Loss_G: 0.7564 acc: 78.1%\n",
      "[BATCH 116/149] Loss_D: 0.8015 Loss_G: 0.7805 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7541 Loss_G: 0.7550 acc: 78.1%\n",
      "[BATCH 118/149] Loss_D: 0.7320 Loss_G: 0.7669 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.7191 Loss_G: 0.7571 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7244 Loss_G: 0.7558 acc: 78.1%\n",
      "[BATCH 121/149] Loss_D: 0.7528 Loss_G: 0.7487 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.6993 Loss_G: 0.7431 acc: 78.1%\n",
      "[BATCH 123/149] Loss_D: 0.7152 Loss_G: 0.7420 acc: 75.0%\n",
      "[BATCH 124/149] Loss_D: 0.7194 Loss_G: 0.7319 acc: 79.7%\n",
      "[BATCH 125/149] Loss_D: 0.7497 Loss_G: 0.7423 acc: 75.0%\n",
      "[BATCH 126/149] Loss_D: 0.7168 Loss_G: 0.7444 acc: 71.9%\n",
      "[BATCH 127/149] Loss_D: 0.7164 Loss_G: 0.7410 acc: 79.7%\n",
      "[BATCH 128/149] Loss_D: 0.7215 Loss_G: 0.7362 acc: 75.0%\n",
      "[BATCH 129/149] Loss_D: 0.7683 Loss_G: 0.7472 acc: 76.6%\n",
      "[BATCH 130/149] Loss_D: 0.7020 Loss_G: 0.7452 acc: 71.9%\n",
      "[BATCH 131/149] Loss_D: 0.7056 Loss_G: 0.7395 acc: 75.0%\n",
      "[BATCH 132/149] Loss_D: 0.6860 Loss_G: 0.7427 acc: 76.6%\n",
      "[BATCH 133/149] Loss_D: 0.6874 Loss_G: 0.7216 acc: 76.6%\n",
      "[BATCH 134/149] Loss_D: 0.7312 Loss_G: 0.7396 acc: 64.1%\n",
      "[EPOCH 5200] TEST ACC is : 71.1%\n",
      "[BATCH 135/149] Loss_D: 0.7076 Loss_G: 0.7215 acc: 82.8%\n",
      "[BATCH 136/149] Loss_D: 0.7779 Loss_G: 0.7618 acc: 70.3%\n",
      "[BATCH 137/149] Loss_D: 0.7082 Loss_G: 0.7347 acc: 76.6%\n",
      "[BATCH 138/149] Loss_D: 0.7362 Loss_G: 0.7509 acc: 76.6%\n",
      "[BATCH 139/149] Loss_D: 0.7219 Loss_G: 0.7468 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.7571 Loss_G: 0.7645 acc: 79.7%\n",
      "[BATCH 141/149] Loss_D: 0.7590 Loss_G: 0.7600 acc: 75.0%\n",
      "[BATCH 142/149] Loss_D: 0.7047 Loss_G: 0.7339 acc: 78.1%\n",
      "[BATCH 143/149] Loss_D: 0.7412 Loss_G: 0.7515 acc: 76.6%\n",
      "[BATCH 144/149] Loss_D: 0.7405 Loss_G: 0.7474 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7526 Loss_G: 0.7583 acc: 75.0%\n",
      "[BATCH 146/149] Loss_D: 0.7596 Loss_G: 0.7673 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.7565 Loss_G: 0.7660 acc: 82.8%\n",
      "[BATCH 148/149] Loss_D: 0.7039 Loss_G: 0.7511 acc: 75.0%\n",
      "[BATCH 149/149] Loss_D: 0.7719 Loss_G: 0.7560 acc: 71.9%\n",
      "-----THE [35/50] epoch end-----\n",
      "-----THE [36/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7496 Loss_G: 0.7357 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7216 Loss_G: 0.7581 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.7204 Loss_G: 0.7288 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.7105 Loss_G: 0.7508 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7201 Loss_G: 0.7497 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.7038 Loss_G: 0.7359 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7407 Loss_G: 0.7519 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.6854 Loss_G: 0.7275 acc: 78.1%\n",
      "[BATCH 9/149] Loss_D: 0.7687 Loss_G: 0.7595 acc: 81.2%\n",
      "[BATCH 10/149] Loss_D: 0.7039 Loss_G: 0.7514 acc: 76.6%\n",
      "[BATCH 11/149] Loss_D: 0.7403 Loss_G: 0.7514 acc: 68.8%\n",
      "[BATCH 12/149] Loss_D: 0.7364 Loss_G: 0.7476 acc: 75.0%\n",
      "[BATCH 13/149] Loss_D: 0.7127 Loss_G: 0.7594 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.7044 Loss_G: 0.7623 acc: 76.6%\n",
      "[BATCH 15/149] Loss_D: 0.7223 Loss_G: 0.7506 acc: 73.4%\n",
      "[BATCH 16/149] Loss_D: 0.6918 Loss_G: 0.7359 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7864 Loss_G: 0.7753 acc: 75.0%\n",
      "[BATCH 18/149] Loss_D: 0.7496 Loss_G: 0.7666 acc: 81.2%\n",
      "[BATCH 19/149] Loss_D: 0.7145 Loss_G: 0.7459 acc: 73.4%\n",
      "[BATCH 20/149] Loss_D: 0.6962 Loss_G: 0.7332 acc: 71.9%\n",
      "[BATCH 21/149] Loss_D: 0.7111 Loss_G: 0.7537 acc: 79.7%\n",
      "[BATCH 22/149] Loss_D: 0.7484 Loss_G: 0.7595 acc: 76.6%\n",
      "[BATCH 23/149] Loss_D: 0.7421 Loss_G: 0.7514 acc: 67.2%\n",
      "[BATCH 24/149] Loss_D: 0.7528 Loss_G: 0.7483 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.7097 Loss_G: 0.7350 acc: 68.8%\n",
      "[BATCH 26/149] Loss_D: 0.7285 Loss_G: 0.7436 acc: 78.1%\n",
      "[BATCH 27/149] Loss_D: 0.7315 Loss_G: 0.7390 acc: 84.4%\n",
      "[BATCH 28/149] Loss_D: 0.7144 Loss_G: 0.7417 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.7359 Loss_G: 0.7441 acc: 78.1%\n",
      "[BATCH 30/149] Loss_D: 0.7451 Loss_G: 0.7431 acc: 79.7%\n",
      "[BATCH 31/149] Loss_D: 0.7175 Loss_G: 0.7341 acc: 79.7%\n",
      "[BATCH 32/149] Loss_D: 0.7577 Loss_G: 0.7417 acc: 73.4%\n",
      "[BATCH 33/149] Loss_D: 0.6562 Loss_G: 0.7236 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7483 Loss_G: 0.7227 acc: 76.6%\n",
      "[BATCH 35/149] Loss_D: 0.6941 Loss_G: 0.7397 acc: 76.6%\n",
      "[EPOCH 5250] TEST ACC is : 70.7%\n",
      "[BATCH 36/149] Loss_D: 0.7654 Loss_G: 0.7589 acc: 75.0%\n",
      "[BATCH 37/149] Loss_D: 0.7262 Loss_G: 0.7548 acc: 76.6%\n",
      "[BATCH 38/149] Loss_D: 0.7779 Loss_G: 0.7584 acc: 71.9%\n",
      "[BATCH 39/149] Loss_D: 0.7288 Loss_G: 0.7584 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.7355 Loss_G: 0.7553 acc: 68.8%\n",
      "[BATCH 41/149] Loss_D: 0.7432 Loss_G: 0.7508 acc: 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7545 Loss_G: 0.7428 acc: 79.7%\n",
      "[BATCH 43/149] Loss_D: 0.6890 Loss_G: 0.7284 acc: 79.7%\n",
      "[BATCH 44/149] Loss_D: 0.7564 Loss_G: 0.7410 acc: 68.8%\n",
      "[BATCH 45/149] Loss_D: 0.7501 Loss_G: 0.7532 acc: 65.6%\n",
      "[BATCH 46/149] Loss_D: 0.7691 Loss_G: 0.7518 acc: 78.1%\n",
      "[BATCH 47/149] Loss_D: 0.7378 Loss_G: 0.7640 acc: 71.9%\n",
      "[BATCH 48/149] Loss_D: 0.7034 Loss_G: 0.7355 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.7546 Loss_G: 0.7569 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.6952 Loss_G: 0.7308 acc: 79.7%\n",
      "[BATCH 51/149] Loss_D: 0.7426 Loss_G: 0.7354 acc: 81.2%\n",
      "[BATCH 52/149] Loss_D: 0.7223 Loss_G: 0.7398 acc: 71.9%\n",
      "[BATCH 53/149] Loss_D: 0.7166 Loss_G: 0.7390 acc: 76.6%\n",
      "[BATCH 54/149] Loss_D: 0.7293 Loss_G: 0.7376 acc: 73.4%\n",
      "[BATCH 55/149] Loss_D: 0.7552 Loss_G: 0.7445 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.7722 Loss_G: 0.7669 acc: 71.9%\n",
      "[BATCH 57/149] Loss_D: 0.7688 Loss_G: 0.7478 acc: 68.8%\n",
      "[BATCH 58/149] Loss_D: 0.7377 Loss_G: 0.7539 acc: 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.7163 Loss_G: 0.7576 acc: 73.4%\n",
      "[BATCH 60/149] Loss_D: 0.7402 Loss_G: 0.7520 acc: 76.6%\n",
      "[BATCH 61/149] Loss_D: 0.7267 Loss_G: 0.7493 acc: 71.9%\n",
      "[BATCH 62/149] Loss_D: 0.7427 Loss_G: 0.7451 acc: 76.6%\n",
      "[BATCH 63/149] Loss_D: 0.7262 Loss_G: 0.7578 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.7479 Loss_G: 0.7795 acc: 71.9%\n",
      "[BATCH 65/149] Loss_D: 0.8039 Loss_G: 0.8046 acc: 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.7161 Loss_G: 0.7621 acc: 76.6%\n",
      "[BATCH 67/149] Loss_D: 0.7149 Loss_G: 0.7296 acc: 76.6%\n",
      "[BATCH 68/149] Loss_D: 0.7813 Loss_G: 0.7594 acc: 75.0%\n",
      "[BATCH 69/149] Loss_D: 0.7403 Loss_G: 0.7531 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.7374 Loss_G: 0.7550 acc: 81.2%\n",
      "[BATCH 71/149] Loss_D: 0.7037 Loss_G: 0.7533 acc: 82.8%\n",
      "[BATCH 72/149] Loss_D: 0.7279 Loss_G: 0.7678 acc: 76.6%\n",
      "[BATCH 73/149] Loss_D: 0.7338 Loss_G: 0.7588 acc: 78.1%\n",
      "[BATCH 74/149] Loss_D: 0.7705 Loss_G: 0.7781 acc: 75.0%\n",
      "[BATCH 75/149] Loss_D: 0.7085 Loss_G: 0.7701 acc: 82.8%\n",
      "[BATCH 76/149] Loss_D: 0.7851 Loss_G: 0.7762 acc: 79.7%\n",
      "[BATCH 77/149] Loss_D: 0.7351 Loss_G: 0.7818 acc: 75.0%\n",
      "[BATCH 78/149] Loss_D: 0.7498 Loss_G: 0.7752 acc: 78.1%\n",
      "[BATCH 79/149] Loss_D: 0.7075 Loss_G: 0.7456 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.7514 Loss_G: 0.7476 acc: 78.1%\n",
      "[BATCH 81/149] Loss_D: 0.7436 Loss_G: 0.7427 acc: 75.0%\n",
      "[BATCH 82/149] Loss_D: 0.7675 Loss_G: 0.7557 acc: 75.0%\n",
      "[BATCH 83/149] Loss_D: 0.7205 Loss_G: 0.7646 acc: 70.3%\n",
      "[BATCH 84/149] Loss_D: 0.7111 Loss_G: 0.7342 acc: 73.4%\n",
      "[BATCH 85/149] Loss_D: 0.7017 Loss_G: 0.7542 acc: 79.7%\n",
      "[EPOCH 5300] TEST ACC is : 71.1%\n",
      "[BATCH 86/149] Loss_D: 0.7438 Loss_G: 0.7437 acc: 78.1%\n",
      "[BATCH 87/149] Loss_D: 0.7064 Loss_G: 0.7338 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.7126 Loss_G: 0.7412 acc: 76.6%\n",
      "[BATCH 89/149] Loss_D: 0.6975 Loss_G: 0.7277 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7705 Loss_G: 0.7672 acc: 70.3%\n",
      "[BATCH 91/149] Loss_D: 0.7094 Loss_G: 0.7516 acc: 70.3%\n",
      "[BATCH 92/149] Loss_D: 0.7181 Loss_G: 0.7448 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7460 Loss_G: 0.7485 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.7123 Loss_G: 0.7627 acc: 73.4%\n",
      "[BATCH 95/149] Loss_D: 0.7747 Loss_G: 0.7744 acc: 79.7%\n",
      "[BATCH 96/149] Loss_D: 0.7298 Loss_G: 0.7798 acc: 79.7%\n",
      "[BATCH 97/149] Loss_D: 0.7417 Loss_G: 0.7680 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.7714 Loss_G: 0.7877 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7228 Loss_G: 0.7866 acc: 78.1%\n",
      "[BATCH 100/149] Loss_D: 0.7348 Loss_G: 0.7634 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.7286 Loss_G: 0.7647 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.7577 Loss_G: 0.7567 acc: 78.1%\n",
      "[BATCH 103/149] Loss_D: 0.7201 Loss_G: 0.7510 acc: 71.9%\n",
      "[BATCH 104/149] Loss_D: 0.7051 Loss_G: 0.7557 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.7495 Loss_G: 0.7620 acc: 79.7%\n",
      "[BATCH 106/149] Loss_D: 0.7154 Loss_G: 0.7429 acc: 71.9%\n",
      "[BATCH 107/149] Loss_D: 0.7268 Loss_G: 0.7572 acc: 79.7%\n",
      "[BATCH 108/149] Loss_D: 0.7563 Loss_G: 0.7589 acc: 70.3%\n",
      "[BATCH 109/149] Loss_D: 0.7289 Loss_G: 0.7378 acc: 76.6%\n",
      "[BATCH 110/149] Loss_D: 0.7049 Loss_G: 0.7241 acc: 79.7%\n",
      "[BATCH 111/149] Loss_D: 0.7207 Loss_G: 0.7312 acc: 70.3%\n",
      "[BATCH 112/149] Loss_D: 0.7017 Loss_G: 0.7234 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.7569 Loss_G: 0.7460 acc: 76.6%\n",
      "[BATCH 114/149] Loss_D: 0.7166 Loss_G: 0.7548 acc: 67.2%\n",
      "[BATCH 115/149] Loss_D: 0.7379 Loss_G: 0.7509 acc: 73.4%\n",
      "[BATCH 116/149] Loss_D: 0.7345 Loss_G: 0.7446 acc: 79.7%\n",
      "[BATCH 117/149] Loss_D: 0.7333 Loss_G: 0.7544 acc: 75.0%\n",
      "[BATCH 118/149] Loss_D: 0.7275 Loss_G: 0.7583 acc: 71.9%\n",
      "[BATCH 119/149] Loss_D: 0.7597 Loss_G: 0.7595 acc: 78.1%\n",
      "[BATCH 120/149] Loss_D: 0.7722 Loss_G: 0.7935 acc: 71.9%\n",
      "[BATCH 121/149] Loss_D: 0.7033 Loss_G: 0.7506 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7145 Loss_G: 0.7471 acc: 68.8%\n",
      "[BATCH 123/149] Loss_D: 0.7310 Loss_G: 0.7286 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.7806 Loss_G: 0.7711 acc: 79.7%\n",
      "[BATCH 125/149] Loss_D: 0.7610 Loss_G: 0.7738 acc: 76.6%\n",
      "[BATCH 126/149] Loss_D: 0.7267 Loss_G: 0.7557 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.7429 Loss_G: 0.7567 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.7562 Loss_G: 0.7586 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.7376 Loss_G: 0.7485 acc: 71.9%\n",
      "[BATCH 130/149] Loss_D: 0.6957 Loss_G: 0.7313 acc: 79.7%\n",
      "[BATCH 131/149] Loss_D: 0.7845 Loss_G: 0.7453 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.7596 Loss_G: 0.7641 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7252 Loss_G: 0.7517 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7114 Loss_G: 0.7430 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.7641 Loss_G: 0.7718 acc: 73.4%\n",
      "[EPOCH 5350] TEST ACC is : 69.5%\n",
      "[BATCH 136/149] Loss_D: 0.7454 Loss_G: 0.7726 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7333 Loss_G: 0.7533 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6810 Loss_G: 0.7504 acc: 81.2%\n",
      "[BATCH 139/149] Loss_D: 0.7478 Loss_G: 0.7716 acc: 79.7%\n",
      "[BATCH 140/149] Loss_D: 0.6997 Loss_G: 0.7465 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.7512 Loss_G: 0.7571 acc: 78.1%\n",
      "[BATCH 142/149] Loss_D: 0.7272 Loss_G: 0.7498 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7529 Loss_G: 0.7647 acc: 73.4%\n",
      "[BATCH 144/149] Loss_D: 0.7596 Loss_G: 0.7705 acc: 73.4%\n",
      "[BATCH 145/149] Loss_D: 0.7466 Loss_G: 0.7690 acc: 75.0%\n",
      "[BATCH 146/149] Loss_D: 0.7569 Loss_G: 0.7649 acc: 75.0%\n",
      "[BATCH 147/149] Loss_D: 0.7202 Loss_G: 0.7324 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6934 Loss_G: 0.7454 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.7421 Loss_G: 0.7632 acc: 71.9%\n",
      "-----THE [36/50] epoch end-----\n",
      "-----THE [37/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6934 Loss_G: 0.7548 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.7662 Loss_G: 0.7598 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.7494 Loss_G: 0.7573 acc: 78.1%\n",
      "[BATCH 4/149] Loss_D: 0.7556 Loss_G: 0.7579 acc: 81.2%\n",
      "[BATCH 5/149] Loss_D: 0.7406 Loss_G: 0.7610 acc: 79.7%\n",
      "[BATCH 6/149] Loss_D: 0.7438 Loss_G: 0.7309 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7110 Loss_G: 0.7324 acc: 79.7%\n",
      "[BATCH 8/149] Loss_D: 0.7068 Loss_G: 0.7330 acc: 76.6%\n",
      "[BATCH 9/149] Loss_D: 0.7234 Loss_G: 0.7519 acc: 67.2%\n",
      "[BATCH 10/149] Loss_D: 0.7389 Loss_G: 0.7581 acc: 79.7%\n",
      "[BATCH 11/149] Loss_D: 0.7273 Loss_G: 0.7437 acc: 76.6%\n",
      "[BATCH 12/149] Loss_D: 0.6881 Loss_G: 0.7298 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.7351 Loss_G: 0.7456 acc: 78.1%\n",
      "[BATCH 14/149] Loss_D: 0.7208 Loss_G: 0.7567 acc: 67.2%\n",
      "[BATCH 15/149] Loss_D: 0.7095 Loss_G: 0.7508 acc: 81.2%\n",
      "[BATCH 16/149] Loss_D: 0.7129 Loss_G: 0.7419 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.7088 Loss_G: 0.7436 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.7342 Loss_G: 0.7462 acc: 73.4%\n",
      "[BATCH 19/149] Loss_D: 0.7179 Loss_G: 0.7449 acc: 79.7%\n",
      "[BATCH 20/149] Loss_D: 0.7331 Loss_G: 0.7698 acc: 71.9%\n",
      "[BATCH 21/149] Loss_D: 0.7971 Loss_G: 0.7693 acc: 76.6%\n",
      "[BATCH 22/149] Loss_D: 0.7471 Loss_G: 0.7521 acc: 78.1%\n",
      "[BATCH 23/149] Loss_D: 0.7532 Loss_G: 0.7500 acc: 73.4%\n",
      "[BATCH 24/149] Loss_D: 0.7173 Loss_G: 0.7436 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7687 Loss_G: 0.7535 acc: 76.6%\n",
      "[BATCH 26/149] Loss_D: 0.7460 Loss_G: 0.7523 acc: 73.4%\n",
      "[BATCH 27/149] Loss_D: 0.7368 Loss_G: 0.7469 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.7559 Loss_G: 0.7439 acc: 73.4%\n",
      "[BATCH 29/149] Loss_D: 0.7459 Loss_G: 0.7543 acc: 78.1%\n",
      "[BATCH 30/149] Loss_D: 0.7874 Loss_G: 0.7813 acc: 71.9%\n",
      "[BATCH 31/149] Loss_D: 0.7071 Loss_G: 0.7650 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7413 Loss_G: 0.7749 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.7541 Loss_G: 0.7852 acc: 67.2%\n",
      "[BATCH 34/149] Loss_D: 0.7019 Loss_G: 0.7638 acc: 81.2%\n",
      "[BATCH 35/149] Loss_D: 0.7323 Loss_G: 0.7505 acc: 71.9%\n",
      "[BATCH 36/149] Loss_D: 0.7650 Loss_G: 0.7509 acc: 73.4%\n",
      "[EPOCH 5400] TEST ACC is : 71.5%\n",
      "[BATCH 37/149] Loss_D: 0.7722 Loss_G: 0.7592 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.7161 Loss_G: 0.7697 acc: 75.0%\n",
      "[BATCH 39/149] Loss_D: 0.7203 Loss_G: 0.7496 acc: 78.1%\n",
      "[BATCH 40/149] Loss_D: 0.7091 Loss_G: 0.7464 acc: 79.7%\n",
      "[BATCH 41/149] Loss_D: 0.7213 Loss_G: 0.7419 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.7352 Loss_G: 0.7460 acc: 81.2%\n",
      "[BATCH 43/149] Loss_D: 0.7390 Loss_G: 0.7589 acc: 70.3%\n",
      "[BATCH 44/149] Loss_D: 0.7542 Loss_G: 0.7369 acc: 78.1%\n",
      "[BATCH 45/149] Loss_D: 0.7837 Loss_G: 0.7456 acc: 79.7%\n",
      "[BATCH 46/149] Loss_D: 0.6810 Loss_G: 0.7358 acc: 78.1%\n",
      "[BATCH 47/149] Loss_D: 0.7302 Loss_G: 0.7492 acc: 78.1%\n",
      "[BATCH 48/149] Loss_D: 0.7526 Loss_G: 0.7541 acc: 79.7%\n",
      "[BATCH 49/149] Loss_D: 0.7106 Loss_G: 0.7535 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7150 Loss_G: 0.7457 acc: 76.6%\n",
      "[BATCH 51/149] Loss_D: 0.7211 Loss_G: 0.7325 acc: 71.9%\n",
      "[BATCH 52/149] Loss_D: 0.6833 Loss_G: 0.7328 acc: 81.2%\n",
      "[BATCH 53/149] Loss_D: 0.7418 Loss_G: 0.7536 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.7136 Loss_G: 0.7655 acc: 71.9%\n",
      "[BATCH 55/149] Loss_D: 0.7308 Loss_G: 0.7599 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.7278 Loss_G: 0.7592 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7270 Loss_G: 0.7443 acc: 75.0%\n",
      "[BATCH 58/149] Loss_D: 0.7336 Loss_G: 0.7438 acc: 79.7%\n",
      "[BATCH 59/149] Loss_D: 0.7610 Loss_G: 0.7629 acc: 82.8%\n",
      "[BATCH 60/149] Loss_D: 0.7233 Loss_G: 0.7398 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7391 Loss_G: 0.7547 acc: 65.6%\n",
      "[BATCH 62/149] Loss_D: 0.7732 Loss_G: 0.7644 acc: 79.7%\n",
      "[BATCH 63/149] Loss_D: 0.7438 Loss_G: 0.7599 acc: 79.7%\n",
      "[BATCH 64/149] Loss_D: 0.6996 Loss_G: 0.7457 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7439 Loss_G: 0.7442 acc: 81.2%\n",
      "[BATCH 66/149] Loss_D: 0.7414 Loss_G: 0.7504 acc: 79.7%\n",
      "[BATCH 67/149] Loss_D: 0.7521 Loss_G: 0.7709 acc: 79.7%\n",
      "[BATCH 68/149] Loss_D: 0.7705 Loss_G: 0.7691 acc: 82.8%\n",
      "[BATCH 69/149] Loss_D: 0.7495 Loss_G: 0.7580 acc: 76.6%\n",
      "[BATCH 70/149] Loss_D: 0.7619 Loss_G: 0.7642 acc: 71.9%\n",
      "[BATCH 71/149] Loss_D: 0.7299 Loss_G: 0.7392 acc: 82.8%\n",
      "[BATCH 72/149] Loss_D: 0.7256 Loss_G: 0.7509 acc: 81.2%\n",
      "[BATCH 73/149] Loss_D: 0.6848 Loss_G: 0.7242 acc: 78.1%\n",
      "[BATCH 74/149] Loss_D: 0.7666 Loss_G: 0.7513 acc: 73.4%\n",
      "[BATCH 75/149] Loss_D: 0.7124 Loss_G: 0.7493 acc: 81.2%\n",
      "[BATCH 76/149] Loss_D: 0.7492 Loss_G: 0.7458 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.7130 Loss_G: 0.7481 acc: 73.4%\n",
      "[BATCH 78/149] Loss_D: 0.7741 Loss_G: 0.7509 acc: 73.4%\n",
      "[BATCH 79/149] Loss_D: 0.7365 Loss_G: 0.7474 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.7536 Loss_G: 0.7453 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7592 Loss_G: 0.7593 acc: 73.4%\n",
      "[BATCH 82/149] Loss_D: 0.7326 Loss_G: 0.7360 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.7115 Loss_G: 0.7477 acc: 76.6%\n",
      "[BATCH 84/149] Loss_D: 0.7077 Loss_G: 0.7526 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.7505 Loss_G: 0.7660 acc: 79.7%\n",
      "[BATCH 86/149] Loss_D: 0.7683 Loss_G: 0.7772 acc: 85.9%\n",
      "[EPOCH 5450] TEST ACC is : 70.1%\n",
      "[BATCH 87/149] Loss_D: 0.7271 Loss_G: 0.7653 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.7143 Loss_G: 0.7429 acc: 79.7%\n",
      "[BATCH 89/149] Loss_D: 0.7236 Loss_G: 0.7464 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7145 Loss_G: 0.7504 acc: 79.7%\n",
      "[BATCH 91/149] Loss_D: 0.7271 Loss_G: 0.7508 acc: 79.7%\n",
      "[BATCH 92/149] Loss_D: 0.7247 Loss_G: 0.7476 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7179 Loss_G: 0.7337 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.7584 Loss_G: 0.7609 acc: 76.6%\n",
      "[BATCH 95/149] Loss_D: 0.7344 Loss_G: 0.7450 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.7616 Loss_G: 0.7650 acc: 78.1%\n",
      "[BATCH 97/149] Loss_D: 0.7597 Loss_G: 0.7739 acc: 76.6%\n",
      "[BATCH 98/149] Loss_D: 0.7594 Loss_G: 0.7567 acc: 81.2%\n",
      "[BATCH 99/149] Loss_D: 0.6970 Loss_G: 0.7442 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.7327 Loss_G: 0.7489 acc: 78.1%\n",
      "[BATCH 101/149] Loss_D: 0.6984 Loss_G: 0.7313 acc: 81.2%\n",
      "[BATCH 102/149] Loss_D: 0.7155 Loss_G: 0.7561 acc: 78.1%\n",
      "[BATCH 103/149] Loss_D: 0.7230 Loss_G: 0.7544 acc: 68.8%\n",
      "[BATCH 104/149] Loss_D: 0.7260 Loss_G: 0.7641 acc: 70.3%\n",
      "[BATCH 105/149] Loss_D: 0.7333 Loss_G: 0.7560 acc: 79.7%\n",
      "[BATCH 106/149] Loss_D: 0.7396 Loss_G: 0.7583 acc: 78.1%\n",
      "[BATCH 107/149] Loss_D: 0.7500 Loss_G: 0.7522 acc: 81.2%\n",
      "[BATCH 108/149] Loss_D: 0.7305 Loss_G: 0.7537 acc: 79.7%\n",
      "[BATCH 109/149] Loss_D: 0.7065 Loss_G: 0.7308 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.7474 Loss_G: 0.7486 acc: 68.8%\n",
      "[BATCH 111/149] Loss_D: 0.7836 Loss_G: 0.7591 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.7460 Loss_G: 0.7683 acc: 76.6%\n",
      "[BATCH 113/149] Loss_D: 0.7462 Loss_G: 0.7610 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.7449 Loss_G: 0.7456 acc: 79.7%\n",
      "[BATCH 115/149] Loss_D: 0.7534 Loss_G: 0.7653 acc: 79.7%\n",
      "[BATCH 116/149] Loss_D: 0.7826 Loss_G: 0.7651 acc: 73.4%\n",
      "[BATCH 117/149] Loss_D: 0.7272 Loss_G: 0.7465 acc: 79.7%\n",
      "[BATCH 118/149] Loss_D: 0.7637 Loss_G: 0.7348 acc: 76.6%\n",
      "[BATCH 119/149] Loss_D: 0.7526 Loss_G: 0.7582 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.7179 Loss_G: 0.7560 acc: 81.2%\n",
      "[BATCH 121/149] Loss_D: 0.7552 Loss_G: 0.7586 acc: 81.2%\n",
      "[BATCH 122/149] Loss_D: 0.7136 Loss_G: 0.7481 acc: 79.7%\n",
      "[BATCH 123/149] Loss_D: 0.7279 Loss_G: 0.7500 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.7383 Loss_G: 0.7459 acc: 73.4%\n",
      "[BATCH 125/149] Loss_D: 0.7439 Loss_G: 0.7699 acc: 79.7%\n",
      "[BATCH 126/149] Loss_D: 0.7217 Loss_G: 0.7527 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.7241 Loss_G: 0.7427 acc: 73.4%\n",
      "[BATCH 128/149] Loss_D: 0.7387 Loss_G: 0.7569 acc: 76.6%\n",
      "[BATCH 129/149] Loss_D: 0.7337 Loss_G: 0.7575 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7212 Loss_G: 0.7622 acc: 79.7%\n",
      "[BATCH 131/149] Loss_D: 0.7605 Loss_G: 0.7737 acc: 67.2%\n",
      "[BATCH 132/149] Loss_D: 0.7476 Loss_G: 0.7511 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.6907 Loss_G: 0.7258 acc: 71.9%\n",
      "[BATCH 134/149] Loss_D: 0.7170 Loss_G: 0.7397 acc: 78.1%\n",
      "[BATCH 135/149] Loss_D: 0.7297 Loss_G: 0.7431 acc: 79.7%\n",
      "[BATCH 136/149] Loss_D: 0.7720 Loss_G: 0.7555 acc: 76.6%\n",
      "[EPOCH 5500] TEST ACC is : 71.1%\n",
      "[BATCH 137/149] Loss_D: 0.7231 Loss_G: 0.7271 acc: 81.2%\n",
      "[BATCH 138/149] Loss_D: 0.7051 Loss_G: 0.7480 acc: 82.8%\n",
      "[BATCH 139/149] Loss_D: 0.7050 Loss_G: 0.7283 acc: 79.7%\n",
      "[BATCH 140/149] Loss_D: 0.7076 Loss_G: 0.7322 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.6940 Loss_G: 0.7332 acc: 76.6%\n",
      "[BATCH 142/149] Loss_D: 0.7195 Loss_G: 0.7418 acc: 78.1%\n",
      "[BATCH 143/149] Loss_D: 0.6999 Loss_G: 0.7448 acc: 71.9%\n",
      "[BATCH 144/149] Loss_D: 0.7084 Loss_G: 0.7363 acc: 79.7%\n",
      "[BATCH 145/149] Loss_D: 0.7489 Loss_G: 0.7495 acc: 75.0%\n",
      "[BATCH 146/149] Loss_D: 0.7220 Loss_G: 0.7556 acc: 76.6%\n",
      "[BATCH 147/149] Loss_D: 0.6851 Loss_G: 0.7342 acc: 79.7%\n",
      "[BATCH 148/149] Loss_D: 0.7509 Loss_G: 0.7517 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7300 Loss_G: 0.7610 acc: 59.4%\n",
      "-----THE [37/50] epoch end-----\n",
      "-----THE [38/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7677 Loss_G: 0.7525 acc: 67.2%\n",
      "[BATCH 2/149] Loss_D: 0.7353 Loss_G: 0.7423 acc: 79.7%\n",
      "[BATCH 3/149] Loss_D: 0.7441 Loss_G: 0.7521 acc: 71.9%\n",
      "[BATCH 4/149] Loss_D: 0.7339 Loss_G: 0.7581 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.7118 Loss_G: 0.7499 acc: 75.0%\n",
      "[BATCH 6/149] Loss_D: 0.7156 Loss_G: 0.7444 acc: 75.0%\n",
      "[BATCH 7/149] Loss_D: 0.6937 Loss_G: 0.7342 acc: 76.6%\n",
      "[BATCH 8/149] Loss_D: 0.6970 Loss_G: 0.7406 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.7141 Loss_G: 0.7460 acc: 75.0%\n",
      "[BATCH 10/149] Loss_D: 0.7417 Loss_G: 0.7517 acc: 76.6%\n",
      "[BATCH 11/149] Loss_D: 0.7680 Loss_G: 0.7644 acc: 76.6%\n",
      "[BATCH 12/149] Loss_D: 0.7403 Loss_G: 0.7576 acc: 67.2%\n",
      "[BATCH 13/149] Loss_D: 0.7439 Loss_G: 0.7474 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.7064 Loss_G: 0.7248 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7465 Loss_G: 0.7654 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7246 Loss_G: 0.7554 acc: 76.6%\n",
      "[BATCH 17/149] Loss_D: 0.7396 Loss_G: 0.7478 acc: 81.2%\n",
      "[BATCH 18/149] Loss_D: 0.7332 Loss_G: 0.7427 acc: 71.9%\n",
      "[BATCH 19/149] Loss_D: 0.7181 Loss_G: 0.7330 acc: 78.1%\n",
      "[BATCH 20/149] Loss_D: 0.7279 Loss_G: 0.7364 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.7380 Loss_G: 0.7433 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.7534 Loss_G: 0.7474 acc: 81.2%\n",
      "[BATCH 23/149] Loss_D: 0.7344 Loss_G: 0.7583 acc: 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.7267 Loss_G: 0.7474 acc: 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.7154 Loss_G: 0.7429 acc: 76.6%\n",
      "[BATCH 26/149] Loss_D: 0.7714 Loss_G: 0.7564 acc: 78.1%\n",
      "[BATCH 27/149] Loss_D: 0.7626 Loss_G: 0.7544 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7490 Loss_G: 0.7589 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.7570 Loss_G: 0.7620 acc: 79.7%\n",
      "[BATCH 30/149] Loss_D: 0.7316 Loss_G: 0.7584 acc: 76.6%\n",
      "[BATCH 31/149] Loss_D: 0.7337 Loss_G: 0.7444 acc: 81.2%\n",
      "[BATCH 32/149] Loss_D: 0.7596 Loss_G: 0.7503 acc: 65.6%\n",
      "[BATCH 33/149] Loss_D: 0.7293 Loss_G: 0.7490 acc: 79.7%\n",
      "[BATCH 34/149] Loss_D: 0.7748 Loss_G: 0.7480 acc: 76.6%\n",
      "[BATCH 35/149] Loss_D: 0.7237 Loss_G: 0.7523 acc: 64.1%\n",
      "[BATCH 36/149] Loss_D: 0.6846 Loss_G: 0.7362 acc: 96.9%\n",
      "[BATCH 37/149] Loss_D: 0.7453 Loss_G: 0.7590 acc: 78.1%\n",
      "[EPOCH 5550] TEST ACC is : 69.9%\n",
      "[BATCH 38/149] Loss_D: 0.7451 Loss_G: 0.7730 acc: 79.7%\n",
      "[BATCH 39/149] Loss_D: 0.7373 Loss_G: 0.7598 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.7362 Loss_G: 0.7555 acc: 79.7%\n",
      "[BATCH 41/149] Loss_D: 0.7293 Loss_G: 0.7452 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7362 Loss_G: 0.7515 acc: 75.0%\n",
      "[BATCH 43/149] Loss_D: 0.7065 Loss_G: 0.7445 acc: 79.7%\n",
      "[BATCH 44/149] Loss_D: 0.7249 Loss_G: 0.7565 acc: 76.6%\n",
      "[BATCH 45/149] Loss_D: 0.6890 Loss_G: 0.7345 acc: 78.1%\n",
      "[BATCH 46/149] Loss_D: 0.7070 Loss_G: 0.7357 acc: 79.7%\n",
      "[BATCH 47/149] Loss_D: 0.7233 Loss_G: 0.7438 acc: 78.1%\n",
      "[BATCH 48/149] Loss_D: 0.7371 Loss_G: 0.7520 acc: 79.7%\n",
      "[BATCH 49/149] Loss_D: 0.7100 Loss_G: 0.7441 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7207 Loss_G: 0.7397 acc: 79.7%\n",
      "[BATCH 51/149] Loss_D: 0.7569 Loss_G: 0.7675 acc: 75.0%\n",
      "[BATCH 52/149] Loss_D: 0.7642 Loss_G: 0.7551 acc: 79.7%\n",
      "[BATCH 53/149] Loss_D: 0.7348 Loss_G: 0.7472 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.7095 Loss_G: 0.7542 acc: 71.9%\n",
      "[BATCH 55/149] Loss_D: 0.7367 Loss_G: 0.7590 acc: 73.4%\n",
      "[BATCH 56/149] Loss_D: 0.7550 Loss_G: 0.7692 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.7169 Loss_G: 0.7606 acc: 78.1%\n",
      "[BATCH 58/149] Loss_D: 0.7249 Loss_G: 0.7402 acc: 71.9%\n",
      "[BATCH 59/149] Loss_D: 0.7432 Loss_G: 0.7416 acc: 79.7%\n",
      "[BATCH 60/149] Loss_D: 0.7232 Loss_G: 0.7557 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.6801 Loss_G: 0.7319 acc: 76.6%\n",
      "[BATCH 62/149] Loss_D: 0.7165 Loss_G: 0.7334 acc: 78.1%\n",
      "[BATCH 63/149] Loss_D: 0.7174 Loss_G: 0.7656 acc: 75.0%\n",
      "[BATCH 64/149] Loss_D: 0.6950 Loss_G: 0.7301 acc: 79.7%\n",
      "[BATCH 65/149] Loss_D: 0.6959 Loss_G: 0.7197 acc: 79.7%\n",
      "[BATCH 66/149] Loss_D: 0.7104 Loss_G: 0.7371 acc: 78.1%\n",
      "[BATCH 67/149] Loss_D: 0.7107 Loss_G: 0.7419 acc: 79.7%\n",
      "[BATCH 68/149] Loss_D: 0.7555 Loss_G: 0.7519 acc: 79.7%\n",
      "[BATCH 69/149] Loss_D: 0.7096 Loss_G: 0.7454 acc: 76.6%\n",
      "[BATCH 70/149] Loss_D: 0.7507 Loss_G: 0.7549 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7215 Loss_G: 0.7419 acc: 79.7%\n",
      "[BATCH 72/149] Loss_D: 0.7103 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7466 Loss_G: 0.7562 acc: 79.7%\n",
      "[BATCH 74/149] Loss_D: 0.7007 Loss_G: 0.7530 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7102 Loss_G: 0.7479 acc: 79.7%\n",
      "[BATCH 76/149] Loss_D: 0.8096 Loss_G: 0.7622 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.7492 Loss_G: 0.7593 acc: 81.2%\n",
      "[BATCH 78/149] Loss_D: 0.6950 Loss_G: 0.7472 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.7739 Loss_G: 0.7663 acc: 75.0%\n",
      "[BATCH 80/149] Loss_D: 0.7705 Loss_G: 0.7595 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.7343 Loss_G: 0.7490 acc: 76.6%\n",
      "[BATCH 82/149] Loss_D: 0.7153 Loss_G: 0.7384 acc: 76.6%\n",
      "[BATCH 83/149] Loss_D: 0.7226 Loss_G: 0.7393 acc: 76.6%\n",
      "[BATCH 84/149] Loss_D: 0.7439 Loss_G: 0.7418 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7347 Loss_G: 0.7620 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7303 Loss_G: 0.7600 acc: 79.7%\n",
      "[BATCH 87/149] Loss_D: 0.7190 Loss_G: 0.7571 acc: 92.2%\n",
      "[EPOCH 5600] TEST ACC is : 71.1%\n",
      "[BATCH 88/149] Loss_D: 0.7654 Loss_G: 0.7708 acc: 76.6%\n",
      "[BATCH 89/149] Loss_D: 0.7716 Loss_G: 0.7594 acc: 75.0%\n",
      "[BATCH 90/149] Loss_D: 0.7293 Loss_G: 0.7440 acc: 76.6%\n",
      "[BATCH 91/149] Loss_D: 0.7376 Loss_G: 0.7583 acc: 71.9%\n",
      "[BATCH 92/149] Loss_D: 0.7313 Loss_G: 0.7579 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7046 Loss_G: 0.7424 acc: 79.7%\n",
      "[BATCH 94/149] Loss_D: 0.7019 Loss_G: 0.7485 acc: 81.2%\n",
      "[BATCH 95/149] Loss_D: 0.7582 Loss_G: 0.7451 acc: 70.3%\n",
      "[BATCH 96/149] Loss_D: 0.7596 Loss_G: 0.7471 acc: 82.8%\n",
      "[BATCH 97/149] Loss_D: 0.7130 Loss_G: 0.7439 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7457 Loss_G: 0.7586 acc: 68.8%\n",
      "[BATCH 99/149] Loss_D: 0.7152 Loss_G: 0.7483 acc: 79.7%\n",
      "[BATCH 100/149] Loss_D: 0.7225 Loss_G: 0.7448 acc: 79.7%\n",
      "[BATCH 101/149] Loss_D: 0.7544 Loss_G: 0.7726 acc: 78.1%\n",
      "[BATCH 102/149] Loss_D: 0.7418 Loss_G: 0.7602 acc: 75.0%\n",
      "[BATCH 103/149] Loss_D: 0.7299 Loss_G: 0.7473 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.7520 Loss_G: 0.7413 acc: 73.4%\n",
      "[BATCH 105/149] Loss_D: 0.7329 Loss_G: 0.7479 acc: 75.0%\n",
      "[BATCH 106/149] Loss_D: 0.6979 Loss_G: 0.7406 acc: 75.0%\n",
      "[BATCH 107/149] Loss_D: 0.7838 Loss_G: 0.7586 acc: 78.1%\n",
      "[BATCH 108/149] Loss_D: 0.7370 Loss_G: 0.7483 acc: 79.7%\n",
      "[BATCH 109/149] Loss_D: 0.7268 Loss_G: 0.7555 acc: 79.7%\n",
      "[BATCH 110/149] Loss_D: 0.6977 Loss_G: 0.7325 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7213 Loss_G: 0.7488 acc: 75.0%\n",
      "[BATCH 112/149] Loss_D: 0.7378 Loss_G: 0.7423 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7438 Loss_G: 0.7487 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.7905 Loss_G: 0.7781 acc: 78.1%\n",
      "[BATCH 115/149] Loss_D: 0.6793 Loss_G: 0.7321 acc: 76.6%\n",
      "[BATCH 116/149] Loss_D: 0.7217 Loss_G: 0.7361 acc: 81.2%\n",
      "[BATCH 117/149] Loss_D: 0.7593 Loss_G: 0.7556 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.8211 Loss_G: 0.7843 acc: 76.6%\n",
      "[BATCH 119/149] Loss_D: 0.7186 Loss_G: 0.7461 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.7373 Loss_G: 0.7554 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.7273 Loss_G: 0.7573 acc: 71.9%\n",
      "[BATCH 122/149] Loss_D: 0.7369 Loss_G: 0.7471 acc: 76.6%\n",
      "[BATCH 123/149] Loss_D: 0.7242 Loss_G: 0.7333 acc: 76.6%\n",
      "[BATCH 124/149] Loss_D: 0.7087 Loss_G: 0.7449 acc: 79.7%\n",
      "[BATCH 125/149] Loss_D: 0.7472 Loss_G: 0.7455 acc: 79.7%\n",
      "[BATCH 126/149] Loss_D: 0.7299 Loss_G: 0.7414 acc: 73.4%\n",
      "[BATCH 127/149] Loss_D: 0.7482 Loss_G: 0.7551 acc: 75.0%\n",
      "[BATCH 128/149] Loss_D: 0.7158 Loss_G: 0.7397 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.7843 Loss_G: 0.7694 acc: 79.7%\n",
      "[BATCH 130/149] Loss_D: 0.7092 Loss_G: 0.7713 acc: 70.3%\n",
      "[BATCH 131/149] Loss_D: 0.7331 Loss_G: 0.7470 acc: 79.7%\n",
      "[BATCH 132/149] Loss_D: 0.7672 Loss_G: 0.7484 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.7247 Loss_G: 0.7535 acc: 79.7%\n",
      "[BATCH 134/149] Loss_D: 0.7387 Loss_G: 0.7588 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.7155 Loss_G: 0.7335 acc: 78.1%\n",
      "[BATCH 136/149] Loss_D: 0.7048 Loss_G: 0.7285 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.7285 Loss_G: 0.7474 acc: 81.2%\n",
      "[EPOCH 5650] TEST ACC is : 72.5%\n",
      "[BATCH 138/149] Loss_D: 0.6847 Loss_G: 0.7179 acc: 84.4%\n",
      "[BATCH 139/149] Loss_D: 0.7356 Loss_G: 0.7435 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.6957 Loss_G: 0.7326 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.6828 Loss_G: 0.7318 acc: 79.7%\n",
      "[BATCH 142/149] Loss_D: 0.7077 Loss_G: 0.7226 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7571 Loss_G: 0.7530 acc: 76.6%\n",
      "[BATCH 144/149] Loss_D: 0.6973 Loss_G: 0.7338 acc: 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.7585 Loss_G: 0.7567 acc: 78.1%\n",
      "[BATCH 146/149] Loss_D: 0.7259 Loss_G: 0.7465 acc: 78.1%\n",
      "[BATCH 147/149] Loss_D: 0.7505 Loss_G: 0.7569 acc: 73.4%\n",
      "[BATCH 148/149] Loss_D: 0.7446 Loss_G: 0.7637 acc: 67.2%\n",
      "[BATCH 149/149] Loss_D: 0.7269 Loss_G: 0.7501 acc: 71.9%\n",
      "-----THE [38/50] epoch end-----\n",
      "-----THE [39/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7689 Loss_G: 0.7538 acc: 71.9%\n",
      "[BATCH 2/149] Loss_D: 0.7748 Loss_G: 0.7589 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7331 Loss_G: 0.7612 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.7043 Loss_G: 0.7443 acc: 79.7%\n",
      "[BATCH 5/149] Loss_D: 0.7618 Loss_G: 0.7667 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.7121 Loss_G: 0.7341 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7213 Loss_G: 0.7490 acc: 79.7%\n",
      "[BATCH 8/149] Loss_D: 0.6967 Loss_G: 0.7220 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7418 Loss_G: 0.7393 acc: 81.2%\n",
      "[BATCH 10/149] Loss_D: 0.7244 Loss_G: 0.7496 acc: 79.7%\n",
      "[BATCH 11/149] Loss_D: 0.7306 Loss_G: 0.7435 acc: 73.4%\n",
      "[BATCH 12/149] Loss_D: 0.7072 Loss_G: 0.7347 acc: 78.1%\n",
      "[BATCH 13/149] Loss_D: 0.6986 Loss_G: 0.7264 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.7503 Loss_G: 0.7374 acc: 70.3%\n",
      "[BATCH 15/149] Loss_D: 0.7152 Loss_G: 0.7469 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7231 Loss_G: 0.7386 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7283 Loss_G: 0.7499 acc: 76.6%\n",
      "[BATCH 18/149] Loss_D: 0.7181 Loss_G: 0.7386 acc: 81.2%\n",
      "[BATCH 19/149] Loss_D: 0.7172 Loss_G: 0.7492 acc: 75.0%\n",
      "[BATCH 20/149] Loss_D: 0.7491 Loss_G: 0.7539 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.7304 Loss_G: 0.7517 acc: 73.4%\n",
      "[BATCH 22/149] Loss_D: 0.7184 Loss_G: 0.7500 acc: 73.4%\n",
      "[BATCH 23/149] Loss_D: 0.7349 Loss_G: 0.7342 acc: 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.7479 Loss_G: 0.7481 acc: 76.6%\n",
      "[BATCH 25/149] Loss_D: 0.7453 Loss_G: 0.7533 acc: 70.3%\n",
      "[BATCH 26/149] Loss_D: 0.7142 Loss_G: 0.7566 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.7084 Loss_G: 0.7348 acc: 75.0%\n",
      "[BATCH 28/149] Loss_D: 0.6960 Loss_G: 0.7363 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7446 Loss_G: 0.7269 acc: 79.7%\n",
      "[BATCH 30/149] Loss_D: 0.7232 Loss_G: 0.7499 acc: 78.1%\n",
      "[BATCH 31/149] Loss_D: 0.7029 Loss_G: 0.7340 acc: 76.6%\n",
      "[BATCH 32/149] Loss_D: 0.7326 Loss_G: 0.7428 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7195 Loss_G: 0.7527 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.7166 Loss_G: 0.7427 acc: 71.9%\n",
      "[BATCH 35/149] Loss_D: 0.7757 Loss_G: 0.7647 acc: 79.7%\n",
      "[BATCH 36/149] Loss_D: 0.7002 Loss_G: 0.7451 acc: 79.7%\n",
      "[BATCH 37/149] Loss_D: 0.7443 Loss_G: 0.7564 acc: 76.6%\n",
      "[BATCH 38/149] Loss_D: 0.7173 Loss_G: 0.7368 acc: 78.1%\n",
      "[EPOCH 5700] TEST ACC is : 71.1%\n",
      "[BATCH 39/149] Loss_D: 0.7190 Loss_G: 0.7412 acc: 79.7%\n",
      "[BATCH 40/149] Loss_D: 0.7541 Loss_G: 0.7398 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7150 Loss_G: 0.7523 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.7266 Loss_G: 0.7442 acc: 73.4%\n",
      "[BATCH 43/149] Loss_D: 0.7131 Loss_G: 0.7262 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.7299 Loss_G: 0.7613 acc: 79.7%\n",
      "[BATCH 45/149] Loss_D: 0.6890 Loss_G: 0.7357 acc: 76.6%\n",
      "[BATCH 46/149] Loss_D: 0.7592 Loss_G: 0.7453 acc: 79.7%\n",
      "[BATCH 47/149] Loss_D: 0.7312 Loss_G: 0.7553 acc: 79.7%\n",
      "[BATCH 48/149] Loss_D: 0.7230 Loss_G: 0.7314 acc: 79.7%\n",
      "[BATCH 49/149] Loss_D: 0.7595 Loss_G: 0.7508 acc: 71.9%\n",
      "[BATCH 50/149] Loss_D: 0.7227 Loss_G: 0.7606 acc: 76.6%\n",
      "[BATCH 51/149] Loss_D: 0.7174 Loss_G: 0.7510 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6984 Loss_G: 0.7457 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7400 Loss_G: 0.7548 acc: 68.8%\n",
      "[BATCH 54/149] Loss_D: 0.7611 Loss_G: 0.7625 acc: 73.4%\n",
      "[BATCH 55/149] Loss_D: 0.7063 Loss_G: 0.7605 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7226 Loss_G: 0.7710 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7369 Loss_G: 0.7667 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7750 Loss_G: 0.7740 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.7292 Loss_G: 0.7569 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7314 Loss_G: 0.7515 acc: 76.6%\n",
      "[BATCH 61/149] Loss_D: 0.7604 Loss_G: 0.7527 acc: 81.2%\n",
      "[BATCH 62/149] Loss_D: 0.7488 Loss_G: 0.7386 acc: 81.2%\n",
      "[BATCH 63/149] Loss_D: 0.6886 Loss_G: 0.7427 acc: 75.0%\n",
      "[BATCH 64/149] Loss_D: 0.7180 Loss_G: 0.7223 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.7331 Loss_G: 0.7359 acc: 76.6%\n",
      "[BATCH 66/149] Loss_D: 0.7675 Loss_G: 0.7619 acc: 73.4%\n",
      "[BATCH 67/149] Loss_D: 0.6713 Loss_G: 0.7400 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.7103 Loss_G: 0.7330 acc: 78.1%\n",
      "[BATCH 69/149] Loss_D: 0.7599 Loss_G: 0.7610 acc: 73.4%\n",
      "[BATCH 70/149] Loss_D: 0.7121 Loss_G: 0.7543 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.7905 Loss_G: 0.7581 acc: 75.0%\n",
      "[BATCH 72/149] Loss_D: 0.7541 Loss_G: 0.7639 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7447 Loss_G: 0.7735 acc: 96.9%\n",
      "[BATCH 74/149] Loss_D: 0.7173 Loss_G: 0.7567 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.7309 Loss_G: 0.7499 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7703 Loss_G: 0.7834 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.7312 Loss_G: 0.7569 acc: 73.4%\n",
      "[BATCH 78/149] Loss_D: 0.7016 Loss_G: 0.7363 acc: 79.7%\n",
      "[BATCH 79/149] Loss_D: 0.7425 Loss_G: 0.7572 acc: 82.8%\n",
      "[BATCH 80/149] Loss_D: 0.6930 Loss_G: 0.7367 acc: 81.2%\n",
      "[BATCH 81/149] Loss_D: 0.7590 Loss_G: 0.7549 acc: 73.4%\n",
      "[BATCH 82/149] Loss_D: 0.7743 Loss_G: 0.7462 acc: 81.2%\n",
      "[BATCH 83/149] Loss_D: 0.7201 Loss_G: 0.7581 acc: 78.1%\n",
      "[BATCH 84/149] Loss_D: 0.7796 Loss_G: 0.7485 acc: 78.1%\n",
      "[BATCH 85/149] Loss_D: 0.7712 Loss_G: 0.7650 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7113 Loss_G: 0.7458 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.7880 Loss_G: 0.7528 acc: 75.0%\n",
      "[BATCH 88/149] Loss_D: 0.7676 Loss_G: 0.7645 acc: 73.4%\n",
      "[EPOCH 5750] TEST ACC is : 70.3%\n",
      "[BATCH 89/149] Loss_D: 0.7370 Loss_G: 0.7739 acc: 79.7%\n",
      "[BATCH 90/149] Loss_D: 0.6818 Loss_G: 0.7407 acc: 76.6%\n",
      "[BATCH 91/149] Loss_D: 0.7060 Loss_G: 0.7234 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6812 Loss_G: 0.7339 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.7427 Loss_G: 0.7440 acc: 78.1%\n",
      "[BATCH 94/149] Loss_D: 0.7425 Loss_G: 0.7604 acc: 76.6%\n",
      "[BATCH 95/149] Loss_D: 0.7278 Loss_G: 0.7436 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7282 Loss_G: 0.7353 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.7333 Loss_G: 0.7368 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7187 Loss_G: 0.7380 acc: 81.2%\n",
      "[BATCH 99/149] Loss_D: 0.7011 Loss_G: 0.7352 acc: 79.7%\n",
      "[BATCH 100/149] Loss_D: 0.6755 Loss_G: 0.7418 acc: 76.6%\n",
      "[BATCH 101/149] Loss_D: 0.7537 Loss_G: 0.7679 acc: 65.6%\n",
      "[BATCH 102/149] Loss_D: 0.7439 Loss_G: 0.7466 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7885 Loss_G: 0.7810 acc: 75.0%\n",
      "[BATCH 104/149] Loss_D: 0.7706 Loss_G: 0.7688 acc: 75.0%\n",
      "[BATCH 105/149] Loss_D: 0.7253 Loss_G: 0.7651 acc: 78.1%\n",
      "[BATCH 106/149] Loss_D: 0.7137 Loss_G: 0.7657 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7622 Loss_G: 0.7585 acc: 76.6%\n",
      "[BATCH 108/149] Loss_D: 0.6996 Loss_G: 0.7417 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7325 Loss_G: 0.7190 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.7368 Loss_G: 0.7475 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.7286 Loss_G: 0.7412 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.7256 Loss_G: 0.7500 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.7318 Loss_G: 0.7728 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7101 Loss_G: 0.7473 acc: 71.9%\n",
      "[BATCH 115/149] Loss_D: 0.7013 Loss_G: 0.7391 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.7327 Loss_G: 0.7432 acc: 71.9%\n",
      "[BATCH 117/149] Loss_D: 0.7743 Loss_G: 0.7687 acc: 76.6%\n",
      "[BATCH 118/149] Loss_D: 0.7175 Loss_G: 0.7398 acc: 76.6%\n",
      "[BATCH 119/149] Loss_D: 0.6911 Loss_G: 0.7444 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.7682 Loss_G: 0.7561 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7359 Loss_G: 0.7371 acc: 79.7%\n",
      "[BATCH 122/149] Loss_D: 0.7230 Loss_G: 0.7474 acc: 81.2%\n",
      "[BATCH 123/149] Loss_D: 0.7170 Loss_G: 0.7419 acc: 81.2%\n",
      "[BATCH 124/149] Loss_D: 0.7891 Loss_G: 0.7481 acc: 79.7%\n",
      "[BATCH 125/149] Loss_D: 0.7424 Loss_G: 0.7544 acc: 78.1%\n",
      "[BATCH 126/149] Loss_D: 0.6911 Loss_G: 0.7383 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.7490 Loss_G: 0.7217 acc: 75.0%\n",
      "[BATCH 128/149] Loss_D: 0.7263 Loss_G: 0.7454 acc: 71.9%\n",
      "[BATCH 129/149] Loss_D: 0.7355 Loss_G: 0.7431 acc: 79.7%\n",
      "[BATCH 130/149] Loss_D: 0.6808 Loss_G: 0.7465 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6986 Loss_G: 0.7442 acc: 79.7%\n",
      "[BATCH 132/149] Loss_D: 0.7834 Loss_G: 0.7655 acc: 70.3%\n",
      "[BATCH 133/149] Loss_D: 0.7353 Loss_G: 0.7386 acc: 76.6%\n",
      "[BATCH 134/149] Loss_D: 0.7509 Loss_G: 0.7415 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.7495 Loss_G: 0.7488 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7291 Loss_G: 0.7668 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7951 Loss_G: 0.7714 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.7487 Loss_G: 0.7758 acc: 85.9%\n",
      "[EPOCH 5800] TEST ACC is : 70.9%\n",
      "[BATCH 139/149] Loss_D: 0.7467 Loss_G: 0.7623 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.7873 Loss_G: 0.7672 acc: 78.1%\n",
      "[BATCH 141/149] Loss_D: 0.6982 Loss_G: 0.7403 acc: 79.7%\n",
      "[BATCH 142/149] Loss_D: 0.7622 Loss_G: 0.7418 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6966 Loss_G: 0.7347 acc: 75.0%\n",
      "[BATCH 144/149] Loss_D: 0.7194 Loss_G: 0.7326 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.7232 Loss_G: 0.7540 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6954 Loss_G: 0.7587 acc: 78.1%\n",
      "[BATCH 147/149] Loss_D: 0.6742 Loss_G: 0.7462 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.7569 Loss_G: 0.7790 acc: 75.0%\n",
      "[BATCH 149/149] Loss_D: 0.7503 Loss_G: 0.7493 acc: 71.9%\n",
      "-----THE [39/50] epoch end-----\n",
      "-----THE [40/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7398 Loss_G: 0.7360 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.7377 Loss_G: 0.7564 acc: 68.8%\n",
      "[BATCH 3/149] Loss_D: 0.7120 Loss_G: 0.7343 acc: 76.6%\n",
      "[BATCH 4/149] Loss_D: 0.7244 Loss_G: 0.7579 acc: 75.0%\n",
      "[BATCH 5/149] Loss_D: 0.6986 Loss_G: 0.7266 acc: 76.6%\n",
      "[BATCH 6/149] Loss_D: 0.7826 Loss_G: 0.7422 acc: 68.8%\n",
      "[BATCH 7/149] Loss_D: 0.7330 Loss_G: 0.7328 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.7315 Loss_G: 0.7450 acc: 79.7%\n",
      "[BATCH 9/149] Loss_D: 0.7045 Loss_G: 0.7270 acc: 71.9%\n",
      "[BATCH 10/149] Loss_D: 0.7255 Loss_G: 0.7447 acc: 76.6%\n",
      "[BATCH 11/149] Loss_D: 0.7436 Loss_G: 0.7389 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.7222 Loss_G: 0.7370 acc: 78.1%\n",
      "[BATCH 13/149] Loss_D: 0.7213 Loss_G: 0.7286 acc: 79.7%\n",
      "[BATCH 14/149] Loss_D: 0.6970 Loss_G: 0.7152 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.7234 Loss_G: 0.7540 acc: 71.9%\n",
      "[BATCH 16/149] Loss_D: 0.7428 Loss_G: 0.7300 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.7545 Loss_G: 0.7556 acc: 78.1%\n",
      "[BATCH 18/149] Loss_D: 0.7298 Loss_G: 0.7422 acc: 78.1%\n",
      "[BATCH 19/149] Loss_D: 0.7288 Loss_G: 0.7668 acc: 73.4%\n",
      "[BATCH 20/149] Loss_D: 0.7189 Loss_G: 0.7455 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.7681 Loss_G: 0.7714 acc: 76.6%\n",
      "[BATCH 22/149] Loss_D: 0.7578 Loss_G: 0.7741 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7392 Loss_G: 0.7544 acc: 81.2%\n",
      "[BATCH 24/149] Loss_D: 0.6934 Loss_G: 0.7478 acc: 75.0%\n",
      "[BATCH 25/149] Loss_D: 0.7301 Loss_G: 0.7724 acc: 79.7%\n",
      "[BATCH 26/149] Loss_D: 0.7237 Loss_G: 0.7580 acc: 68.8%\n",
      "[BATCH 27/149] Loss_D: 0.6929 Loss_G: 0.7448 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6992 Loss_G: 0.7401 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7312 Loss_G: 0.7593 acc: 78.1%\n",
      "[BATCH 30/149] Loss_D: 0.6915 Loss_G: 0.7405 acc: 81.2%\n",
      "[BATCH 31/149] Loss_D: 0.7085 Loss_G: 0.7280 acc: 81.2%\n",
      "[BATCH 32/149] Loss_D: 0.7238 Loss_G: 0.7108 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.7624 Loss_G: 0.7299 acc: 78.1%\n",
      "[BATCH 34/149] Loss_D: 0.7252 Loss_G: 0.7441 acc: 76.6%\n",
      "[BATCH 35/149] Loss_D: 0.7020 Loss_G: 0.7313 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.7306 Loss_G: 0.7418 acc: 79.7%\n",
      "[BATCH 37/149] Loss_D: 0.7197 Loss_G: 0.7502 acc: 79.7%\n",
      "[BATCH 38/149] Loss_D: 0.7665 Loss_G: 0.7382 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7888 Loss_G: 0.7582 acc: 75.0%\n",
      "[EPOCH 5850] TEST ACC is : 72.5%\n",
      "[BATCH 40/149] Loss_D: 0.7641 Loss_G: 0.7499 acc: 78.1%\n",
      "[BATCH 41/149] Loss_D: 0.7267 Loss_G: 0.7446 acc: 65.6%\n",
      "[BATCH 42/149] Loss_D: 0.7613 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7673 Loss_G: 0.7696 acc: 73.4%\n",
      "[BATCH 44/149] Loss_D: 0.7312 Loss_G: 0.7520 acc: 79.7%\n",
      "[BATCH 45/149] Loss_D: 0.7222 Loss_G: 0.7413 acc: 76.6%\n",
      "[BATCH 46/149] Loss_D: 0.7713 Loss_G: 0.7378 acc: 75.0%\n",
      "[BATCH 47/149] Loss_D: 0.7635 Loss_G: 0.7502 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.7146 Loss_G: 0.7440 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7971 Loss_G: 0.7842 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.6811 Loss_G: 0.7334 acc: 76.6%\n",
      "[BATCH 51/149] Loss_D: 0.7183 Loss_G: 0.7534 acc: 81.2%\n",
      "[BATCH 52/149] Loss_D: 0.7501 Loss_G: 0.7525 acc: 71.9%\n",
      "[BATCH 53/149] Loss_D: 0.7354 Loss_G: 0.7773 acc: 71.9%\n",
      "[BATCH 54/149] Loss_D: 0.6904 Loss_G: 0.7320 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.7923 Loss_G: 0.7591 acc: 79.7%\n",
      "[BATCH 56/149] Loss_D: 0.7360 Loss_G: 0.7689 acc: 79.7%\n",
      "[BATCH 57/149] Loss_D: 0.7406 Loss_G: 0.7623 acc: 75.0%\n",
      "[BATCH 58/149] Loss_D: 0.7224 Loss_G: 0.7378 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7573 Loss_G: 0.7512 acc: 82.8%\n",
      "[BATCH 60/149] Loss_D: 0.7388 Loss_G: 0.7597 acc: 81.2%\n",
      "[BATCH 61/149] Loss_D: 0.7092 Loss_G: 0.7448 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7435 Loss_G: 0.7490 acc: 68.8%\n",
      "[BATCH 63/149] Loss_D: 0.7085 Loss_G: 0.7509 acc: 79.7%\n",
      "[BATCH 64/149] Loss_D: 0.6979 Loss_G: 0.7406 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7231 Loss_G: 0.7450 acc: 78.1%\n",
      "[BATCH 66/149] Loss_D: 0.7553 Loss_G: 0.7453 acc: 79.7%\n",
      "[BATCH 67/149] Loss_D: 0.7216 Loss_G: 0.7474 acc: 78.1%\n",
      "[BATCH 68/149] Loss_D: 0.7138 Loss_G: 0.7504 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7430 Loss_G: 0.7444 acc: 67.2%\n",
      "[BATCH 70/149] Loss_D: 0.7368 Loss_G: 0.7641 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.7308 Loss_G: 0.7485 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7286 Loss_G: 0.7446 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7563 Loss_G: 0.7614 acc: 81.2%\n",
      "[BATCH 74/149] Loss_D: 0.7540 Loss_G: 0.7577 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7079 Loss_G: 0.7585 acc: 78.1%\n",
      "[BATCH 76/149] Loss_D: 0.7416 Loss_G: 0.7567 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.7171 Loss_G: 0.7560 acc: 71.9%\n",
      "[BATCH 78/149] Loss_D: 0.7004 Loss_G: 0.7312 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.7678 Loss_G: 0.7477 acc: 79.7%\n",
      "[BATCH 80/149] Loss_D: 0.7265 Loss_G: 0.7336 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6992 Loss_G: 0.7421 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.7362 Loss_G: 0.7588 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7128 Loss_G: 0.7583 acc: 79.7%\n",
      "[BATCH 84/149] Loss_D: 0.7804 Loss_G: 0.7560 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.6903 Loss_G: 0.7446 acc: 81.2%\n",
      "[BATCH 86/149] Loss_D: 0.7245 Loss_G: 0.7355 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6939 Loss_G: 0.7442 acc: 81.2%\n",
      "[BATCH 88/149] Loss_D: 0.7101 Loss_G: 0.7545 acc: 78.1%\n",
      "[BATCH 89/149] Loss_D: 0.7471 Loss_G: 0.7548 acc: 81.2%\n",
      "[EPOCH 5900] TEST ACC is : 70.7%\n",
      "[BATCH 90/149] Loss_D: 0.6941 Loss_G: 0.7480 acc: 82.8%\n",
      "[BATCH 91/149] Loss_D: 0.7145 Loss_G: 0.7520 acc: 81.2%\n",
      "[BATCH 92/149] Loss_D: 0.7135 Loss_G: 0.7216 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.7576 Loss_G: 0.7395 acc: 71.9%\n",
      "[BATCH 94/149] Loss_D: 0.6928 Loss_G: 0.7419 acc: 75.0%\n",
      "[BATCH 95/149] Loss_D: 0.7300 Loss_G: 0.7189 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.7203 Loss_G: 0.7321 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.7350 Loss_G: 0.7493 acc: 75.0%\n",
      "[BATCH 98/149] Loss_D: 0.7136 Loss_G: 0.7420 acc: 82.8%\n",
      "[BATCH 99/149] Loss_D: 0.7645 Loss_G: 0.7647 acc: 82.8%\n",
      "[BATCH 100/149] Loss_D: 0.6897 Loss_G: 0.7324 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6940 Loss_G: 0.7379 acc: 78.1%\n",
      "[BATCH 102/149] Loss_D: 0.7161 Loss_G: 0.7456 acc: 78.1%\n",
      "[BATCH 103/149] Loss_D: 0.7320 Loss_G: 0.7590 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.7153 Loss_G: 0.7359 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.7226 Loss_G: 0.7502 acc: 82.8%\n",
      "[BATCH 106/149] Loss_D: 0.7394 Loss_G: 0.7567 acc: 75.0%\n",
      "[BATCH 107/149] Loss_D: 0.7200 Loss_G: 0.7318 acc: 75.0%\n",
      "[BATCH 108/149] Loss_D: 0.7281 Loss_G: 0.7416 acc: 78.1%\n",
      "[BATCH 109/149] Loss_D: 0.7339 Loss_G: 0.7436 acc: 78.1%\n",
      "[BATCH 110/149] Loss_D: 0.7087 Loss_G: 0.7407 acc: 81.2%\n",
      "[BATCH 111/149] Loss_D: 0.7678 Loss_G: 0.7530 acc: 79.7%\n",
      "[BATCH 112/149] Loss_D: 0.8027 Loss_G: 0.7645 acc: 71.9%\n",
      "[BATCH 113/149] Loss_D: 0.7798 Loss_G: 0.7752 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.7602 Loss_G: 0.7721 acc: 76.6%\n",
      "[BATCH 115/149] Loss_D: 0.7324 Loss_G: 0.7627 acc: 81.2%\n",
      "[BATCH 116/149] Loss_D: 0.7590 Loss_G: 0.7575 acc: 75.0%\n",
      "[BATCH 117/149] Loss_D: 0.7375 Loss_G: 0.7379 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.7051 Loss_G: 0.7295 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.7358 Loss_G: 0.7648 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.6972 Loss_G: 0.7377 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.6897 Loss_G: 0.7192 acc: 79.7%\n",
      "[BATCH 122/149] Loss_D: 0.8257 Loss_G: 0.7770 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7386 Loss_G: 0.7683 acc: 81.2%\n",
      "[BATCH 124/149] Loss_D: 0.7334 Loss_G: 0.7502 acc: 79.7%\n",
      "[BATCH 125/149] Loss_D: 0.7403 Loss_G: 0.7564 acc: 79.7%\n",
      "[BATCH 126/149] Loss_D: 0.7493 Loss_G: 0.7578 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7354 Loss_G: 0.7548 acc: 82.8%\n",
      "[BATCH 128/149] Loss_D: 0.7439 Loss_G: 0.7695 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7723 Loss_G: 0.7857 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.7200 Loss_G: 0.7690 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.6923 Loss_G: 0.7417 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.7070 Loss_G: 0.7376 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7407 Loss_G: 0.7353 acc: 79.7%\n",
      "[BATCH 134/149] Loss_D: 0.8010 Loss_G: 0.7629 acc: 75.0%\n",
      "[BATCH 135/149] Loss_D: 0.7161 Loss_G: 0.7591 acc: 76.6%\n",
      "[BATCH 136/149] Loss_D: 0.7180 Loss_G: 0.7617 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6828 Loss_G: 0.7307 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7321 Loss_G: 0.7347 acc: 78.1%\n",
      "[BATCH 139/149] Loss_D: 0.7206 Loss_G: 0.7443 acc: 73.4%\n",
      "[EPOCH 5950] TEST ACC is : 71.1%\n",
      "[BATCH 140/149] Loss_D: 0.6805 Loss_G: 0.7310 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.7574 Loss_G: 0.7620 acc: 76.6%\n",
      "[BATCH 142/149] Loss_D: 0.7329 Loss_G: 0.7650 acc: 75.0%\n",
      "[BATCH 143/149] Loss_D: 0.7792 Loss_G: 0.7769 acc: 75.0%\n",
      "[BATCH 144/149] Loss_D: 0.7484 Loss_G: 0.7546 acc: 82.8%\n",
      "[BATCH 145/149] Loss_D: 0.7226 Loss_G: 0.7411 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.7304 Loss_G: 0.7523 acc: 76.6%\n",
      "[BATCH 147/149] Loss_D: 0.7562 Loss_G: 0.7464 acc: 82.8%\n",
      "[BATCH 148/149] Loss_D: 0.7536 Loss_G: 0.7545 acc: 79.7%\n",
      "[BATCH 149/149] Loss_D: 0.6858 Loss_G: 0.7380 acc: 81.2%\n",
      "-----THE [40/50] epoch end-----\n",
      "-----THE [41/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7072 Loss_G: 0.7402 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.7621 Loss_G: 0.7557 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.7067 Loss_G: 0.7303 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7449 Loss_G: 0.7490 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6875 Loss_G: 0.7423 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.7536 Loss_G: 0.7471 acc: 79.7%\n",
      "[BATCH 7/149] Loss_D: 0.7111 Loss_G: 0.7348 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.7161 Loss_G: 0.7320 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.7084 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7692 Loss_G: 0.7544 acc: 78.1%\n",
      "[BATCH 11/149] Loss_D: 0.7693 Loss_G: 0.7604 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.7242 Loss_G: 0.7377 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.7113 Loss_G: 0.7430 acc: 75.0%\n",
      "[BATCH 14/149] Loss_D: 0.7292 Loss_G: 0.7448 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.7870 Loss_G: 0.7683 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7238 Loss_G: 0.7550 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7668 Loss_G: 0.7637 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.7485 Loss_G: 0.7768 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7183 Loss_G: 0.7781 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.7491 Loss_G: 0.7789 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7531 Loss_G: 0.7700 acc: 79.7%\n",
      "[BATCH 22/149] Loss_D: 0.7297 Loss_G: 0.7507 acc: 71.9%\n",
      "[BATCH 23/149] Loss_D: 0.7240 Loss_G: 0.7311 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7013 Loss_G: 0.7453 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7488 Loss_G: 0.7408 acc: 81.2%\n",
      "[BATCH 26/149] Loss_D: 0.7373 Loss_G: 0.7368 acc: 76.6%\n",
      "[BATCH 27/149] Loss_D: 0.7023 Loss_G: 0.7231 acc: 81.2%\n",
      "[BATCH 28/149] Loss_D: 0.7555 Loss_G: 0.7627 acc: 76.6%\n",
      "[BATCH 29/149] Loss_D: 0.7712 Loss_G: 0.7432 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7408 Loss_G: 0.7511 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.6850 Loss_G: 0.7239 acc: 79.7%\n",
      "[BATCH 32/149] Loss_D: 0.6931 Loss_G: 0.7235 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.7775 Loss_G: 0.7668 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.7415 Loss_G: 0.7569 acc: 75.0%\n",
      "[BATCH 35/149] Loss_D: 0.7409 Loss_G: 0.7481 acc: 78.1%\n",
      "[BATCH 36/149] Loss_D: 0.7364 Loss_G: 0.7452 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.6968 Loss_G: 0.7352 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.7355 Loss_G: 0.7610 acc: 78.1%\n",
      "[BATCH 39/149] Loss_D: 0.7156 Loss_G: 0.7492 acc: 73.4%\n",
      "[BATCH 40/149] Loss_D: 0.7451 Loss_G: 0.7399 acc: 76.6%\n",
      "[EPOCH 6000] TEST ACC is : 71.5%\n",
      "[BATCH 41/149] Loss_D: 0.7530 Loss_G: 0.7341 acc: 78.1%\n",
      "[BATCH 42/149] Loss_D: 0.7431 Loss_G: 0.7373 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7252 Loss_G: 0.7334 acc: 79.7%\n",
      "[BATCH 44/149] Loss_D: 0.6912 Loss_G: 0.7353 acc: 78.1%\n",
      "[BATCH 45/149] Loss_D: 0.7407 Loss_G: 0.7477 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7399 Loss_G: 0.7558 acc: 65.6%\n",
      "[BATCH 47/149] Loss_D: 0.7396 Loss_G: 0.7564 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.6790 Loss_G: 0.7237 acc: 76.6%\n",
      "[BATCH 49/149] Loss_D: 0.7430 Loss_G: 0.7466 acc: 81.2%\n",
      "[BATCH 50/149] Loss_D: 0.7721 Loss_G: 0.7583 acc: 76.6%\n",
      "[BATCH 51/149] Loss_D: 0.6962 Loss_G: 0.7376 acc: 76.6%\n",
      "[BATCH 52/149] Loss_D: 0.7168 Loss_G: 0.7384 acc: 78.1%\n",
      "[BATCH 53/149] Loss_D: 0.7303 Loss_G: 0.7177 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.7025 Loss_G: 0.7319 acc: 70.3%\n",
      "[BATCH 55/149] Loss_D: 0.6857 Loss_G: 0.7266 acc: 79.7%\n",
      "[BATCH 56/149] Loss_D: 0.7253 Loss_G: 0.7296 acc: 76.6%\n",
      "[BATCH 57/149] Loss_D: 0.7352 Loss_G: 0.7408 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7323 Loss_G: 0.7334 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7225 Loss_G: 0.7592 acc: 71.9%\n",
      "[BATCH 60/149] Loss_D: 0.7502 Loss_G: 0.7365 acc: 73.4%\n",
      "[BATCH 61/149] Loss_D: 0.7468 Loss_G: 0.7516 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.7213 Loss_G: 0.7536 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7174 Loss_G: 0.7626 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.7074 Loss_G: 0.7465 acc: 73.4%\n",
      "[BATCH 65/149] Loss_D: 0.7357 Loss_G: 0.7523 acc: 76.6%\n",
      "[BATCH 66/149] Loss_D: 0.7607 Loss_G: 0.7760 acc: 76.6%\n",
      "[BATCH 67/149] Loss_D: 0.7333 Loss_G: 0.7603 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7589 Loss_G: 0.7518 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.7370 Loss_G: 0.7368 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7360 Loss_G: 0.7480 acc: 76.6%\n",
      "[BATCH 71/149] Loss_D: 0.7766 Loss_G: 0.7572 acc: 78.1%\n",
      "[BATCH 72/149] Loss_D: 0.7611 Loss_G: 0.7791 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7345 Loss_G: 0.7603 acc: 79.7%\n",
      "[BATCH 74/149] Loss_D: 0.7224 Loss_G: 0.7688 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7474 Loss_G: 0.7766 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7445 Loss_G: 0.7673 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.7349 Loss_G: 0.7498 acc: 81.2%\n",
      "[BATCH 78/149] Loss_D: 0.7526 Loss_G: 0.7592 acc: 79.7%\n",
      "[BATCH 79/149] Loss_D: 0.7377 Loss_G: 0.7594 acc: 82.8%\n",
      "[BATCH 80/149] Loss_D: 0.7113 Loss_G: 0.7568 acc: 76.6%\n",
      "[BATCH 81/149] Loss_D: 0.7093 Loss_G: 0.7451 acc: 71.9%\n",
      "[BATCH 82/149] Loss_D: 0.7319 Loss_G: 0.7421 acc: 81.2%\n",
      "[BATCH 83/149] Loss_D: 0.7141 Loss_G: 0.7309 acc: 75.0%\n",
      "[BATCH 84/149] Loss_D: 0.6994 Loss_G: 0.7178 acc: 79.7%\n",
      "[BATCH 85/149] Loss_D: 0.7177 Loss_G: 0.7340 acc: 75.0%\n",
      "[BATCH 86/149] Loss_D: 0.7223 Loss_G: 0.7361 acc: 73.4%\n",
      "[BATCH 87/149] Loss_D: 0.6964 Loss_G: 0.7389 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.7796 Loss_G: 0.7733 acc: 78.1%\n",
      "[BATCH 89/149] Loss_D: 0.7457 Loss_G: 0.7549 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.7214 Loss_G: 0.7561 acc: 81.2%\n",
      "[EPOCH 6050] TEST ACC is : 70.7%\n",
      "[BATCH 91/149] Loss_D: 0.7025 Loss_G: 0.7501 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.7459 Loss_G: 0.7655 acc: 76.6%\n",
      "[BATCH 93/149] Loss_D: 0.7442 Loss_G: 0.7681 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.6990 Loss_G: 0.7381 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7337 Loss_G: 0.7398 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7149 Loss_G: 0.7464 acc: 78.1%\n",
      "[BATCH 97/149] Loss_D: 0.7456 Loss_G: 0.7544 acc: 78.1%\n",
      "[BATCH 98/149] Loss_D: 0.7380 Loss_G: 0.7451 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7191 Loss_G: 0.7492 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.7851 Loss_G: 0.7740 acc: 75.0%\n",
      "[BATCH 101/149] Loss_D: 0.7612 Loss_G: 0.7628 acc: 79.7%\n",
      "[BATCH 102/149] Loss_D: 0.7597 Loss_G: 0.7665 acc: 78.1%\n",
      "[BATCH 103/149] Loss_D: 0.7825 Loss_G: 0.7713 acc: 79.7%\n",
      "[BATCH 104/149] Loss_D: 0.7466 Loss_G: 0.7708 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.7293 Loss_G: 0.7557 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7007 Loss_G: 0.7598 acc: 76.6%\n",
      "[BATCH 107/149] Loss_D: 0.7520 Loss_G: 0.7420 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7402 Loss_G: 0.7380 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7568 Loss_G: 0.7431 acc: 75.0%\n",
      "[BATCH 110/149] Loss_D: 0.7075 Loss_G: 0.7345 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.7618 Loss_G: 0.7516 acc: 73.4%\n",
      "[BATCH 112/149] Loss_D: 0.7512 Loss_G: 0.7616 acc: 79.7%\n",
      "[BATCH 113/149] Loss_D: 0.6965 Loss_G: 0.7376 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.7030 Loss_G: 0.7254 acc: 70.3%\n",
      "[BATCH 115/149] Loss_D: 0.7105 Loss_G: 0.7361 acc: 82.8%\n",
      "[BATCH 116/149] Loss_D: 0.7040 Loss_G: 0.7421 acc: 81.2%\n",
      "[BATCH 117/149] Loss_D: 0.7497 Loss_G: 0.7396 acc: 71.9%\n",
      "[BATCH 118/149] Loss_D: 0.7183 Loss_G: 0.7489 acc: 78.1%\n",
      "[BATCH 119/149] Loss_D: 0.7783 Loss_G: 0.7789 acc: 73.4%\n",
      "[BATCH 120/149] Loss_D: 0.7421 Loss_G: 0.7657 acc: 65.6%\n",
      "[BATCH 121/149] Loss_D: 0.7092 Loss_G: 0.7364 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.7117 Loss_G: 0.7354 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6993 Loss_G: 0.7308 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.7829 Loss_G: 0.7455 acc: 79.7%\n",
      "[BATCH 125/149] Loss_D: 0.7608 Loss_G: 0.7654 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.7195 Loss_G: 0.7758 acc: 78.1%\n",
      "[BATCH 127/149] Loss_D: 0.7292 Loss_G: 0.7674 acc: 79.7%\n",
      "[BATCH 128/149] Loss_D: 0.7334 Loss_G: 0.7541 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.7534 Loss_G: 0.7589 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7556 Loss_G: 0.7640 acc: 82.8%\n",
      "[BATCH 131/149] Loss_D: 0.7314 Loss_G: 0.7553 acc: 78.1%\n",
      "[BATCH 132/149] Loss_D: 0.7306 Loss_G: 0.7491 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.7672 Loss_G: 0.7620 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7572 Loss_G: 0.7677 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.7028 Loss_G: 0.7664 acc: 78.1%\n",
      "[BATCH 136/149] Loss_D: 0.7109 Loss_G: 0.7473 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7039 Loss_G: 0.7619 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.7229 Loss_G: 0.7554 acc: 73.4%\n",
      "[BATCH 139/149] Loss_D: 0.7244 Loss_G: 0.7442 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7139 Loss_G: 0.7590 acc: 75.0%\n",
      "[EPOCH 6100] TEST ACC is : 70.7%\n",
      "[BATCH 141/149] Loss_D: 0.7634 Loss_G: 0.7524 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6924 Loss_G: 0.7430 acc: 78.1%\n",
      "[BATCH 143/149] Loss_D: 0.7401 Loss_G: 0.7421 acc: 79.7%\n",
      "[BATCH 144/149] Loss_D: 0.6906 Loss_G: 0.7234 acc: 79.7%\n",
      "[BATCH 145/149] Loss_D: 0.7002 Loss_G: 0.7311 acc: 81.2%\n",
      "[BATCH 146/149] Loss_D: 0.7130 Loss_G: 0.7493 acc: 64.1%\n",
      "[BATCH 147/149] Loss_D: 0.6821 Loss_G: 0.7339 acc: 75.0%\n",
      "[BATCH 148/149] Loss_D: 0.6986 Loss_G: 0.7280 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.6877 Loss_G: 0.7101 acc: 82.8%\n",
      "-----THE [41/50] epoch end-----\n",
      "-----THE [42/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7532 Loss_G: 0.7343 acc: 78.1%\n",
      "[BATCH 2/149] Loss_D: 0.7126 Loss_G: 0.7394 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.7545 Loss_G: 0.7527 acc: 73.4%\n",
      "[BATCH 4/149] Loss_D: 0.7024 Loss_G: 0.7267 acc: 75.0%\n",
      "[BATCH 5/149] Loss_D: 0.7747 Loss_G: 0.7543 acc: 73.4%\n",
      "[BATCH 6/149] Loss_D: 0.7159 Loss_G: 0.7312 acc: 76.6%\n",
      "[BATCH 7/149] Loss_D: 0.7222 Loss_G: 0.7345 acc: 70.3%\n",
      "[BATCH 8/149] Loss_D: 0.7300 Loss_G: 0.7493 acc: 71.9%\n",
      "[BATCH 9/149] Loss_D: 0.7160 Loss_G: 0.7523 acc: 78.1%\n",
      "[BATCH 10/149] Loss_D: 0.7036 Loss_G: 0.7421 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7165 Loss_G: 0.7464 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.7162 Loss_G: 0.7445 acc: 79.7%\n",
      "[BATCH 13/149] Loss_D: 0.7806 Loss_G: 0.7585 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.7330 Loss_G: 0.7628 acc: 81.2%\n",
      "[BATCH 15/149] Loss_D: 0.7374 Loss_G: 0.7551 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.6915 Loss_G: 0.7363 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7114 Loss_G: 0.7345 acc: 76.6%\n",
      "[BATCH 18/149] Loss_D: 0.7215 Loss_G: 0.7377 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7517 Loss_G: 0.7432 acc: 82.8%\n",
      "[BATCH 20/149] Loss_D: 0.7099 Loss_G: 0.7369 acc: 73.4%\n",
      "[BATCH 21/149] Loss_D: 0.6812 Loss_G: 0.7261 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7133 Loss_G: 0.7489 acc: 81.2%\n",
      "[BATCH 23/149] Loss_D: 0.7155 Loss_G: 0.7516 acc: 78.1%\n",
      "[BATCH 24/149] Loss_D: 0.7676 Loss_G: 0.7603 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7402 Loss_G: 0.7513 acc: 71.9%\n",
      "[BATCH 26/149] Loss_D: 0.7627 Loss_G: 0.7537 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7426 Loss_G: 0.7712 acc: 76.6%\n",
      "[BATCH 28/149] Loss_D: 0.7117 Loss_G: 0.7552 acc: 78.1%\n",
      "[BATCH 29/149] Loss_D: 0.7259 Loss_G: 0.7422 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7360 Loss_G: 0.7395 acc: 79.7%\n",
      "[BATCH 31/149] Loss_D: 0.8001 Loss_G: 0.7688 acc: 81.2%\n",
      "[BATCH 32/149] Loss_D: 0.7108 Loss_G: 0.7437 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7402 Loss_G: 0.7633 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7285 Loss_G: 0.7507 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7060 Loss_G: 0.7466 acc: 75.0%\n",
      "[BATCH 36/149] Loss_D: 0.7727 Loss_G: 0.7662 acc: 78.1%\n",
      "[BATCH 37/149] Loss_D: 0.7082 Loss_G: 0.7217 acc: 81.2%\n",
      "[BATCH 38/149] Loss_D: 0.7533 Loss_G: 0.7329 acc: 81.2%\n",
      "[BATCH 39/149] Loss_D: 0.6993 Loss_G: 0.7287 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.7447 Loss_G: 0.7503 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.7395 Loss_G: 0.7596 acc: 76.6%\n",
      "[EPOCH 6150] TEST ACC is : 70.3%\n",
      "[BATCH 42/149] Loss_D: 0.7382 Loss_G: 0.7714 acc: 79.7%\n",
      "[BATCH 43/149] Loss_D: 0.6971 Loss_G: 0.7524 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7602 Loss_G: 0.7619 acc: 75.0%\n",
      "[BATCH 45/149] Loss_D: 0.7364 Loss_G: 0.7537 acc: 78.1%\n",
      "[BATCH 46/149] Loss_D: 0.7118 Loss_G: 0.7479 acc: 81.2%\n",
      "[BATCH 47/149] Loss_D: 0.7300 Loss_G: 0.7550 acc: 81.2%\n",
      "[BATCH 48/149] Loss_D: 0.7347 Loss_G: 0.7690 acc: 76.6%\n",
      "[BATCH 49/149] Loss_D: 0.7817 Loss_G: 0.7729 acc: 73.4%\n",
      "[BATCH 50/149] Loss_D: 0.7333 Loss_G: 0.7480 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7781 Loss_G: 0.7715 acc: 79.7%\n",
      "[BATCH 52/149] Loss_D: 0.7485 Loss_G: 0.7842 acc: 81.2%\n",
      "[BATCH 53/149] Loss_D: 0.6955 Loss_G: 0.7406 acc: 81.2%\n",
      "[BATCH 54/149] Loss_D: 0.7468 Loss_G: 0.7606 acc: 81.2%\n",
      "[BATCH 55/149] Loss_D: 0.7618 Loss_G: 0.7605 acc: 78.1%\n",
      "[BATCH 56/149] Loss_D: 0.7512 Loss_G: 0.7323 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6816 Loss_G: 0.7304 acc: 79.7%\n",
      "[BATCH 58/149] Loss_D: 0.7792 Loss_G: 0.7658 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.7155 Loss_G: 0.7742 acc: 76.6%\n",
      "[BATCH 60/149] Loss_D: 0.7447 Loss_G: 0.7721 acc: 75.0%\n",
      "[BATCH 61/149] Loss_D: 0.7221 Loss_G: 0.7414 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7398 Loss_G: 0.7548 acc: 70.3%\n",
      "[BATCH 63/149] Loss_D: 0.7044 Loss_G: 0.7410 acc: 78.1%\n",
      "[BATCH 64/149] Loss_D: 0.7035 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.7258 Loss_G: 0.7479 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6849 Loss_G: 0.7453 acc: 71.9%\n",
      "[BATCH 67/149] Loss_D: 0.7456 Loss_G: 0.7413 acc: 73.4%\n",
      "[BATCH 68/149] Loss_D: 0.7287 Loss_G: 0.7406 acc: 81.2%\n",
      "[BATCH 69/149] Loss_D: 0.7188 Loss_G: 0.7361 acc: 79.7%\n",
      "[BATCH 70/149] Loss_D: 0.7392 Loss_G: 0.7470 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.7166 Loss_G: 0.7514 acc: 71.9%\n",
      "[BATCH 72/149] Loss_D: 0.7472 Loss_G: 0.7600 acc: 75.0%\n",
      "[BATCH 73/149] Loss_D: 0.7266 Loss_G: 0.7445 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7527 Loss_G: 0.7419 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7350 Loss_G: 0.7555 acc: 76.6%\n",
      "[BATCH 76/149] Loss_D: 0.6997 Loss_G: 0.7470 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.7047 Loss_G: 0.7308 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7328 Loss_G: 0.7430 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.7503 Loss_G: 0.7623 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.7219 Loss_G: 0.7721 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.7380 Loss_G: 0.7682 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.7569 Loss_G: 0.7625 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7176 Loss_G: 0.7455 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7071 Loss_G: 0.7392 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.7744 Loss_G: 0.7513 acc: 79.7%\n",
      "[BATCH 86/149] Loss_D: 0.7149 Loss_G: 0.7531 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7088 Loss_G: 0.7260 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7481 Loss_G: 0.7377 acc: 79.7%\n",
      "[BATCH 89/149] Loss_D: 0.7310 Loss_G: 0.7354 acc: 79.7%\n",
      "[BATCH 90/149] Loss_D: 0.7201 Loss_G: 0.7598 acc: 82.8%\n",
      "[BATCH 91/149] Loss_D: 0.7518 Loss_G: 0.7574 acc: 82.8%\n",
      "[EPOCH 6200] TEST ACC is : 72.3%\n",
      "[BATCH 92/149] Loss_D: 0.7454 Loss_G: 0.7547 acc: 75.0%\n",
      "[BATCH 93/149] Loss_D: 0.7922 Loss_G: 0.7801 acc: 73.4%\n",
      "[BATCH 94/149] Loss_D: 0.6916 Loss_G: 0.7387 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.7448 Loss_G: 0.7352 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7811 Loss_G: 0.7723 acc: 75.0%\n",
      "[BATCH 97/149] Loss_D: 0.7451 Loss_G: 0.7539 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.7360 Loss_G: 0.7414 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7273 Loss_G: 0.7664 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.7209 Loss_G: 0.7573 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.7012 Loss_G: 0.7660 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.7517 Loss_G: 0.7607 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7270 Loss_G: 0.7402 acc: 81.2%\n",
      "[BATCH 104/149] Loss_D: 0.7055 Loss_G: 0.7309 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7406 Loss_G: 0.7407 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7513 Loss_G: 0.7456 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.7479 Loss_G: 0.7498 acc: 81.2%\n",
      "[BATCH 108/149] Loss_D: 0.7199 Loss_G: 0.7441 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7496 Loss_G: 0.7464 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.7076 Loss_G: 0.7386 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7404 Loss_G: 0.7379 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.7343 Loss_G: 0.7406 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.7354 Loss_G: 0.7401 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.7203 Loss_G: 0.7505 acc: 75.0%\n",
      "[BATCH 115/149] Loss_D: 0.7452 Loss_G: 0.7736 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7370 Loss_G: 0.7720 acc: 76.6%\n",
      "[BATCH 117/149] Loss_D: 0.7047 Loss_G: 0.7621 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.7913 Loss_G: 0.7658 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7232 Loss_G: 0.7612 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.7786 Loss_G: 0.7785 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.7303 Loss_G: 0.7742 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.7597 Loss_G: 0.7537 acc: 79.7%\n",
      "[BATCH 123/149] Loss_D: 0.7452 Loss_G: 0.7439 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.7090 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.7310 Loss_G: 0.7433 acc: 78.1%\n",
      "[BATCH 126/149] Loss_D: 0.7086 Loss_G: 0.7486 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.7185 Loss_G: 0.7360 acc: 70.3%\n",
      "[BATCH 128/149] Loss_D: 0.7041 Loss_G: 0.7226 acc: 81.2%\n",
      "[BATCH 129/149] Loss_D: 0.7051 Loss_G: 0.7301 acc: 78.1%\n",
      "[BATCH 130/149] Loss_D: 0.7086 Loss_G: 0.7261 acc: 79.7%\n",
      "[BATCH 131/149] Loss_D: 0.6807 Loss_G: 0.6994 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6990 Loss_G: 0.7210 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6994 Loss_G: 0.7173 acc: 75.0%\n",
      "[BATCH 134/149] Loss_D: 0.7136 Loss_G: 0.7344 acc: 64.1%\n",
      "[BATCH 135/149] Loss_D: 0.7664 Loss_G: 0.7591 acc: 71.9%\n",
      "[BATCH 136/149] Loss_D: 0.7132 Loss_G: 0.7314 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7301 Loss_G: 0.7543 acc: 79.7%\n",
      "[BATCH 138/149] Loss_D: 0.7266 Loss_G: 0.7178 acc: 82.8%\n",
      "[BATCH 139/149] Loss_D: 0.7329 Loss_G: 0.7459 acc: 73.4%\n",
      "[BATCH 140/149] Loss_D: 0.7240 Loss_G: 0.7317 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.6772 Loss_G: 0.7378 acc: 81.2%\n",
      "[EPOCH 6250] TEST ACC is : 72.1%\n",
      "[BATCH 142/149] Loss_D: 0.7203 Loss_G: 0.7515 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.7439 Loss_G: 0.7590 acc: 76.6%\n",
      "[BATCH 144/149] Loss_D: 0.7239 Loss_G: 0.7527 acc: 79.7%\n",
      "[BATCH 145/149] Loss_D: 0.7783 Loss_G: 0.7540 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.7161 Loss_G: 0.7374 acc: 81.2%\n",
      "[BATCH 147/149] Loss_D: 0.7128 Loss_G: 0.7339 acc: 70.3%\n",
      "[BATCH 148/149] Loss_D: 0.6965 Loss_G: 0.7276 acc: 79.7%\n",
      "[BATCH 149/149] Loss_D: 0.7313 Loss_G: 0.7427 acc: 81.2%\n",
      "-----THE [42/50] epoch end-----\n",
      "-----THE [43/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7330 Loss_G: 0.7566 acc: 78.1%\n",
      "[BATCH 2/149] Loss_D: 0.7324 Loss_G: 0.7655 acc: 79.7%\n",
      "[BATCH 3/149] Loss_D: 0.7502 Loss_G: 0.7644 acc: 76.6%\n",
      "[BATCH 4/149] Loss_D: 0.7423 Loss_G: 0.7631 acc: 81.2%\n",
      "[BATCH 5/149] Loss_D: 0.7684 Loss_G: 0.7484 acc: 75.0%\n",
      "[BATCH 6/149] Loss_D: 0.7066 Loss_G: 0.7303 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.7700 Loss_G: 0.7291 acc: 79.7%\n",
      "[BATCH 8/149] Loss_D: 0.7174 Loss_G: 0.7466 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.7015 Loss_G: 0.7289 acc: 78.1%\n",
      "[BATCH 10/149] Loss_D: 0.7770 Loss_G: 0.7331 acc: 81.2%\n",
      "[BATCH 11/149] Loss_D: 0.7196 Loss_G: 0.7672 acc: 75.0%\n",
      "[BATCH 12/149] Loss_D: 0.6926 Loss_G: 0.7390 acc: 81.2%\n",
      "[BATCH 13/149] Loss_D: 0.7321 Loss_G: 0.7409 acc: 76.6%\n",
      "[BATCH 14/149] Loss_D: 0.7132 Loss_G: 0.7514 acc: 79.7%\n",
      "[BATCH 15/149] Loss_D: 0.7275 Loss_G: 0.7458 acc: 75.0%\n",
      "[BATCH 16/149] Loss_D: 0.7598 Loss_G: 0.7553 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7610 Loss_G: 0.7665 acc: 78.1%\n",
      "[BATCH 18/149] Loss_D: 0.7478 Loss_G: 0.7536 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.7309 Loss_G: 0.7499 acc: 78.1%\n",
      "[BATCH 20/149] Loss_D: 0.7206 Loss_G: 0.7339 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7618 Loss_G: 0.7588 acc: 81.2%\n",
      "[BATCH 22/149] Loss_D: 0.7086 Loss_G: 0.7468 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7428 Loss_G: 0.7520 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7546 Loss_G: 0.7678 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.7489 Loss_G: 0.7750 acc: 82.8%\n",
      "[BATCH 26/149] Loss_D: 0.7104 Loss_G: 0.7451 acc: 75.0%\n",
      "[BATCH 27/149] Loss_D: 0.7761 Loss_G: 0.7708 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7354 Loss_G: 0.7656 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.7434 Loss_G: 0.7651 acc: 75.0%\n",
      "[BATCH 30/149] Loss_D: 0.6963 Loss_G: 0.7404 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.6954 Loss_G: 0.7407 acc: 76.6%\n",
      "[BATCH 32/149] Loss_D: 0.6866 Loss_G: 0.7361 acc: 81.2%\n",
      "[BATCH 33/149] Loss_D: 0.7550 Loss_G: 0.7694 acc: 70.3%\n",
      "[BATCH 34/149] Loss_D: 0.7221 Loss_G: 0.7482 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7701 Loss_G: 0.7677 acc: 81.2%\n",
      "[BATCH 36/149] Loss_D: 0.6971 Loss_G: 0.7458 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7278 Loss_G: 0.7382 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.6650 Loss_G: 0.7193 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7757 Loss_G: 0.7724 acc: 78.1%\n",
      "[BATCH 40/149] Loss_D: 0.7484 Loss_G: 0.7595 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7378 Loss_G: 0.7697 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.7176 Loss_G: 0.7465 acc: 87.5%\n",
      "[EPOCH 6300] TEST ACC is : 71.9%\n",
      "[BATCH 43/149] Loss_D: 0.7437 Loss_G: 0.7671 acc: 75.0%\n",
      "[BATCH 44/149] Loss_D: 0.7531 Loss_G: 0.7612 acc: 81.2%\n",
      "[BATCH 45/149] Loss_D: 0.7094 Loss_G: 0.7480 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7359 Loss_G: 0.7524 acc: 78.1%\n",
      "[BATCH 47/149] Loss_D: 0.7295 Loss_G: 0.7378 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.7049 Loss_G: 0.7404 acc: 81.2%\n",
      "[BATCH 49/149] Loss_D: 0.7356 Loss_G: 0.7221 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7630 Loss_G: 0.7618 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.7416 Loss_G: 0.7552 acc: 78.1%\n",
      "[BATCH 52/149] Loss_D: 0.7237 Loss_G: 0.7383 acc: 81.2%\n",
      "[BATCH 53/149] Loss_D: 0.7260 Loss_G: 0.7195 acc: 75.0%\n",
      "[BATCH 54/149] Loss_D: 0.7021 Loss_G: 0.7360 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.7721 Loss_G: 0.7472 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.7564 Loss_G: 0.7639 acc: 76.6%\n",
      "[BATCH 57/149] Loss_D: 0.7608 Loss_G: 0.7414 acc: 71.9%\n",
      "[BATCH 58/149] Loss_D: 0.7383 Loss_G: 0.7439 acc: 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.7144 Loss_G: 0.7341 acc: 76.6%\n",
      "[BATCH 60/149] Loss_D: 0.7634 Loss_G: 0.7474 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7537 Loss_G: 0.7493 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7358 Loss_G: 0.7729 acc: 81.2%\n",
      "[BATCH 63/149] Loss_D: 0.7280 Loss_G: 0.7620 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.6796 Loss_G: 0.7349 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.8324 Loss_G: 0.7486 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7541 Loss_G: 0.7715 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7364 Loss_G: 0.7502 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7490 Loss_G: 0.7537 acc: 82.8%\n",
      "[BATCH 69/149] Loss_D: 0.7745 Loss_G: 0.7700 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.7436 Loss_G: 0.7730 acc: 93.8%\n",
      "[BATCH 71/149] Loss_D: 0.7216 Loss_G: 0.7407 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.7323 Loss_G: 0.7510 acc: 79.7%\n",
      "[BATCH 73/149] Loss_D: 0.7325 Loss_G: 0.7597 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7599 Loss_G: 0.7628 acc: 75.0%\n",
      "[BATCH 75/149] Loss_D: 0.7231 Loss_G: 0.7477 acc: 81.2%\n",
      "[BATCH 76/149] Loss_D: 0.7309 Loss_G: 0.7416 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.7572 Loss_G: 0.7500 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.7267 Loss_G: 0.7619 acc: 76.6%\n",
      "[BATCH 79/149] Loss_D: 0.7266 Loss_G: 0.7484 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.7488 Loss_G: 0.7477 acc: 67.2%\n",
      "[BATCH 81/149] Loss_D: 0.7732 Loss_G: 0.7452 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.6858 Loss_G: 0.7364 acc: 76.6%\n",
      "[BATCH 83/149] Loss_D: 0.7347 Loss_G: 0.7547 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.7457 Loss_G: 0.7801 acc: 75.0%\n",
      "[BATCH 85/149] Loss_D: 0.7015 Loss_G: 0.7645 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7025 Loss_G: 0.7531 acc: 79.7%\n",
      "[BATCH 87/149] Loss_D: 0.7378 Loss_G: 0.7301 acc: 78.1%\n",
      "[BATCH 88/149] Loss_D: 0.7748 Loss_G: 0.7699 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.7620 Loss_G: 0.7601 acc: 75.0%\n",
      "[BATCH 90/149] Loss_D: 0.7599 Loss_G: 0.7606 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.7223 Loss_G: 0.7800 acc: 78.1%\n",
      "[BATCH 92/149] Loss_D: 0.6895 Loss_G: 0.7579 acc: 79.7%\n",
      "[EPOCH 6350] TEST ACC is : 71.9%\n",
      "[BATCH 93/149] Loss_D: 0.7059 Loss_G: 0.7453 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7404 Loss_G: 0.7720 acc: 73.4%\n",
      "[BATCH 95/149] Loss_D: 0.6964 Loss_G: 0.7460 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.7035 Loss_G: 0.7241 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7245 Loss_G: 0.7381 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.7443 Loss_G: 0.7682 acc: 78.1%\n",
      "[BATCH 99/149] Loss_D: 0.7345 Loss_G: 0.7710 acc: 78.1%\n",
      "[BATCH 100/149] Loss_D: 0.6928 Loss_G: 0.7364 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.7385 Loss_G: 0.7489 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.7322 Loss_G: 0.7683 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.6956 Loss_G: 0.7524 acc: 78.1%\n",
      "[BATCH 104/149] Loss_D: 0.7289 Loss_G: 0.7376 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.7589 Loss_G: 0.7436 acc: 82.8%\n",
      "[BATCH 106/149] Loss_D: 0.6983 Loss_G: 0.7346 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.7095 Loss_G: 0.7204 acc: 79.7%\n",
      "[BATCH 108/149] Loss_D: 0.7269 Loss_G: 0.7210 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.7288 Loss_G: 0.7337 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.7848 Loss_G: 0.7815 acc: 79.7%\n",
      "[BATCH 111/149] Loss_D: 0.7239 Loss_G: 0.7535 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.6956 Loss_G: 0.7445 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7250 Loss_G: 0.7257 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.7273 Loss_G: 0.7294 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7312 Loss_G: 0.7417 acc: 71.9%\n",
      "[BATCH 116/149] Loss_D: 0.6934 Loss_G: 0.7271 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.7212 Loss_G: 0.7377 acc: 79.7%\n",
      "[BATCH 118/149] Loss_D: 0.7045 Loss_G: 0.7445 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.7092 Loss_G: 0.7365 acc: 71.9%\n",
      "[BATCH 120/149] Loss_D: 0.7581 Loss_G: 0.7338 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7401 Loss_G: 0.7483 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7436 Loss_G: 0.7614 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.7267 Loss_G: 0.7490 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7354 Loss_G: 0.7476 acc: 78.1%\n",
      "[BATCH 125/149] Loss_D: 0.7508 Loss_G: 0.7408 acc: 75.0%\n",
      "[BATCH 126/149] Loss_D: 0.7196 Loss_G: 0.7428 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7329 Loss_G: 0.7566 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.6898 Loss_G: 0.7440 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7266 Loss_G: 0.7383 acc: 75.0%\n",
      "[BATCH 130/149] Loss_D: 0.7374 Loss_G: 0.7426 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6933 Loss_G: 0.7390 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6895 Loss_G: 0.7259 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.7269 Loss_G: 0.7347 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.7394 Loss_G: 0.7203 acc: 76.6%\n",
      "[BATCH 135/149] Loss_D: 0.7430 Loss_G: 0.7414 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7021 Loss_G: 0.7398 acc: 75.0%\n",
      "[BATCH 137/149] Loss_D: 0.6927 Loss_G: 0.7310 acc: 81.2%\n",
      "[BATCH 138/149] Loss_D: 0.6905 Loss_G: 0.7323 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7230 Loss_G: 0.7636 acc: 75.0%\n",
      "[BATCH 140/149] Loss_D: 0.7363 Loss_G: 0.7488 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.7247 Loss_G: 0.7522 acc: 78.1%\n",
      "[BATCH 142/149] Loss_D: 0.7231 Loss_G: 0.7382 acc: 84.4%\n",
      "[EPOCH 6400] TEST ACC is : 71.7%\n",
      "[BATCH 143/149] Loss_D: 0.7217 Loss_G: 0.7386 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.6861 Loss_G: 0.7459 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6841 Loss_G: 0.7219 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7006 Loss_G: 0.7154 acc: 81.2%\n",
      "[BATCH 147/149] Loss_D: 0.7320 Loss_G: 0.7490 acc: 81.2%\n",
      "[BATCH 148/149] Loss_D: 0.7398 Loss_G: 0.7385 acc: 76.6%\n",
      "[BATCH 149/149] Loss_D: 0.7086 Loss_G: 0.7245 acc: 84.4%\n",
      "-----THE [43/50] epoch end-----\n",
      "-----THE [44/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7045 Loss_G: 0.7333 acc: 75.0%\n",
      "[BATCH 2/149] Loss_D: 0.7435 Loss_G: 0.7381 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.7445 Loss_G: 0.7433 acc: 81.2%\n",
      "[BATCH 4/149] Loss_D: 0.7021 Loss_G: 0.7566 acc: 81.2%\n",
      "[BATCH 5/149] Loss_D: 0.7267 Loss_G: 0.7604 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.7331 Loss_G: 0.7820 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.7542 Loss_G: 0.7671 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.7273 Loss_G: 0.7490 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7163 Loss_G: 0.7423 acc: 76.6%\n",
      "[BATCH 10/149] Loss_D: 0.7257 Loss_G: 0.7171 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7638 Loss_G: 0.7618 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.7448 Loss_G: 0.7642 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7309 Loss_G: 0.7649 acc: 76.6%\n",
      "[BATCH 14/149] Loss_D: 0.7974 Loss_G: 0.7761 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.7065 Loss_G: 0.7478 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.7040 Loss_G: 0.7357 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7797 Loss_G: 0.7651 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7396 Loss_G: 0.7696 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7046 Loss_G: 0.7459 acc: 81.2%\n",
      "[BATCH 20/149] Loss_D: 0.6749 Loss_G: 0.7349 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7435 Loss_G: 0.7470 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.7379 Loss_G: 0.7427 acc: 79.7%\n",
      "[BATCH 23/149] Loss_D: 0.7009 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.7632 Loss_G: 0.7347 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.6934 Loss_G: 0.7389 acc: 82.8%\n",
      "[BATCH 26/149] Loss_D: 0.7423 Loss_G: 0.7511 acc: 78.1%\n",
      "[BATCH 27/149] Loss_D: 0.7325 Loss_G: 0.7372 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7145 Loss_G: 0.7506 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.7337 Loss_G: 0.7483 acc: 79.7%\n",
      "[BATCH 30/149] Loss_D: 0.7169 Loss_G: 0.7421 acc: 79.7%\n",
      "[BATCH 31/149] Loss_D: 0.7369 Loss_G: 0.7473 acc: 70.3%\n",
      "[BATCH 32/149] Loss_D: 0.7484 Loss_G: 0.7439 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.7247 Loss_G: 0.7266 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.6850 Loss_G: 0.7262 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7228 Loss_G: 0.7280 acc: 79.7%\n",
      "[BATCH 36/149] Loss_D: 0.7663 Loss_G: 0.7576 acc: 75.0%\n",
      "[BATCH 37/149] Loss_D: 0.7182 Loss_G: 0.7540 acc: 79.7%\n",
      "[BATCH 38/149] Loss_D: 0.7347 Loss_G: 0.7427 acc: 81.2%\n",
      "[BATCH 39/149] Loss_D: 0.7409 Loss_G: 0.7613 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.7118 Loss_G: 0.7473 acc: 79.7%\n",
      "[BATCH 41/149] Loss_D: 0.7370 Loss_G: 0.7440 acc: 79.7%\n",
      "[BATCH 42/149] Loss_D: 0.7295 Loss_G: 0.7277 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7392 Loss_G: 0.7552 acc: 84.4%\n",
      "[EPOCH 6450] TEST ACC is : 71.9%\n",
      "[BATCH 44/149] Loss_D: 0.7229 Loss_G: 0.7532 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.6970 Loss_G: 0.7419 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.6987 Loss_G: 0.7403 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7012 Loss_G: 0.7395 acc: 71.9%\n",
      "[BATCH 48/149] Loss_D: 0.7542 Loss_G: 0.7436 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7105 Loss_G: 0.7461 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6933 Loss_G: 0.7414 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.7335 Loss_G: 0.7320 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.6884 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6931 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7363 Loss_G: 0.7404 acc: 81.2%\n",
      "[BATCH 55/149] Loss_D: 0.6994 Loss_G: 0.7231 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.7312 Loss_G: 0.7441 acc: 81.2%\n",
      "[BATCH 57/149] Loss_D: 0.6986 Loss_G: 0.7371 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7142 Loss_G: 0.7292 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.7388 Loss_G: 0.7318 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6918 Loss_G: 0.7392 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.7334 Loss_G: 0.7482 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7284 Loss_G: 0.7732 acc: 78.1%\n",
      "[BATCH 63/149] Loss_D: 0.7449 Loss_G: 0.7421 acc: 78.1%\n",
      "[BATCH 64/149] Loss_D: 0.7151 Loss_G: 0.7409 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.7378 Loss_G: 0.7603 acc: 81.2%\n",
      "[BATCH 66/149] Loss_D: 0.7343 Loss_G: 0.7598 acc: 65.6%\n",
      "[BATCH 67/149] Loss_D: 0.7081 Loss_G: 0.7516 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7643 Loss_G: 0.7578 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7177 Loss_G: 0.7554 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.7408 Loss_G: 0.7659 acc: 79.7%\n",
      "[BATCH 71/149] Loss_D: 0.7902 Loss_G: 0.7713 acc: 78.1%\n",
      "[BATCH 72/149] Loss_D: 0.7679 Loss_G: 0.7691 acc: 95.3%\n",
      "[BATCH 73/149] Loss_D: 0.7578 Loss_G: 0.7993 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.7090 Loss_G: 0.7749 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.7143 Loss_G: 0.7507 acc: 82.8%\n",
      "[BATCH 76/149] Loss_D: 0.7209 Loss_G: 0.7528 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.7407 Loss_G: 0.7474 acc: 81.2%\n",
      "[BATCH 78/149] Loss_D: 0.6933 Loss_G: 0.7296 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.7703 Loss_G: 0.7656 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.7102 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.7080 Loss_G: 0.7418 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.7715 Loss_G: 0.7491 acc: 78.1%\n",
      "[BATCH 83/149] Loss_D: 0.7349 Loss_G: 0.7534 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.7359 Loss_G: 0.7551 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.7536 Loss_G: 0.7455 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.6986 Loss_G: 0.7405 acc: 75.0%\n",
      "[BATCH 87/149] Loss_D: 0.7161 Loss_G: 0.7362 acc: 81.2%\n",
      "[BATCH 88/149] Loss_D: 0.7354 Loss_G: 0.7401 acc: 78.1%\n",
      "[BATCH 89/149] Loss_D: 0.7244 Loss_G: 0.7369 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.7034 Loss_G: 0.7402 acc: 78.1%\n",
      "[BATCH 91/149] Loss_D: 0.7143 Loss_G: 0.7438 acc: 76.6%\n",
      "[BATCH 92/149] Loss_D: 0.7210 Loss_G: 0.7219 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.7586 Loss_G: 0.7662 acc: 89.1%\n",
      "[EPOCH 6500] TEST ACC is : 71.1%\n",
      "[BATCH 94/149] Loss_D: 0.7173 Loss_G: 0.7415 acc: 73.4%\n",
      "[BATCH 95/149] Loss_D: 0.7406 Loss_G: 0.7677 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.7130 Loss_G: 0.7704 acc: 70.3%\n",
      "[BATCH 97/149] Loss_D: 0.7190 Loss_G: 0.7595 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.7253 Loss_G: 0.7643 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7341 Loss_G: 0.7325 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7024 Loss_G: 0.7420 acc: 79.7%\n",
      "[BATCH 101/149] Loss_D: 0.7519 Loss_G: 0.7333 acc: 75.0%\n",
      "[BATCH 102/149] Loss_D: 0.7566 Loss_G: 0.7490 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7288 Loss_G: 0.7510 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7585 Loss_G: 0.7423 acc: 79.7%\n",
      "[BATCH 105/149] Loss_D: 0.7061 Loss_G: 0.7409 acc: 79.7%\n",
      "[BATCH 106/149] Loss_D: 0.6995 Loss_G: 0.7315 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.7148 Loss_G: 0.7349 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7382 Loss_G: 0.7356 acc: 75.0%\n",
      "[BATCH 109/149] Loss_D: 0.7850 Loss_G: 0.7548 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.7300 Loss_G: 0.7515 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.7239 Loss_G: 0.7491 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.7576 Loss_G: 0.7668 acc: 78.1%\n",
      "[BATCH 113/149] Loss_D: 0.7090 Loss_G: 0.7284 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7133 Loss_G: 0.7466 acc: 78.1%\n",
      "[BATCH 115/149] Loss_D: 0.7288 Loss_G: 0.7449 acc: 78.1%\n",
      "[BATCH 116/149] Loss_D: 0.7091 Loss_G: 0.7291 acc: 81.2%\n",
      "[BATCH 117/149] Loss_D: 0.7583 Loss_G: 0.7390 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.7471 Loss_G: 0.7524 acc: 78.1%\n",
      "[BATCH 119/149] Loss_D: 0.7710 Loss_G: 0.7503 acc: 78.1%\n",
      "[BATCH 120/149] Loss_D: 0.7308 Loss_G: 0.7469 acc: 81.2%\n",
      "[BATCH 121/149] Loss_D: 0.7038 Loss_G: 0.7270 acc: 79.7%\n",
      "[BATCH 122/149] Loss_D: 0.7681 Loss_G: 0.7656 acc: 79.7%\n",
      "[BATCH 123/149] Loss_D: 0.7187 Loss_G: 0.7471 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7368 Loss_G: 0.7635 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7812 Loss_G: 0.7790 acc: 79.7%\n",
      "[BATCH 126/149] Loss_D: 0.7297 Loss_G: 0.7748 acc: 79.7%\n",
      "[BATCH 127/149] Loss_D: 0.7115 Loss_G: 0.7635 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7379 Loss_G: 0.7786 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7720 Loss_G: 0.7752 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.7342 Loss_G: 0.7608 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.7216 Loss_G: 0.7733 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.6923 Loss_G: 0.7361 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.8017 Loss_G: 0.7873 acc: 73.4%\n",
      "[BATCH 134/149] Loss_D: 0.7070 Loss_G: 0.7385 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7244 Loss_G: 0.7425 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.6973 Loss_G: 0.7384 acc: 71.9%\n",
      "[BATCH 137/149] Loss_D: 0.7292 Loss_G: 0.7208 acc: 76.6%\n",
      "[BATCH 138/149] Loss_D: 0.7257 Loss_G: 0.7322 acc: 84.4%\n",
      "[BATCH 139/149] Loss_D: 0.7786 Loss_G: 0.7503 acc: 81.2%\n",
      "[BATCH 140/149] Loss_D: 0.7155 Loss_G: 0.7607 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.7250 Loss_G: 0.7207 acc: 81.2%\n",
      "[BATCH 142/149] Loss_D: 0.7075 Loss_G: 0.7434 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6998 Loss_G: 0.7460 acc: 90.6%\n",
      "[EPOCH 6550] TEST ACC is : 73.4%\n",
      "[BATCH 144/149] Loss_D: 0.7802 Loss_G: 0.7370 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.7149 Loss_G: 0.7445 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7286 Loss_G: 0.7462 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.7057 Loss_G: 0.7311 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7433 Loss_G: 0.7425 acc: 81.2%\n",
      "[BATCH 149/149] Loss_D: 0.7195 Loss_G: 0.7280 acc: 82.8%\n",
      "-----THE [44/50] epoch end-----\n",
      "-----THE [45/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6935 Loss_G: 0.7271 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.7132 Loss_G: 0.7276 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7405 Loss_G: 0.7297 acc: 76.6%\n",
      "[BATCH 4/149] Loss_D: 0.7454 Loss_G: 0.7396 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.7356 Loss_G: 0.7524 acc: 79.7%\n",
      "[BATCH 6/149] Loss_D: 0.7474 Loss_G: 0.7419 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7271 Loss_G: 0.7457 acc: 75.0%\n",
      "[BATCH 8/149] Loss_D: 0.7085 Loss_G: 0.7340 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.6930 Loss_G: 0.7264 acc: 79.7%\n",
      "[BATCH 10/149] Loss_D: 0.7946 Loss_G: 0.7777 acc: 68.8%\n",
      "[BATCH 11/149] Loss_D: 0.7075 Loss_G: 0.7402 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.7137 Loss_G: 0.7515 acc: 70.3%\n",
      "[BATCH 13/149] Loss_D: 0.7172 Loss_G: 0.7457 acc: 79.7%\n",
      "[BATCH 14/149] Loss_D: 0.7527 Loss_G: 0.7538 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.7113 Loss_G: 0.7432 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6948 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7130 Loss_G: 0.7288 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7638 Loss_G: 0.7521 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7629 Loss_G: 0.7654 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.7405 Loss_G: 0.7514 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.7042 Loss_G: 0.7404 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.7197 Loss_G: 0.7517 acc: 78.1%\n",
      "[BATCH 23/149] Loss_D: 0.6886 Loss_G: 0.7223 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7453 Loss_G: 0.7515 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7271 Loss_G: 0.7373 acc: 76.6%\n",
      "[BATCH 26/149] Loss_D: 0.7077 Loss_G: 0.7351 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.7495 Loss_G: 0.7586 acc: 79.7%\n",
      "[BATCH 28/149] Loss_D: 0.7462 Loss_G: 0.7578 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.6936 Loss_G: 0.7444 acc: 81.2%\n",
      "[BATCH 30/149] Loss_D: 0.7390 Loss_G: 0.7416 acc: 81.2%\n",
      "[BATCH 31/149] Loss_D: 0.7527 Loss_G: 0.7712 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6819 Loss_G: 0.7305 acc: 78.1%\n",
      "[BATCH 33/149] Loss_D: 0.7186 Loss_G: 0.7396 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7272 Loss_G: 0.7295 acc: 78.1%\n",
      "[BATCH 35/149] Loss_D: 0.7151 Loss_G: 0.7353 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.7301 Loss_G: 0.7363 acc: 82.8%\n",
      "[BATCH 37/149] Loss_D: 0.7916 Loss_G: 0.7713 acc: 78.1%\n",
      "[BATCH 38/149] Loss_D: 0.7199 Loss_G: 0.7573 acc: 78.1%\n",
      "[BATCH 39/149] Loss_D: 0.7682 Loss_G: 0.7588 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7111 Loss_G: 0.7382 acc: 81.2%\n",
      "[BATCH 41/149] Loss_D: 0.7442 Loss_G: 0.7490 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6959 Loss_G: 0.7314 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.8165 Loss_G: 0.7592 acc: 82.8%\n",
      "[BATCH 44/149] Loss_D: 0.7137 Loss_G: 0.7490 acc: 81.2%\n",
      "[EPOCH 6600] TEST ACC is : 71.3%\n",
      "[BATCH 45/149] Loss_D: 0.7431 Loss_G: 0.7510 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6747 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7098 Loss_G: 0.7389 acc: 78.1%\n",
      "[BATCH 48/149] Loss_D: 0.8332 Loss_G: 0.7845 acc: 79.7%\n",
      "[BATCH 49/149] Loss_D: 0.7561 Loss_G: 0.7558 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.7625 Loss_G: 0.7626 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6955 Loss_G: 0.7474 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.6965 Loss_G: 0.7518 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.7530 Loss_G: 0.7531 acc: 79.7%\n",
      "[BATCH 54/149] Loss_D: 0.7364 Loss_G: 0.7503 acc: 78.1%\n",
      "[BATCH 55/149] Loss_D: 0.7747 Loss_G: 0.7510 acc: 81.2%\n",
      "[BATCH 56/149] Loss_D: 0.7061 Loss_G: 0.7632 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.7034 Loss_G: 0.7345 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6962 Loss_G: 0.7199 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.7382 Loss_G: 0.7575 acc: 70.3%\n",
      "[BATCH 60/149] Loss_D: 0.7536 Loss_G: 0.7558 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.7178 Loss_G: 0.7489 acc: 76.6%\n",
      "[BATCH 62/149] Loss_D: 0.7371 Loss_G: 0.7364 acc: 79.7%\n",
      "[BATCH 63/149] Loss_D: 0.7007 Loss_G: 0.7399 acc: 73.4%\n",
      "[BATCH 64/149] Loss_D: 0.7386 Loss_G: 0.7337 acc: 76.6%\n",
      "[BATCH 65/149] Loss_D: 0.7333 Loss_G: 0.7285 acc: 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.6995 Loss_G: 0.7196 acc: 81.2%\n",
      "[BATCH 67/149] Loss_D: 0.7296 Loss_G: 0.7179 acc: 76.6%\n",
      "[BATCH 68/149] Loss_D: 0.7251 Loss_G: 0.7362 acc: 71.9%\n",
      "[BATCH 69/149] Loss_D: 0.7319 Loss_G: 0.7290 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.7383 Loss_G: 0.7373 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6799 Loss_G: 0.7195 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.7186 Loss_G: 0.7315 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6922 Loss_G: 0.7408 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.7398 Loss_G: 0.7429 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.7380 Loss_G: 0.7671 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.6965 Loss_G: 0.7471 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.6810 Loss_G: 0.7325 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7285 Loss_G: 0.7410 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.7673 Loss_G: 0.7671 acc: 73.4%\n",
      "[BATCH 80/149] Loss_D: 0.7520 Loss_G: 0.7471 acc: 78.1%\n",
      "[BATCH 81/149] Loss_D: 0.7720 Loss_G: 0.7563 acc: 81.2%\n",
      "[BATCH 82/149] Loss_D: 0.7235 Loss_G: 0.7367 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7468 Loss_G: 0.7637 acc: 73.4%\n",
      "[BATCH 84/149] Loss_D: 0.7007 Loss_G: 0.7486 acc: 79.7%\n",
      "[BATCH 85/149] Loss_D: 0.7332 Loss_G: 0.7489 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7517 Loss_G: 0.7534 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7163 Loss_G: 0.7488 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7122 Loss_G: 0.7585 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7586 Loss_G: 0.7487 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.7205 Loss_G: 0.7517 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.7366 Loss_G: 0.7517 acc: 78.1%\n",
      "[BATCH 92/149] Loss_D: 0.7232 Loss_G: 0.7543 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7227 Loss_G: 0.7653 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.7539 Loss_G: 0.7531 acc: 64.1%\n",
      "[EPOCH 6650] TEST ACC is : 72.9%\n",
      "[BATCH 95/149] Loss_D: 0.7312 Loss_G: 0.7602 acc: 76.6%\n",
      "[BATCH 96/149] Loss_D: 0.7245 Loss_G: 0.7333 acc: 82.8%\n",
      "[BATCH 97/149] Loss_D: 0.7661 Loss_G: 0.7647 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7201 Loss_G: 0.7546 acc: 76.6%\n",
      "[BATCH 99/149] Loss_D: 0.7577 Loss_G: 0.7711 acc: 75.0%\n",
      "[BATCH 100/149] Loss_D: 0.7091 Loss_G: 0.7671 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.7465 Loss_G: 0.7539 acc: 81.2%\n",
      "[BATCH 102/149] Loss_D: 0.7480 Loss_G: 0.7534 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7257 Loss_G: 0.7215 acc: 78.1%\n",
      "[BATCH 104/149] Loss_D: 0.7296 Loss_G: 0.7396 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.7242 Loss_G: 0.7353 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7194 Loss_G: 0.7340 acc: 79.7%\n",
      "[BATCH 107/149] Loss_D: 0.7151 Loss_G: 0.7173 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7283 Loss_G: 0.7536 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7355 Loss_G: 0.7550 acc: 76.6%\n",
      "[BATCH 110/149] Loss_D: 0.7373 Loss_G: 0.7734 acc: 79.7%\n",
      "[BATCH 111/149] Loss_D: 0.7419 Loss_G: 0.7510 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.7200 Loss_G: 0.7340 acc: 92.2%\n",
      "[BATCH 113/149] Loss_D: 0.7159 Loss_G: 0.7269 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7040 Loss_G: 0.7520 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.7228 Loss_G: 0.7242 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7318 Loss_G: 0.7389 acc: 75.0%\n",
      "[BATCH 117/149] Loss_D: 0.7417 Loss_G: 0.7429 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7393 Loss_G: 0.7440 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6943 Loss_G: 0.7403 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.7155 Loss_G: 0.7458 acc: 79.7%\n",
      "[BATCH 121/149] Loss_D: 0.7114 Loss_G: 0.7193 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.7619 Loss_G: 0.7605 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7277 Loss_G: 0.7526 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.7564 Loss_G: 0.7674 acc: 75.0%\n",
      "[BATCH 125/149] Loss_D: 0.7410 Loss_G: 0.7662 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.6777 Loss_G: 0.7548 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.7330 Loss_G: 0.7530 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.7571 Loss_G: 0.7491 acc: 73.4%\n",
      "[BATCH 129/149] Loss_D: 0.7630 Loss_G: 0.7588 acc: 78.1%\n",
      "[BATCH 130/149] Loss_D: 0.7563 Loss_G: 0.7641 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7203 Loss_G: 0.7493 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.7228 Loss_G: 0.7665 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7370 Loss_G: 0.7695 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7646 Loss_G: 0.7525 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.7278 Loss_G: 0.7550 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.6847 Loss_G: 0.7379 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.7348 Loss_G: 0.7327 acc: 81.2%\n",
      "[BATCH 138/149] Loss_D: 0.7236 Loss_G: 0.7410 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7526 Loss_G: 0.7639 acc: 79.7%\n",
      "[BATCH 140/149] Loss_D: 0.7511 Loss_G: 0.7652 acc: 75.0%\n",
      "[BATCH 141/149] Loss_D: 0.7071 Loss_G: 0.7499 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7288 Loss_G: 0.7489 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7100 Loss_G: 0.7419 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.7285 Loss_G: 0.7425 acc: 71.9%\n",
      "[EPOCH 6700] TEST ACC is : 71.9%\n",
      "[BATCH 145/149] Loss_D: 0.7100 Loss_G: 0.7452 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.7367 Loss_G: 0.7457 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.7268 Loss_G: 0.7430 acc: 78.1%\n",
      "[BATCH 148/149] Loss_D: 0.7671 Loss_G: 0.7623 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7587 Loss_G: 0.7754 acc: 89.1%\n",
      "-----THE [45/50] epoch end-----\n",
      "-----THE [46/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7340 Loss_G: 0.7714 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.7401 Loss_G: 0.7596 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7059 Loss_G: 0.7531 acc: 79.7%\n",
      "[BATCH 4/149] Loss_D: 0.7216 Loss_G: 0.7481 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.7659 Loss_G: 0.7614 acc: 79.7%\n",
      "[BATCH 6/149] Loss_D: 0.7377 Loss_G: 0.7461 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7356 Loss_G: 0.7551 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6922 Loss_G: 0.7281 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.7441 Loss_G: 0.7463 acc: 81.2%\n",
      "[BATCH 10/149] Loss_D: 0.7644 Loss_G: 0.7479 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7045 Loss_G: 0.7448 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.7147 Loss_G: 0.7292 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7363 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.7104 Loss_G: 0.7427 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.7193 Loss_G: 0.7406 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.7114 Loss_G: 0.7310 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7538 Loss_G: 0.7420 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.7108 Loss_G: 0.7330 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.7246 Loss_G: 0.7311 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7161 Loss_G: 0.7350 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.7728 Loss_G: 0.7549 acc: 76.6%\n",
      "[BATCH 22/149] Loss_D: 0.7133 Loss_G: 0.7308 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7574 Loss_G: 0.7462 acc: 75.0%\n",
      "[BATCH 24/149] Loss_D: 0.7362 Loss_G: 0.7350 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.7172 Loss_G: 0.7446 acc: 78.1%\n",
      "[BATCH 26/149] Loss_D: 0.6657 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7202 Loss_G: 0.7383 acc: 70.3%\n",
      "[BATCH 28/149] Loss_D: 0.6814 Loss_G: 0.7368 acc: 78.1%\n",
      "[BATCH 29/149] Loss_D: 0.7336 Loss_G: 0.7426 acc: 78.1%\n",
      "[BATCH 30/149] Loss_D: 0.7210 Loss_G: 0.7387 acc: 81.2%\n",
      "[BATCH 31/149] Loss_D: 0.7350 Loss_G: 0.7416 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7366 Loss_G: 0.7622 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6989 Loss_G: 0.7362 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6895 Loss_G: 0.7251 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7549 Loss_G: 0.7428 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7308 Loss_G: 0.7636 acc: 73.4%\n",
      "[BATCH 37/149] Loss_D: 0.7907 Loss_G: 0.7810 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.7452 Loss_G: 0.7541 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7439 Loss_G: 0.7554 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.7056 Loss_G: 0.7523 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.6965 Loss_G: 0.7161 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.6766 Loss_G: 0.7232 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7459 Loss_G: 0.7369 acc: 82.8%\n",
      "[BATCH 44/149] Loss_D: 0.7337 Loss_G: 0.7464 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7708 Loss_G: 0.7665 acc: 81.2%\n",
      "[EPOCH 6750] TEST ACC is : 72.7%\n",
      "[BATCH 46/149] Loss_D: 0.7315 Loss_G: 0.7414 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.7065 Loss_G: 0.7491 acc: 82.8%\n",
      "[BATCH 48/149] Loss_D: 0.7473 Loss_G: 0.7592 acc: 81.2%\n",
      "[BATCH 49/149] Loss_D: 0.7546 Loss_G: 0.7687 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7306 Loss_G: 0.7385 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.7031 Loss_G: 0.7348 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.7231 Loss_G: 0.7523 acc: 81.2%\n",
      "[BATCH 53/149] Loss_D: 0.7167 Loss_G: 0.7545 acc: 79.7%\n",
      "[BATCH 54/149] Loss_D: 0.7184 Loss_G: 0.7445 acc: 79.7%\n",
      "[BATCH 55/149] Loss_D: 0.7187 Loss_G: 0.7757 acc: 76.6%\n",
      "[BATCH 56/149] Loss_D: 0.7015 Loss_G: 0.7472 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.7455 Loss_G: 0.7564 acc: 81.2%\n",
      "[BATCH 58/149] Loss_D: 0.7225 Loss_G: 0.7490 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.7343 Loss_G: 0.7687 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6904 Loss_G: 0.7336 acc: 79.7%\n",
      "[BATCH 61/149] Loss_D: 0.6840 Loss_G: 0.7226 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7702 Loss_G: 0.7560 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7274 Loss_G: 0.7518 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7265 Loss_G: 0.7565 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7629 Loss_G: 0.7559 acc: 81.2%\n",
      "[BATCH 66/149] Loss_D: 0.7262 Loss_G: 0.7598 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7582 Loss_G: 0.7487 acc: 78.1%\n",
      "[BATCH 68/149] Loss_D: 0.7289 Loss_G: 0.7534 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.7296 Loss_G: 0.7566 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.7562 Loss_G: 0.7915 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.7674 Loss_G: 0.7766 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.7406 Loss_G: 0.7591 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.7678 Loss_G: 0.7553 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7522 Loss_G: 0.7687 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7830 Loss_G: 0.7575 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.7135 Loss_G: 0.7473 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7539 Loss_G: 0.7403 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7477 Loss_G: 0.7532 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.7291 Loss_G: 0.7565 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.7262 Loss_G: 0.7635 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.7410 Loss_G: 0.7595 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7484 Loss_G: 0.7708 acc: 75.0%\n",
      "[BATCH 83/149] Loss_D: 0.7305 Loss_G: 0.7412 acc: 82.8%\n",
      "[BATCH 84/149] Loss_D: 0.7241 Loss_G: 0.7696 acc: 75.0%\n",
      "[BATCH 85/149] Loss_D: 0.7495 Loss_G: 0.7482 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6943 Loss_G: 0.7304 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7853 Loss_G: 0.7700 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6861 Loss_G: 0.7382 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7274 Loss_G: 0.7367 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7545 Loss_G: 0.7626 acc: 76.6%\n",
      "[BATCH 91/149] Loss_D: 0.6930 Loss_G: 0.7479 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7602 Loss_G: 0.7488 acc: 81.2%\n",
      "[BATCH 93/149] Loss_D: 0.6951 Loss_G: 0.7418 acc: 79.7%\n",
      "[BATCH 94/149] Loss_D: 0.7301 Loss_G: 0.7388 acc: 81.2%\n",
      "[BATCH 95/149] Loss_D: 0.7559 Loss_G: 0.7418 acc: 81.2%\n",
      "[EPOCH 6800] TEST ACC is : 73.0%\n",
      "[BATCH 96/149] Loss_D: 0.7132 Loss_G: 0.7340 acc: 82.8%\n",
      "[BATCH 97/149] Loss_D: 0.7395 Loss_G: 0.7625 acc: 71.9%\n",
      "[BATCH 98/149] Loss_D: 0.7443 Loss_G: 0.7715 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7146 Loss_G: 0.7318 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.7374 Loss_G: 0.7547 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6953 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.7014 Loss_G: 0.7424 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7528 Loss_G: 0.7717 acc: 76.6%\n",
      "[BATCH 104/149] Loss_D: 0.6940 Loss_G: 0.7296 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.7319 Loss_G: 0.7587 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.7214 Loss_G: 0.7470 acc: 81.2%\n",
      "[BATCH 107/149] Loss_D: 0.6959 Loss_G: 0.7344 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7270 Loss_G: 0.7348 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.7540 Loss_G: 0.7553 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7520 Loss_G: 0.7524 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7341 Loss_G: 0.7218 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7029 Loss_G: 0.7319 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.7328 Loss_G: 0.7477 acc: 81.2%\n",
      "[BATCH 114/149] Loss_D: 0.6681 Loss_G: 0.7366 acc: 79.7%\n",
      "[BATCH 115/149] Loss_D: 0.7167 Loss_G: 0.7321 acc: 75.0%\n",
      "[BATCH 116/149] Loss_D: 0.7176 Loss_G: 0.7115 acc: 78.1%\n",
      "[BATCH 117/149] Loss_D: 0.6928 Loss_G: 0.7212 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.6998 Loss_G: 0.7307 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.7157 Loss_G: 0.7311 acc: 76.6%\n",
      "[BATCH 120/149] Loss_D: 0.7188 Loss_G: 0.7256 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.7377 Loss_G: 0.7387 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7487 Loss_G: 0.7410 acc: 81.2%\n",
      "[BATCH 123/149] Loss_D: 0.7309 Loss_G: 0.7402 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.7691 Loss_G: 0.7448 acc: 81.2%\n",
      "[BATCH 125/149] Loss_D: 0.7126 Loss_G: 0.7495 acc: 75.0%\n",
      "[BATCH 126/149] Loss_D: 0.7121 Loss_G: 0.7404 acc: 79.7%\n",
      "[BATCH 127/149] Loss_D: 0.6774 Loss_G: 0.7214 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.7375 Loss_G: 0.7321 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.7356 Loss_G: 0.7619 acc: 78.1%\n",
      "[BATCH 130/149] Loss_D: 0.7363 Loss_G: 0.7496 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.7807 Loss_G: 0.7591 acc: 95.3%\n",
      "[BATCH 132/149] Loss_D: 0.7142 Loss_G: 0.7447 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7584 Loss_G: 0.7519 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7238 Loss_G: 0.7396 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.7729 Loss_G: 0.7561 acc: 79.7%\n",
      "[BATCH 136/149] Loss_D: 0.7045 Loss_G: 0.7268 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.7195 Loss_G: 0.7346 acc: 79.7%\n",
      "[BATCH 138/149] Loss_D: 0.7017 Loss_G: 0.7397 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6827 Loss_G: 0.7290 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.7331 Loss_G: 0.7356 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.7287 Loss_G: 0.7416 acc: 81.2%\n",
      "[BATCH 142/149] Loss_D: 0.7447 Loss_G: 0.7669 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.7312 Loss_G: 0.7702 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.7662 Loss_G: 0.7796 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7661 Loss_G: 0.7667 acc: 78.1%\n",
      "[EPOCH 6850] TEST ACC is : 74.2%\n",
      "[BATCH 146/149] Loss_D: 0.7460 Loss_G: 0.7808 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.7783 Loss_G: 0.7790 acc: 79.7%\n",
      "[BATCH 148/149] Loss_D: 0.7248 Loss_G: 0.7670 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7006 Loss_G: 0.7483 acc: 81.2%\n",
      "-----THE [46/50] epoch end-----\n",
      "-----THE [47/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7443 Loss_G: 0.7363 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7152 Loss_G: 0.7601 acc: 76.6%\n",
      "[BATCH 3/149] Loss_D: 0.6915 Loss_G: 0.7282 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.7384 Loss_G: 0.7351 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7022 Loss_G: 0.7318 acc: 79.7%\n",
      "[BATCH 6/149] Loss_D: 0.7598 Loss_G: 0.7394 acc: 78.1%\n",
      "[BATCH 7/149] Loss_D: 0.7177 Loss_G: 0.7494 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6843 Loss_G: 0.7289 acc: 78.1%\n",
      "[BATCH 9/149] Loss_D: 0.7057 Loss_G: 0.7310 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7552 Loss_G: 0.7353 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6845 Loss_G: 0.7341 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7273 Loss_G: 0.7443 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7361 Loss_G: 0.7685 acc: 79.7%\n",
      "[BATCH 14/149] Loss_D: 0.7378 Loss_G: 0.7352 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.7568 Loss_G: 0.7518 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7443 Loss_G: 0.7449 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.7350 Loss_G: 0.7358 acc: 79.7%\n",
      "[BATCH 18/149] Loss_D: 0.6933 Loss_G: 0.7263 acc: 78.1%\n",
      "[BATCH 19/149] Loss_D: 0.7249 Loss_G: 0.7262 acc: 82.8%\n",
      "[BATCH 20/149] Loss_D: 0.7082 Loss_G: 0.7319 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.7086 Loss_G: 0.7363 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7531 Loss_G: 0.7436 acc: 78.1%\n",
      "[BATCH 23/149] Loss_D: 0.7494 Loss_G: 0.7588 acc: 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.6995 Loss_G: 0.7528 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.7350 Loss_G: 0.7557 acc: 82.8%\n",
      "[BATCH 26/149] Loss_D: 0.7860 Loss_G: 0.7768 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7731 Loss_G: 0.7688 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7434 Loss_G: 0.7504 acc: 79.7%\n",
      "[BATCH 29/149] Loss_D: 0.7312 Loss_G: 0.7410 acc: 75.0%\n",
      "[BATCH 30/149] Loss_D: 0.7502 Loss_G: 0.7453 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7319 Loss_G: 0.7554 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7489 Loss_G: 0.7718 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.7086 Loss_G: 0.7533 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7346 Loss_G: 0.7580 acc: 81.2%\n",
      "[BATCH 35/149] Loss_D: 0.7213 Loss_G: 0.7480 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7403 Loss_G: 0.7647 acc: 78.1%\n",
      "[BATCH 37/149] Loss_D: 0.7366 Loss_G: 0.7493 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7185 Loss_G: 0.7530 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7105 Loss_G: 0.7344 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.7241 Loss_G: 0.7335 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.7747 Loss_G: 0.7580 acc: 71.9%\n",
      "[BATCH 42/149] Loss_D: 0.7164 Loss_G: 0.7581 acc: 81.2%\n",
      "[BATCH 43/149] Loss_D: 0.7247 Loss_G: 0.7531 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7294 Loss_G: 0.7581 acc: 76.6%\n",
      "[BATCH 45/149] Loss_D: 0.7087 Loss_G: 0.7385 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7597 Loss_G: 0.7387 acc: 89.1%\n",
      "[EPOCH 6900] TEST ACC is : 71.9%\n",
      "[BATCH 47/149] Loss_D: 0.7187 Loss_G: 0.7362 acc: 81.2%\n",
      "[BATCH 48/149] Loss_D: 0.7268 Loss_G: 0.7430 acc: 75.0%\n",
      "[BATCH 49/149] Loss_D: 0.7011 Loss_G: 0.7326 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.7342 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6982 Loss_G: 0.7434 acc: 78.1%\n",
      "[BATCH 52/149] Loss_D: 0.7113 Loss_G: 0.7303 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.7360 Loss_G: 0.7539 acc: 79.7%\n",
      "[BATCH 54/149] Loss_D: 0.7260 Loss_G: 0.7338 acc: 75.0%\n",
      "[BATCH 55/149] Loss_D: 0.7210 Loss_G: 0.7367 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.7152 Loss_G: 0.7234 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7216 Loss_G: 0.7305 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.7183 Loss_G: 0.7459 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.7630 Loss_G: 0.7637 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7000 Loss_G: 0.7484 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6951 Loss_G: 0.7592 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7382 Loss_G: 0.7631 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.7485 Loss_G: 0.7697 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.7200 Loss_G: 0.7481 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7115 Loss_G: 0.7513 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7207 Loss_G: 0.7523 acc: 79.7%\n",
      "[BATCH 67/149] Loss_D: 0.7051 Loss_G: 0.7369 acc: 75.0%\n",
      "[BATCH 68/149] Loss_D: 0.6883 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7293 Loss_G: 0.7362 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.7516 Loss_G: 0.7553 acc: 78.1%\n",
      "[BATCH 71/149] Loss_D: 0.7389 Loss_G: 0.7540 acc: 79.7%\n",
      "[BATCH 72/149] Loss_D: 0.7088 Loss_G: 0.7423 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7387 Loss_G: 0.7538 acc: 79.7%\n",
      "[BATCH 74/149] Loss_D: 0.7316 Loss_G: 0.7543 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.7469 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.7204 Loss_G: 0.7359 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.7089 Loss_G: 0.7310 acc: 78.1%\n",
      "[BATCH 78/149] Loss_D: 0.7738 Loss_G: 0.7444 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.7249 Loss_G: 0.7401 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7126 Loss_G: 0.7500 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.8253 Loss_G: 0.7925 acc: 79.7%\n",
      "[BATCH 82/149] Loss_D: 0.7502 Loss_G: 0.7763 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.7397 Loss_G: 0.7747 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.7477 Loss_G: 0.7663 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7260 Loss_G: 0.7583 acc: 79.7%\n",
      "[BATCH 86/149] Loss_D: 0.7186 Loss_G: 0.7331 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7205 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.7475 Loss_G: 0.7511 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7820 Loss_G: 0.7690 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.7672 Loss_G: 0.7467 acc: 79.7%\n",
      "[BATCH 91/149] Loss_D: 0.7226 Loss_G: 0.7415 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7341 Loss_G: 0.7479 acc: 75.0%\n",
      "[BATCH 93/149] Loss_D: 0.7007 Loss_G: 0.7306 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7144 Loss_G: 0.7375 acc: 79.7%\n",
      "[BATCH 95/149] Loss_D: 0.7447 Loss_G: 0.7603 acc: 78.1%\n",
      "[BATCH 96/149] Loss_D: 0.7256 Loss_G: 0.7643 acc: 84.4%\n",
      "[EPOCH 6950] TEST ACC is : 72.3%\n",
      "[BATCH 97/149] Loss_D: 0.7692 Loss_G: 0.7565 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.7249 Loss_G: 0.7520 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7094 Loss_G: 0.7434 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7429 Loss_G: 0.7551 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.7576 Loss_G: 0.7674 acc: 79.7%\n",
      "[BATCH 102/149] Loss_D: 0.7569 Loss_G: 0.7569 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.6781 Loss_G: 0.7316 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.7311 Loss_G: 0.7246 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.7148 Loss_G: 0.7211 acc: 76.6%\n",
      "[BATCH 106/149] Loss_D: 0.6856 Loss_G: 0.7157 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.7416 Loss_G: 0.7490 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7655 Loss_G: 0.7564 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7562 Loss_G: 0.7786 acc: 78.1%\n",
      "[BATCH 110/149] Loss_D: 0.7095 Loss_G: 0.7485 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7103 Loss_G: 0.7340 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.7370 Loss_G: 0.7406 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.7181 Loss_G: 0.7386 acc: 81.2%\n",
      "[BATCH 114/149] Loss_D: 0.6760 Loss_G: 0.7424 acc: 78.1%\n",
      "[BATCH 115/149] Loss_D: 0.7004 Loss_G: 0.7255 acc: 82.8%\n",
      "[BATCH 116/149] Loss_D: 0.7104 Loss_G: 0.7275 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.6996 Loss_G: 0.7329 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.7336 Loss_G: 0.7370 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7361 Loss_G: 0.7479 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.7314 Loss_G: 0.7562 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7102 Loss_G: 0.7595 acc: 81.2%\n",
      "[BATCH 122/149] Loss_D: 0.7145 Loss_G: 0.7549 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7420 Loss_G: 0.7592 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.7095 Loss_G: 0.7322 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.7610 Loss_G: 0.7403 acc: 79.7%\n",
      "[BATCH 126/149] Loss_D: 0.7090 Loss_G: 0.7369 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.7303 Loss_G: 0.7610 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7135 Loss_G: 0.7534 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.6996 Loss_G: 0.7172 acc: 81.2%\n",
      "[BATCH 130/149] Loss_D: 0.7319 Loss_G: 0.7339 acc: 78.1%\n",
      "[BATCH 131/149] Loss_D: 0.6844 Loss_G: 0.7229 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.7141 Loss_G: 0.7202 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.7511 Loss_G: 0.7547 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7179 Loss_G: 0.7482 acc: 81.2%\n",
      "[BATCH 135/149] Loss_D: 0.7151 Loss_G: 0.7513 acc: 75.0%\n",
      "[BATCH 136/149] Loss_D: 0.7141 Loss_G: 0.7325 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.7141 Loss_G: 0.7352 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.7853 Loss_G: 0.7664 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.7728 Loss_G: 0.7923 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7200 Loss_G: 0.7872 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7480 Loss_G: 0.7567 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7294 Loss_G: 0.7365 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6874 Loss_G: 0.7317 acc: 78.1%\n",
      "[BATCH 144/149] Loss_D: 0.7169 Loss_G: 0.7167 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7317 Loss_G: 0.7328 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.7647 Loss_G: 0.7457 acc: 81.2%\n",
      "[EPOCH 7000] TEST ACC is : 72.1%\n",
      "[BATCH 147/149] Loss_D: 0.7187 Loss_G: 0.7543 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.7501 Loss_G: 0.7713 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7205 Loss_G: 0.7616 acc: 79.7%\n",
      "-----THE [47/50] epoch end-----\n",
      "-----THE [48/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7184 Loss_G: 0.7441 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7897 Loss_G: 0.7633 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.7339 Loss_G: 0.7493 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.7304 Loss_G: 0.7581 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.7127 Loss_G: 0.7536 acc: 78.1%\n",
      "[BATCH 6/149] Loss_D: 0.7300 Loss_G: 0.7298 acc: 79.7%\n",
      "[BATCH 7/149] Loss_D: 0.7242 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6745 Loss_G: 0.7250 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.7151 Loss_G: 0.7362 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7076 Loss_G: 0.7413 acc: 81.2%\n",
      "[BATCH 11/149] Loss_D: 0.7706 Loss_G: 0.7516 acc: 71.9%\n",
      "[BATCH 12/149] Loss_D: 0.7290 Loss_G: 0.7349 acc: 76.6%\n",
      "[BATCH 13/149] Loss_D: 0.7347 Loss_G: 0.7370 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.7592 Loss_G: 0.7457 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.7562 Loss_G: 0.7627 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7111 Loss_G: 0.7501 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6941 Loss_G: 0.7213 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.7315 Loss_G: 0.7398 acc: 75.0%\n",
      "[BATCH 19/149] Loss_D: 0.7568 Loss_G: 0.7465 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.7065 Loss_G: 0.7373 acc: 81.2%\n",
      "[BATCH 21/149] Loss_D: 0.7032 Loss_G: 0.7431 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.7198 Loss_G: 0.7464 acc: 79.7%\n",
      "[BATCH 23/149] Loss_D: 0.7247 Loss_G: 0.7405 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7364 Loss_G: 0.7394 acc: 76.6%\n",
      "[BATCH 25/149] Loss_D: 0.7073 Loss_G: 0.7342 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6845 Loss_G: 0.7263 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7234 Loss_G: 0.7496 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6962 Loss_G: 0.7474 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.7006 Loss_G: 0.7488 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7450 Loss_G: 0.7620 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6966 Loss_G: 0.7474 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.7023 Loss_G: 0.7462 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7688 Loss_G: 0.7610 acc: 78.1%\n",
      "[BATCH 34/149] Loss_D: 0.7151 Loss_G: 0.7319 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.7184 Loss_G: 0.7335 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7199 Loss_G: 0.7386 acc: 82.8%\n",
      "[BATCH 37/149] Loss_D: 0.7255 Loss_G: 0.7340 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.7178 Loss_G: 0.7265 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.7361 Loss_G: 0.7567 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7554 Loss_G: 0.7513 acc: 73.4%\n",
      "[BATCH 41/149] Loss_D: 0.7012 Loss_G: 0.7409 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7516 Loss_G: 0.7514 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7128 Loss_G: 0.7486 acc: 81.2%\n",
      "[BATCH 44/149] Loss_D: 0.7381 Loss_G: 0.7490 acc: 79.7%\n",
      "[BATCH 45/149] Loss_D: 0.6951 Loss_G: 0.7190 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.7239 Loss_G: 0.7196 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.7480 Loss_G: 0.7463 acc: 78.1%\n",
      "[EPOCH 7050] TEST ACC is : 73.6%\n",
      "[BATCH 48/149] Loss_D: 0.7140 Loss_G: 0.7676 acc: 81.2%\n",
      "[BATCH 49/149] Loss_D: 0.7711 Loss_G: 0.7693 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.6927 Loss_G: 0.7551 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7370 Loss_G: 0.7786 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.7320 Loss_G: 0.7610 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.7341 Loss_G: 0.7457 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7205 Loss_G: 0.7335 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.7146 Loss_G: 0.7391 acc: 81.2%\n",
      "[BATCH 56/149] Loss_D: 0.7314 Loss_G: 0.7366 acc: 79.7%\n",
      "[BATCH 57/149] Loss_D: 0.7052 Loss_G: 0.7624 acc: 81.2%\n",
      "[BATCH 58/149] Loss_D: 0.7872 Loss_G: 0.7702 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6903 Loss_G: 0.7491 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6935 Loss_G: 0.7247 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.7096 Loss_G: 0.7377 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7256 Loss_G: 0.7202 acc: 95.3%\n",
      "[BATCH 63/149] Loss_D: 0.7505 Loss_G: 0.7331 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7199 Loss_G: 0.7333 acc: 79.7%\n",
      "[BATCH 65/149] Loss_D: 0.8394 Loss_G: 0.8060 acc: 79.7%\n",
      "[BATCH 66/149] Loss_D: 0.7733 Loss_G: 0.7911 acc: 79.7%\n",
      "[BATCH 67/149] Loss_D: 0.7195 Loss_G: 0.7464 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.7132 Loss_G: 0.7420 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7511 Loss_G: 0.7608 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.7125 Loss_G: 0.7330 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6987 Loss_G: 0.7460 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7177 Loss_G: 0.7608 acc: 82.8%\n",
      "[BATCH 73/149] Loss_D: 0.7245 Loss_G: 0.7263 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.6992 Loss_G: 0.7280 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.7211 Loss_G: 0.7302 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7711 Loss_G: 0.7452 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.7275 Loss_G: 0.7471 acc: 75.0%\n",
      "[BATCH 78/149] Loss_D: 0.7770 Loss_G: 0.7474 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.7457 Loss_G: 0.7553 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.6941 Loss_G: 0.7373 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.7290 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7462 Loss_G: 0.7542 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.7210 Loss_G: 0.7492 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.7360 Loss_G: 0.7504 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.7383 Loss_G: 0.7449 acc: 96.9%\n",
      "[BATCH 86/149] Loss_D: 0.7163 Loss_G: 0.7451 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.7654 Loss_G: 0.7528 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.6990 Loss_G: 0.7507 acc: 78.1%\n",
      "[BATCH 89/149] Loss_D: 0.7307 Loss_G: 0.7467 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7683 Loss_G: 0.7659 acc: 78.1%\n",
      "[BATCH 91/149] Loss_D: 0.7092 Loss_G: 0.7362 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.7354 Loss_G: 0.7511 acc: 76.6%\n",
      "[BATCH 93/149] Loss_D: 0.7190 Loss_G: 0.7403 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.7544 Loss_G: 0.7646 acc: 81.2%\n",
      "[BATCH 95/149] Loss_D: 0.7460 Loss_G: 0.7481 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7129 Loss_G: 0.7527 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.7303 Loss_G: 0.7391 acc: 82.8%\n",
      "[EPOCH 7100] TEST ACC is : 71.9%\n",
      "[BATCH 98/149] Loss_D: 0.7505 Loss_G: 0.7679 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7343 Loss_G: 0.7716 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.7302 Loss_G: 0.7570 acc: 78.1%\n",
      "[BATCH 101/149] Loss_D: 0.7396 Loss_G: 0.7735 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.7137 Loss_G: 0.7660 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6829 Loss_G: 0.7406 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7532 Loss_G: 0.7315 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.7130 Loss_G: 0.7318 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7172 Loss_G: 0.7343 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7020 Loss_G: 0.7309 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7806 Loss_G: 0.7633 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.7053 Loss_G: 0.7580 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.7088 Loss_G: 0.7388 acc: 79.7%\n",
      "[BATCH 111/149] Loss_D: 0.7571 Loss_G: 0.7302 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.6847 Loss_G: 0.7274 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6883 Loss_G: 0.7331 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6872 Loss_G: 0.7198 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.7335 Loss_G: 0.7341 acc: 79.7%\n",
      "[BATCH 116/149] Loss_D: 0.6830 Loss_G: 0.7343 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7011 Loss_G: 0.7395 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7114 Loss_G: 0.7402 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7616 Loss_G: 0.7296 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.7389 Loss_G: 0.7358 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7563 Loss_G: 0.7246 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.7347 Loss_G: 0.7320 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.7843 Loss_G: 0.7626 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.7382 Loss_G: 0.7416 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.7008 Loss_G: 0.7473 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7319 Loss_G: 0.7436 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7179 Loss_G: 0.7623 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7053 Loss_G: 0.7634 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.7122 Loss_G: 0.7626 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7439 Loss_G: 0.7671 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7416 Loss_G: 0.7703 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.6843 Loss_G: 0.7582 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7172 Loss_G: 0.7518 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7114 Loss_G: 0.7572 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.7195 Loss_G: 0.7192 acc: 79.7%\n",
      "[BATCH 136/149] Loss_D: 0.7173 Loss_G: 0.7343 acc: 81.2%\n",
      "[BATCH 137/149] Loss_D: 0.7362 Loss_G: 0.7421 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.7479 Loss_G: 0.7535 acc: 82.8%\n",
      "[BATCH 139/149] Loss_D: 0.7079 Loss_G: 0.7333 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7550 Loss_G: 0.7537 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.7585 Loss_G: 0.7280 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7588 Loss_G: 0.7565 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.7185 Loss_G: 0.7259 acc: 79.7%\n",
      "[BATCH 144/149] Loss_D: 0.7406 Loss_G: 0.7326 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7729 Loss_G: 0.7524 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.7465 Loss_G: 0.7508 acc: 76.6%\n",
      "[BATCH 147/149] Loss_D: 0.7476 Loss_G: 0.7611 acc: 81.2%\n",
      "[EPOCH 7150] TEST ACC is : 73.0%\n",
      "[BATCH 148/149] Loss_D: 0.7525 Loss_G: 0.7622 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.7233 Loss_G: 0.7464 acc: 87.5%\n",
      "-----THE [48/50] epoch end-----\n",
      "-----THE [49/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7323 Loss_G: 0.7510 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7639 Loss_G: 0.7914 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.7432 Loss_G: 0.7613 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.7119 Loss_G: 0.7543 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.7412 Loss_G: 0.7616 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7346 Loss_G: 0.7527 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.7442 Loss_G: 0.7496 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.7346 Loss_G: 0.7538 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.6684 Loss_G: 0.7203 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6932 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7497 Loss_G: 0.7706 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.7285 Loss_G: 0.7486 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.7228 Loss_G: 0.7434 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.7412 Loss_G: 0.7432 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6928 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6834 Loss_G: 0.7215 acc: 81.2%\n",
      "[BATCH 17/149] Loss_D: 0.7292 Loss_G: 0.7387 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.7145 Loss_G: 0.7387 acc: 68.8%\n",
      "[BATCH 19/149] Loss_D: 0.6890 Loss_G: 0.7221 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.7165 Loss_G: 0.7341 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7435 Loss_G: 0.7479 acc: 76.6%\n",
      "[BATCH 22/149] Loss_D: 0.7029 Loss_G: 0.7388 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7604 Loss_G: 0.7545 acc: 82.8%\n",
      "[BATCH 24/149] Loss_D: 0.7422 Loss_G: 0.7538 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7492 Loss_G: 0.7586 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6886 Loss_G: 0.7499 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7226 Loss_G: 0.7436 acc: 84.4%\n",
      "[BATCH 28/149] Loss_D: 0.7062 Loss_G: 0.7520 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7239 Loss_G: 0.7396 acc: 79.7%\n",
      "[BATCH 30/149] Loss_D: 0.7293 Loss_G: 0.7340 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6979 Loss_G: 0.7497 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.7710 Loss_G: 0.7644 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.7478 Loss_G: 0.7545 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.7255 Loss_G: 0.7597 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.7413 Loss_G: 0.7483 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.7743 Loss_G: 0.7630 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7449 Loss_G: 0.7611 acc: 81.2%\n",
      "[BATCH 38/149] Loss_D: 0.7870 Loss_G: 0.7611 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7155 Loss_G: 0.7611 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.7345 Loss_G: 0.7598 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7134 Loss_G: 0.7582 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.7224 Loss_G: 0.7408 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.7110 Loss_G: 0.7348 acc: 81.2%\n",
      "[BATCH 44/149] Loss_D: 0.7289 Loss_G: 0.7434 acc: 79.7%\n",
      "[BATCH 45/149] Loss_D: 0.7429 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6824 Loss_G: 0.7257 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7535 Loss_G: 0.7540 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7584 Loss_G: 0.7782 acc: 82.8%\n",
      "[EPOCH 7200] TEST ACC is : 73.0%\n",
      "[BATCH 49/149] Loss_D: 0.7718 Loss_G: 0.7683 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.7137 Loss_G: 0.7610 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.7547 Loss_G: 0.7644 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.7543 Loss_G: 0.7707 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7142 Loss_G: 0.7584 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7234 Loss_G: 0.7503 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.7500 Loss_G: 0.7559 acc: 81.2%\n",
      "[BATCH 56/149] Loss_D: 0.7559 Loss_G: 0.7701 acc: 79.7%\n",
      "[BATCH 57/149] Loss_D: 0.7675 Loss_G: 0.7545 acc: 78.1%\n",
      "[BATCH 58/149] Loss_D: 0.7110 Loss_G: 0.7412 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.7161 Loss_G: 0.7332 acc: 81.2%\n",
      "[BATCH 60/149] Loss_D: 0.7752 Loss_G: 0.7514 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.7107 Loss_G: 0.7423 acc: 81.2%\n",
      "[BATCH 62/149] Loss_D: 0.7347 Loss_G: 0.7611 acc: 79.7%\n",
      "[BATCH 63/149] Loss_D: 0.7363 Loss_G: 0.7665 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.7518 Loss_G: 0.7594 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7439 Loss_G: 0.7522 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.7186 Loss_G: 0.7438 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7110 Loss_G: 0.7241 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7037 Loss_G: 0.7350 acc: 81.2%\n",
      "[BATCH 69/149] Loss_D: 0.7298 Loss_G: 0.7320 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.7499 Loss_G: 0.7642 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7162 Loss_G: 0.7527 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7679 Loss_G: 0.7677 acc: 78.1%\n",
      "[BATCH 73/149] Loss_D: 0.7062 Loss_G: 0.7639 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7488 Loss_G: 0.7637 acc: 73.4%\n",
      "[BATCH 75/149] Loss_D: 0.7720 Loss_G: 0.7511 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7031 Loss_G: 0.7294 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.7178 Loss_G: 0.7218 acc: 73.4%\n",
      "[BATCH 78/149] Loss_D: 0.7010 Loss_G: 0.7218 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.7332 Loss_G: 0.7379 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.7038 Loss_G: 0.7386 acc: 76.6%\n",
      "[BATCH 81/149] Loss_D: 0.7009 Loss_G: 0.7413 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7382 Loss_G: 0.7216 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.7295 Loss_G: 0.7285 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7100 Loss_G: 0.7427 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6985 Loss_G: 0.7222 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7463 Loss_G: 0.7362 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7441 Loss_G: 0.7424 acc: 81.2%\n",
      "[BATCH 88/149] Loss_D: 0.7426 Loss_G: 0.7369 acc: 75.0%\n",
      "[BATCH 89/149] Loss_D: 0.6755 Loss_G: 0.7303 acc: 95.3%\n",
      "[BATCH 90/149] Loss_D: 0.7266 Loss_G: 0.7306 acc: 71.9%\n",
      "[BATCH 91/149] Loss_D: 0.7189 Loss_G: 0.7250 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7505 Loss_G: 0.7452 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.7145 Loss_G: 0.7543 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.7263 Loss_G: 0.7459 acc: 82.8%\n",
      "[BATCH 95/149] Loss_D: 0.7276 Loss_G: 0.7359 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7253 Loss_G: 0.7311 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.7712 Loss_G: 0.7531 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.7534 Loss_G: 0.7777 acc: 92.2%\n",
      "[EPOCH 7250] TEST ACC is : 71.7%\n",
      "[BATCH 99/149] Loss_D: 0.7146 Loss_G: 0.7587 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7175 Loss_G: 0.7426 acc: 76.6%\n",
      "[BATCH 101/149] Loss_D: 0.7333 Loss_G: 0.7505 acc: 78.1%\n",
      "[BATCH 102/149] Loss_D: 0.7500 Loss_G: 0.7438 acc: 84.4%\n",
      "[BATCH 103/149] Loss_D: 0.7492 Loss_G: 0.7451 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7681 Loss_G: 0.7492 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6829 Loss_G: 0.7467 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7901 Loss_G: 0.7771 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6870 Loss_G: 0.7445 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6956 Loss_G: 0.7370 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7117 Loss_G: 0.7395 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.7010 Loss_G: 0.7463 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.7319 Loss_G: 0.7406 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7480 Loss_G: 0.7380 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7270 Loss_G: 0.7424 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7468 Loss_G: 0.7659 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.6880 Loss_G: 0.7219 acc: 81.2%\n",
      "[BATCH 116/149] Loss_D: 0.7322 Loss_G: 0.7376 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.7065 Loss_G: 0.7625 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.7179 Loss_G: 0.7255 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6918 Loss_G: 0.7246 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.7124 Loss_G: 0.7397 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.7119 Loss_G: 0.7341 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.7495 Loss_G: 0.7357 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7081 Loss_G: 0.7327 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.7244 Loss_G: 0.7336 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.7147 Loss_G: 0.7392 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.7440 Loss_G: 0.7440 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6797 Loss_G: 0.7127 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7408 Loss_G: 0.7515 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.7193 Loss_G: 0.7499 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7073 Loss_G: 0.7369 acc: 79.7%\n",
      "[BATCH 131/149] Loss_D: 0.7706 Loss_G: 0.7632 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7419 Loss_G: 0.7663 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7512 Loss_G: 0.7734 acc: 79.7%\n",
      "[BATCH 134/149] Loss_D: 0.7329 Loss_G: 0.7553 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7458 Loss_G: 0.7661 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7476 Loss_G: 0.7656 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6953 Loss_G: 0.7347 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.7026 Loss_G: 0.7290 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7202 Loss_G: 0.7439 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7537 Loss_G: 0.7576 acc: 79.7%\n",
      "[BATCH 141/149] Loss_D: 0.7128 Loss_G: 0.7478 acc: 81.2%\n",
      "[BATCH 142/149] Loss_D: 0.7208 Loss_G: 0.7416 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7344 Loss_G: 0.7480 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.7041 Loss_G: 0.7364 acc: 82.8%\n",
      "[BATCH 145/149] Loss_D: 0.7375 Loss_G: 0.7285 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7342 Loss_G: 0.7401 acc: 81.2%\n",
      "[BATCH 147/149] Loss_D: 0.6919 Loss_G: 0.7294 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7095 Loss_G: 0.7301 acc: 95.3%\n",
      "[EPOCH 7300] TEST ACC is : 73.0%\n",
      "[BATCH 149/149] Loss_D: 0.6997 Loss_G: 0.7298 acc: 89.1%\n",
      "-----THE [49/50] epoch end-----\n",
      "-----THE [50/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7117 Loss_G: 0.7314 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7607 Loss_G: 0.7253 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.6848 Loss_G: 0.7221 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6735 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.7268 Loss_G: 0.7290 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.7126 Loss_G: 0.7435 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7203 Loss_G: 0.7409 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.7595 Loss_G: 0.7550 acc: 76.6%\n",
      "[BATCH 9/149] Loss_D: 0.7498 Loss_G: 0.7498 acc: 78.1%\n",
      "[BATCH 10/149] Loss_D: 0.6913 Loss_G: 0.7301 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7088 Loss_G: 0.7406 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6837 Loss_G: 0.7257 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.6991 Loss_G: 0.7221 acc: 81.2%\n",
      "[BATCH 14/149] Loss_D: 0.7357 Loss_G: 0.7368 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.7483 Loss_G: 0.7659 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.7629 Loss_G: 0.7576 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.7036 Loss_G: 0.7210 acc: 81.2%\n",
      "[BATCH 18/149] Loss_D: 0.7352 Loss_G: 0.7327 acc: 81.2%\n",
      "[BATCH 19/149] Loss_D: 0.7271 Loss_G: 0.7532 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.7432 Loss_G: 0.7699 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6912 Loss_G: 0.7517 acc: 79.7%\n",
      "[BATCH 22/149] Loss_D: 0.7437 Loss_G: 0.7362 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7138 Loss_G: 0.7334 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7332 Loss_G: 0.7278 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.7499 Loss_G: 0.7429 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.7479 Loss_G: 0.7467 acc: 79.7%\n",
      "[BATCH 27/149] Loss_D: 0.7220 Loss_G: 0.7316 acc: 81.2%\n",
      "[BATCH 28/149] Loss_D: 0.7082 Loss_G: 0.7471 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7461 Loss_G: 0.7522 acc: 81.2%\n",
      "[BATCH 30/149] Loss_D: 0.7112 Loss_G: 0.7526 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.7095 Loss_G: 0.7490 acc: 79.7%\n",
      "[BATCH 32/149] Loss_D: 0.7331 Loss_G: 0.7480 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7022 Loss_G: 0.7439 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.7064 Loss_G: 0.7318 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.6931 Loss_G: 0.7368 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6944 Loss_G: 0.7379 acc: 93.8%\n",
      "[BATCH 37/149] Loss_D: 0.7269 Loss_G: 0.7370 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.7210 Loss_G: 0.7615 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7824 Loss_G: 0.7888 acc: 95.3%\n",
      "[BATCH 40/149] Loss_D: 0.7341 Loss_G: 0.7703 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7206 Loss_G: 0.7541 acc: 81.2%\n",
      "[BATCH 42/149] Loss_D: 0.7016 Loss_G: 0.7256 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7868 Loss_G: 0.7761 acc: 81.2%\n",
      "[BATCH 44/149] Loss_D: 0.7452 Loss_G: 0.7576 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.7205 Loss_G: 0.7525 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.6959 Loss_G: 0.7347 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.7452 Loss_G: 0.7618 acc: 82.8%\n",
      "[BATCH 48/149] Loss_D: 0.7012 Loss_G: 0.7417 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7297 Loss_G: 0.7495 acc: 90.6%\n",
      "[EPOCH 7350] TEST ACC is : 72.5%\n",
      "[BATCH 50/149] Loss_D: 0.7187 Loss_G: 0.7394 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7183 Loss_G: 0.7510 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.7598 Loss_G: 0.7533 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7403 Loss_G: 0.7583 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.7478 Loss_G: 0.7561 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6742 Loss_G: 0.7290 acc: 79.7%\n",
      "[BATCH 56/149] Loss_D: 0.7462 Loss_G: 0.7571 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.8053 Loss_G: 0.7720 acc: 75.0%\n",
      "[BATCH 58/149] Loss_D: 0.7057 Loss_G: 0.7318 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.7522 Loss_G: 0.7350 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6915 Loss_G: 0.7213 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.7098 Loss_G: 0.7313 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7013 Loss_G: 0.7310 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7577 Loss_G: 0.7376 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.7224 Loss_G: 0.7345 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.7068 Loss_G: 0.7343 acc: 81.2%\n",
      "[BATCH 66/149] Loss_D: 0.7425 Loss_G: 0.7391 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.7026 Loss_G: 0.7606 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7575 Loss_G: 0.7503 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7157 Loss_G: 0.7528 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.7300 Loss_G: 0.7492 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.7141 Loss_G: 0.7388 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6928 Loss_G: 0.7386 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6953 Loss_G: 0.7274 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7180 Loss_G: 0.7481 acc: 79.7%\n",
      "[BATCH 75/149] Loss_D: 0.7565 Loss_G: 0.7681 acc: 82.8%\n",
      "[BATCH 76/149] Loss_D: 0.6936 Loss_G: 0.7303 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6845 Loss_G: 0.7389 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.7485 Loss_G: 0.7466 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.7465 Loss_G: 0.7487 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6799 Loss_G: 0.7342 acc: 78.1%\n",
      "[BATCH 81/149] Loss_D: 0.7074 Loss_G: 0.7204 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7255 Loss_G: 0.7420 acc: 81.2%\n",
      "[BATCH 83/149] Loss_D: 0.7439 Loss_G: 0.7388 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6802 Loss_G: 0.7293 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.7085 Loss_G: 0.7360 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6849 Loss_G: 0.7321 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.7517 Loss_G: 0.7370 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.7673 Loss_G: 0.7572 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.7252 Loss_G: 0.7428 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7137 Loss_G: 0.7370 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.7521 Loss_G: 0.7323 acc: 81.2%\n",
      "[BATCH 92/149] Loss_D: 0.7545 Loss_G: 0.7467 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.7409 Loss_G: 0.7582 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.7487 Loss_G: 0.7737 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7245 Loss_G: 0.7747 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7335 Loss_G: 0.7299 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.7424 Loss_G: 0.7544 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.7138 Loss_G: 0.7493 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7309 Loss_G: 0.7383 acc: 76.6%\n",
      "[EPOCH 7400] TEST ACC is : 73.8%\n",
      "[BATCH 100/149] Loss_D: 0.7485 Loss_G: 0.7281 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7225 Loss_G: 0.7309 acc: 81.2%\n",
      "[BATCH 102/149] Loss_D: 0.7375 Loss_G: 0.7621 acc: 78.1%\n",
      "[BATCH 103/149] Loss_D: 0.8073 Loss_G: 0.7384 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.6925 Loss_G: 0.7230 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7326 Loss_G: 0.7533 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.7068 Loss_G: 0.7550 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.7225 Loss_G: 0.7463 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7406 Loss_G: 0.7478 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7419 Loss_G: 0.7511 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6883 Loss_G: 0.7311 acc: 79.7%\n",
      "[BATCH 111/149] Loss_D: 0.7324 Loss_G: 0.7485 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.7082 Loss_G: 0.7330 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7067 Loss_G: 0.7489 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7248 Loss_G: 0.7385 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.7478 Loss_G: 0.7491 acc: 95.3%\n",
      "[BATCH 116/149] Loss_D: 0.6771 Loss_G: 0.7278 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7359 Loss_G: 0.7472 acc: 81.2%\n",
      "[BATCH 118/149] Loss_D: 0.7654 Loss_G: 0.7547 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7163 Loss_G: 0.7245 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.7264 Loss_G: 0.7271 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.7241 Loss_G: 0.7547 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.7319 Loss_G: 0.7424 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6802 Loss_G: 0.7356 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.7393 Loss_G: 0.7396 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.7414 Loss_G: 0.7438 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7262 Loss_G: 0.7233 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7326 Loss_G: 0.7387 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7624 Loss_G: 0.7519 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.7542 Loss_G: 0.7525 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7649 Loss_G: 0.7748 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.7489 Loss_G: 0.7602 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.7081 Loss_G: 0.7347 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7718 Loss_G: 0.7558 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7509 Loss_G: 0.7585 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7446 Loss_G: 0.7474 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.7089 Loss_G: 0.7614 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7444 Loss_G: 0.7705 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.7509 Loss_G: 0.7720 acc: 78.1%\n",
      "[BATCH 139/149] Loss_D: 0.7428 Loss_G: 0.7734 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7485 Loss_G: 0.7682 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.7518 Loss_G: 0.7601 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7391 Loss_G: 0.7477 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7016 Loss_G: 0.7483 acc: 79.7%\n",
      "[BATCH 144/149] Loss_D: 0.7033 Loss_G: 0.7469 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.7129 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7118 Loss_G: 0.7564 acc: 81.2%\n",
      "[BATCH 147/149] Loss_D: 0.6962 Loss_G: 0.7434 acc: 82.8%\n",
      "[BATCH 148/149] Loss_D: 0.7322 Loss_G: 0.7352 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.7285 Loss_G: 0.7485 acc: 82.8%\n",
      "[EPOCH 7450] TEST ACC is : 73.0%\n",
      "-----THE [50/50] epoch end-----\n",
      "The 4 * 50 epochs train starts:\n",
      "-----THE [1/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7225 Loss_G: 0.7499 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7415 Loss_G: 0.7610 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7166 Loss_G: 0.7617 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.7415 Loss_G: 0.7517 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7361 Loss_G: 0.7633 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.7189 Loss_G: 0.7521 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7421 Loss_G: 0.7538 acc: 76.6%\n",
      "[BATCH 8/149] Loss_D: 0.7292 Loss_G: 0.7475 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6954 Loss_G: 0.7344 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6915 Loss_G: 0.7113 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6850 Loss_G: 0.7191 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.7163 Loss_G: 0.7260 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7316 Loss_G: 0.7351 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.7680 Loss_G: 0.7424 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.7403 Loss_G: 0.7384 acc: 78.1%\n",
      "[BATCH 16/149] Loss_D: 0.7185 Loss_G: 0.7535 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.7098 Loss_G: 0.7284 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6739 Loss_G: 0.7175 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7158 Loss_G: 0.7264 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7091 Loss_G: 0.7280 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7670 Loss_G: 0.7513 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7537 Loss_G: 0.7332 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7046 Loss_G: 0.7488 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7140 Loss_G: 0.7254 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7357 Loss_G: 0.7521 acc: 82.8%\n",
      "[BATCH 26/149] Loss_D: 0.6771 Loss_G: 0.7220 acc: 79.7%\n",
      "[BATCH 27/149] Loss_D: 0.7514 Loss_G: 0.7280 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7283 Loss_G: 0.7422 acc: 73.4%\n",
      "[BATCH 29/149] Loss_D: 0.7361 Loss_G: 0.7411 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7054 Loss_G: 0.7479 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.6987 Loss_G: 0.7395 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7320 Loss_G: 0.7534 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.7267 Loss_G: 0.7724 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7563 Loss_G: 0.7522 acc: 81.2%\n",
      "[BATCH 35/149] Loss_D: 0.7116 Loss_G: 0.7602 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.7452 Loss_G: 0.7410 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.8175 Loss_G: 0.7718 acc: 73.4%\n",
      "[BATCH 38/149] Loss_D: 0.7181 Loss_G: 0.7527 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.6916 Loss_G: 0.7408 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6870 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.7218 Loss_G: 0.7269 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.7164 Loss_G: 0.7386 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.7269 Loss_G: 0.7654 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6903 Loss_G: 0.7404 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7226 Loss_G: 0.7328 acc: 79.7%\n",
      "[BATCH 46/149] Loss_D: 0.7084 Loss_G: 0.7240 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6807 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6883 Loss_G: 0.7279 acc: 81.2%\n",
      "[BATCH 49/149] Loss_D: 0.7458 Loss_G: 0.7494 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7102 Loss_G: 0.7202 acc: 85.9%\n",
      "[EPOCH 50] TEST ACC is : 72.7%\n",
      "[BATCH 51/149] Loss_D: 0.7455 Loss_G: 0.7468 acc: 81.2%\n",
      "[BATCH 52/149] Loss_D: 0.7173 Loss_G: 0.7456 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7099 Loss_G: 0.7311 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.6851 Loss_G: 0.7249 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6947 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7390 Loss_G: 0.7308 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7420 Loss_G: 0.7556 acc: 81.2%\n",
      "[BATCH 58/149] Loss_D: 0.7371 Loss_G: 0.7657 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7857 Loss_G: 0.7567 acc: 78.1%\n",
      "[BATCH 60/149] Loss_D: 0.7174 Loss_G: 0.7547 acc: 79.7%\n",
      "[BATCH 61/149] Loss_D: 0.6952 Loss_G: 0.7364 acc: 81.2%\n",
      "[BATCH 62/149] Loss_D: 0.7246 Loss_G: 0.7314 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.6985 Loss_G: 0.7101 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7100 Loss_G: 0.7347 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7436 Loss_G: 0.7462 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.7185 Loss_G: 0.7402 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7564 Loss_G: 0.7585 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7250 Loss_G: 0.7489 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7124 Loss_G: 0.7459 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.7200 Loss_G: 0.7402 acc: 79.7%\n",
      "[BATCH 71/149] Loss_D: 0.7699 Loss_G: 0.7541 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7545 Loss_G: 0.7702 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7346 Loss_G: 0.7644 acc: 79.7%\n",
      "[BATCH 74/149] Loss_D: 0.7255 Loss_G: 0.7505 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7274 Loss_G: 0.7486 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7256 Loss_G: 0.7475 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.6985 Loss_G: 0.7456 acc: 95.3%\n",
      "[BATCH 78/149] Loss_D: 0.7626 Loss_G: 0.7647 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7250 Loss_G: 0.7544 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.7201 Loss_G: 0.7576 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.7657 Loss_G: 0.7734 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.7557 Loss_G: 0.7550 acc: 75.0%\n",
      "[BATCH 83/149] Loss_D: 0.7261 Loss_G: 0.7433 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7345 Loss_G: 0.7296 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7371 Loss_G: 0.7579 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7552 Loss_G: 0.7641 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7295 Loss_G: 0.7475 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7088 Loss_G: 0.7570 acc: 95.3%\n",
      "[BATCH 89/149] Loss_D: 0.7520 Loss_G: 0.7675 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.7154 Loss_G: 0.7214 acc: 78.1%\n",
      "[BATCH 91/149] Loss_D: 0.7223 Loss_G: 0.7436 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7191 Loss_G: 0.7570 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.7083 Loss_G: 0.7657 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7582 Loss_G: 0.7692 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7621 Loss_G: 0.7612 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7206 Loss_G: 0.7374 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.7601 Loss_G: 0.7503 acc: 78.1%\n",
      "[BATCH 98/149] Loss_D: 0.7409 Loss_G: 0.7336 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7333 Loss_G: 0.7382 acc: 79.7%\n",
      "[BATCH 100/149] Loss_D: 0.7619 Loss_G: 0.7379 acc: 89.1%\n",
      "[EPOCH 100] TEST ACC is : 72.3%\n",
      "[BATCH 101/149] Loss_D: 0.7159 Loss_G: 0.7459 acc: 84.4%\n",
      "[BATCH 102/149] Loss_D: 0.7093 Loss_G: 0.7430 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.7216 Loss_G: 0.7460 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.7395 Loss_G: 0.7519 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7227 Loss_G: 0.7417 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.7751 Loss_G: 0.7631 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.7390 Loss_G: 0.7554 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7237 Loss_G: 0.7458 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7400 Loss_G: 0.7488 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.7143 Loss_G: 0.7398 acc: 79.7%\n",
      "[BATCH 111/149] Loss_D: 0.7124 Loss_G: 0.7411 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6807 Loss_G: 0.7249 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7426 Loss_G: 0.7354 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.7146 Loss_G: 0.7273 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.7567 Loss_G: 0.7596 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7088 Loss_G: 0.7464 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7326 Loss_G: 0.7454 acc: 81.2%\n",
      "[BATCH 118/149] Loss_D: 0.7022 Loss_G: 0.7408 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7095 Loss_G: 0.7389 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7007 Loss_G: 0.7137 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7111 Loss_G: 0.7467 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.7384 Loss_G: 0.7419 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7060 Loss_G: 0.7549 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.7138 Loss_G: 0.7338 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.8077 Loss_G: 0.7491 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.7403 Loss_G: 0.7458 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7010 Loss_G: 0.7241 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7541 Loss_G: 0.7494 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.7111 Loss_G: 0.7334 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7609 Loss_G: 0.7379 acc: 79.7%\n",
      "[BATCH 131/149] Loss_D: 0.6897 Loss_G: 0.7313 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7485 Loss_G: 0.7494 acc: 95.3%\n",
      "[BATCH 133/149] Loss_D: 0.7071 Loss_G: 0.7546 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7210 Loss_G: 0.7374 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7348 Loss_G: 0.7628 acc: 82.8%\n",
      "[BATCH 136/149] Loss_D: 0.7557 Loss_G: 0.7553 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7374 Loss_G: 0.7410 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.7211 Loss_G: 0.7535 acc: 82.8%\n",
      "[BATCH 139/149] Loss_D: 0.6925 Loss_G: 0.7288 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7296 Loss_G: 0.7555 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.7584 Loss_G: 0.7617 acc: 81.2%\n",
      "[BATCH 142/149] Loss_D: 0.7073 Loss_G: 0.7578 acc: 76.6%\n",
      "[BATCH 143/149] Loss_D: 0.7707 Loss_G: 0.7735 acc: 81.2%\n",
      "[BATCH 144/149] Loss_D: 0.7156 Loss_G: 0.7579 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7254 Loss_G: 0.7464 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7816 Loss_G: 0.7573 acc: 81.2%\n",
      "[BATCH 147/149] Loss_D: 0.7067 Loss_G: 0.7348 acc: 81.2%\n",
      "[BATCH 148/149] Loss_D: 0.7571 Loss_G: 0.7554 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7389 Loss_G: 0.7519 acc: 92.2%\n",
      "-----THE [1/50] epoch end-----\n",
      "-----THE [2/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7361 Loss_G: 0.7493 acc: 81.2%\n",
      "[EPOCH 150] TEST ACC is : 72.7%\n",
      "[BATCH 2/149] Loss_D: 0.6954 Loss_G: 0.7384 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.6977 Loss_G: 0.7230 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.7153 Loss_G: 0.7355 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.7649 Loss_G: 0.7581 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7234 Loss_G: 0.7509 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.6994 Loss_G: 0.7197 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.7510 Loss_G: 0.7437 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.7537 Loss_G: 0.7653 acc: 95.3%\n",
      "[BATCH 10/149] Loss_D: 0.6894 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7719 Loss_G: 0.7363 acc: 79.7%\n",
      "[BATCH 12/149] Loss_D: 0.7102 Loss_G: 0.7448 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6829 Loss_G: 0.7299 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.7313 Loss_G: 0.7444 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.7013 Loss_G: 0.7438 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.7536 Loss_G: 0.7379 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.7397 Loss_G: 0.7522 acc: 78.1%\n",
      "[BATCH 18/149] Loss_D: 0.6941 Loss_G: 0.7437 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7750 Loss_G: 0.7575 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.7266 Loss_G: 0.7548 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7083 Loss_G: 0.7256 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7562 Loss_G: 0.7533 acc: 81.2%\n",
      "[BATCH 23/149] Loss_D: 0.7796 Loss_G: 0.7807 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.7493 Loss_G: 0.7640 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7040 Loss_G: 0.7449 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7341 Loss_G: 0.7465 acc: 73.4%\n",
      "[BATCH 27/149] Loss_D: 0.7358 Loss_G: 0.7409 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7502 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7334 Loss_G: 0.7639 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.7032 Loss_G: 0.7471 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.7062 Loss_G: 0.7460 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7025 Loss_G: 0.7563 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.7448 Loss_G: 0.7557 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7575 Loss_G: 0.7516 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7559 Loss_G: 0.7567 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7041 Loss_G: 0.7283 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7071 Loss_G: 0.7384 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.7178 Loss_G: 0.7219 acc: 81.2%\n",
      "[BATCH 39/149] Loss_D: 0.7339 Loss_G: 0.7341 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.7269 Loss_G: 0.7455 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.7395 Loss_G: 0.7385 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.7047 Loss_G: 0.7399 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.7548 Loss_G: 0.7544 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6962 Loss_G: 0.7402 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7172 Loss_G: 0.7363 acc: 95.3%\n",
      "[BATCH 46/149] Loss_D: 0.7911 Loss_G: 0.7558 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.7323 Loss_G: 0.7603 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6984 Loss_G: 0.7365 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7307 Loss_G: 0.7432 acc: 78.1%\n",
      "[BATCH 50/149] Loss_D: 0.7778 Loss_G: 0.7548 acc: 79.7%\n",
      "[BATCH 51/149] Loss_D: 0.6844 Loss_G: 0.7234 acc: 87.5%\n",
      "[EPOCH 200] TEST ACC is : 73.0%\n",
      "[BATCH 52/149] Loss_D: 0.7332 Loss_G: 0.7523 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7509 Loss_G: 0.7621 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.7498 Loss_G: 0.7574 acc: 79.7%\n",
      "[BATCH 55/149] Loss_D: 0.7126 Loss_G: 0.7478 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7066 Loss_G: 0.7302 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7511 Loss_G: 0.7584 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.7281 Loss_G: 0.7299 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.7162 Loss_G: 0.7387 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7373 Loss_G: 0.7314 acc: 78.1%\n",
      "[BATCH 61/149] Loss_D: 0.7399 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7478 Loss_G: 0.7555 acc: 78.1%\n",
      "[BATCH 63/149] Loss_D: 0.7631 Loss_G: 0.7562 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7142 Loss_G: 0.7414 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7131 Loss_G: 0.7646 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7412 Loss_G: 0.7599 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7610 Loss_G: 0.7528 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.7083 Loss_G: 0.7605 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.7443 Loss_G: 0.7610 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.7407 Loss_G: 0.7642 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7571 Loss_G: 0.7874 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7191 Loss_G: 0.7583 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7245 Loss_G: 0.7445 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.7449 Loss_G: 0.7516 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.7377 Loss_G: 0.7478 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.7174 Loss_G: 0.7468 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7217 Loss_G: 0.7574 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7269 Loss_G: 0.7563 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7152 Loss_G: 0.7536 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7402 Loss_G: 0.7324 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.7613 Loss_G: 0.7583 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7065 Loss_G: 0.7534 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.7403 Loss_G: 0.7483 acc: 78.1%\n",
      "[BATCH 84/149] Loss_D: 0.7426 Loss_G: 0.7253 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7372 Loss_G: 0.7530 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7393 Loss_G: 0.7637 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.7256 Loss_G: 0.7523 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.7518 Loss_G: 0.7457 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.7171 Loss_G: 0.7524 acc: 96.9%\n",
      "[BATCH 90/149] Loss_D: 0.7205 Loss_G: 0.7643 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.7468 Loss_G: 0.7664 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7594 Loss_G: 0.7581 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7017 Loss_G: 0.7495 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7474 Loss_G: 0.7444 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.7503 Loss_G: 0.7563 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7270 Loss_G: 0.7584 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6904 Loss_G: 0.7437 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.7465 Loss_G: 0.7536 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7309 Loss_G: 0.7349 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.7104 Loss_G: 0.7374 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.7037 Loss_G: 0.7188 acc: 82.8%\n",
      "[EPOCH 250] TEST ACC is : 73.8%\n",
      "[BATCH 102/149] Loss_D: 0.6854 Loss_G: 0.7266 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7420 Loss_G: 0.7489 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7192 Loss_G: 0.7427 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.7249 Loss_G: 0.7439 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7509 Loss_G: 0.7513 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7224 Loss_G: 0.7457 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6874 Loss_G: 0.7395 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7096 Loss_G: 0.7354 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6890 Loss_G: 0.7233 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.6715 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7516 Loss_G: 0.7292 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7084 Loss_G: 0.7381 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7307 Loss_G: 0.7289 acc: 81.2%\n",
      "[BATCH 115/149] Loss_D: 0.7177 Loss_G: 0.7449 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6800 Loss_G: 0.7210 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.7530 Loss_G: 0.7407 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.7188 Loss_G: 0.7315 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7144 Loss_G: 0.7415 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7515 Loss_G: 0.7512 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7013 Loss_G: 0.7279 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.7207 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.7224 Loss_G: 0.7458 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6981 Loss_G: 0.7353 acc: 93.8%\n",
      "[BATCH 125/149] Loss_D: 0.7165 Loss_G: 0.7455 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7263 Loss_G: 0.7367 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7129 Loss_G: 0.7485 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6999 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6933 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7666 Loss_G: 0.7775 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.7306 Loss_G: 0.7510 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6948 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7226 Loss_G: 0.7248 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7354 Loss_G: 0.7533 acc: 79.7%\n",
      "[BATCH 135/149] Loss_D: 0.7404 Loss_G: 0.7405 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.7024 Loss_G: 0.7299 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.7212 Loss_G: 0.7512 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6918 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7410 Loss_G: 0.7449 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.7251 Loss_G: 0.7473 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.7131 Loss_G: 0.7589 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.7608 Loss_G: 0.7583 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.7012 Loss_G: 0.7350 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.7337 Loss_G: 0.7467 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6856 Loss_G: 0.7088 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.6942 Loss_G: 0.7439 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.6994 Loss_G: 0.7110 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.7611 Loss_G: 0.7433 acc: 79.7%\n",
      "[BATCH 149/149] Loss_D: 0.7291 Loss_G: 0.7277 acc: 92.2%\n",
      "-----THE [2/50] epoch end-----\n",
      "-----THE [3/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7228 Loss_G: 0.7476 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.7082 Loss_G: 0.7428 acc: 87.5%\n",
      "[EPOCH 300] TEST ACC is : 72.7%\n",
      "[BATCH 3/149] Loss_D: 0.6755 Loss_G: 0.7302 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.6965 Loss_G: 0.7387 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7244 Loss_G: 0.7525 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6749 Loss_G: 0.7252 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7016 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.7264 Loss_G: 0.7370 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.7743 Loss_G: 0.7573 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7056 Loss_G: 0.7630 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7015 Loss_G: 0.7352 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7629 Loss_G: 0.7471 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.7156 Loss_G: 0.7386 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.7515 Loss_G: 0.7766 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.7675 Loss_G: 0.7769 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6916 Loss_G: 0.7586 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.7213 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.7305 Loss_G: 0.7529 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7142 Loss_G: 0.7323 acc: 79.7%\n",
      "[BATCH 20/149] Loss_D: 0.7131 Loss_G: 0.7161 acc: 81.2%\n",
      "[BATCH 21/149] Loss_D: 0.7421 Loss_G: 0.7411 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.7165 Loss_G: 0.7432 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7247 Loss_G: 0.7321 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7138 Loss_G: 0.7459 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.7479 Loss_G: 0.7464 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.7484 Loss_G: 0.7505 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.7133 Loss_G: 0.7396 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.7265 Loss_G: 0.7602 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6930 Loss_G: 0.7448 acc: 81.2%\n",
      "[BATCH 30/149] Loss_D: 0.7265 Loss_G: 0.7454 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6926 Loss_G: 0.7281 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7479 Loss_G: 0.7425 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7716 Loss_G: 0.7552 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7191 Loss_G: 0.7417 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6837 Loss_G: 0.7215 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.7180 Loss_G: 0.7423 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7507 Loss_G: 0.7481 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.7434 Loss_G: 0.7587 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7424 Loss_G: 0.7715 acc: 95.3%\n",
      "[BATCH 40/149] Loss_D: 0.6907 Loss_G: 0.7400 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.7692 Loss_G: 0.7492 acc: 81.2%\n",
      "[BATCH 42/149] Loss_D: 0.7365 Loss_G: 0.7456 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7156 Loss_G: 0.7414 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7528 Loss_G: 0.7551 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.7189 Loss_G: 0.7357 acc: 95.3%\n",
      "[BATCH 46/149] Loss_D: 0.7489 Loss_G: 0.7667 acc: 78.1%\n",
      "[BATCH 47/149] Loss_D: 0.7396 Loss_G: 0.7520 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7195 Loss_G: 0.7421 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.6812 Loss_G: 0.7323 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6900 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7711 Loss_G: 0.7288 acc: 79.7%\n",
      "[BATCH 52/149] Loss_D: 0.6934 Loss_G: 0.7428 acc: 92.2%\n",
      "[EPOCH 350] TEST ACC is : 73.6%\n",
      "[BATCH 53/149] Loss_D: 0.7152 Loss_G: 0.7455 acc: 79.7%\n",
      "[BATCH 54/149] Loss_D: 0.7148 Loss_G: 0.7268 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7225 Loss_G: 0.7348 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.7277 Loss_G: 0.7562 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7674 Loss_G: 0.7482 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7096 Loss_G: 0.7601 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.7196 Loss_G: 0.7602 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.7429 Loss_G: 0.7635 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.7593 Loss_G: 0.7647 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7475 Loss_G: 0.7583 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.6907 Loss_G: 0.7462 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7461 Loss_G: 0.7778 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7196 Loss_G: 0.7541 acc: 81.2%\n",
      "[BATCH 66/149] Loss_D: 0.7068 Loss_G: 0.7422 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.7207 Loss_G: 0.7530 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7356 Loss_G: 0.7453 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7365 Loss_G: 0.7475 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.7549 Loss_G: 0.7473 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7300 Loss_G: 0.7308 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7172 Loss_G: 0.7475 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.8000 Loss_G: 0.7817 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.7746 Loss_G: 0.7794 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7204 Loss_G: 0.7726 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7858 Loss_G: 0.7869 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.7162 Loss_G: 0.7624 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6968 Loss_G: 0.7433 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7281 Loss_G: 0.7421 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.7025 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7179 Loss_G: 0.7447 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.7605 Loss_G: 0.7409 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.7067 Loss_G: 0.7324 acc: 82.8%\n",
      "[BATCH 84/149] Loss_D: 0.7211 Loss_G: 0.7359 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.6867 Loss_G: 0.7245 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7457 Loss_G: 0.7552 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7346 Loss_G: 0.7419 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7228 Loss_G: 0.7356 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7124 Loss_G: 0.7426 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7206 Loss_G: 0.7548 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6821 Loss_G: 0.7478 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.7498 Loss_G: 0.7463 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7048 Loss_G: 0.7449 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.7001 Loss_G: 0.7564 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7648 Loss_G: 0.7494 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.7517 Loss_G: 0.7574 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6692 Loss_G: 0.7261 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7198 Loss_G: 0.7416 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7365 Loss_G: 0.7384 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7460 Loss_G: 0.7456 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7144 Loss_G: 0.7342 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.7352 Loss_G: 0.7578 acc: 90.6%\n",
      "[EPOCH 400] TEST ACC is : 72.9%\n",
      "[BATCH 103/149] Loss_D: 0.7248 Loss_G: 0.7435 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7318 Loss_G: 0.7492 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6959 Loss_G: 0.7407 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7445 Loss_G: 0.7457 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7180 Loss_G: 0.7387 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7213 Loss_G: 0.7351 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7215 Loss_G: 0.7353 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.7492 Loss_G: 0.7343 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6942 Loss_G: 0.7298 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7370 Loss_G: 0.7567 acc: 79.7%\n",
      "[BATCH 113/149] Loss_D: 0.7185 Loss_G: 0.7409 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7511 Loss_G: 0.7584 acc: 81.2%\n",
      "[BATCH 115/149] Loss_D: 0.7180 Loss_G: 0.7367 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.7292 Loss_G: 0.7622 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.6990 Loss_G: 0.7330 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.7489 Loss_G: 0.7492 acc: 81.2%\n",
      "[BATCH 119/149] Loss_D: 0.7702 Loss_G: 0.7729 acc: 75.0%\n",
      "[BATCH 120/149] Loss_D: 0.6939 Loss_G: 0.7296 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7357 Loss_G: 0.7419 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.7714 Loss_G: 0.7822 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7172 Loss_G: 0.7453 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.7344 Loss_G: 0.7554 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.6968 Loss_G: 0.7445 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.7142 Loss_G: 0.7496 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6874 Loss_G: 0.7288 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6843 Loss_G: 0.7385 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.7428 Loss_G: 0.7448 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7171 Loss_G: 0.7372 acc: 79.7%\n",
      "[BATCH 131/149] Loss_D: 0.7177 Loss_G: 0.7429 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7656 Loss_G: 0.7667 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7025 Loss_G: 0.7371 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7045 Loss_G: 0.7040 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7115 Loss_G: 0.7301 acc: 79.7%\n",
      "[BATCH 136/149] Loss_D: 0.6833 Loss_G: 0.7162 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7376 Loss_G: 0.7223 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.7491 Loss_G: 0.7512 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7333 Loss_G: 0.7384 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7549 Loss_G: 0.7587 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.6910 Loss_G: 0.7271 acc: 75.0%\n",
      "[BATCH 142/149] Loss_D: 0.7156 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.7801 Loss_G: 0.7481 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.6962 Loss_G: 0.7298 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7324 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7411 Loss_G: 0.7367 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6972 Loss_G: 0.7571 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7349 Loss_G: 0.7535 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7015 Loss_G: 0.7348 acc: 85.9%\n",
      "-----THE [3/50] epoch end-----\n",
      "-----THE [4/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7387 Loss_G: 0.7197 acc: 95.3%\n",
      "[BATCH 2/149] Loss_D: 0.7227 Loss_G: 0.7542 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7023 Loss_G: 0.7371 acc: 89.1%\n",
      "[EPOCH 450] TEST ACC is : 73.2%\n",
      "[BATCH 4/149] Loss_D: 0.7510 Loss_G: 0.7631 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7291 Loss_G: 0.7511 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6919 Loss_G: 0.7289 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.7677 Loss_G: 0.7487 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.7452 Loss_G: 0.7519 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7282 Loss_G: 0.7507 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.7398 Loss_G: 0.7468 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.7450 Loss_G: 0.7524 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.7412 Loss_G: 0.7712 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7175 Loss_G: 0.7388 acc: 78.1%\n",
      "[BATCH 14/149] Loss_D: 0.6894 Loss_G: 0.7374 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.7059 Loss_G: 0.7526 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.6894 Loss_G: 0.7355 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.7595 Loss_G: 0.7581 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6977 Loss_G: 0.7554 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.7254 Loss_G: 0.7583 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7636 Loss_G: 0.7660 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.7580 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7786 Loss_G: 0.7579 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7184 Loss_G: 0.7430 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7159 Loss_G: 0.7406 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.7372 Loss_G: 0.7398 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.7329 Loss_G: 0.7486 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7021 Loss_G: 0.7219 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6937 Loss_G: 0.7315 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.7642 Loss_G: 0.7417 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7431 Loss_G: 0.7480 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6804 Loss_G: 0.7384 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.7669 Loss_G: 0.7656 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.7123 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7672 Loss_G: 0.7581 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.7519 Loss_G: 0.7415 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6949 Loss_G: 0.7323 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7127 Loss_G: 0.7282 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.7120 Loss_G: 0.7392 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7490 Loss_G: 0.7522 acc: 81.2%\n",
      "[BATCH 40/149] Loss_D: 0.7486 Loss_G: 0.7349 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7150 Loss_G: 0.7436 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.7213 Loss_G: 0.7370 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7177 Loss_G: 0.7224 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7083 Loss_G: 0.7236 acc: 79.7%\n",
      "[BATCH 45/149] Loss_D: 0.7171 Loss_G: 0.7323 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.7057 Loss_G: 0.7511 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.7434 Loss_G: 0.7484 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7070 Loss_G: 0.7503 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7811 Loss_G: 0.7598 acc: 78.1%\n",
      "[BATCH 50/149] Loss_D: 0.7513 Loss_G: 0.7515 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6972 Loss_G: 0.7333 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.7226 Loss_G: 0.7595 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7261 Loss_G: 0.7549 acc: 84.4%\n",
      "[EPOCH 500] TEST ACC is : 74.8%\n",
      "[BATCH 54/149] Loss_D: 0.7369 Loss_G: 0.7450 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.7407 Loss_G: 0.7621 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.6819 Loss_G: 0.7433 acc: 95.3%\n",
      "[BATCH 57/149] Loss_D: 0.7057 Loss_G: 0.7432 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.7111 Loss_G: 0.7357 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7190 Loss_G: 0.7396 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7817 Loss_G: 0.7427 acc: 70.3%\n",
      "[BATCH 61/149] Loss_D: 0.7830 Loss_G: 0.7522 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7438 Loss_G: 0.7541 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.7136 Loss_G: 0.7418 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.6874 Loss_G: 0.7335 acc: 81.2%\n",
      "[BATCH 65/149] Loss_D: 0.6908 Loss_G: 0.7335 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.7613 Loss_G: 0.7502 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6986 Loss_G: 0.7487 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7494 Loss_G: 0.7365 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7456 Loss_G: 0.7639 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.7640 Loss_G: 0.7681 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6593 Loss_G: 0.7236 acc: 95.3%\n",
      "[BATCH 72/149] Loss_D: 0.7294 Loss_G: 0.7434 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7003 Loss_G: 0.7353 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.7039 Loss_G: 0.7209 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7040 Loss_G: 0.7389 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6777 Loss_G: 0.7239 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7617 Loss_G: 0.7543 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7872 Loss_G: 0.7537 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7519 Loss_G: 0.7499 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.7258 Loss_G: 0.7650 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6981 Loss_G: 0.7406 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7024 Loss_G: 0.7355 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7273 Loss_G: 0.7252 acc: 81.2%\n",
      "[BATCH 84/149] Loss_D: 0.7025 Loss_G: 0.7216 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7414 Loss_G: 0.7390 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7030 Loss_G: 0.7550 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6987 Loss_G: 0.7343 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7418 Loss_G: 0.7567 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.7355 Loss_G: 0.7469 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7136 Loss_G: 0.7503 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.7522 Loss_G: 0.7610 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7354 Loss_G: 0.7569 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7414 Loss_G: 0.7662 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.7539 Loss_G: 0.7628 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7520 Loss_G: 0.7640 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7332 Loss_G: 0.7466 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7078 Loss_G: 0.7431 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.7521 Loss_G: 0.7666 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7380 Loss_G: 0.7530 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.7622 Loss_G: 0.7841 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7049 Loss_G: 0.7563 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7234 Loss_G: 0.7266 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.7331 Loss_G: 0.7774 acc: 89.1%\n",
      "[EPOCH 550] TEST ACC is : 75.6%\n",
      "[BATCH 104/149] Loss_D: 0.6951 Loss_G: 0.7321 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.7131 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.7081 Loss_G: 0.7331 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6819 Loss_G: 0.7074 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7255 Loss_G: 0.7120 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.7407 Loss_G: 0.7412 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.7650 Loss_G: 0.7616 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.7225 Loss_G: 0.7454 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6912 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7628 Loss_G: 0.7592 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7257 Loss_G: 0.7463 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.7615 Loss_G: 0.7493 acc: 75.0%\n",
      "[BATCH 116/149] Loss_D: 0.7286 Loss_G: 0.7465 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7546 Loss_G: 0.7637 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7009 Loss_G: 0.7456 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6976 Loss_G: 0.7474 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7085 Loss_G: 0.7416 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7114 Loss_G: 0.7497 acc: 81.2%\n",
      "[BATCH 122/149] Loss_D: 0.7046 Loss_G: 0.7518 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7269 Loss_G: 0.7321 acc: 81.2%\n",
      "[BATCH 124/149] Loss_D: 0.7148 Loss_G: 0.7421 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6992 Loss_G: 0.7296 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6783 Loss_G: 0.7151 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7158 Loss_G: 0.7136 acc: 82.8%\n",
      "[BATCH 128/149] Loss_D: 0.7584 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6875 Loss_G: 0.7382 acc: 95.3%\n",
      "[BATCH 130/149] Loss_D: 0.7811 Loss_G: 0.7594 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7510 Loss_G: 0.7617 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7131 Loss_G: 0.7543 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7050 Loss_G: 0.7356 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6905 Loss_G: 0.7323 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.7708 Loss_G: 0.7458 acc: 78.1%\n",
      "[BATCH 136/149] Loss_D: 0.7477 Loss_G: 0.7510 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.7254 Loss_G: 0.7511 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7370 Loss_G: 0.7458 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.6857 Loss_G: 0.7334 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7306 Loss_G: 0.7418 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.6951 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7022 Loss_G: 0.7336 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7345 Loss_G: 0.7172 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.7193 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7080 Loss_G: 0.7400 acc: 95.3%\n",
      "[BATCH 146/149] Loss_D: 0.7115 Loss_G: 0.7503 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.7102 Loss_G: 0.7537 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.7166 Loss_G: 0.7252 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.7405 Loss_G: 0.7417 acc: 89.1%\n",
      "-----THE [4/50] epoch end-----\n",
      "-----THE [5/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7485 Loss_G: 0.7510 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7088 Loss_G: 0.7321 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7210 Loss_G: 0.7420 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7270 Loss_G: 0.7492 acc: 84.4%\n",
      "[EPOCH 600] TEST ACC is : 73.8%\n",
      "[BATCH 5/149] Loss_D: 0.7682 Loss_G: 0.7627 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6805 Loss_G: 0.7330 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7010 Loss_G: 0.7219 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.7638 Loss_G: 0.7612 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.8077 Loss_G: 0.7834 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.7182 Loss_G: 0.7549 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7060 Loss_G: 0.7483 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.7013 Loss_G: 0.7423 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.7172 Loss_G: 0.7405 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6965 Loss_G: 0.7321 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.7061 Loss_G: 0.7237 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6986 Loss_G: 0.7362 acc: 93.8%\n",
      "[BATCH 17/149] Loss_D: 0.6840 Loss_G: 0.7391 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.7467 Loss_G: 0.7605 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7414 Loss_G: 0.7524 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7199 Loss_G: 0.7387 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.7362 Loss_G: 0.7413 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7677 Loss_G: 0.7519 acc: 79.7%\n",
      "[BATCH 23/149] Loss_D: 0.7317 Loss_G: 0.7423 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7167 Loss_G: 0.7451 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.7500 Loss_G: 0.7662 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.7443 Loss_G: 0.7924 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7748 Loss_G: 0.7848 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.7563 Loss_G: 0.7762 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.7459 Loss_G: 0.7591 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7005 Loss_G: 0.7428 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7300 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7026 Loss_G: 0.7506 acc: 93.8%\n",
      "[BATCH 33/149] Loss_D: 0.7013 Loss_G: 0.7328 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.7701 Loss_G: 0.7328 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.7098 Loss_G: 0.7443 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6962 Loss_G: 0.7247 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.7292 Loss_G: 0.7487 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7049 Loss_G: 0.7408 acc: 81.2%\n",
      "[BATCH 39/149] Loss_D: 0.7215 Loss_G: 0.7353 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6852 Loss_G: 0.7312 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6901 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7063 Loss_G: 0.7259 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7280 Loss_G: 0.7185 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7176 Loss_G: 0.7261 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7686 Loss_G: 0.7653 acc: 79.7%\n",
      "[BATCH 46/149] Loss_D: 0.7271 Loss_G: 0.7696 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7448 Loss_G: 0.7598 acc: 79.7%\n",
      "[BATCH 48/149] Loss_D: 0.7052 Loss_G: 0.7310 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7230 Loss_G: 0.7379 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7489 Loss_G: 0.7546 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7687 Loss_G: 0.7589 acc: 81.2%\n",
      "[BATCH 52/149] Loss_D: 0.7633 Loss_G: 0.7529 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.6968 Loss_G: 0.7293 acc: 76.6%\n",
      "[BATCH 54/149] Loss_D: 0.7444 Loss_G: 0.7369 acc: 81.2%\n",
      "[EPOCH 650] TEST ACC is : 74.8%\n",
      "[BATCH 55/149] Loss_D: 0.7792 Loss_G: 0.7655 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.7240 Loss_G: 0.7551 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.7527 Loss_G: 0.7507 acc: 81.2%\n",
      "[BATCH 58/149] Loss_D: 0.7379 Loss_G: 0.7461 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7122 Loss_G: 0.7502 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.6943 Loss_G: 0.7332 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7150 Loss_G: 0.7473 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7159 Loss_G: 0.7455 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7429 Loss_G: 0.7491 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7162 Loss_G: 0.7558 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6993 Loss_G: 0.7334 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6920 Loss_G: 0.7305 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7404 Loss_G: 0.7272 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.6996 Loss_G: 0.7196 acc: 78.1%\n",
      "[BATCH 69/149] Loss_D: 0.7403 Loss_G: 0.7343 acc: 79.7%\n",
      "[BATCH 70/149] Loss_D: 0.7210 Loss_G: 0.7626 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7235 Loss_G: 0.7391 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7296 Loss_G: 0.7343 acc: 73.4%\n",
      "[BATCH 73/149] Loss_D: 0.7163 Loss_G: 0.7336 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.6913 Loss_G: 0.7188 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7560 Loss_G: 0.7548 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.7083 Loss_G: 0.7595 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.7133 Loss_G: 0.7564 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.7520 Loss_G: 0.7804 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6658 Loss_G: 0.7352 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.7492 Loss_G: 0.7675 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.7053 Loss_G: 0.7341 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7055 Loss_G: 0.7371 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.7033 Loss_G: 0.7463 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.7146 Loss_G: 0.7531 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6900 Loss_G: 0.7225 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7565 Loss_G: 0.7400 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7339 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6944 Loss_G: 0.7391 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7656 Loss_G: 0.7509 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.7398 Loss_G: 0.7667 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7095 Loss_G: 0.7488 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7222 Loss_G: 0.7472 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6808 Loss_G: 0.7315 acc: 95.3%\n",
      "[BATCH 94/149] Loss_D: 0.7292 Loss_G: 0.7364 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6841 Loss_G: 0.7202 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6969 Loss_G: 0.7226 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6820 Loss_G: 0.7272 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.7183 Loss_G: 0.7529 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7303 Loss_G: 0.7464 acc: 78.1%\n",
      "[BATCH 100/149] Loss_D: 0.7243 Loss_G: 0.7358 acc: 78.1%\n",
      "[BATCH 101/149] Loss_D: 0.7773 Loss_G: 0.7720 acc: 84.4%\n",
      "[BATCH 102/149] Loss_D: 0.6985 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7072 Loss_G: 0.7420 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7027 Loss_G: 0.7291 acc: 95.3%\n",
      "[EPOCH 700] TEST ACC is : 75.4%\n",
      "[BATCH 105/149] Loss_D: 0.7399 Loss_G: 0.7441 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6844 Loss_G: 0.7275 acc: 95.3%\n",
      "[BATCH 107/149] Loss_D: 0.6995 Loss_G: 0.7281 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7211 Loss_G: 0.7609 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.6970 Loss_G: 0.7245 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6972 Loss_G: 0.7441 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7402 Loss_G: 0.7419 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.7971 Loss_G: 0.7829 acc: 79.7%\n",
      "[BATCH 113/149] Loss_D: 0.7040 Loss_G: 0.7606 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7448 Loss_G: 0.7567 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.7215 Loss_G: 0.7425 acc: 82.8%\n",
      "[BATCH 116/149] Loss_D: 0.7626 Loss_G: 0.7350 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7294 Loss_G: 0.7452 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.8118 Loss_G: 0.7676 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6926 Loss_G: 0.7529 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6913 Loss_G: 0.7238 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7221 Loss_G: 0.7303 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.7181 Loss_G: 0.7381 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.7343 Loss_G: 0.7603 acc: 76.6%\n",
      "[BATCH 124/149] Loss_D: 0.7624 Loss_G: 0.7368 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.7599 Loss_G: 0.7529 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7428 Loss_G: 0.7430 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6797 Loss_G: 0.7211 acc: 81.2%\n",
      "[BATCH 128/149] Loss_D: 0.7491 Loss_G: 0.7307 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7410 Loss_G: 0.7559 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.7112 Loss_G: 0.7328 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7291 Loss_G: 0.7470 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.7072 Loss_G: 0.7258 acc: 81.2%\n",
      "[BATCH 133/149] Loss_D: 0.7376 Loss_G: 0.7249 acc: 76.6%\n",
      "[BATCH 134/149] Loss_D: 0.7180 Loss_G: 0.7407 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7404 Loss_G: 0.7358 acc: 76.6%\n",
      "[BATCH 136/149] Loss_D: 0.7512 Loss_G: 0.7391 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7637 Loss_G: 0.7400 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7273 Loss_G: 0.7408 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.7142 Loss_G: 0.7305 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6911 Loss_G: 0.7328 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6993 Loss_G: 0.7434 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6979 Loss_G: 0.7368 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7145 Loss_G: 0.7465 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.6970 Loss_G: 0.7409 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7567 Loss_G: 0.7620 acc: 81.2%\n",
      "[BATCH 146/149] Loss_D: 0.7192 Loss_G: 0.7529 acc: 79.7%\n",
      "[BATCH 147/149] Loss_D: 0.7029 Loss_G: 0.7406 acc: 82.8%\n",
      "[BATCH 148/149] Loss_D: 0.7298 Loss_G: 0.7614 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6943 Loss_G: 0.7429 acc: 85.9%\n",
      "-----THE [5/50] epoch end-----\n",
      "-----THE [6/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7281 Loss_G: 0.7429 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7379 Loss_G: 0.7469 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7028 Loss_G: 0.7436 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.7297 Loss_G: 0.7492 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7382 Loss_G: 0.7713 acc: 84.4%\n",
      "[EPOCH 750] TEST ACC is : 76.0%\n",
      "[BATCH 6/149] Loss_D: 0.7249 Loss_G: 0.7497 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.7633 Loss_G: 0.7530 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.7103 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6925 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6964 Loss_G: 0.7363 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7361 Loss_G: 0.7592 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.7178 Loss_G: 0.7316 acc: 79.7%\n",
      "[BATCH 13/149] Loss_D: 0.7374 Loss_G: 0.7607 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.7471 Loss_G: 0.7573 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.7334 Loss_G: 0.7584 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7490 Loss_G: 0.7484 acc: 78.1%\n",
      "[BATCH 17/149] Loss_D: 0.7011 Loss_G: 0.7271 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6740 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6842 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7421 Loss_G: 0.7514 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.7071 Loss_G: 0.7397 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.7497 Loss_G: 0.7400 acc: 82.8%\n",
      "[BATCH 23/149] Loss_D: 0.7549 Loss_G: 0.7609 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.7625 Loss_G: 0.7549 acc: 79.7%\n",
      "[BATCH 25/149] Loss_D: 0.7286 Loss_G: 0.7612 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7334 Loss_G: 0.7325 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7201 Loss_G: 0.7365 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6950 Loss_G: 0.7294 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.7017 Loss_G: 0.7195 acc: 82.8%\n",
      "[BATCH 30/149] Loss_D: 0.7203 Loss_G: 0.7211 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7192 Loss_G: 0.7288 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.7725 Loss_G: 0.7416 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.7326 Loss_G: 0.7570 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7065 Loss_G: 0.7383 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.7250 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7321 Loss_G: 0.7433 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7271 Loss_G: 0.7389 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6998 Loss_G: 0.7250 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7365 Loss_G: 0.7448 acc: 79.7%\n",
      "[BATCH 40/149] Loss_D: 0.7398 Loss_G: 0.7613 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7253 Loss_G: 0.7405 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7628 Loss_G: 0.7543 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7843 Loss_G: 0.7710 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7378 Loss_G: 0.7583 acc: 76.6%\n",
      "[BATCH 45/149] Loss_D: 0.7452 Loss_G: 0.7473 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6954 Loss_G: 0.7407 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6667 Loss_G: 0.7205 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.7685 Loss_G: 0.7304 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.7338 Loss_G: 0.7468 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7036 Loss_G: 0.7633 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.7370 Loss_G: 0.7520 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.7132 Loss_G: 0.7337 acc: 95.3%\n",
      "[BATCH 53/149] Loss_D: 0.7318 Loss_G: 0.7326 acc: 75.0%\n",
      "[BATCH 54/149] Loss_D: 0.7486 Loss_G: 0.7543 acc: 76.6%\n",
      "[BATCH 55/149] Loss_D: 0.7649 Loss_G: 0.7592 acc: 81.2%\n",
      "[EPOCH 800] TEST ACC is : 76.2%\n",
      "[BATCH 56/149] Loss_D: 0.7061 Loss_G: 0.7429 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.7419 Loss_G: 0.7431 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7087 Loss_G: 0.7462 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7249 Loss_G: 0.7453 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6973 Loss_G: 0.7317 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7251 Loss_G: 0.7372 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7195 Loss_G: 0.7355 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.6833 Loss_G: 0.7293 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.7309 Loss_G: 0.7351 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7090 Loss_G: 0.7392 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7159 Loss_G: 0.7548 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.7345 Loss_G: 0.7521 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6955 Loss_G: 0.7413 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7085 Loss_G: 0.7302 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.7148 Loss_G: 0.7352 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6970 Loss_G: 0.7325 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6971 Loss_G: 0.7300 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7271 Loss_G: 0.7424 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.7183 Loss_G: 0.7477 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.7431 Loss_G: 0.7346 acc: 81.2%\n",
      "[BATCH 76/149] Loss_D: 0.7240 Loss_G: 0.7427 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.7326 Loss_G: 0.7448 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7309 Loss_G: 0.7414 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7153 Loss_G: 0.7486 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6980 Loss_G: 0.7356 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.7268 Loss_G: 0.7386 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7425 Loss_G: 0.7479 acc: 81.2%\n",
      "[BATCH 83/149] Loss_D: 0.7154 Loss_G: 0.7514 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.7218 Loss_G: 0.7459 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.7421 Loss_G: 0.7538 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7369 Loss_G: 0.7524 acc: 81.2%\n",
      "[BATCH 87/149] Loss_D: 0.7201 Loss_G: 0.7217 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.7478 Loss_G: 0.7439 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.6793 Loss_G: 0.7219 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7201 Loss_G: 0.7306 acc: 82.8%\n",
      "[BATCH 91/149] Loss_D: 0.7070 Loss_G: 0.7326 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7152 Loss_G: 0.7479 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7173 Loss_G: 0.7449 acc: 95.3%\n",
      "[BATCH 94/149] Loss_D: 0.7151 Loss_G: 0.7418 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7011 Loss_G: 0.7655 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7640 Loss_G: 0.7673 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6833 Loss_G: 0.7245 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7032 Loss_G: 0.7324 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7259 Loss_G: 0.7127 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.7293 Loss_G: 0.7123 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.7213 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6660 Loss_G: 0.7195 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7319 Loss_G: 0.7342 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.7115 Loss_G: 0.7595 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7311 Loss_G: 0.7484 acc: 89.1%\n",
      "[EPOCH 850] TEST ACC is : 74.0%\n",
      "[BATCH 106/149] Loss_D: 0.7675 Loss_G: 0.7496 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.7116 Loss_G: 0.7357 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7041 Loss_G: 0.7417 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7444 Loss_G: 0.7387 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.7029 Loss_G: 0.7428 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7109 Loss_G: 0.7459 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7217 Loss_G: 0.7549 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7160 Loss_G: 0.7473 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7208 Loss_G: 0.7417 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6919 Loss_G: 0.7330 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.7273 Loss_G: 0.7399 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7478 Loss_G: 0.7465 acc: 81.2%\n",
      "[BATCH 118/149] Loss_D: 0.7008 Loss_G: 0.7277 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7400 Loss_G: 0.7518 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.7362 Loss_G: 0.7520 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7017 Loss_G: 0.7284 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.7075 Loss_G: 0.7196 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.7281 Loss_G: 0.7328 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.7525 Loss_G: 0.7554 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.7327 Loss_G: 0.7542 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7351 Loss_G: 0.7506 acc: 81.2%\n",
      "[BATCH 127/149] Loss_D: 0.6947 Loss_G: 0.7402 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7267 Loss_G: 0.7465 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.7044 Loss_G: 0.7325 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7610 Loss_G: 0.7448 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7340 Loss_G: 0.7414 acc: 95.3%\n",
      "[BATCH 132/149] Loss_D: 0.7247 Loss_G: 0.7508 acc: 78.1%\n",
      "[BATCH 133/149] Loss_D: 0.6974 Loss_G: 0.7292 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.7459 Loss_G: 0.7561 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.7267 Loss_G: 0.7516 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7086 Loss_G: 0.7468 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.7420 Loss_G: 0.7438 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.7175 Loss_G: 0.7445 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7364 Loss_G: 0.7342 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7553 Loss_G: 0.7456 acc: 79.7%\n",
      "[BATCH 141/149] Loss_D: 0.7436 Loss_G: 0.7519 acc: 75.0%\n",
      "[BATCH 142/149] Loss_D: 0.7069 Loss_G: 0.7286 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7417 Loss_G: 0.7285 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7194 Loss_G: 0.7319 acc: 76.6%\n",
      "[BATCH 145/149] Loss_D: 0.6790 Loss_G: 0.7199 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.7346 Loss_G: 0.7322 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.7207 Loss_G: 0.7304 acc: 82.8%\n",
      "[BATCH 148/149] Loss_D: 0.7785 Loss_G: 0.7577 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.7329 Loss_G: 0.7475 acc: 89.1%\n",
      "-----THE [6/50] epoch end-----\n",
      "-----THE [7/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7072 Loss_G: 0.7485 acc: 79.7%\n",
      "[BATCH 2/149] Loss_D: 0.7438 Loss_G: 0.7445 acc: 78.1%\n",
      "[BATCH 3/149] Loss_D: 0.7292 Loss_G: 0.7315 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7203 Loss_G: 0.7472 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.6731 Loss_G: 0.7211 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7168 Loss_G: 0.7175 acc: 84.4%\n",
      "[EPOCH 900] TEST ACC is : 74.8%\n",
      "[BATCH 7/149] Loss_D: 0.7551 Loss_G: 0.7529 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.7359 Loss_G: 0.7572 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.7107 Loss_G: 0.7335 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.7352 Loss_G: 0.7461 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7484 Loss_G: 0.7637 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.7106 Loss_G: 0.7502 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.7235 Loss_G: 0.7488 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.7518 Loss_G: 0.7709 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.7525 Loss_G: 0.7388 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7279 Loss_G: 0.7579 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7299 Loss_G: 0.7516 acc: 75.0%\n",
      "[BATCH 18/149] Loss_D: 0.6912 Loss_G: 0.7249 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7228 Loss_G: 0.7392 acc: 79.7%\n",
      "[BATCH 20/149] Loss_D: 0.7190 Loss_G: 0.7404 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6821 Loss_G: 0.7428 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.7754 Loss_G: 0.7509 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7090 Loss_G: 0.7412 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.7091 Loss_G: 0.7319 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7475 Loss_G: 0.7542 acc: 82.8%\n",
      "[BATCH 26/149] Loss_D: 0.8002 Loss_G: 0.7496 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7227 Loss_G: 0.7429 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7401 Loss_G: 0.7676 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.7711 Loss_G: 0.7683 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7636 Loss_G: 0.7612 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7193 Loss_G: 0.7629 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.7109 Loss_G: 0.7631 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7230 Loss_G: 0.7372 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.7125 Loss_G: 0.7215 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.7200 Loss_G: 0.7148 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7147 Loss_G: 0.7283 acc: 82.8%\n",
      "[BATCH 37/149] Loss_D: 0.6817 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7304 Loss_G: 0.7569 acc: 95.3%\n",
      "[BATCH 39/149] Loss_D: 0.7425 Loss_G: 0.7779 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.7166 Loss_G: 0.7986 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6911 Loss_G: 0.7381 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7227 Loss_G: 0.7523 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7357 Loss_G: 0.7362 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7151 Loss_G: 0.7321 acc: 82.8%\n",
      "[BATCH 45/149] Loss_D: 0.7229 Loss_G: 0.7498 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.7029 Loss_G: 0.7462 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6766 Loss_G: 0.7417 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7483 Loss_G: 0.7601 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7251 Loss_G: 0.7480 acc: 98.4%\n",
      "[BATCH 50/149] Loss_D: 0.7419 Loss_G: 0.7286 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7300 Loss_G: 0.7313 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.7247 Loss_G: 0.7455 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.7113 Loss_G: 0.7421 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7589 Loss_G: 0.7377 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.7604 Loss_G: 0.7484 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.7512 Loss_G: 0.7705 acc: 82.8%\n",
      "[EPOCH 950] TEST ACC is : 76.2%\n",
      "[BATCH 57/149] Loss_D: 0.7519 Loss_G: 0.7532 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7562 Loss_G: 0.7708 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.7056 Loss_G: 0.7581 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7258 Loss_G: 0.7371 acc: 81.2%\n",
      "[BATCH 61/149] Loss_D: 0.7108 Loss_G: 0.7344 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7398 Loss_G: 0.7450 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.6793 Loss_G: 0.7372 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7475 Loss_G: 0.7403 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.7384 Loss_G: 0.7340 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7250 Loss_G: 0.7365 acc: 81.2%\n",
      "[BATCH 67/149] Loss_D: 0.7041 Loss_G: 0.7547 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6980 Loss_G: 0.7324 acc: 79.7%\n",
      "[BATCH 69/149] Loss_D: 0.7122 Loss_G: 0.7384 acc: 75.0%\n",
      "[BATCH 70/149] Loss_D: 0.7245 Loss_G: 0.7433 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.7167 Loss_G: 0.7394 acc: 82.8%\n",
      "[BATCH 72/149] Loss_D: 0.6948 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7330 Loss_G: 0.7438 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.7097 Loss_G: 0.7581 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7369 Loss_G: 0.7603 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7108 Loss_G: 0.7472 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.7271 Loss_G: 0.7385 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.7230 Loss_G: 0.7437 acc: 78.1%\n",
      "[BATCH 79/149] Loss_D: 0.7511 Loss_G: 0.7531 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.7182 Loss_G: 0.7495 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.6910 Loss_G: 0.7239 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7388 Loss_G: 0.7560 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6983 Loss_G: 0.7494 acc: 82.8%\n",
      "[BATCH 84/149] Loss_D: 0.7220 Loss_G: 0.7625 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.7619 Loss_G: 0.7670 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.7069 Loss_G: 0.7360 acc: 78.1%\n",
      "[BATCH 87/149] Loss_D: 0.7321 Loss_G: 0.7308 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7008 Loss_G: 0.7371 acc: 78.1%\n",
      "[BATCH 89/149] Loss_D: 0.6925 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7508 Loss_G: 0.7447 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7325 Loss_G: 0.7423 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7526 Loss_G: 0.7688 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7495 Loss_G: 0.7459 acc: 73.4%\n",
      "[BATCH 94/149] Loss_D: 0.7473 Loss_G: 0.7557 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7342 Loss_G: 0.7424 acc: 79.7%\n",
      "[BATCH 96/149] Loss_D: 0.7393 Loss_G: 0.7466 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7297 Loss_G: 0.7599 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.7022 Loss_G: 0.7374 acc: 75.0%\n",
      "[BATCH 99/149] Loss_D: 0.7270 Loss_G: 0.7517 acc: 93.8%\n",
      "[BATCH 100/149] Loss_D: 0.6755 Loss_G: 0.7306 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.7352 Loss_G: 0.7317 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7093 Loss_G: 0.7473 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.7407 Loss_G: 0.7467 acc: 81.2%\n",
      "[BATCH 104/149] Loss_D: 0.7321 Loss_G: 0.7318 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.7443 Loss_G: 0.7326 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.7166 Loss_G: 0.7548 acc: 90.6%\n",
      "[EPOCH 1000] TEST ACC is : 76.6%\n",
      "[BATCH 107/149] Loss_D: 0.7223 Loss_G: 0.7463 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7275 Loss_G: 0.7369 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7176 Loss_G: 0.7162 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.7272 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6878 Loss_G: 0.7430 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.7250 Loss_G: 0.7499 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.7232 Loss_G: 0.7551 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7511 Loss_G: 0.7432 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7512 Loss_G: 0.7497 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6928 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7242 Loss_G: 0.7280 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7519 Loss_G: 0.7410 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7225 Loss_G: 0.7192 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.7251 Loss_G: 0.7309 acc: 95.3%\n",
      "[BATCH 121/149] Loss_D: 0.7131 Loss_G: 0.7334 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.7482 Loss_G: 0.7637 acc: 79.7%\n",
      "[BATCH 123/149] Loss_D: 0.7442 Loss_G: 0.7486 acc: 81.2%\n",
      "[BATCH 124/149] Loss_D: 0.7051 Loss_G: 0.7508 acc: 81.2%\n",
      "[BATCH 125/149] Loss_D: 0.7114 Loss_G: 0.7400 acc: 81.2%\n",
      "[BATCH 126/149] Loss_D: 0.7138 Loss_G: 0.7381 acc: 81.2%\n",
      "[BATCH 127/149] Loss_D: 0.7197 Loss_G: 0.7307 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.7262 Loss_G: 0.7519 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6785 Loss_G: 0.7190 acc: 81.2%\n",
      "[BATCH 130/149] Loss_D: 0.6986 Loss_G: 0.7233 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.7068 Loss_G: 0.7253 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7119 Loss_G: 0.7362 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.7122 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7081 Loss_G: 0.7292 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.7264 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7799 Loss_G: 0.7484 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7462 Loss_G: 0.7389 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6744 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.6733 Loss_G: 0.7245 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.6837 Loss_G: 0.7259 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7454 Loss_G: 0.7257 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7165 Loss_G: 0.7334 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7360 Loss_G: 0.7506 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.6936 Loss_G: 0.7402 acc: 82.8%\n",
      "[BATCH 145/149] Loss_D: 0.7088 Loss_G: 0.7404 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7036 Loss_G: 0.7318 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.7329 Loss_G: 0.7433 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7365 Loss_G: 0.7396 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.6867 Loss_G: 0.7329 acc: 89.1%\n",
      "-----THE [7/50] epoch end-----\n",
      "-----THE [8/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7164 Loss_G: 0.7437 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.7354 Loss_G: 0.7593 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7499 Loss_G: 0.7549 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.7205 Loss_G: 0.7268 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7157 Loss_G: 0.7445 acc: 78.1%\n",
      "[BATCH 6/149] Loss_D: 0.7519 Loss_G: 0.7462 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.7105 Loss_G: 0.7445 acc: 82.8%\n",
      "[EPOCH 1050] TEST ACC is : 76.4%\n",
      "[BATCH 8/149] Loss_D: 0.7441 Loss_G: 0.7429 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6970 Loss_G: 0.7508 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7140 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7143 Loss_G: 0.7263 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.7024 Loss_G: 0.7379 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.7087 Loss_G: 0.7309 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.7283 Loss_G: 0.7430 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7034 Loss_G: 0.7427 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7414 Loss_G: 0.7529 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.7389 Loss_G: 0.7583 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.7736 Loss_G: 0.7589 acc: 81.2%\n",
      "[BATCH 19/149] Loss_D: 0.7743 Loss_G: 0.7558 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.7121 Loss_G: 0.7593 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6747 Loss_G: 0.7391 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.7121 Loss_G: 0.7204 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7293 Loss_G: 0.7337 acc: 82.8%\n",
      "[BATCH 24/149] Loss_D: 0.7179 Loss_G: 0.7460 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6731 Loss_G: 0.7387 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.7223 Loss_G: 0.7502 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7127 Loss_G: 0.7361 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.7451 Loss_G: 0.7563 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7347 Loss_G: 0.7524 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6931 Loss_G: 0.7349 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.7079 Loss_G: 0.7397 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7246 Loss_G: 0.7629 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7081 Loss_G: 0.7481 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.7072 Loss_G: 0.7437 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.6915 Loss_G: 0.7421 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7128 Loss_G: 0.7455 acc: 82.8%\n",
      "[BATCH 37/149] Loss_D: 0.7253 Loss_G: 0.7440 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6959 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.7307 Loss_G: 0.7314 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7578 Loss_G: 0.7452 acc: 81.2%\n",
      "[BATCH 41/149] Loss_D: 0.7595 Loss_G: 0.7565 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.7399 Loss_G: 0.7481 acc: 81.2%\n",
      "[BATCH 43/149] Loss_D: 0.7193 Loss_G: 0.7348 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.7385 Loss_G: 0.7708 acc: 95.3%\n",
      "[BATCH 45/149] Loss_D: 0.7867 Loss_G: 0.8059 acc: 82.8%\n",
      "[BATCH 46/149] Loss_D: 0.7217 Loss_G: 0.7479 acc: 81.2%\n",
      "[BATCH 47/149] Loss_D: 0.7356 Loss_G: 0.7484 acc: 79.7%\n",
      "[BATCH 48/149] Loss_D: 0.6717 Loss_G: 0.7083 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.7602 Loss_G: 0.7368 acc: 81.2%\n",
      "[BATCH 50/149] Loss_D: 0.7164 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7279 Loss_G: 0.7406 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6937 Loss_G: 0.7354 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7241 Loss_G: 0.7280 acc: 81.2%\n",
      "[BATCH 54/149] Loss_D: 0.7427 Loss_G: 0.7436 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.7308 Loss_G: 0.7273 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.7043 Loss_G: 0.7347 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7073 Loss_G: 0.7423 acc: 81.2%\n",
      "[EPOCH 1100] TEST ACC is : 75.8%\n",
      "[BATCH 58/149] Loss_D: 0.7528 Loss_G: 0.7730 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7069 Loss_G: 0.7487 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7558 Loss_G: 0.7476 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.7033 Loss_G: 0.7530 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7378 Loss_G: 0.7575 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7647 Loss_G: 0.7736 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.7274 Loss_G: 0.7519 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7647 Loss_G: 0.7589 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7374 Loss_G: 0.7621 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7173 Loss_G: 0.7436 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7010 Loss_G: 0.7480 acc: 78.1%\n",
      "[BATCH 69/149] Loss_D: 0.7470 Loss_G: 0.7460 acc: 79.7%\n",
      "[BATCH 70/149] Loss_D: 0.6915 Loss_G: 0.7428 acc: 81.2%\n",
      "[BATCH 71/149] Loss_D: 0.6717 Loss_G: 0.7237 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7250 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7356 Loss_G: 0.7421 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7093 Loss_G: 0.7457 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6920 Loss_G: 0.7340 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6967 Loss_G: 0.7447 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6872 Loss_G: 0.7206 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7435 Loss_G: 0.7316 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.7117 Loss_G: 0.7286 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6986 Loss_G: 0.7239 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.7371 Loss_G: 0.7413 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.7511 Loss_G: 0.7648 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7468 Loss_G: 0.7530 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6994 Loss_G: 0.7578 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.7247 Loss_G: 0.7510 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.6565 Loss_G: 0.7293 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7126 Loss_G: 0.7227 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.7355 Loss_G: 0.7322 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.7436 Loss_G: 0.7502 acc: 81.2%\n",
      "[BATCH 90/149] Loss_D: 0.7363 Loss_G: 0.7571 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.7090 Loss_G: 0.7469 acc: 78.1%\n",
      "[BATCH 92/149] Loss_D: 0.7329 Loss_G: 0.7630 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7632 Loss_G: 0.7647 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6992 Loss_G: 0.7555 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6940 Loss_G: 0.7259 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7295 Loss_G: 0.7229 acc: 76.6%\n",
      "[BATCH 97/149] Loss_D: 0.7359 Loss_G: 0.7418 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7004 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7182 Loss_G: 0.7274 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7301 Loss_G: 0.7312 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7304 Loss_G: 0.7614 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.7032 Loss_G: 0.7515 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.7423 Loss_G: 0.7581 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.7292 Loss_G: 0.7702 acc: 95.3%\n",
      "[BATCH 105/149] Loss_D: 0.7080 Loss_G: 0.7526 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7360 Loss_G: 0.7499 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.6910 Loss_G: 0.7520 acc: 90.6%\n",
      "[EPOCH 1150] TEST ACC is : 76.0%\n",
      "[BATCH 108/149] Loss_D: 0.6964 Loss_G: 0.7349 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.6883 Loss_G: 0.7184 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.7283 Loss_G: 0.7484 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6687 Loss_G: 0.7409 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.7731 Loss_G: 0.7630 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.6963 Loss_G: 0.7301 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7439 Loss_G: 0.7655 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6907 Loss_G: 0.7329 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6897 Loss_G: 0.7452 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7331 Loss_G: 0.7411 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.7482 Loss_G: 0.7542 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7638 Loss_G: 0.7567 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.7693 Loss_G: 0.7705 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7711 Loss_G: 0.7578 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7204 Loss_G: 0.7420 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7408 Loss_G: 0.7558 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.7080 Loss_G: 0.7640 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.7613 Loss_G: 0.7695 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.7347 Loss_G: 0.7466 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7166 Loss_G: 0.7502 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6931 Loss_G: 0.7205 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6925 Loss_G: 0.7333 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6999 Loss_G: 0.7130 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7158 Loss_G: 0.7349 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.7214 Loss_G: 0.7511 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6784 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7286 Loss_G: 0.7434 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7450 Loss_G: 0.7436 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6591 Loss_G: 0.7364 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.6792 Loss_G: 0.7073 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.7630 Loss_G: 0.7435 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6961 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7150 Loss_G: 0.7241 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.7380 Loss_G: 0.7549 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7266 Loss_G: 0.7818 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.7551 Loss_G: 0.7404 acc: 81.2%\n",
      "[BATCH 144/149] Loss_D: 0.6978 Loss_G: 0.7358 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.7447 Loss_G: 0.7533 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7299 Loss_G: 0.7494 acc: 81.2%\n",
      "[BATCH 147/149] Loss_D: 0.7494 Loss_G: 0.7426 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6994 Loss_G: 0.7297 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.7301 Loss_G: 0.7437 acc: 85.9%\n",
      "-----THE [8/50] epoch end-----\n",
      "-----THE [9/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6992 Loss_G: 0.7421 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7073 Loss_G: 0.7271 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7123 Loss_G: 0.7329 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.8060 Loss_G: 0.7566 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6939 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6945 Loss_G: 0.7199 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7365 Loss_G: 0.7343 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6932 Loss_G: 0.7256 acc: 87.5%\n",
      "[EPOCH 1200] TEST ACC is : 75.4%\n",
      "[BATCH 9/149] Loss_D: 0.6710 Loss_G: 0.7261 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.7319 Loss_G: 0.7485 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7454 Loss_G: 0.7726 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.7228 Loss_G: 0.7494 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7241 Loss_G: 0.7464 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.7090 Loss_G: 0.7312 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.7104 Loss_G: 0.7399 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.7252 Loss_G: 0.7345 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.7339 Loss_G: 0.7576 acc: 78.1%\n",
      "[BATCH 18/149] Loss_D: 0.7408 Loss_G: 0.7621 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7533 Loss_G: 0.7632 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6879 Loss_G: 0.7400 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.7173 Loss_G: 0.7430 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.6810 Loss_G: 0.7411 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6820 Loss_G: 0.7153 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7092 Loss_G: 0.7427 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.7352 Loss_G: 0.7438 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.7094 Loss_G: 0.7316 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7343 Loss_G: 0.7605 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6938 Loss_G: 0.7283 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7230 Loss_G: 0.7634 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7166 Loss_G: 0.7615 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.6959 Loss_G: 0.7260 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.6961 Loss_G: 0.7368 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.7383 Loss_G: 0.7571 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7134 Loss_G: 0.7390 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7102 Loss_G: 0.7545 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7192 Loss_G: 0.7473 acc: 81.2%\n",
      "[BATCH 37/149] Loss_D: 0.7099 Loss_G: 0.7372 acc: 73.4%\n",
      "[BATCH 38/149] Loss_D: 0.6933 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7102 Loss_G: 0.7312 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.7333 Loss_G: 0.7626 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7098 Loss_G: 0.7690 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.6953 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7603 Loss_G: 0.7570 acc: 81.2%\n",
      "[BATCH 44/149] Loss_D: 0.7399 Loss_G: 0.7572 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.7532 Loss_G: 0.7750 acc: 79.7%\n",
      "[BATCH 46/149] Loss_D: 0.7179 Loss_G: 0.7632 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7189 Loss_G: 0.7602 acc: 82.8%\n",
      "[BATCH 48/149] Loss_D: 0.7237 Loss_G: 0.7460 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.7345 Loss_G: 0.7707 acc: 96.9%\n",
      "[BATCH 50/149] Loss_D: 0.7365 Loss_G: 0.7556 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7473 Loss_G: 0.7611 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7178 Loss_G: 0.7456 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7110 Loss_G: 0.7297 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.7106 Loss_G: 0.7479 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6959 Loss_G: 0.7356 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.7285 Loss_G: 0.7366 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.7253 Loss_G: 0.7379 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.7582 Loss_G: 0.7647 acc: 85.9%\n",
      "[EPOCH 1250] TEST ACC is : 75.4%\n",
      "[BATCH 59/149] Loss_D: 0.7082 Loss_G: 0.7564 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7087 Loss_G: 0.7547 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7459 Loss_G: 0.7363 acc: 81.2%\n",
      "[BATCH 62/149] Loss_D: 0.6783 Loss_G: 0.7652 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7325 Loss_G: 0.7285 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7564 Loss_G: 0.7574 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7197 Loss_G: 0.7446 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.7197 Loss_G: 0.7454 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6938 Loss_G: 0.7181 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7078 Loss_G: 0.7322 acc: 81.2%\n",
      "[BATCH 69/149] Loss_D: 0.7072 Loss_G: 0.7524 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7117 Loss_G: 0.7470 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7142 Loss_G: 0.7508 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.7454 Loss_G: 0.7731 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.7095 Loss_G: 0.7476 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.6902 Loss_G: 0.7653 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7778 Loss_G: 0.7588 acc: 79.7%\n",
      "[BATCH 76/149] Loss_D: 0.7649 Loss_G: 0.7498 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.7629 Loss_G: 0.7501 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7582 Loss_G: 0.7350 acc: 79.7%\n",
      "[BATCH 79/149] Loss_D: 0.7751 Loss_G: 0.7520 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.7587 Loss_G: 0.7649 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7771 Loss_G: 0.7683 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.6920 Loss_G: 0.7465 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.7151 Loss_G: 0.7487 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7672 Loss_G: 0.7443 acc: 78.1%\n",
      "[BATCH 85/149] Loss_D: 0.7245 Loss_G: 0.7460 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.6915 Loss_G: 0.7337 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7507 Loss_G: 0.7628 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7350 Loss_G: 0.7661 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7226 Loss_G: 0.7578 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6965 Loss_G: 0.7384 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7085 Loss_G: 0.7411 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.6988 Loss_G: 0.7319 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7156 Loss_G: 0.7340 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.7320 Loss_G: 0.7289 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6690 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7322 Loss_G: 0.7413 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7206 Loss_G: 0.7516 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6848 Loss_G: 0.7281 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7242 Loss_G: 0.7536 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.7297 Loss_G: 0.7361 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7084 Loss_G: 0.7425 acc: 84.4%\n",
      "[BATCH 102/149] Loss_D: 0.6918 Loss_G: 0.7371 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.7681 Loss_G: 0.7614 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7197 Loss_G: 0.7601 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.7476 Loss_G: 0.7491 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7116 Loss_G: 0.7406 acc: 81.2%\n",
      "[BATCH 107/149] Loss_D: 0.7229 Loss_G: 0.7229 acc: 82.8%\n",
      "[BATCH 108/149] Loss_D: 0.6971 Loss_G: 0.7255 acc: 85.9%\n",
      "[EPOCH 1300] TEST ACC is : 75.4%\n",
      "[BATCH 109/149] Loss_D: 0.7240 Loss_G: 0.7442 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.6793 Loss_G: 0.7401 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.7465 Loss_G: 0.7362 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.7522 Loss_G: 0.7432 acc: 79.7%\n",
      "[BATCH 113/149] Loss_D: 0.7189 Loss_G: 0.7537 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7390 Loss_G: 0.7480 acc: 79.7%\n",
      "[BATCH 115/149] Loss_D: 0.7674 Loss_G: 0.7351 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6959 Loss_G: 0.7336 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6844 Loss_G: 0.7197 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7507 Loss_G: 0.7388 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6802 Loss_G: 0.7374 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.7466 Loss_G: 0.7547 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7225 Loss_G: 0.7645 acc: 79.7%\n",
      "[BATCH 122/149] Loss_D: 0.7070 Loss_G: 0.7231 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7007 Loss_G: 0.7528 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.7096 Loss_G: 0.7295 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.7316 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7117 Loss_G: 0.7302 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7464 Loss_G: 0.7410 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7361 Loss_G: 0.7641 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.7378 Loss_G: 0.7709 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7203 Loss_G: 0.7634 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.6925 Loss_G: 0.7321 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7162 Loss_G: 0.7276 acc: 93.8%\n",
      "[BATCH 133/149] Loss_D: 0.7258 Loss_G: 0.7291 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7130 Loss_G: 0.7275 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.7405 Loss_G: 0.7451 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6908 Loss_G: 0.7345 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7534 Loss_G: 0.7486 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.7256 Loss_G: 0.7649 acc: 84.4%\n",
      "[BATCH 139/149] Loss_D: 0.7609 Loss_G: 0.7572 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7098 Loss_G: 0.7727 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7015 Loss_G: 0.7482 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7375 Loss_G: 0.7320 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.6872 Loss_G: 0.7246 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.7766 Loss_G: 0.7743 acc: 81.2%\n",
      "[BATCH 145/149] Loss_D: 0.7039 Loss_G: 0.7477 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7295 Loss_G: 0.7501 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.7299 Loss_G: 0.7251 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6771 Loss_G: 0.7284 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6774 Loss_G: 0.7287 acc: 84.4%\n",
      "-----THE [9/50] epoch end-----\n",
      "-----THE [10/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7705 Loss_G: 0.7369 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7128 Loss_G: 0.7470 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6953 Loss_G: 0.7306 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7082 Loss_G: 0.7395 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.7151 Loss_G: 0.7512 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7203 Loss_G: 0.7443 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.7160 Loss_G: 0.7508 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.7234 Loss_G: 0.7520 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6664 Loss_G: 0.7399 acc: 90.6%\n",
      "[EPOCH 1350] TEST ACC is : 76.4%\n",
      "[BATCH 10/149] Loss_D: 0.6448 Loss_G: 0.7205 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7221 Loss_G: 0.7174 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.7179 Loss_G: 0.7434 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.6825 Loss_G: 0.7306 acc: 81.2%\n",
      "[BATCH 14/149] Loss_D: 0.7422 Loss_G: 0.7461 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.7347 Loss_G: 0.7283 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6963 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.7258 Loss_G: 0.7421 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.7164 Loss_G: 0.7389 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7129 Loss_G: 0.7227 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.7940 Loss_G: 0.7759 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.7314 Loss_G: 0.7492 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7515 Loss_G: 0.7512 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7210 Loss_G: 0.7547 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6809 Loss_G: 0.7340 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.7288 Loss_G: 0.7585 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.7793 Loss_G: 0.7879 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6902 Loss_G: 0.7489 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.7136 Loss_G: 0.7406 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.7389 Loss_G: 0.7522 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7565 Loss_G: 0.7662 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7272 Loss_G: 0.7548 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7733 Loss_G: 0.7680 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.7282 Loss_G: 0.7536 acc: 95.3%\n",
      "[BATCH 34/149] Loss_D: 0.7357 Loss_G: 0.7562 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.7017 Loss_G: 0.7519 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7322 Loss_G: 0.7460 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7135 Loss_G: 0.7549 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.6986 Loss_G: 0.7645 acc: 93.8%\n",
      "[BATCH 39/149] Loss_D: 0.7091 Loss_G: 0.7574 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6816 Loss_G: 0.7420 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7145 Loss_G: 0.7557 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7123 Loss_G: 0.7476 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.7220 Loss_G: 0.7380 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7137 Loss_G: 0.7402 acc: 79.7%\n",
      "[BATCH 45/149] Loss_D: 0.7275 Loss_G: 0.7338 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6642 Loss_G: 0.7241 acc: 95.3%\n",
      "[BATCH 47/149] Loss_D: 0.7632 Loss_G: 0.7506 acc: 79.7%\n",
      "[BATCH 48/149] Loss_D: 0.6887 Loss_G: 0.7279 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7143 Loss_G: 0.7318 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.7313 Loss_G: 0.7453 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6884 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7205 Loss_G: 0.7183 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.7513 Loss_G: 0.7423 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6969 Loss_G: 0.7460 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.7087 Loss_G: 0.7553 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7877 Loss_G: 0.7790 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.7040 Loss_G: 0.7444 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6953 Loss_G: 0.7175 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7433 Loss_G: 0.7404 acc: 89.1%\n",
      "[EPOCH 1400] TEST ACC is : 76.2%\n",
      "[BATCH 60/149] Loss_D: 0.6976 Loss_G: 0.7307 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7002 Loss_G: 0.7414 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7263 Loss_G: 0.7383 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6953 Loss_G: 0.7218 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.7171 Loss_G: 0.7486 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6860 Loss_G: 0.7585 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7316 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6998 Loss_G: 0.7550 acc: 93.8%\n",
      "[BATCH 68/149] Loss_D: 0.7984 Loss_G: 0.7812 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6963 Loss_G: 0.7586 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7416 Loss_G: 0.7714 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.7283 Loss_G: 0.7663 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7168 Loss_G: 0.7287 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.6909 Loss_G: 0.7341 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6857 Loss_G: 0.7262 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.7229 Loss_G: 0.7553 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.7135 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6989 Loss_G: 0.7407 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.6753 Loss_G: 0.7414 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7656 Loss_G: 0.7822 acc: 79.7%\n",
      "[BATCH 80/149] Loss_D: 0.7115 Loss_G: 0.7410 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6860 Loss_G: 0.7269 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7478 Loss_G: 0.7301 acc: 78.1%\n",
      "[BATCH 83/149] Loss_D: 0.7019 Loss_G: 0.7238 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.7062 Loss_G: 0.7303 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.7455 Loss_G: 0.7288 acc: 76.6%\n",
      "[BATCH 86/149] Loss_D: 0.6979 Loss_G: 0.7450 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6794 Loss_G: 0.7309 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.7243 Loss_G: 0.7581 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7040 Loss_G: 0.7435 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6909 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7102 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7412 Loss_G: 0.7697 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7476 Loss_G: 0.7995 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.7417 Loss_G: 0.7855 acc: 79.7%\n",
      "[BATCH 95/149] Loss_D: 0.7245 Loss_G: 0.7348 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.7104 Loss_G: 0.7276 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.6745 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6929 Loss_G: 0.7247 acc: 95.3%\n",
      "[BATCH 99/149] Loss_D: 0.7246 Loss_G: 0.7398 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.7115 Loss_G: 0.7251 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7521 Loss_G: 0.7487 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7038 Loss_G: 0.7344 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.7121 Loss_G: 0.7270 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7535 Loss_G: 0.7465 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7139 Loss_G: 0.7471 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7566 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6960 Loss_G: 0.7381 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7011 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7164 Loss_G: 0.7353 acc: 93.8%\n",
      "[EPOCH 1450] TEST ACC is : 77.0%\n",
      "[BATCH 110/149] Loss_D: 0.6932 Loss_G: 0.7398 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7240 Loss_G: 0.7190 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.7276 Loss_G: 0.7341 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7342 Loss_G: 0.7531 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.6773 Loss_G: 0.7351 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.7611 Loss_G: 0.7730 acc: 95.3%\n",
      "[BATCH 116/149] Loss_D: 0.7763 Loss_G: 0.7798 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.7132 Loss_G: 0.7429 acc: 81.2%\n",
      "[BATCH 118/149] Loss_D: 0.7286 Loss_G: 0.7275 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7255 Loss_G: 0.7453 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.6916 Loss_G: 0.7300 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.7071 Loss_G: 0.7401 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7427 Loss_G: 0.7645 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6928 Loss_G: 0.7393 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7730 Loss_G: 0.7537 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.6991 Loss_G: 0.7450 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.7138 Loss_G: 0.7459 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.7142 Loss_G: 0.7462 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7144 Loss_G: 0.7408 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.6895 Loss_G: 0.7106 acc: 96.9%\n",
      "[BATCH 130/149] Loss_D: 0.7163 Loss_G: 0.7281 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7007 Loss_G: 0.7341 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6962 Loss_G: 0.7470 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.7381 Loss_G: 0.7597 acc: 95.3%\n",
      "[BATCH 134/149] Loss_D: 0.7789 Loss_G: 0.7789 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.6951 Loss_G: 0.7352 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7621 Loss_G: 0.7533 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7867 Loss_G: 0.7790 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.7012 Loss_G: 0.7297 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7598 Loss_G: 0.7591 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6662 Loss_G: 0.7333 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.7064 Loss_G: 0.7249 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.7242 Loss_G: 0.7422 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.7523 Loss_G: 0.7470 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.6979 Loss_G: 0.7634 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7331 Loss_G: 0.7433 acc: 79.7%\n",
      "[BATCH 146/149] Loss_D: 0.7298 Loss_G: 0.7478 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.7005 Loss_G: 0.7478 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.7393 Loss_G: 0.7540 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7268 Loss_G: 0.7669 acc: 89.1%\n",
      "-----THE [10/50] epoch end-----\n",
      "-----THE [11/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7026 Loss_G: 0.7439 acc: 82.8%\n",
      "[BATCH 2/149] Loss_D: 0.7380 Loss_G: 0.7597 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7182 Loss_G: 0.7465 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.7026 Loss_G: 0.7349 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.7507 Loss_G: 0.7545 acc: 78.1%\n",
      "[BATCH 6/149] Loss_D: 0.7254 Loss_G: 0.7466 acc: 79.7%\n",
      "[BATCH 7/149] Loss_D: 0.6599 Loss_G: 0.7306 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7128 Loss_G: 0.7274 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.6827 Loss_G: 0.7236 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.7163 Loss_G: 0.7271 acc: 90.6%\n",
      "[EPOCH 1500] TEST ACC is : 76.2%\n",
      "[BATCH 11/149] Loss_D: 0.7266 Loss_G: 0.7506 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.7358 Loss_G: 0.7407 acc: 79.7%\n",
      "[BATCH 13/149] Loss_D: 0.7036 Loss_G: 0.7310 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.7589 Loss_G: 0.7453 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6899 Loss_G: 0.7331 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6986 Loss_G: 0.7236 acc: 76.6%\n",
      "[BATCH 17/149] Loss_D: 0.7666 Loss_G: 0.7552 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.7060 Loss_G: 0.7714 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7306 Loss_G: 0.7571 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6714 Loss_G: 0.7367 acc: 81.2%\n",
      "[BATCH 21/149] Loss_D: 0.7082 Loss_G: 0.7417 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7004 Loss_G: 0.7443 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7379 Loss_G: 0.7570 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7831 Loss_G: 0.7935 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6924 Loss_G: 0.7672 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6873 Loss_G: 0.7486 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7820 Loss_G: 0.7650 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7430 Loss_G: 0.7606 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7693 Loss_G: 0.7538 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7489 Loss_G: 0.7550 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.7275 Loss_G: 0.7653 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.7232 Loss_G: 0.7655 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6897 Loss_G: 0.7342 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.7231 Loss_G: 0.7347 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.7126 Loss_G: 0.7472 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7589 Loss_G: 0.7611 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6701 Loss_G: 0.7445 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.7061 Loss_G: 0.7276 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.7228 Loss_G: 0.7440 acc: 81.2%\n",
      "[BATCH 40/149] Loss_D: 0.7606 Loss_G: 0.7519 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.6883 Loss_G: 0.7307 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.7331 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6931 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.7316 Loss_G: 0.7447 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6925 Loss_G: 0.7322 acc: 82.8%\n",
      "[BATCH 46/149] Loss_D: 0.6823 Loss_G: 0.7349 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.7293 Loss_G: 0.7382 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7032 Loss_G: 0.7299 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7207 Loss_G: 0.7377 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6982 Loss_G: 0.7262 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7296 Loss_G: 0.7305 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7143 Loss_G: 0.7364 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7399 Loss_G: 0.7461 acc: 78.1%\n",
      "[BATCH 54/149] Loss_D: 0.6833 Loss_G: 0.7263 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.7368 Loss_G: 0.7454 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.7551 Loss_G: 0.7684 acc: 81.2%\n",
      "[BATCH 57/149] Loss_D: 0.7092 Loss_G: 0.7580 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6693 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7590 Loss_G: 0.7488 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6971 Loss_G: 0.7420 acc: 85.9%\n",
      "[EPOCH 1550] TEST ACC is : 75.6%\n",
      "[BATCH 61/149] Loss_D: 0.6706 Loss_G: 0.7416 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7126 Loss_G: 0.7428 acc: 95.3%\n",
      "[BATCH 63/149] Loss_D: 0.7234 Loss_G: 0.7538 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7477 Loss_G: 0.7511 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7073 Loss_G: 0.7595 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7048 Loss_G: 0.7234 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.7138 Loss_G: 0.7282 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.7122 Loss_G: 0.7236 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6973 Loss_G: 0.7294 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.7322 Loss_G: 0.7456 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6989 Loss_G: 0.7426 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6765 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7276 Loss_G: 0.7506 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7003 Loss_G: 0.7471 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6880 Loss_G: 0.7217 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.7418 Loss_G: 0.7407 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7110 Loss_G: 0.7412 acc: 95.3%\n",
      "[BATCH 78/149] Loss_D: 0.7332 Loss_G: 0.7304 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.7247 Loss_G: 0.7427 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7430 Loss_G: 0.7445 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.7150 Loss_G: 0.7442 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6935 Loss_G: 0.7339 acc: 93.8%\n",
      "[BATCH 83/149] Loss_D: 0.7170 Loss_G: 0.7202 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.7387 Loss_G: 0.7210 acc: 81.2%\n",
      "[BATCH 85/149] Loss_D: 0.7380 Loss_G: 0.7481 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.7140 Loss_G: 0.7221 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.7237 Loss_G: 0.7393 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7595 Loss_G: 0.7540 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.7162 Loss_G: 0.7511 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.7083 Loss_G: 0.7190 acc: 79.7%\n",
      "[BATCH 91/149] Loss_D: 0.7167 Loss_G: 0.7141 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7436 Loss_G: 0.7256 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7223 Loss_G: 0.7366 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7068 Loss_G: 0.7518 acc: 93.8%\n",
      "[BATCH 95/149] Loss_D: 0.7141 Loss_G: 0.7522 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7136 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6998 Loss_G: 0.7356 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.7455 Loss_G: 0.7529 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7173 Loss_G: 0.7525 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.7059 Loss_G: 0.7389 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7164 Loss_G: 0.7355 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7378 Loss_G: 0.7485 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7472 Loss_G: 0.7645 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7151 Loss_G: 0.7741 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.6986 Loss_G: 0.7624 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.7313 Loss_G: 0.7577 acc: 81.2%\n",
      "[BATCH 107/149] Loss_D: 0.7394 Loss_G: 0.7602 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7111 Loss_G: 0.7593 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7368 Loss_G: 0.7375 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.7416 Loss_G: 0.7549 acc: 89.1%\n",
      "[EPOCH 1600] TEST ACC is : 75.4%\n",
      "[BATCH 111/149] Loss_D: 0.7049 Loss_G: 0.7426 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7022 Loss_G: 0.7331 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7975 Loss_G: 0.7740 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7155 Loss_G: 0.7649 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7175 Loss_G: 0.7696 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.7332 Loss_G: 0.7392 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.6770 Loss_G: 0.7372 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.7443 Loss_G: 0.7509 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7351 Loss_G: 0.7624 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7002 Loss_G: 0.7395 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7166 Loss_G: 0.7520 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.7358 Loss_G: 0.7577 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7124 Loss_G: 0.7449 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.7192 Loss_G: 0.7481 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.7056 Loss_G: 0.7319 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7863 Loss_G: 0.7567 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7138 Loss_G: 0.7595 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7033 Loss_G: 0.7582 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6868 Loss_G: 0.7419 acc: 79.7%\n",
      "[BATCH 130/149] Loss_D: 0.7212 Loss_G: 0.7499 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.6689 Loss_G: 0.7195 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.7340 Loss_G: 0.7591 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.7073 Loss_G: 0.7463 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7362 Loss_G: 0.7374 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6837 Loss_G: 0.7324 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.7416 Loss_G: 0.7537 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.7349 Loss_G: 0.7611 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.7139 Loss_G: 0.7533 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.7263 Loss_G: 0.7822 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.7035 Loss_G: 0.7615 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.6908 Loss_G: 0.7279 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7079 Loss_G: 0.7550 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.7163 Loss_G: 0.7645 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.7115 Loss_G: 0.7494 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7366 Loss_G: 0.7459 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6987 Loss_G: 0.7402 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7098 Loss_G: 0.7414 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.7019 Loss_G: 0.7234 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.8226 Loss_G: 0.7592 acc: 87.5%\n",
      "-----THE [11/50] epoch end-----\n",
      "-----THE [12/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7077 Loss_G: 0.7407 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7530 Loss_G: 0.7599 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7176 Loss_G: 0.7453 acc: 95.3%\n",
      "[BATCH 4/149] Loss_D: 0.7050 Loss_G: 0.7533 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6905 Loss_G: 0.7407 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6665 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.7372 Loss_G: 0.7384 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.7826 Loss_G: 0.7798 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7043 Loss_G: 0.7212 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7434 Loss_G: 0.7549 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7025 Loss_G: 0.7305 acc: 92.2%\n",
      "[EPOCH 1650] TEST ACC is : 74.8%\n",
      "[BATCH 12/149] Loss_D: 0.7342 Loss_G: 0.7566 acc: 75.0%\n",
      "[BATCH 13/149] Loss_D: 0.7375 Loss_G: 0.7566 acc: 76.6%\n",
      "[BATCH 14/149] Loss_D: 0.6988 Loss_G: 0.7513 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6977 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6922 Loss_G: 0.7084 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6882 Loss_G: 0.7322 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.7004 Loss_G: 0.7460 acc: 93.8%\n",
      "[BATCH 19/149] Loss_D: 0.7402 Loss_G: 0.7382 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.7222 Loss_G: 0.7528 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7435 Loss_G: 0.7565 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6634 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6934 Loss_G: 0.7126 acc: 81.2%\n",
      "[BATCH 24/149] Loss_D: 0.6852 Loss_G: 0.7197 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.7390 Loss_G: 0.7472 acc: 79.7%\n",
      "[BATCH 26/149] Loss_D: 0.6806 Loss_G: 0.7485 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7243 Loss_G: 0.7403 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6857 Loss_G: 0.7336 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.7354 Loss_G: 0.7475 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7174 Loss_G: 0.7466 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6889 Loss_G: 0.7391 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.6741 Loss_G: 0.7323 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7597 Loss_G: 0.7479 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7944 Loss_G: 0.7922 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7197 Loss_G: 0.7573 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7232 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6751 Loss_G: 0.7179 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6977 Loss_G: 0.7756 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7480 Loss_G: 0.7680 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7415 Loss_G: 0.7554 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.7112 Loss_G: 0.7315 acc: 81.2%\n",
      "[BATCH 42/149] Loss_D: 0.7064 Loss_G: 0.7395 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.7207 Loss_G: 0.7479 acc: 81.2%\n",
      "[BATCH 44/149] Loss_D: 0.7190 Loss_G: 0.7350 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.7206 Loss_G: 0.7129 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.7467 Loss_G: 0.7390 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7115 Loss_G: 0.7416 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7185 Loss_G: 0.7427 acc: 81.2%\n",
      "[BATCH 49/149] Loss_D: 0.7309 Loss_G: 0.7203 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7053 Loss_G: 0.7240 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6893 Loss_G: 0.7391 acc: 95.3%\n",
      "[BATCH 52/149] Loss_D: 0.7406 Loss_G: 0.7434 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7078 Loss_G: 0.7409 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7261 Loss_G: 0.7396 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.7152 Loss_G: 0.7390 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7220 Loss_G: 0.7520 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7195 Loss_G: 0.7297 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7566 Loss_G: 0.7454 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7133 Loss_G: 0.7554 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6931 Loss_G: 0.7367 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.7704 Loss_G: 0.7607 acc: 92.2%\n",
      "[EPOCH 1700] TEST ACC is : 75.2%\n",
      "[BATCH 62/149] Loss_D: 0.7585 Loss_G: 0.7597 acc: 95.3%\n",
      "[BATCH 63/149] Loss_D: 0.6953 Loss_G: 0.7460 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.6791 Loss_G: 0.7323 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.7272 Loss_G: 0.7402 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6927 Loss_G: 0.7632 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6937 Loss_G: 0.7284 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7217 Loss_G: 0.7356 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6966 Loss_G: 0.7449 acc: 95.3%\n",
      "[BATCH 70/149] Loss_D: 0.7164 Loss_G: 0.7534 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.7201 Loss_G: 0.7584 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6753 Loss_G: 0.7501 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.7195 Loss_G: 0.7482 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.7289 Loss_G: 0.7332 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.7014 Loss_G: 0.7466 acc: 82.8%\n",
      "[BATCH 76/149] Loss_D: 0.7089 Loss_G: 0.7320 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.6782 Loss_G: 0.7434 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.7068 Loss_G: 0.7291 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7026 Loss_G: 0.7414 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.7154 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7620 Loss_G: 0.7540 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.7596 Loss_G: 0.7600 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6915 Loss_G: 0.7364 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6826 Loss_G: 0.7207 acc: 81.2%\n",
      "[BATCH 85/149] Loss_D: 0.7007 Loss_G: 0.7364 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7127 Loss_G: 0.7425 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.7338 Loss_G: 0.7458 acc: 81.2%\n",
      "[BATCH 88/149] Loss_D: 0.7642 Loss_G: 0.7676 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7111 Loss_G: 0.7486 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7207 Loss_G: 0.7576 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.6952 Loss_G: 0.7333 acc: 98.4%\n",
      "[BATCH 92/149] Loss_D: 0.7564 Loss_G: 0.7583 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7224 Loss_G: 0.7626 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.7473 Loss_G: 0.7712 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7285 Loss_G: 0.7628 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7203 Loss_G: 0.7580 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7231 Loss_G: 0.7434 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6915 Loss_G: 0.7461 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7104 Loss_G: 0.7624 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7447 Loss_G: 0.7823 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.7270 Loss_G: 0.7529 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7268 Loss_G: 0.7378 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7172 Loss_G: 0.7520 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7049 Loss_G: 0.7305 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.7066 Loss_G: 0.7349 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7494 Loss_G: 0.7517 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.7112 Loss_G: 0.7395 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7432 Loss_G: 0.7304 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6875 Loss_G: 0.7276 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.7229 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6884 Loss_G: 0.7466 acc: 93.8%\n",
      "[EPOCH 1750] TEST ACC is : 76.8%\n",
      "[BATCH 112/149] Loss_D: 0.7132 Loss_G: 0.7505 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7303 Loss_G: 0.7700 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7284 Loss_G: 0.7520 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.7266 Loss_G: 0.7225 acc: 78.1%\n",
      "[BATCH 116/149] Loss_D: 0.6862 Loss_G: 0.7216 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.7240 Loss_G: 0.7405 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6940 Loss_G: 0.7263 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.7465 Loss_G: 0.7582 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.7123 Loss_G: 0.7363 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7142 Loss_G: 0.7603 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.7015 Loss_G: 0.7545 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.7128 Loss_G: 0.7514 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6820 Loss_G: 0.7525 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.7098 Loss_G: 0.7511 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.7261 Loss_G: 0.7449 acc: 81.2%\n",
      "[BATCH 127/149] Loss_D: 0.7163 Loss_G: 0.7382 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7066 Loss_G: 0.7350 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7062 Loss_G: 0.7273 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7447 Loss_G: 0.7374 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7051 Loss_G: 0.7325 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.7201 Loss_G: 0.7490 acc: 95.3%\n",
      "[BATCH 133/149] Loss_D: 0.6959 Loss_G: 0.7426 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7349 Loss_G: 0.7616 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7374 Loss_G: 0.7530 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7518 Loss_G: 0.7670 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7049 Loss_G: 0.7615 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.7271 Loss_G: 0.7409 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.7029 Loss_G: 0.7288 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7260 Loss_G: 0.7397 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7182 Loss_G: 0.7264 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.7435 Loss_G: 0.7418 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7168 Loss_G: 0.7444 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6960 Loss_G: 0.7379 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.7113 Loss_G: 0.7362 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7130 Loss_G: 0.7353 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.7401 Loss_G: 0.7531 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7101 Loss_G: 0.7695 acc: 78.1%\n",
      "[BATCH 149/149] Loss_D: 0.7466 Loss_G: 0.7511 acc: 92.2%\n",
      "-----THE [12/50] epoch end-----\n",
      "-----THE [13/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7320 Loss_G: 0.7428 acc: 78.1%\n",
      "[BATCH 2/149] Loss_D: 0.7229 Loss_G: 0.7405 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7294 Loss_G: 0.7611 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.7423 Loss_G: 0.7748 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.7124 Loss_G: 0.7573 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6976 Loss_G: 0.7521 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.7570 Loss_G: 0.7643 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.6826 Loss_G: 0.7456 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.7282 Loss_G: 0.7471 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7397 Loss_G: 0.7562 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.6910 Loss_G: 0.7388 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7074 Loss_G: 0.7449 acc: 79.7%\n",
      "[EPOCH 1800] TEST ACC is : 76.0%\n",
      "[BATCH 13/149] Loss_D: 0.7232 Loss_G: 0.7535 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6811 Loss_G: 0.7273 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.7131 Loss_G: 0.7585 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.7234 Loss_G: 0.7353 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6922 Loss_G: 0.7046 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.7119 Loss_G: 0.7218 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7068 Loss_G: 0.7102 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6940 Loss_G: 0.7306 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.7260 Loss_G: 0.7544 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7332 Loss_G: 0.7575 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7093 Loss_G: 0.7663 acc: 93.8%\n",
      "[BATCH 24/149] Loss_D: 0.6976 Loss_G: 0.7402 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.6919 Loss_G: 0.7254 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7352 Loss_G: 0.7311 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6774 Loss_G: 0.7400 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.7097 Loss_G: 0.7439 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7048 Loss_G: 0.7496 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7163 Loss_G: 0.7466 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6755 Loss_G: 0.7232 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7450 Loss_G: 0.7380 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.7022 Loss_G: 0.7347 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.7116 Loss_G: 0.7406 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7221 Loss_G: 0.7397 acc: 95.3%\n",
      "[BATCH 36/149] Loss_D: 0.7259 Loss_G: 0.7284 acc: 81.2%\n",
      "[BATCH 37/149] Loss_D: 0.7317 Loss_G: 0.7437 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.7078 Loss_G: 0.7387 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7194 Loss_G: 0.7480 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.7553 Loss_G: 0.7334 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7439 Loss_G: 0.7396 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.6813 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.7343 Loss_G: 0.7761 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.6988 Loss_G: 0.7507 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.7397 Loss_G: 0.7470 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6883 Loss_G: 0.7275 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7331 Loss_G: 0.7363 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7414 Loss_G: 0.7408 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.6980 Loss_G: 0.7076 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.7083 Loss_G: 0.7276 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.6825 Loss_G: 0.7188 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.6801 Loss_G: 0.7084 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6687 Loss_G: 0.6978 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.7018 Loss_G: 0.7241 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.7250 Loss_G: 0.7471 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.6774 Loss_G: 0.7366 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7127 Loss_G: 0.7390 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.7595 Loss_G: 0.7700 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6905 Loss_G: 0.7678 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7058 Loss_G: 0.7381 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6936 Loss_G: 0.7561 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6918 Loss_G: 0.7277 acc: 87.5%\n",
      "[EPOCH 1850] TEST ACC is : 77.0%\n",
      "[BATCH 63/149] Loss_D: 0.7312 Loss_G: 0.7457 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6993 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7093 Loss_G: 0.7455 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7004 Loss_G: 0.7296 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6931 Loss_G: 0.7463 acc: 96.9%\n",
      "[BATCH 68/149] Loss_D: 0.6985 Loss_G: 0.7302 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.7177 Loss_G: 0.7251 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.7767 Loss_G: 0.7449 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7336 Loss_G: 0.7550 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7210 Loss_G: 0.7407 acc: 82.8%\n",
      "[BATCH 73/149] Loss_D: 0.7461 Loss_G: 0.7250 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7324 Loss_G: 0.7494 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.7145 Loss_G: 0.7445 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7554 Loss_G: 0.7970 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6952 Loss_G: 0.7504 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7186 Loss_G: 0.7588 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.7567 Loss_G: 0.7406 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7047 Loss_G: 0.7375 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7159 Loss_G: 0.7488 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7259 Loss_G: 0.7599 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6782 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7057 Loss_G: 0.7550 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7206 Loss_G: 0.7322 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7112 Loss_G: 0.7358 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7264 Loss_G: 0.7363 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7586 Loss_G: 0.7695 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7507 Loss_G: 0.7719 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7164 Loss_G: 0.7527 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7471 Loss_G: 0.7599 acc: 79.7%\n",
      "[BATCH 92/149] Loss_D: 0.7144 Loss_G: 0.7412 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7690 Loss_G: 0.7503 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.7043 Loss_G: 0.7366 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7066 Loss_G: 0.7546 acc: 95.3%\n",
      "[BATCH 96/149] Loss_D: 0.7108 Loss_G: 0.7279 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7184 Loss_G: 0.7299 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.7027 Loss_G: 0.7263 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7035 Loss_G: 0.7461 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6960 Loss_G: 0.7183 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7600 Loss_G: 0.7360 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6803 Loss_G: 0.7411 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6984 Loss_G: 0.7347 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.7016 Loss_G: 0.7449 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.7218 Loss_G: 0.7381 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7487 Loss_G: 0.7649 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.7150 Loss_G: 0.7645 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7065 Loss_G: 0.7670 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7254 Loss_G: 0.7526 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7475 Loss_G: 0.7721 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6836 Loss_G: 0.7309 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7329 Loss_G: 0.7387 acc: 87.5%\n",
      "[EPOCH 1900] TEST ACC is : 75.2%\n",
      "[BATCH 113/149] Loss_D: 0.7192 Loss_G: 0.7405 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.7655 Loss_G: 0.7452 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.7155 Loss_G: 0.7432 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.7324 Loss_G: 0.7385 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7176 Loss_G: 0.7265 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.7210 Loss_G: 0.7543 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6883 Loss_G: 0.7377 acc: 96.9%\n",
      "[BATCH 120/149] Loss_D: 0.7453 Loss_G: 0.7539 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.7391 Loss_G: 0.7603 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.7067 Loss_G: 0.7408 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7491 Loss_G: 0.7417 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.7475 Loss_G: 0.7472 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7181 Loss_G: 0.7235 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.6985 Loss_G: 0.7343 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7159 Loss_G: 0.7369 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7219 Loss_G: 0.7552 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7217 Loss_G: 0.7637 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7102 Loss_G: 0.7465 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7486 Loss_G: 0.7678 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7302 Loss_G: 0.7590 acc: 93.8%\n",
      "[BATCH 133/149] Loss_D: 0.7164 Loss_G: 0.7432 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.7258 Loss_G: 0.7515 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6892 Loss_G: 0.7566 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6736 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7013 Loss_G: 0.7299 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6780 Loss_G: 0.7435 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7177 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7006 Loss_G: 0.7329 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6986 Loss_G: 0.7345 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7388 Loss_G: 0.7293 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7231 Loss_G: 0.7445 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.7167 Loss_G: 0.7655 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7103 Loss_G: 0.7475 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7007 Loss_G: 0.7377 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.7444 Loss_G: 0.7666 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.7242 Loss_G: 0.7613 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6961 Loss_G: 0.7432 acc: 90.6%\n",
      "-----THE [13/50] epoch end-----\n",
      "-----THE [14/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7360 Loss_G: 0.7425 acc: 81.2%\n",
      "[BATCH 2/149] Loss_D: 0.6886 Loss_G: 0.7400 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.7250 Loss_G: 0.7483 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.7549 Loss_G: 0.7767 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7145 Loss_G: 0.7478 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6880 Loss_G: 0.7270 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6815 Loss_G: 0.7342 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7093 Loss_G: 0.7380 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.6838 Loss_G: 0.7218 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.7227 Loss_G: 0.7462 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.7132 Loss_G: 0.7358 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.7058 Loss_G: 0.7273 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7146 Loss_G: 0.7391 acc: 82.8%\n",
      "[EPOCH 1950] TEST ACC is : 75.4%\n",
      "[BATCH 14/149] Loss_D: 0.7135 Loss_G: 0.7434 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.7026 Loss_G: 0.7449 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.7480 Loss_G: 0.7442 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7085 Loss_G: 0.7403 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7385 Loss_G: 0.7458 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6965 Loss_G: 0.7479 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.7662 Loss_G: 0.7553 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7363 Loss_G: 0.7509 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.7058 Loss_G: 0.7253 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.6921 Loss_G: 0.7364 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7438 Loss_G: 0.7593 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.7060 Loss_G: 0.7434 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.7018 Loss_G: 0.7311 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7040 Loss_G: 0.7252 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7192 Loss_G: 0.7264 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7106 Loss_G: 0.7481 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7361 Loss_G: 0.7577 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.6906 Loss_G: 0.7515 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.7330 Loss_G: 0.7586 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7641 Loss_G: 0.7646 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.7481 Loss_G: 0.7826 acc: 95.3%\n",
      "[BATCH 35/149] Loss_D: 0.6979 Loss_G: 0.7440 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.7655 Loss_G: 0.7338 acc: 81.2%\n",
      "[BATCH 37/149] Loss_D: 0.7527 Loss_G: 0.7400 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7353 Loss_G: 0.7544 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7236 Loss_G: 0.7607 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.7450 Loss_G: 0.7558 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7165 Loss_G: 0.7576 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6914 Loss_G: 0.7745 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.7140 Loss_G: 0.7510 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.7087 Loss_G: 0.7429 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.7024 Loss_G: 0.7578 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6870 Loss_G: 0.7373 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.7092 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.7170 Loss_G: 0.7242 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7078 Loss_G: 0.7237 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6914 Loss_G: 0.7251 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.7439 Loss_G: 0.7483 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.6990 Loss_G: 0.7498 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7040 Loss_G: 0.7466 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6372 Loss_G: 0.7136 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.7281 Loss_G: 0.7434 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.7457 Loss_G: 0.7426 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.7055 Loss_G: 0.7362 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.7152 Loss_G: 0.7363 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.7321 Loss_G: 0.7546 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7422 Loss_G: 0.7632 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7314 Loss_G: 0.7695 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7237 Loss_G: 0.7782 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.7341 Loss_G: 0.7549 acc: 81.2%\n",
      "[EPOCH 2000] TEST ACC is : 75.6%\n",
      "[BATCH 64/149] Loss_D: 0.7164 Loss_G: 0.7587 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7420 Loss_G: 0.7444 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.7259 Loss_G: 0.7341 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6937 Loss_G: 0.7349 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7296 Loss_G: 0.7202 acc: 82.8%\n",
      "[BATCH 69/149] Loss_D: 0.6900 Loss_G: 0.7145 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6797 Loss_G: 0.7146 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.7187 Loss_G: 0.7321 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6788 Loss_G: 0.7379 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7144 Loss_G: 0.7337 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6599 Loss_G: 0.7129 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.7232 Loss_G: 0.7251 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6997 Loss_G: 0.7416 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6915 Loss_G: 0.7381 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7263 Loss_G: 0.7453 acc: 95.3%\n",
      "[BATCH 79/149] Loss_D: 0.7265 Loss_G: 0.7613 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7335 Loss_G: 0.7549 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6995 Loss_G: 0.7303 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7053 Loss_G: 0.7589 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.7153 Loss_G: 0.7487 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.7435 Loss_G: 0.7611 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7696 Loss_G: 0.7727 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7565 Loss_G: 0.7744 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.7059 Loss_G: 0.7476 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6765 Loss_G: 0.7216 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7300 Loss_G: 0.7264 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7470 Loss_G: 0.7366 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7347 Loss_G: 0.7482 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7082 Loss_G: 0.7337 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.7198 Loss_G: 0.7297 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.7205 Loss_G: 0.7270 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7087 Loss_G: 0.7262 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7214 Loss_G: 0.7569 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7180 Loss_G: 0.7538 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.7156 Loss_G: 0.7605 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6966 Loss_G: 0.7132 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.6911 Loss_G: 0.7275 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7026 Loss_G: 0.7280 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6852 Loss_G: 0.7197 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.7063 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7069 Loss_G: 0.7466 acc: 95.3%\n",
      "[BATCH 105/149] Loss_D: 0.7349 Loss_G: 0.7952 acc: 93.8%\n",
      "[BATCH 106/149] Loss_D: 0.7053 Loss_G: 0.7781 acc: 96.9%\n",
      "[BATCH 107/149] Loss_D: 0.6782 Loss_G: 0.7473 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7009 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6894 Loss_G: 0.7364 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7368 Loss_G: 0.7422 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7437 Loss_G: 0.7441 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7211 Loss_G: 0.7578 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7005 Loss_G: 0.7448 acc: 82.8%\n",
      "[EPOCH 2050] TEST ACC is : 74.2%\n",
      "[BATCH 114/149] Loss_D: 0.6870 Loss_G: 0.7515 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.7001 Loss_G: 0.7393 acc: 79.7%\n",
      "[BATCH 116/149] Loss_D: 0.7139 Loss_G: 0.7363 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7648 Loss_G: 0.7582 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.7422 Loss_G: 0.7413 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7172 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.7133 Loss_G: 0.7198 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7281 Loss_G: 0.7435 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6838 Loss_G: 0.7491 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7608 Loss_G: 0.7492 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7289 Loss_G: 0.7395 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7394 Loss_G: 0.7336 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6745 Loss_G: 0.7299 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7020 Loss_G: 0.7233 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7341 Loss_G: 0.7522 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7218 Loss_G: 0.7869 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7004 Loss_G: 0.7410 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.7066 Loss_G: 0.7445 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.7097 Loss_G: 0.7494 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7016 Loss_G: 0.7301 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.7282 Loss_G: 0.7511 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6837 Loss_G: 0.7242 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7366 Loss_G: 0.7430 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6920 Loss_G: 0.7555 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.7174 Loss_G: 0.7519 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7010 Loss_G: 0.7385 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.6973 Loss_G: 0.7415 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.7560 Loss_G: 0.7560 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.7092 Loss_G: 0.7312 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.7643 Loss_G: 0.7533 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.7262 Loss_G: 0.7580 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7218 Loss_G: 0.7316 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7133 Loss_G: 0.7541 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.7217 Loss_G: 0.7481 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6956 Loss_G: 0.7415 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.7323 Loss_G: 0.7458 acc: 81.2%\n",
      "-----THE [14/50] epoch end-----\n",
      "-----THE [15/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7261 Loss_G: 0.7541 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6965 Loss_G: 0.7499 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7523 Loss_G: 0.7482 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.7117 Loss_G: 0.7470 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.7459 Loss_G: 0.7540 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6933 Loss_G: 0.7512 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7129 Loss_G: 0.7486 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.7543 Loss_G: 0.7515 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.7465 Loss_G: 0.7537 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7666 Loss_G: 0.7685 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7556 Loss_G: 0.7660 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6862 Loss_G: 0.7515 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6979 Loss_G: 0.7437 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.7215 Loss_G: 0.7630 acc: 89.1%\n",
      "[EPOCH 2100] TEST ACC is : 74.0%\n",
      "[BATCH 15/149] Loss_D: 0.7607 Loss_G: 0.7680 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6977 Loss_G: 0.7594 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.7282 Loss_G: 0.7425 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.7287 Loss_G: 0.7528 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7151 Loss_G: 0.7511 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.7076 Loss_G: 0.7426 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7195 Loss_G: 0.7527 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6982 Loss_G: 0.7669 acc: 95.3%\n",
      "[BATCH 23/149] Loss_D: 0.7261 Loss_G: 0.7553 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6892 Loss_G: 0.7302 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6836 Loss_G: 0.7175 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7340 Loss_G: 0.7339 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.7120 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.7219 Loss_G: 0.7548 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.7409 Loss_G: 0.7594 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7591 Loss_G: 0.7624 acc: 81.2%\n",
      "[BATCH 31/149] Loss_D: 0.7357 Loss_G: 0.7472 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.7397 Loss_G: 0.7614 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6925 Loss_G: 0.7428 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.7233 Loss_G: 0.7609 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6923 Loss_G: 0.7550 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6829 Loss_G: 0.7550 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.7073 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6841 Loss_G: 0.7516 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6987 Loss_G: 0.7587 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6642 Loss_G: 0.7219 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7190 Loss_G: 0.7433 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.7089 Loss_G: 0.7406 acc: 96.9%\n",
      "[BATCH 43/149] Loss_D: 0.6918 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7069 Loss_G: 0.7215 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.7204 Loss_G: 0.7267 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7314 Loss_G: 0.7486 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6895 Loss_G: 0.7441 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7049 Loss_G: 0.7292 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7189 Loss_G: 0.7407 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7438 Loss_G: 0.7522 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7148 Loss_G: 0.7327 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.7058 Loss_G: 0.7404 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7441 Loss_G: 0.7562 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7396 Loss_G: 0.7590 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6931 Loss_G: 0.7459 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.7303 Loss_G: 0.7351 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7272 Loss_G: 0.7492 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7199 Loss_G: 0.7608 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.6757 Loss_G: 0.7384 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7181 Loss_G: 0.7411 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6929 Loss_G: 0.7403 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7392 Loss_G: 0.7417 acc: 81.2%\n",
      "[BATCH 63/149] Loss_D: 0.7226 Loss_G: 0.7451 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6930 Loss_G: 0.7386 acc: 92.2%\n",
      "[EPOCH 2150] TEST ACC is : 75.2%\n",
      "[BATCH 65/149] Loss_D: 0.7359 Loss_G: 0.7541 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7357 Loss_G: 0.7533 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6875 Loss_G: 0.7403 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6800 Loss_G: 0.7355 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7139 Loss_G: 0.7470 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7334 Loss_G: 0.7456 acc: 81.2%\n",
      "[BATCH 71/149] Loss_D: 0.7659 Loss_G: 0.7899 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7304 Loss_G: 0.7619 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.7184 Loss_G: 0.7410 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7192 Loss_G: 0.7326 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7253 Loss_G: 0.7321 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6840 Loss_G: 0.7364 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6976 Loss_G: 0.7552 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7164 Loss_G: 0.7539 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.7250 Loss_G: 0.7650 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.7446 Loss_G: 0.7435 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6896 Loss_G: 0.7440 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.6869 Loss_G: 0.7238 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6890 Loss_G: 0.7160 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6956 Loss_G: 0.7383 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.7308 Loss_G: 0.7407 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.7335 Loss_G: 0.7352 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7420 Loss_G: 0.7506 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6945 Loss_G: 0.7351 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.6956 Loss_G: 0.7323 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7105 Loss_G: 0.7281 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.7409 Loss_G: 0.7433 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6851 Loss_G: 0.7434 acc: 95.3%\n",
      "[BATCH 93/149] Loss_D: 0.6866 Loss_G: 0.7252 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6931 Loss_G: 0.7446 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.7150 Loss_G: 0.7576 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7284 Loss_G: 0.7640 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.6934 Loss_G: 0.7387 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.7210 Loss_G: 0.7492 acc: 95.3%\n",
      "[BATCH 99/149] Loss_D: 0.7032 Loss_G: 0.7450 acc: 82.8%\n",
      "[BATCH 100/149] Loss_D: 0.6942 Loss_G: 0.7545 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7336 Loss_G: 0.7513 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7458 Loss_G: 0.7591 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7074 Loss_G: 0.7542 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.7163 Loss_G: 0.7233 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6800 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6894 Loss_G: 0.7328 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7009 Loss_G: 0.7470 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.7198 Loss_G: 0.7447 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7013 Loss_G: 0.7379 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7229 Loss_G: 0.7278 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.7020 Loss_G: 0.7220 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7246 Loss_G: 0.7348 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6866 Loss_G: 0.7549 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7398 Loss_G: 0.7468 acc: 89.1%\n",
      "[EPOCH 2200] TEST ACC is : 75.4%\n",
      "[BATCH 115/149] Loss_D: 0.7343 Loss_G: 0.7525 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6888 Loss_G: 0.7449 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6942 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6882 Loss_G: 0.7212 acc: 96.9%\n",
      "[BATCH 119/149] Loss_D: 0.7395 Loss_G: 0.7262 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.7329 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7197 Loss_G: 0.7334 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.7091 Loss_G: 0.7338 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.7233 Loss_G: 0.7398 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.7498 Loss_G: 0.7536 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.7049 Loss_G: 0.7392 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7045 Loss_G: 0.7424 acc: 95.3%\n",
      "[BATCH 127/149] Loss_D: 0.7036 Loss_G: 0.7262 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6919 Loss_G: 0.7366 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.7500 Loss_G: 0.7442 acc: 81.2%\n",
      "[BATCH 130/149] Loss_D: 0.6957 Loss_G: 0.7145 acc: 82.8%\n",
      "[BATCH 131/149] Loss_D: 0.7008 Loss_G: 0.7138 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7071 Loss_G: 0.7273 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6711 Loss_G: 0.7114 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6763 Loss_G: 0.7346 acc: 95.3%\n",
      "[BATCH 135/149] Loss_D: 0.7289 Loss_G: 0.7488 acc: 92.2%\n",
      "[BATCH 136/149] Loss_D: 0.6983 Loss_G: 0.7546 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7648 Loss_G: 0.7961 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6905 Loss_G: 0.7603 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7369 Loss_G: 0.7408 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.7119 Loss_G: 0.7699 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7325 Loss_G: 0.7613 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.7549 Loss_G: 0.7515 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.7016 Loss_G: 0.7386 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.7288 Loss_G: 0.7676 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7174 Loss_G: 0.7539 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7283 Loss_G: 0.7362 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.6982 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7195 Loss_G: 0.7294 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7193 Loss_G: 0.7496 acc: 92.2%\n",
      "-----THE [15/50] epoch end-----\n",
      "-----THE [16/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7047 Loss_G: 0.7325 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6806 Loss_G: 0.7500 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.7378 Loss_G: 0.7678 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.7147 Loss_G: 0.7564 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.6926 Loss_G: 0.7477 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7139 Loss_G: 0.7554 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7246 Loss_G: 0.7381 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6842 Loss_G: 0.7243 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.7183 Loss_G: 0.7532 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.7400 Loss_G: 0.7475 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.6997 Loss_G: 0.7246 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6897 Loss_G: 0.7310 acc: 95.3%\n",
      "[BATCH 13/149] Loss_D: 0.7013 Loss_G: 0.7378 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.7312 Loss_G: 0.7578 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7139 Loss_G: 0.7409 acc: 92.2%\n",
      "[EPOCH 2250] TEST ACC is : 74.8%\n",
      "[BATCH 16/149] Loss_D: 0.6979 Loss_G: 0.7157 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7322 Loss_G: 0.7436 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6942 Loss_G: 0.7364 acc: 95.3%\n",
      "[BATCH 19/149] Loss_D: 0.7347 Loss_G: 0.7578 acc: 79.7%\n",
      "[BATCH 20/149] Loss_D: 0.7189 Loss_G: 0.7600 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6932 Loss_G: 0.7455 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.6780 Loss_G: 0.7294 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.7350 Loss_G: 0.7279 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.7043 Loss_G: 0.7365 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6832 Loss_G: 0.7306 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7209 Loss_G: 0.7245 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7119 Loss_G: 0.7347 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7081 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.6773 Loss_G: 0.7252 acc: 93.8%\n",
      "[BATCH 30/149] Loss_D: 0.6732 Loss_G: 0.7306 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7299 Loss_G: 0.7244 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6927 Loss_G: 0.7237 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6798 Loss_G: 0.7144 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.7011 Loss_G: 0.7243 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7486 Loss_G: 0.7468 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7189 Loss_G: 0.7594 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7211 Loss_G: 0.7761 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.7245 Loss_G: 0.7544 acc: 81.2%\n",
      "[BATCH 39/149] Loss_D: 0.6799 Loss_G: 0.7207 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.7443 Loss_G: 0.7374 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.7240 Loss_G: 0.7558 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6948 Loss_G: 0.7853 acc: 95.3%\n",
      "[BATCH 43/149] Loss_D: 0.6944 Loss_G: 0.7518 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6970 Loss_G: 0.7032 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.7018 Loss_G: 0.7440 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.7392 Loss_G: 0.7465 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7101 Loss_G: 0.7455 acc: 82.8%\n",
      "[BATCH 48/149] Loss_D: 0.7337 Loss_G: 0.7410 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6895 Loss_G: 0.7416 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7318 Loss_G: 0.7451 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7358 Loss_G: 0.7280 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6882 Loss_G: 0.7279 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6870 Loss_G: 0.7394 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6889 Loss_G: 0.7585 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.7257 Loss_G: 0.7470 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.7185 Loss_G: 0.7614 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6803 Loss_G: 0.7357 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6763 Loss_G: 0.7223 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7106 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7518 Loss_G: 0.7491 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.7221 Loss_G: 0.7512 acc: 79.7%\n",
      "[BATCH 62/149] Loss_D: 0.7138 Loss_G: 0.7397 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7176 Loss_G: 0.7295 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.6964 Loss_G: 0.7276 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.7431 Loss_G: 0.7693 acc: 92.2%\n",
      "[EPOCH 2300] TEST ACC is : 75.0%\n",
      "[BATCH 66/149] Loss_D: 0.6972 Loss_G: 0.7450 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.7222 Loss_G: 0.7483 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7292 Loss_G: 0.7582 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.7070 Loss_G: 0.7473 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7195 Loss_G: 0.7492 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7211 Loss_G: 0.7370 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7228 Loss_G: 0.7408 acc: 81.2%\n",
      "[BATCH 73/149] Loss_D: 0.6925 Loss_G: 0.7271 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.7292 Loss_G: 0.7595 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.7237 Loss_G: 0.7757 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.6995 Loss_G: 0.7566 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6907 Loss_G: 0.7305 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7795 Loss_G: 0.7596 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7407 Loss_G: 0.7589 acc: 82.8%\n",
      "[BATCH 80/149] Loss_D: 0.7018 Loss_G: 0.7461 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7306 Loss_G: 0.7479 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6717 Loss_G: 0.7326 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.7115 Loss_G: 0.7303 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7453 Loss_G: 0.7441 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.7211 Loss_G: 0.7496 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7219 Loss_G: 0.7448 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6903 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7661 Loss_G: 0.7584 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7377 Loss_G: 0.7525 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7284 Loss_G: 0.7418 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7334 Loss_G: 0.7517 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.6889 Loss_G: 0.7437 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7057 Loss_G: 0.7371 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.7058 Loss_G: 0.7279 acc: 95.3%\n",
      "[BATCH 95/149] Loss_D: 0.7041 Loss_G: 0.7355 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.7205 Loss_G: 0.7175 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6864 Loss_G: 0.7242 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6981 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6800 Loss_G: 0.7171 acc: 79.7%\n",
      "[BATCH 100/149] Loss_D: 0.7356 Loss_G: 0.7215 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7043 Loss_G: 0.7175 acc: 81.2%\n",
      "[BATCH 102/149] Loss_D: 0.7069 Loss_G: 0.7107 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.7124 Loss_G: 0.7282 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6927 Loss_G: 0.7206 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7277 Loss_G: 0.7421 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7062 Loss_G: 0.7576 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.7015 Loss_G: 0.7402 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7454 Loss_G: 0.7349 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7161 Loss_G: 0.7288 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.7622 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7393 Loss_G: 0.7778 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7285 Loss_G: 0.7620 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6872 Loss_G: 0.7637 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7067 Loss_G: 0.7485 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.7084 Loss_G: 0.7509 acc: 90.6%\n",
      "[EPOCH 2350] TEST ACC is : 76.2%\n",
      "[BATCH 116/149] Loss_D: 0.7110 Loss_G: 0.7395 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7052 Loss_G: 0.7369 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.7181 Loss_G: 0.7368 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7141 Loss_G: 0.7301 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6944 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6654 Loss_G: 0.7267 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6979 Loss_G: 0.7107 acc: 78.1%\n",
      "[BATCH 123/149] Loss_D: 0.7088 Loss_G: 0.7184 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.7233 Loss_G: 0.7450 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.7350 Loss_G: 0.7436 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7267 Loss_G: 0.7516 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7073 Loss_G: 0.7618 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.7086 Loss_G: 0.7665 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.7110 Loss_G: 0.7314 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6920 Loss_G: 0.7346 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.7237 Loss_G: 0.7286 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6936 Loss_G: 0.7263 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6956 Loss_G: 0.7226 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7360 Loss_G: 0.7362 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6824 Loss_G: 0.7282 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7103 Loss_G: 0.7234 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7249 Loss_G: 0.7321 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6990 Loss_G: 0.7531 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.7381 Loss_G: 0.7521 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7648 Loss_G: 0.7788 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.7248 Loss_G: 0.7775 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7217 Loss_G: 0.7725 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6843 Loss_G: 0.7391 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.7217 Loss_G: 0.7483 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.7447 Loss_G: 0.7775 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7405 Loss_G: 0.7617 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.7493 Loss_G: 0.7544 acc: 96.9%\n",
      "[BATCH 148/149] Loss_D: 0.7127 Loss_G: 0.7398 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.7039 Loss_G: 0.7600 acc: 95.3%\n",
      "-----THE [16/50] epoch end-----\n",
      "-----THE [17/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7453 Loss_G: 0.7471 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.6893 Loss_G: 0.7323 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7385 Loss_G: 0.7527 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6933 Loss_G: 0.7526 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6898 Loss_G: 0.7281 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.7105 Loss_G: 0.7271 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6771 Loss_G: 0.7121 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6878 Loss_G: 0.7073 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6770 Loss_G: 0.7214 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.7110 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7665 Loss_G: 0.7544 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7469 Loss_G: 0.7609 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.7168 Loss_G: 0.7360 acc: 79.7%\n",
      "[BATCH 14/149] Loss_D: 0.7440 Loss_G: 0.7485 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7245 Loss_G: 0.7343 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.7116 Loss_G: 0.7256 acc: 82.8%\n",
      "[EPOCH 2400] TEST ACC is : 74.8%\n",
      "[BATCH 17/149] Loss_D: 0.7178 Loss_G: 0.7394 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6890 Loss_G: 0.7265 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6803 Loss_G: 0.7456 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6969 Loss_G: 0.7457 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6965 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.7071 Loss_G: 0.7316 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7214 Loss_G: 0.7395 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7376 Loss_G: 0.7554 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7300 Loss_G: 0.7582 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6999 Loss_G: 0.7624 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7603 Loss_G: 0.7636 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6866 Loss_G: 0.7594 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.6984 Loss_G: 0.7245 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7713 Loss_G: 0.7585 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.7609 Loss_G: 0.7682 acc: 81.2%\n",
      "[BATCH 32/149] Loss_D: 0.7520 Loss_G: 0.7664 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.6992 Loss_G: 0.7423 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.7178 Loss_G: 0.7435 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.7299 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7042 Loss_G: 0.7291 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7323 Loss_G: 0.7380 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6720 Loss_G: 0.7245 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6727 Loss_G: 0.7292 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6830 Loss_G: 0.7423 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6621 Loss_G: 0.7194 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.7289 Loss_G: 0.7306 acc: 79.7%\n",
      "[BATCH 43/149] Loss_D: 0.7626 Loss_G: 0.7519 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6843 Loss_G: 0.7459 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6995 Loss_G: 0.7280 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.7324 Loss_G: 0.7279 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7197 Loss_G: 0.7458 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7147 Loss_G: 0.7849 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.7228 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6884 Loss_G: 0.7418 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6762 Loss_G: 0.7261 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7023 Loss_G: 0.7422 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7173 Loss_G: 0.7353 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6661 Loss_G: 0.7347 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6971 Loss_G: 0.7292 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.7586 Loss_G: 0.7692 acc: 79.7%\n",
      "[BATCH 57/149] Loss_D: 0.7586 Loss_G: 0.7973 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.6827 Loss_G: 0.7517 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6915 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7716 Loss_G: 0.7519 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.7220 Loss_G: 0.7338 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7086 Loss_G: 0.7385 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7199 Loss_G: 0.7366 acc: 95.3%\n",
      "[BATCH 64/149] Loss_D: 0.7211 Loss_G: 0.7376 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7026 Loss_G: 0.7295 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7139 Loss_G: 0.7291 acc: 85.9%\n",
      "[EPOCH 2450] TEST ACC is : 74.0%\n",
      "[BATCH 67/149] Loss_D: 0.7413 Loss_G: 0.7593 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7119 Loss_G: 0.7585 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6992 Loss_G: 0.7470 acc: 93.8%\n",
      "[BATCH 70/149] Loss_D: 0.7473 Loss_G: 0.7741 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7173 Loss_G: 0.7551 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7667 Loss_G: 0.7458 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6678 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6938 Loss_G: 0.7169 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.7159 Loss_G: 0.7290 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6761 Loss_G: 0.7301 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.7041 Loss_G: 0.7339 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7374 Loss_G: 0.7409 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.6863 Loss_G: 0.7317 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7182 Loss_G: 0.7530 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.6930 Loss_G: 0.7503 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.7221 Loss_G: 0.7670 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.7085 Loss_G: 0.7420 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.7223 Loss_G: 0.7410 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.7304 Loss_G: 0.7484 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.7254 Loss_G: 0.7535 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7336 Loss_G: 0.7548 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7345 Loss_G: 0.7540 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.7290 Loss_G: 0.7426 acc: 95.3%\n",
      "[BATCH 90/149] Loss_D: 0.7155 Loss_G: 0.7606 acc: 93.8%\n",
      "[BATCH 91/149] Loss_D: 0.7128 Loss_G: 0.7466 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.7152 Loss_G: 0.7541 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7073 Loss_G: 0.7440 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7286 Loss_G: 0.7290 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6858 Loss_G: 0.7296 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.7050 Loss_G: 0.7411 acc: 79.7%\n",
      "[BATCH 97/149] Loss_D: 0.6777 Loss_G: 0.7272 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.7480 Loss_G: 0.7430 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7040 Loss_G: 0.7464 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6971 Loss_G: 0.7543 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.7077 Loss_G: 0.7704 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7000 Loss_G: 0.7537 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.7070 Loss_G: 0.7331 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.6941 Loss_G: 0.7488 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.7565 Loss_G: 0.7538 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7068 Loss_G: 0.7418 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7322 Loss_G: 0.7232 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7083 Loss_G: 0.7347 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.7174 Loss_G: 0.7316 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.6977 Loss_G: 0.7354 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7027 Loss_G: 0.7372 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.7018 Loss_G: 0.7295 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6996 Loss_G: 0.7287 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6907 Loss_G: 0.7276 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.7122 Loss_G: 0.7474 acc: 93.8%\n",
      "[BATCH 116/149] Loss_D: 0.7258 Loss_G: 0.7499 acc: 95.3%\n",
      "[EPOCH 2500] TEST ACC is : 74.6%\n",
      "[BATCH 117/149] Loss_D: 0.6935 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7061 Loss_G: 0.7406 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6892 Loss_G: 0.7306 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7259 Loss_G: 0.7405 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6985 Loss_G: 0.7475 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.7318 Loss_G: 0.7337 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.7008 Loss_G: 0.7305 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.7432 Loss_G: 0.7625 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7450 Loss_G: 0.7494 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7209 Loss_G: 0.7385 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7134 Loss_G: 0.7397 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7044 Loss_G: 0.7397 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6971 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6798 Loss_G: 0.7265 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.7093 Loss_G: 0.7175 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.7562 Loss_G: 0.7367 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.7917 Loss_G: 0.7504 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7332 Loss_G: 0.7443 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7210 Loss_G: 0.7517 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6842 Loss_G: 0.7514 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6847 Loss_G: 0.7374 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.7165 Loss_G: 0.7393 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6915 Loss_G: 0.7294 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.7140 Loss_G: 0.7454 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6861 Loss_G: 0.7316 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6835 Loss_G: 0.7163 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6806 Loss_G: 0.7230 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7347 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7053 Loss_G: 0.7458 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7056 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7275 Loss_G: 0.7397 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.7328 Loss_G: 0.7394 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7002 Loss_G: 0.7280 acc: 87.5%\n",
      "-----THE [17/50] epoch end-----\n",
      "-----THE [18/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6814 Loss_G: 0.7415 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7141 Loss_G: 0.7545 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6747 Loss_G: 0.7311 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.7140 Loss_G: 0.7414 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.7875 Loss_G: 0.7629 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7199 Loss_G: 0.7598 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.7054 Loss_G: 0.7462 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7859 Loss_G: 0.7589 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.6969 Loss_G: 0.7414 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.7034 Loss_G: 0.7284 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7068 Loss_G: 0.7365 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.7039 Loss_G: 0.7415 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6770 Loss_G: 0.7205 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6833 Loss_G: 0.7334 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7583 Loss_G: 0.7495 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7311 Loss_G: 0.7441 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.7339 Loss_G: 0.7599 acc: 92.2%\n",
      "[EPOCH 2550] TEST ACC is : 75.2%\n",
      "[BATCH 18/149] Loss_D: 0.7362 Loss_G: 0.7704 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7038 Loss_G: 0.7451 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7117 Loss_G: 0.7295 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.7117 Loss_G: 0.7164 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7481 Loss_G: 0.7391 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.7296 Loss_G: 0.7743 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.7214 Loss_G: 0.7440 acc: 79.7%\n",
      "[BATCH 25/149] Loss_D: 0.7155 Loss_G: 0.7356 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7165 Loss_G: 0.7446 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7396 Loss_G: 0.7362 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7311 Loss_G: 0.7558 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6947 Loss_G: 0.7475 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7442 Loss_G: 0.7634 acc: 93.8%\n",
      "[BATCH 31/149] Loss_D: 0.7350 Loss_G: 0.7478 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7149 Loss_G: 0.7426 acc: 81.2%\n",
      "[BATCH 33/149] Loss_D: 0.7291 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7129 Loss_G: 0.7285 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7035 Loss_G: 0.7578 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7175 Loss_G: 0.7535 acc: 95.3%\n",
      "[BATCH 37/149] Loss_D: 0.6891 Loss_G: 0.7312 acc: 81.2%\n",
      "[BATCH 38/149] Loss_D: 0.7385 Loss_G: 0.7621 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6636 Loss_G: 0.7357 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6677 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.7135 Loss_G: 0.7324 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7068 Loss_G: 0.7380 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6964 Loss_G: 0.7631 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.7364 Loss_G: 0.7751 acc: 95.3%\n",
      "[BATCH 45/149] Loss_D: 0.7032 Loss_G: 0.7595 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.7061 Loss_G: 0.7494 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7200 Loss_G: 0.7456 acc: 95.3%\n",
      "[BATCH 48/149] Loss_D: 0.6638 Loss_G: 0.7395 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7377 Loss_G: 0.7361 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7034 Loss_G: 0.7482 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7132 Loss_G: 0.7446 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6783 Loss_G: 0.7420 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7270 Loss_G: 0.7608 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.7217 Loss_G: 0.7456 acc: 81.2%\n",
      "[BATCH 55/149] Loss_D: 0.6949 Loss_G: 0.7247 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6698 Loss_G: 0.7123 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7360 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7591 Loss_G: 0.7495 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6864 Loss_G: 0.7331 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6826 Loss_G: 0.7303 acc: 93.8%\n",
      "[BATCH 61/149] Loss_D: 0.7150 Loss_G: 0.7394 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6897 Loss_G: 0.7129 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7153 Loss_G: 0.7358 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.6449 Loss_G: 0.7123 acc: 93.8%\n",
      "[BATCH 65/149] Loss_D: 0.7113 Loss_G: 0.7235 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6598 Loss_G: 0.7328 acc: 95.3%\n",
      "[BATCH 67/149] Loss_D: 0.7046 Loss_G: 0.7187 acc: 90.6%\n",
      "[EPOCH 2600] TEST ACC is : 75.4%\n",
      "[BATCH 68/149] Loss_D: 0.6922 Loss_G: 0.7184 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.7061 Loss_G: 0.7299 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6545 Loss_G: 0.7035 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6984 Loss_G: 0.7274 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7332 Loss_G: 0.7142 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7061 Loss_G: 0.7483 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.6825 Loss_G: 0.7394 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.7463 Loss_G: 0.7432 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7180 Loss_G: 0.7301 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.7370 Loss_G: 0.7399 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7218 Loss_G: 0.7416 acc: 79.7%\n",
      "[BATCH 79/149] Loss_D: 0.7048 Loss_G: 0.7375 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6996 Loss_G: 0.7155 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7534 Loss_G: 0.7289 acc: 96.9%\n",
      "[BATCH 82/149] Loss_D: 0.6798 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.7186 Loss_G: 0.7400 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.6896 Loss_G: 0.7246 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.7221 Loss_G: 0.7380 acc: 81.2%\n",
      "[BATCH 86/149] Loss_D: 0.6927 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6690 Loss_G: 0.7125 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.7256 Loss_G: 0.7245 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7387 Loss_G: 0.7608 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.7662 Loss_G: 0.7710 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.7060 Loss_G: 0.7522 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6704 Loss_G: 0.7549 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.6827 Loss_G: 0.7405 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7202 Loss_G: 0.7402 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.7019 Loss_G: 0.7502 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7094 Loss_G: 0.7252 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.7213 Loss_G: 0.7274 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.7348 Loss_G: 0.7378 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6952 Loss_G: 0.7392 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.7045 Loss_G: 0.7605 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.7266 Loss_G: 0.7578 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7558 Loss_G: 0.7554 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6869 Loss_G: 0.7735 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6949 Loss_G: 0.7429 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7083 Loss_G: 0.7398 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7151 Loss_G: 0.7564 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.7049 Loss_G: 0.7437 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.6961 Loss_G: 0.7197 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7278 Loss_G: 0.7327 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7463 Loss_G: 0.7373 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6694 Loss_G: 0.7191 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7710 Loss_G: 0.7425 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6626 Loss_G: 0.7243 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7268 Loss_G: 0.7128 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.7540 Loss_G: 0.7403 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.7023 Loss_G: 0.7315 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.7619 Loss_G: 0.7586 acc: 85.9%\n",
      "[EPOCH 2650] TEST ACC is : 74.4%\n",
      "[BATCH 118/149] Loss_D: 0.7045 Loss_G: 0.7460 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7042 Loss_G: 0.7342 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.7517 Loss_G: 0.7687 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.7401 Loss_G: 0.7681 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.7133 Loss_G: 0.7386 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7070 Loss_G: 0.7429 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6769 Loss_G: 0.7362 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.7016 Loss_G: 0.7257 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6799 Loss_G: 0.7129 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6712 Loss_G: 0.7163 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7133 Loss_G: 0.7236 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7664 Loss_G: 0.7649 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7009 Loss_G: 0.7435 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7290 Loss_G: 0.7327 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.7275 Loss_G: 0.7513 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6814 Loss_G: 0.7299 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7400 Loss_G: 0.7608 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7003 Loss_G: 0.7554 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6864 Loss_G: 0.7048 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6859 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7280 Loss_G: 0.7506 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6742 Loss_G: 0.7394 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.7121 Loss_G: 0.7420 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.7053 Loss_G: 0.7330 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6619 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.7061 Loss_G: 0.7391 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7222 Loss_G: 0.7530 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7395 Loss_G: 0.7732 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7577 Loss_G: 0.7624 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.7727 Loss_G: 0.7932 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7380 Loss_G: 0.7737 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7119 Loss_G: 0.7439 acc: 89.1%\n",
      "-----THE [18/50] epoch end-----\n",
      "-----THE [19/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7300 Loss_G: 0.7578 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6907 Loss_G: 0.7431 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7068 Loss_G: 0.7443 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6626 Loss_G: 0.7396 acc: 95.3%\n",
      "[BATCH 5/149] Loss_D: 0.7337 Loss_G: 0.7479 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.7287 Loss_G: 0.7555 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7467 Loss_G: 0.7596 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7252 Loss_G: 0.7645 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.7590 Loss_G: 0.7870 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6865 Loss_G: 0.7412 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7228 Loss_G: 0.7323 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6943 Loss_G: 0.7290 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6951 Loss_G: 0.7277 acc: 96.9%\n",
      "[BATCH 14/149] Loss_D: 0.7021 Loss_G: 0.7305 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.7451 Loss_G: 0.7448 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6995 Loss_G: 0.7514 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7150 Loss_G: 0.7370 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.7264 Loss_G: 0.7350 acc: 84.4%\n",
      "[EPOCH 2700] TEST ACC is : 76.0%\n",
      "[BATCH 19/149] Loss_D: 0.7012 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.7047 Loss_G: 0.7345 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.7362 Loss_G: 0.7511 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.7354 Loss_G: 0.7498 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.7227 Loss_G: 0.7486 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7177 Loss_G: 0.7434 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.6950 Loss_G: 0.7430 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.7023 Loss_G: 0.7450 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7217 Loss_G: 0.7677 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.6853 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7036 Loss_G: 0.7280 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.7193 Loss_G: 0.7396 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7174 Loss_G: 0.7370 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7114 Loss_G: 0.7423 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7220 Loss_G: 0.7527 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7059 Loss_G: 0.7219 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7064 Loss_G: 0.7155 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7653 Loss_G: 0.7479 acc: 93.8%\n",
      "[BATCH 37/149] Loss_D: 0.7111 Loss_G: 0.7420 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6957 Loss_G: 0.7389 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7055 Loss_G: 0.7306 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6975 Loss_G: 0.7256 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.7196 Loss_G: 0.7267 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.7503 Loss_G: 0.7615 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6955 Loss_G: 0.7394 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7193 Loss_G: 0.7399 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6615 Loss_G: 0.7287 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.6818 Loss_G: 0.7244 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6857 Loss_G: 0.7104 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6865 Loss_G: 0.7294 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7358 Loss_G: 0.7398 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6769 Loss_G: 0.7189 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.6809 Loss_G: 0.7207 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.7196 Loss_G: 0.7353 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.6858 Loss_G: 0.7204 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.7125 Loss_G: 0.7416 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7288 Loss_G: 0.7553 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6814 Loss_G: 0.7528 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6876 Loss_G: 0.7794 acc: 95.3%\n",
      "[BATCH 58/149] Loss_D: 0.7120 Loss_G: 0.7554 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.7502 Loss_G: 0.7674 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7039 Loss_G: 0.7556 acc: 79.7%\n",
      "[BATCH 61/149] Loss_D: 0.6828 Loss_G: 0.7346 acc: 82.8%\n",
      "[BATCH 62/149] Loss_D: 0.7165 Loss_G: 0.7518 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7497 Loss_G: 0.7693 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.6833 Loss_G: 0.7392 acc: 81.2%\n",
      "[BATCH 65/149] Loss_D: 0.7208 Loss_G: 0.7419 acc: 93.8%\n",
      "[BATCH 66/149] Loss_D: 0.7149 Loss_G: 0.7438 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6873 Loss_G: 0.7448 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7464 Loss_G: 0.7300 acc: 82.8%\n",
      "[EPOCH 2750] TEST ACC is : 74.8%\n",
      "[BATCH 69/149] Loss_D: 0.6762 Loss_G: 0.7381 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7065 Loss_G: 0.7226 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7328 Loss_G: 0.7422 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7055 Loss_G: 0.7694 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.7162 Loss_G: 0.7604 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.7012 Loss_G: 0.7403 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.6992 Loss_G: 0.7593 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7341 Loss_G: 0.7642 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.7008 Loss_G: 0.7429 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6952 Loss_G: 0.7293 acc: 95.3%\n",
      "[BATCH 79/149] Loss_D: 0.7010 Loss_G: 0.7250 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6983 Loss_G: 0.7190 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7288 Loss_G: 0.7311 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6978 Loss_G: 0.7340 acc: 93.8%\n",
      "[BATCH 83/149] Loss_D: 0.7077 Loss_G: 0.7508 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6722 Loss_G: 0.7205 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.7279 Loss_G: 0.7383 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7002 Loss_G: 0.7386 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.7128 Loss_G: 0.7444 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.7133 Loss_G: 0.7427 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.7358 Loss_G: 0.7491 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.6843 Loss_G: 0.7516 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7248 Loss_G: 0.7368 acc: 81.2%\n",
      "[BATCH 92/149] Loss_D: 0.7027 Loss_G: 0.7430 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7161 Loss_G: 0.7476 acc: 98.4%\n",
      "[BATCH 94/149] Loss_D: 0.7606 Loss_G: 0.7634 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.7349 Loss_G: 0.7550 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7505 Loss_G: 0.7685 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.7086 Loss_G: 0.7362 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.7308 Loss_G: 0.7311 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6634 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7205 Loss_G: 0.7261 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.6976 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7525 Loss_G: 0.7613 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.7394 Loss_G: 0.7511 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.7233 Loss_G: 0.7433 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.6424 Loss_G: 0.7105 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7379 Loss_G: 0.7366 acc: 79.7%\n",
      "[BATCH 107/149] Loss_D: 0.7528 Loss_G: 0.7372 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6965 Loss_G: 0.7356 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6944 Loss_G: 0.7323 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6966 Loss_G: 0.7423 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.6984 Loss_G: 0.7481 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7394 Loss_G: 0.7440 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6900 Loss_G: 0.7794 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7040 Loss_G: 0.7490 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7006 Loss_G: 0.7283 acc: 81.2%\n",
      "[BATCH 116/149] Loss_D: 0.7187 Loss_G: 0.7353 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.7096 Loss_G: 0.7339 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7370 Loss_G: 0.7385 acc: 92.2%\n",
      "[EPOCH 2800] TEST ACC is : 74.8%\n",
      "[BATCH 119/149] Loss_D: 0.6992 Loss_G: 0.7390 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.7642 Loss_G: 0.7399 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6649 Loss_G: 0.7241 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6846 Loss_G: 0.7247 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7285 Loss_G: 0.7176 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.7263 Loss_G: 0.7129 acc: 93.8%\n",
      "[BATCH 125/149] Loss_D: 0.6810 Loss_G: 0.7209 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6851 Loss_G: 0.7247 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7193 Loss_G: 0.7261 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6842 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7177 Loss_G: 0.7334 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.6968 Loss_G: 0.7551 acc: 93.8%\n",
      "[BATCH 131/149] Loss_D: 0.7104 Loss_G: 0.7330 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.6979 Loss_G: 0.7432 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.7403 Loss_G: 0.7386 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6895 Loss_G: 0.7202 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7268 Loss_G: 0.7379 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.7125 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7021 Loss_G: 0.7280 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7356 Loss_G: 0.7481 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6994 Loss_G: 0.7361 acc: 79.7%\n",
      "[BATCH 140/149] Loss_D: 0.6962 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7309 Loss_G: 0.7513 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6983 Loss_G: 0.7488 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6907 Loss_G: 0.7387 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.7078 Loss_G: 0.7205 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7079 Loss_G: 0.7382 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7200 Loss_G: 0.7275 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6902 Loss_G: 0.7478 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7421 Loss_G: 0.7490 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7777 Loss_G: 0.7745 acc: 85.9%\n",
      "-----THE [19/50] epoch end-----\n",
      "-----THE [20/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7712 Loss_G: 0.7742 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6837 Loss_G: 0.7457 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7307 Loss_G: 0.7353 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7008 Loss_G: 0.7359 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.7031 Loss_G: 0.7418 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7285 Loss_G: 0.7563 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7294 Loss_G: 0.7702 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.7510 Loss_G: 0.7594 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.6996 Loss_G: 0.7427 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.7200 Loss_G: 0.7338 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.6894 Loss_G: 0.7232 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6727 Loss_G: 0.7258 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7155 Loss_G: 0.7423 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.7026 Loss_G: 0.7368 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6730 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6946 Loss_G: 0.7446 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.7185 Loss_G: 0.7325 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7102 Loss_G: 0.7300 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6556 Loss_G: 0.7331 acc: 89.1%\n",
      "[EPOCH 2850] TEST ACC is : 75.6%\n",
      "[BATCH 20/149] Loss_D: 0.6922 Loss_G: 0.7163 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6850 Loss_G: 0.7258 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6964 Loss_G: 0.7303 acc: 96.9%\n",
      "[BATCH 23/149] Loss_D: 0.7399 Loss_G: 0.7380 acc: 82.8%\n",
      "[BATCH 24/149] Loss_D: 0.7104 Loss_G: 0.7398 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.8027 Loss_G: 0.7838 acc: 81.2%\n",
      "[BATCH 26/149] Loss_D: 0.6905 Loss_G: 0.7614 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.7053 Loss_G: 0.7584 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6817 Loss_G: 0.7376 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7299 Loss_G: 0.7424 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7201 Loss_G: 0.7566 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7410 Loss_G: 0.7580 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7319 Loss_G: 0.7459 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6977 Loss_G: 0.7491 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.7116 Loss_G: 0.7368 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.7169 Loss_G: 0.7764 acc: 81.2%\n",
      "[BATCH 36/149] Loss_D: 0.6881 Loss_G: 0.7331 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6972 Loss_G: 0.7378 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.7188 Loss_G: 0.7450 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6722 Loss_G: 0.7374 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6938 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.7088 Loss_G: 0.7340 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.7387 Loss_G: 0.7492 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.6827 Loss_G: 0.7464 acc: 98.4%\n",
      "[BATCH 44/149] Loss_D: 0.7206 Loss_G: 0.7305 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6823 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7596 Loss_G: 0.7302 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7198 Loss_G: 0.7359 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6815 Loss_G: 0.7342 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7285 Loss_G: 0.7335 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7142 Loss_G: 0.7349 acc: 82.8%\n",
      "[BATCH 51/149] Loss_D: 0.7376 Loss_G: 0.7771 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6866 Loss_G: 0.7384 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7194 Loss_G: 0.7394 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.7194 Loss_G: 0.7421 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.7306 Loss_G: 0.7487 acc: 81.2%\n",
      "[BATCH 56/149] Loss_D: 0.7045 Loss_G: 0.7469 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7200 Loss_G: 0.7570 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.7202 Loss_G: 0.7449 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.6992 Loss_G: 0.7358 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6761 Loss_G: 0.7304 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7372 Loss_G: 0.7351 acc: 93.8%\n",
      "[BATCH 62/149] Loss_D: 0.7395 Loss_G: 0.7471 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6971 Loss_G: 0.7618 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7680 Loss_G: 0.7560 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6966 Loss_G: 0.7554 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6863 Loss_G: 0.7261 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7099 Loss_G: 0.7479 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6841 Loss_G: 0.7340 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.7047 Loss_G: 0.7459 acc: 89.1%\n",
      "[EPOCH 2900] TEST ACC is : 74.6%\n",
      "[BATCH 70/149] Loss_D: 0.7236 Loss_G: 0.7365 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7002 Loss_G: 0.7638 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7216 Loss_G: 0.7570 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7188 Loss_G: 0.7544 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6979 Loss_G: 0.7550 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7066 Loss_G: 0.7466 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.7625 Loss_G: 0.7693 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7137 Loss_G: 0.7533 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.7622 Loss_G: 0.7438 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.7559 Loss_G: 0.7429 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6830 Loss_G: 0.7336 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6972 Loss_G: 0.7418 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.7258 Loss_G: 0.7489 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.7193 Loss_G: 0.7459 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.7391 Loss_G: 0.7554 acc: 78.1%\n",
      "[BATCH 85/149] Loss_D: 0.6896 Loss_G: 0.7337 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7191 Loss_G: 0.7311 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6977 Loss_G: 0.7379 acc: 96.9%\n",
      "[BATCH 88/149] Loss_D: 0.6720 Loss_G: 0.7499 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.7132 Loss_G: 0.7391 acc: 81.2%\n",
      "[BATCH 90/149] Loss_D: 0.6743 Loss_G: 0.7250 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6849 Loss_G: 0.7505 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7006 Loss_G: 0.7284 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.6884 Loss_G: 0.7177 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6818 Loss_G: 0.7244 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7094 Loss_G: 0.7374 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6986 Loss_G: 0.7276 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7088 Loss_G: 0.7238 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.7420 Loss_G: 0.7484 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7299 Loss_G: 0.7445 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7113 Loss_G: 0.7480 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7271 Loss_G: 0.7582 acc: 96.9%\n",
      "[BATCH 102/149] Loss_D: 0.7710 Loss_G: 0.7831 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7009 Loss_G: 0.7487 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.7035 Loss_G: 0.7356 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.7164 Loss_G: 0.7533 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.7499 Loss_G: 0.7415 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.7283 Loss_G: 0.7378 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6593 Loss_G: 0.7285 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6884 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7050 Loss_G: 0.7143 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6800 Loss_G: 0.7093 acc: 78.1%\n",
      "[BATCH 112/149] Loss_D: 0.7169 Loss_G: 0.7259 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7074 Loss_G: 0.7268 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7183 Loss_G: 0.7265 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.7119 Loss_G: 0.7351 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6849 Loss_G: 0.7273 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.7416 Loss_G: 0.7346 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6951 Loss_G: 0.7413 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.6767 Loss_G: 0.7305 acc: 87.5%\n",
      "[EPOCH 2950] TEST ACC is : 75.4%\n",
      "[BATCH 120/149] Loss_D: 0.7300 Loss_G: 0.7527 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.6799 Loss_G: 0.7249 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.7079 Loss_G: 0.7528 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.6757 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6968 Loss_G: 0.7142 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7574 Loss_G: 0.7414 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7109 Loss_G: 0.7445 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7407 Loss_G: 0.7525 acc: 82.8%\n",
      "[BATCH 128/149] Loss_D: 0.7347 Loss_G: 0.7503 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6973 Loss_G: 0.7481 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6769 Loss_G: 0.7275 acc: 82.8%\n",
      "[BATCH 131/149] Loss_D: 0.6980 Loss_G: 0.7223 acc: 79.7%\n",
      "[BATCH 132/149] Loss_D: 0.7184 Loss_G: 0.7182 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.7039 Loss_G: 0.7204 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7167 Loss_G: 0.7463 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6903 Loss_G: 0.7446 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6808 Loss_G: 0.7476 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6830 Loss_G: 0.7228 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.6803 Loss_G: 0.7364 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7178 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7497 Loss_G: 0.7742 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7478 Loss_G: 0.7719 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6952 Loss_G: 0.7558 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.7209 Loss_G: 0.7524 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7048 Loss_G: 0.7457 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6962 Loss_G: 0.7182 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7094 Loss_G: 0.7423 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6990 Loss_G: 0.7461 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6944 Loss_G: 0.7272 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6828 Loss_G: 0.7018 acc: 85.9%\n",
      "-----THE [20/50] epoch end-----\n",
      "-----THE [21/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6762 Loss_G: 0.7239 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.7102 Loss_G: 0.7396 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7075 Loss_G: 0.7357 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.7339 Loss_G: 0.7806 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7470 Loss_G: 0.7749 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.7296 Loss_G: 0.7638 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7269 Loss_G: 0.7711 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.7490 Loss_G: 0.7674 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.7422 Loss_G: 0.7581 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.6868 Loss_G: 0.7264 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.7448 Loss_G: 0.7390 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7067 Loss_G: 0.7375 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.7059 Loss_G: 0.7565 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.7000 Loss_G: 0.7545 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7115 Loss_G: 0.7410 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7193 Loss_G: 0.7458 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7286 Loss_G: 0.7354 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6978 Loss_G: 0.7385 acc: 78.1%\n",
      "[BATCH 19/149] Loss_D: 0.7042 Loss_G: 0.7271 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6943 Loss_G: 0.7357 acc: 89.1%\n",
      "[EPOCH 3000] TEST ACC is : 73.0%\n",
      "[BATCH 21/149] Loss_D: 0.7093 Loss_G: 0.7464 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.6811 Loss_G: 0.7423 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6927 Loss_G: 0.7355 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.7068 Loss_G: 0.7494 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6744 Loss_G: 0.7212 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.7253 Loss_G: 0.7285 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7866 Loss_G: 0.7526 acc: 71.9%\n",
      "[BATCH 28/149] Loss_D: 0.7502 Loss_G: 0.7693 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.7324 Loss_G: 0.7379 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7112 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7045 Loss_G: 0.7288 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7152 Loss_G: 0.7499 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.7079 Loss_G: 0.7453 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7166 Loss_G: 0.7350 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7327 Loss_G: 0.7415 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6950 Loss_G: 0.7371 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6788 Loss_G: 0.7174 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.7205 Loss_G: 0.7406 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7102 Loss_G: 0.7468 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6720 Loss_G: 0.7432 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6837 Loss_G: 0.7204 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6980 Loss_G: 0.7185 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7447 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.7208 Loss_G: 0.7507 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.7075 Loss_G: 0.7501 acc: 96.9%\n",
      "[BATCH 46/149] Loss_D: 0.6956 Loss_G: 0.7202 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.6791 Loss_G: 0.7293 acc: 93.8%\n",
      "[BATCH 48/149] Loss_D: 0.7084 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7324 Loss_G: 0.7262 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7126 Loss_G: 0.7277 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6811 Loss_G: 0.7430 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6884 Loss_G: 0.7297 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6987 Loss_G: 0.7276 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.7047 Loss_G: 0.7078 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.7026 Loss_G: 0.7142 acc: 95.3%\n",
      "[BATCH 56/149] Loss_D: 0.6997 Loss_G: 0.7126 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7341 Loss_G: 0.7396 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.6878 Loss_G: 0.7328 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.7206 Loss_G: 0.7428 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6821 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6969 Loss_G: 0.7451 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6823 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6720 Loss_G: 0.7334 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6738 Loss_G: 0.7358 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6968 Loss_G: 0.7308 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6796 Loss_G: 0.7254 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7320 Loss_G: 0.7428 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6909 Loss_G: 0.7273 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.7185 Loss_G: 0.7360 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6613 Loss_G: 0.7137 acc: 89.1%\n",
      "[EPOCH 3050] TEST ACC is : 73.8%\n",
      "[BATCH 71/149] Loss_D: 0.6974 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7227 Loss_G: 0.7428 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7142 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7316 Loss_G: 0.7402 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6933 Loss_G: 0.7534 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7040 Loss_G: 0.7434 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7156 Loss_G: 0.7577 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.7177 Loss_G: 0.7585 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.7221 Loss_G: 0.7503 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6972 Loss_G: 0.7309 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6881 Loss_G: 0.7438 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6714 Loss_G: 0.7336 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6874 Loss_G: 0.7467 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7418 Loss_G: 0.7547 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6919 Loss_G: 0.7703 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6948 Loss_G: 0.7458 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6739 Loss_G: 0.7428 acc: 95.3%\n",
      "[BATCH 88/149] Loss_D: 0.6934 Loss_G: 0.7391 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6814 Loss_G: 0.7389 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7205 Loss_G: 0.7616 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.7435 Loss_G: 0.7577 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.6841 Loss_G: 0.7405 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7347 Loss_G: 0.7569 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7548 Loss_G: 0.7587 acc: 79.7%\n",
      "[BATCH 95/149] Loss_D: 0.7266 Loss_G: 0.7373 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.6990 Loss_G: 0.7159 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6992 Loss_G: 0.7154 acc: 81.2%\n",
      "[BATCH 98/149] Loss_D: 0.7216 Loss_G: 0.7446 acc: 81.2%\n",
      "[BATCH 99/149] Loss_D: 0.7152 Loss_G: 0.7228 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6926 Loss_G: 0.7269 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7387 Loss_G: 0.7498 acc: 93.8%\n",
      "[BATCH 102/149] Loss_D: 0.6960 Loss_G: 0.7337 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.7016 Loss_G: 0.7500 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7252 Loss_G: 0.7611 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.7126 Loss_G: 0.7657 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7197 Loss_G: 0.7784 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.7103 Loss_G: 0.7636 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.7420 Loss_G: 0.7569 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.7053 Loss_G: 0.7260 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.7560 Loss_G: 0.7310 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7197 Loss_G: 0.7347 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7160 Loss_G: 0.7233 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7617 Loss_G: 0.7346 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7349 Loss_G: 0.7416 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6819 Loss_G: 0.7257 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6824 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7209 Loss_G: 0.7361 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6825 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7265 Loss_G: 0.7324 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6891 Loss_G: 0.7475 acc: 90.6%\n",
      "[EPOCH 3100] TEST ACC is : 75.2%\n",
      "[BATCH 121/149] Loss_D: 0.7244 Loss_G: 0.7277 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6816 Loss_G: 0.7204 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7146 Loss_G: 0.7443 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6974 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7446 Loss_G: 0.7347 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7089 Loss_G: 0.7320 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6813 Loss_G: 0.7336 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7215 Loss_G: 0.7411 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6810 Loss_G: 0.7304 acc: 93.8%\n",
      "[BATCH 130/149] Loss_D: 0.6819 Loss_G: 0.7548 acc: 95.3%\n",
      "[BATCH 131/149] Loss_D: 0.7087 Loss_G: 0.7535 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.7459 Loss_G: 0.7626 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6875 Loss_G: 0.7284 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7192 Loss_G: 0.7111 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6834 Loss_G: 0.7161 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6688 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.7509 Loss_G: 0.7473 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7036 Loss_G: 0.7336 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6971 Loss_G: 0.7449 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7347 Loss_G: 0.7775 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7243 Loss_G: 0.7493 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7248 Loss_G: 0.7476 acc: 81.2%\n",
      "[BATCH 143/149] Loss_D: 0.7238 Loss_G: 0.7515 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7349 Loss_G: 0.7540 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6957 Loss_G: 0.7437 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7961 Loss_G: 0.7396 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.7186 Loss_G: 0.7382 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6693 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6933 Loss_G: 0.7135 acc: 84.4%\n",
      "-----THE [21/50] epoch end-----\n",
      "-----THE [22/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7612 Loss_G: 0.7376 acc: 82.8%\n",
      "[BATCH 2/149] Loss_D: 0.6941 Loss_G: 0.7365 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7342 Loss_G: 0.7362 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.7025 Loss_G: 0.7413 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7071 Loss_G: 0.7531 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.7023 Loss_G: 0.7521 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7032 Loss_G: 0.7738 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.7037 Loss_G: 0.7736 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6924 Loss_G: 0.7418 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6959 Loss_G: 0.7355 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6970 Loss_G: 0.7222 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.7224 Loss_G: 0.7288 acc: 95.3%\n",
      "[BATCH 13/149] Loss_D: 0.7503 Loss_G: 0.7530 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.7290 Loss_G: 0.7554 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6989 Loss_G: 0.7356 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7044 Loss_G: 0.7282 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7056 Loss_G: 0.7331 acc: 93.8%\n",
      "[BATCH 18/149] Loss_D: 0.6860 Loss_G: 0.7286 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7455 Loss_G: 0.7398 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7248 Loss_G: 0.7431 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6712 Loss_G: 0.7335 acc: 89.1%\n",
      "[EPOCH 3150] TEST ACC is : 75.4%\n",
      "[BATCH 22/149] Loss_D: 0.7140 Loss_G: 0.7409 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7369 Loss_G: 0.7479 acc: 82.8%\n",
      "[BATCH 24/149] Loss_D: 0.7008 Loss_G: 0.7456 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.7083 Loss_G: 0.7351 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6954 Loss_G: 0.7390 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7404 Loss_G: 0.7320 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6962 Loss_G: 0.7241 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.7077 Loss_G: 0.7425 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.7344 Loss_G: 0.7491 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6947 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7269 Loss_G: 0.7467 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.7034 Loss_G: 0.7440 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7247 Loss_G: 0.7399 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7271 Loss_G: 0.7614 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7762 Loss_G: 0.7797 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7060 Loss_G: 0.7622 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6919 Loss_G: 0.7372 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7180 Loss_G: 0.7396 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6958 Loss_G: 0.7433 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.7172 Loss_G: 0.7245 acc: 81.2%\n",
      "[BATCH 42/149] Loss_D: 0.6751 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7200 Loss_G: 0.7294 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.7268 Loss_G: 0.7481 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7316 Loss_G: 0.7522 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6926 Loss_G: 0.7687 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7149 Loss_G: 0.7407 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6929 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7154 Loss_G: 0.7396 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7097 Loss_G: 0.7255 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.7405 Loss_G: 0.7434 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6833 Loss_G: 0.7423 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7265 Loss_G: 0.7424 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.6780 Loss_G: 0.7264 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6850 Loss_G: 0.7641 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.6899 Loss_G: 0.7292 acc: 79.7%\n",
      "[BATCH 57/149] Loss_D: 0.7114 Loss_G: 0.7224 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7249 Loss_G: 0.7517 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.6893 Loss_G: 0.7476 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.7103 Loss_G: 0.7283 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.7265 Loss_G: 0.7426 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6920 Loss_G: 0.7238 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7445 Loss_G: 0.7448 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.7229 Loss_G: 0.7544 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6852 Loss_G: 0.7381 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6969 Loss_G: 0.7307 acc: 79.7%\n",
      "[BATCH 67/149] Loss_D: 0.7178 Loss_G: 0.7300 acc: 81.2%\n",
      "[BATCH 68/149] Loss_D: 0.6873 Loss_G: 0.7176 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.7074 Loss_G: 0.7097 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6899 Loss_G: 0.7245 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7255 Loss_G: 0.7494 acc: 93.8%\n",
      "[EPOCH 3200] TEST ACC is : 74.6%\n",
      "[BATCH 72/149] Loss_D: 0.7103 Loss_G: 0.7389 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.7034 Loss_G: 0.7370 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7210 Loss_G: 0.7477 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.7268 Loss_G: 0.7561 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6994 Loss_G: 0.7295 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.7119 Loss_G: 0.7592 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.7023 Loss_G: 0.7422 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.7181 Loss_G: 0.7409 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.7354 Loss_G: 0.7375 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.7075 Loss_G: 0.7503 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6869 Loss_G: 0.7408 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7150 Loss_G: 0.7372 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.7429 Loss_G: 0.7314 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.7431 Loss_G: 0.7393 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7213 Loss_G: 0.7291 acc: 81.2%\n",
      "[BATCH 87/149] Loss_D: 0.6930 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7073 Loss_G: 0.7371 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6889 Loss_G: 0.7341 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6814 Loss_G: 0.7491 acc: 79.7%\n",
      "[BATCH 91/149] Loss_D: 0.7480 Loss_G: 0.7513 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.6863 Loss_G: 0.7344 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.6895 Loss_G: 0.7265 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6905 Loss_G: 0.7271 acc: 79.7%\n",
      "[BATCH 95/149] Loss_D: 0.6899 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6846 Loss_G: 0.7134 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7406 Loss_G: 0.7350 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6754 Loss_G: 0.7339 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7119 Loss_G: 0.7437 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.6909 Loss_G: 0.7321 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6811 Loss_G: 0.7431 acc: 95.3%\n",
      "[BATCH 102/149] Loss_D: 0.6713 Loss_G: 0.7154 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.7335 Loss_G: 0.7363 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7215 Loss_G: 0.7401 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.7494 Loss_G: 0.7430 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7012 Loss_G: 0.7408 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6959 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6747 Loss_G: 0.7220 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7097 Loss_G: 0.7095 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6877 Loss_G: 0.7151 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7437 Loss_G: 0.7398 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6967 Loss_G: 0.7331 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7067 Loss_G: 0.7280 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7243 Loss_G: 0.7161 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6950 Loss_G: 0.7214 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6811 Loss_G: 0.7229 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7069 Loss_G: 0.7288 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7529 Loss_G: 0.7368 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6854 Loss_G: 0.7277 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7057 Loss_G: 0.7177 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6820 Loss_G: 0.7050 acc: 89.1%\n",
      "[EPOCH 3250] TEST ACC is : 74.4%\n",
      "[BATCH 122/149] Loss_D: 0.7405 Loss_G: 0.7254 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.7435 Loss_G: 0.7430 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7270 Loss_G: 0.7572 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.7174 Loss_G: 0.7639 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6894 Loss_G: 0.7382 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6916 Loss_G: 0.7533 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7005 Loss_G: 0.7369 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6799 Loss_G: 0.7241 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.7038 Loss_G: 0.7281 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6895 Loss_G: 0.7412 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.7057 Loss_G: 0.7356 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6853 Loss_G: 0.7333 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7135 Loss_G: 0.7264 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6911 Loss_G: 0.7226 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.7014 Loss_G: 0.7237 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6710 Loss_G: 0.7325 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.7037 Loss_G: 0.7213 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7005 Loss_G: 0.7230 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.7377 Loss_G: 0.7268 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6986 Loss_G: 0.7274 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.7297 Loss_G: 0.7639 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.6865 Loss_G: 0.7270 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.7047 Loss_G: 0.7273 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7008 Loss_G: 0.7399 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.7299 Loss_G: 0.7364 acc: 79.7%\n",
      "[BATCH 147/149] Loss_D: 0.7259 Loss_G: 0.7367 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6911 Loss_G: 0.7363 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.7210 Loss_G: 0.7387 acc: 92.2%\n",
      "-----THE [22/50] epoch end-----\n",
      "-----THE [23/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6915 Loss_G: 0.7398 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7082 Loss_G: 0.7101 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7042 Loss_G: 0.7212 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7133 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6777 Loss_G: 0.7236 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.6765 Loss_G: 0.7235 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6852 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6860 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6995 Loss_G: 0.7240 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.7204 Loss_G: 0.7372 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7088 Loss_G: 0.7458 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.7241 Loss_G: 0.7541 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.6775 Loss_G: 0.7474 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6821 Loss_G: 0.7408 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.7147 Loss_G: 0.7404 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6965 Loss_G: 0.7197 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.7099 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7063 Loss_G: 0.7393 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7198 Loss_G: 0.7283 acc: 79.7%\n",
      "[BATCH 20/149] Loss_D: 0.7180 Loss_G: 0.7177 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.7005 Loss_G: 0.7071 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7019 Loss_G: 0.7249 acc: 87.5%\n",
      "[EPOCH 3300] TEST ACC is : 75.2%\n",
      "[BATCH 23/149] Loss_D: 0.7393 Loss_G: 0.7596 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7488 Loss_G: 0.7852 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.7264 Loss_G: 0.7662 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.7085 Loss_G: 0.7334 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6715 Loss_G: 0.7170 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.7184 Loss_G: 0.7287 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6990 Loss_G: 0.7380 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7593 Loss_G: 0.7527 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7129 Loss_G: 0.7694 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.7177 Loss_G: 0.7590 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.6929 Loss_G: 0.7788 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6921 Loss_G: 0.7510 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.7416 Loss_G: 0.7404 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6906 Loss_G: 0.7560 acc: 95.3%\n",
      "[BATCH 37/149] Loss_D: 0.6678 Loss_G: 0.6969 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6992 Loss_G: 0.7097 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.7079 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.7754 Loss_G: 0.7548 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6899 Loss_G: 0.7348 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.7379 Loss_G: 0.7548 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.7367 Loss_G: 0.7694 acc: 95.3%\n",
      "[BATCH 44/149] Loss_D: 0.7406 Loss_G: 0.7536 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6916 Loss_G: 0.7419 acc: 82.8%\n",
      "[BATCH 46/149] Loss_D: 0.6912 Loss_G: 0.7329 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7408 Loss_G: 0.7407 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6796 Loss_G: 0.7426 acc: 93.8%\n",
      "[BATCH 49/149] Loss_D: 0.6853 Loss_G: 0.7326 acc: 93.8%\n",
      "[BATCH 50/149] Loss_D: 0.7352 Loss_G: 0.7343 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.6876 Loss_G: 0.7117 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.7161 Loss_G: 0.7267 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.6824 Loss_G: 0.7315 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6724 Loss_G: 0.7184 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.7333 Loss_G: 0.7332 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6963 Loss_G: 0.7584 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6896 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6952 Loss_G: 0.7223 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6894 Loss_G: 0.7186 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6991 Loss_G: 0.7091 acc: 93.8%\n",
      "[BATCH 61/149] Loss_D: 0.6878 Loss_G: 0.7147 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6774 Loss_G: 0.7145 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7005 Loss_G: 0.7401 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7347 Loss_G: 0.7440 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7643 Loss_G: 0.7530 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6973 Loss_G: 0.7748 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.7223 Loss_G: 0.7397 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6831 Loss_G: 0.7365 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.6903 Loss_G: 0.7518 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7220 Loss_G: 0.7492 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6919 Loss_G: 0.7451 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6922 Loss_G: 0.7314 acc: 92.2%\n",
      "[EPOCH 3350] TEST ACC is : 75.4%\n",
      "[BATCH 73/149] Loss_D: 0.6990 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6804 Loss_G: 0.7248 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.6860 Loss_G: 0.7510 acc: 96.9%\n",
      "[BATCH 76/149] Loss_D: 0.7004 Loss_G: 0.7545 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6985 Loss_G: 0.7335 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6831 Loss_G: 0.7241 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7050 Loss_G: 0.7346 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6880 Loss_G: 0.7196 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.7238 Loss_G: 0.7238 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7497 Loss_G: 0.7515 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6906 Loss_G: 0.7351 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6745 Loss_G: 0.7278 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.6983 Loss_G: 0.7272 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6708 Loss_G: 0.7297 acc: 95.3%\n",
      "[BATCH 87/149] Loss_D: 0.7584 Loss_G: 0.7462 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7082 Loss_G: 0.7312 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7173 Loss_G: 0.7593 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7149 Loss_G: 0.7711 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7518 Loss_G: 0.7563 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.7148 Loss_G: 0.7541 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7284 Loss_G: 0.7477 acc: 95.3%\n",
      "[BATCH 94/149] Loss_D: 0.7443 Loss_G: 0.7534 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6946 Loss_G: 0.7314 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7399 Loss_G: 0.7484 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6794 Loss_G: 0.7507 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.7202 Loss_G: 0.7549 acc: 95.3%\n",
      "[BATCH 99/149] Loss_D: 0.7111 Loss_G: 0.7425 acc: 93.8%\n",
      "[BATCH 100/149] Loss_D: 0.7577 Loss_G: 0.7615 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6977 Loss_G: 0.7466 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7444 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6846 Loss_G: 0.7259 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7607 Loss_G: 0.7476 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.7351 Loss_G: 0.7508 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.7062 Loss_G: 0.7309 acc: 79.7%\n",
      "[BATCH 107/149] Loss_D: 0.7050 Loss_G: 0.7332 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6782 Loss_G: 0.7262 acc: 84.4%\n",
      "[BATCH 109/149] Loss_D: 0.7502 Loss_G: 0.7311 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7355 Loss_G: 0.7480 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6874 Loss_G: 0.7313 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.7216 Loss_G: 0.7305 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7334 Loss_G: 0.7539 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7150 Loss_G: 0.7592 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6985 Loss_G: 0.7384 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6893 Loss_G: 0.7498 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.7063 Loss_G: 0.7201 acc: 81.2%\n",
      "[BATCH 118/149] Loss_D: 0.7378 Loss_G: 0.7220 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6974 Loss_G: 0.7331 acc: 78.1%\n",
      "[BATCH 120/149] Loss_D: 0.6830 Loss_G: 0.7501 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.7082 Loss_G: 0.7765 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6913 Loss_G: 0.7607 acc: 90.6%\n",
      "[EPOCH 3400] TEST ACC is : 76.0%\n",
      "[BATCH 123/149] Loss_D: 0.7142 Loss_G: 0.7476 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.6948 Loss_G: 0.7482 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.7505 Loss_G: 0.7498 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.7018 Loss_G: 0.7471 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7098 Loss_G: 0.7425 acc: 79.7%\n",
      "[BATCH 128/149] Loss_D: 0.7085 Loss_G: 0.7290 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.7191 Loss_G: 0.7323 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6953 Loss_G: 0.7209 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7396 Loss_G: 0.7347 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.7457 Loss_G: 0.7616 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7347 Loss_G: 0.7799 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7190 Loss_G: 0.7612 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6814 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7100 Loss_G: 0.7335 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7200 Loss_G: 0.7681 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6880 Loss_G: 0.7320 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6907 Loss_G: 0.7481 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7255 Loss_G: 0.7272 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7009 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7200 Loss_G: 0.7078 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6568 Loss_G: 0.7211 acc: 95.3%\n",
      "[BATCH 144/149] Loss_D: 0.6708 Loss_G: 0.7046 acc: 95.3%\n",
      "[BATCH 145/149] Loss_D: 0.7102 Loss_G: 0.7307 acc: 81.2%\n",
      "[BATCH 146/149] Loss_D: 0.7267 Loss_G: 0.7373 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6996 Loss_G: 0.7411 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7425 Loss_G: 0.7470 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7109 Loss_G: 0.7585 acc: 92.2%\n",
      "-----THE [23/50] epoch end-----\n",
      "-----THE [24/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7068 Loss_G: 0.7704 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6938 Loss_G: 0.7462 acc: 95.3%\n",
      "[BATCH 3/149] Loss_D: 0.7279 Loss_G: 0.7516 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7134 Loss_G: 0.7444 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6807 Loss_G: 0.7250 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6818 Loss_G: 0.7188 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7008 Loss_G: 0.7341 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.7145 Loss_G: 0.7654 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7242 Loss_G: 0.7541 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.7461 Loss_G: 0.7386 acc: 79.7%\n",
      "[BATCH 11/149] Loss_D: 0.7111 Loss_G: 0.7461 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6898 Loss_G: 0.7433 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7300 Loss_G: 0.7502 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.7225 Loss_G: 0.7587 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7049 Loss_G: 0.7499 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.7398 Loss_G: 0.7730 acc: 81.2%\n",
      "[BATCH 17/149] Loss_D: 0.7049 Loss_G: 0.7420 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6983 Loss_G: 0.7424 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.7373 Loss_G: 0.7561 acc: 78.1%\n",
      "[BATCH 20/149] Loss_D: 0.6901 Loss_G: 0.7411 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.7010 Loss_G: 0.7279 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.7030 Loss_G: 0.7284 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6779 Loss_G: 0.7287 acc: 93.8%\n",
      "[EPOCH 3450] TEST ACC is : 76.8%\n",
      "[BATCH 24/149] Loss_D: 0.7396 Loss_G: 0.7561 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6722 Loss_G: 0.7344 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.7235 Loss_G: 0.7175 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.7197 Loss_G: 0.7423 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.7113 Loss_G: 0.7368 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6787 Loss_G: 0.7250 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6865 Loss_G: 0.7307 acc: 93.8%\n",
      "[BATCH 31/149] Loss_D: 0.7313 Loss_G: 0.7393 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6938 Loss_G: 0.7402 acc: 96.9%\n",
      "[BATCH 33/149] Loss_D: 0.7002 Loss_G: 0.7412 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.7227 Loss_G: 0.7287 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7261 Loss_G: 0.7303 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7461 Loss_G: 0.7543 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7256 Loss_G: 0.7485 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.7012 Loss_G: 0.7296 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7205 Loss_G: 0.7535 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.7007 Loss_G: 0.7271 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7344 Loss_G: 0.7355 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7021 Loss_G: 0.7475 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.7324 Loss_G: 0.7355 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6784 Loss_G: 0.7244 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.7139 Loss_G: 0.7335 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.7071 Loss_G: 0.7137 acc: 82.8%\n",
      "[BATCH 47/149] Loss_D: 0.6715 Loss_G: 0.7058 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.7155 Loss_G: 0.7222 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7023 Loss_G: 0.7323 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.7330 Loss_G: 0.7390 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.7170 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.7484 Loss_G: 0.7417 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7010 Loss_G: 0.7719 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6944 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.7132 Loss_G: 0.7285 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6655 Loss_G: 0.7373 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.7481 Loss_G: 0.7468 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7310 Loss_G: 0.7386 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.6666 Loss_G: 0.7362 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7073 Loss_G: 0.7295 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7566 Loss_G: 0.7374 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6941 Loss_G: 0.7415 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7405 Loss_G: 0.7357 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6787 Loss_G: 0.7159 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.7190 Loss_G: 0.7481 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7473 Loss_G: 0.7559 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7124 Loss_G: 0.7333 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6834 Loss_G: 0.7368 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6939 Loss_G: 0.7328 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6841 Loss_G: 0.7253 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6887 Loss_G: 0.7184 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7010 Loss_G: 0.7155 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7025 Loss_G: 0.7266 acc: 90.6%\n",
      "[EPOCH 3500] TEST ACC is : 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.6734 Loss_G: 0.7187 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.7161 Loss_G: 0.7046 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7000 Loss_G: 0.7188 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7127 Loss_G: 0.7283 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7565 Loss_G: 0.7591 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6909 Loss_G: 0.7327 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7348 Loss_G: 0.7460 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6670 Loss_G: 0.7367 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7055 Loss_G: 0.7324 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.7205 Loss_G: 0.7256 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7093 Loss_G: 0.7357 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6735 Loss_G: 0.7191 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6843 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6926 Loss_G: 0.7401 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7353 Loss_G: 0.7314 acc: 81.2%\n",
      "[BATCH 89/149] Loss_D: 0.6879 Loss_G: 0.7343 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7436 Loss_G: 0.7570 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.7341 Loss_G: 0.7472 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7199 Loss_G: 0.7353 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.7183 Loss_G: 0.7556 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6892 Loss_G: 0.7248 acc: 81.2%\n",
      "[BATCH 95/149] Loss_D: 0.7181 Loss_G: 0.7460 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.7142 Loss_G: 0.7392 acc: 82.8%\n",
      "[BATCH 97/149] Loss_D: 0.6658 Loss_G: 0.7215 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6768 Loss_G: 0.7413 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7420 Loss_G: 0.7522 acc: 93.8%\n",
      "[BATCH 100/149] Loss_D: 0.6925 Loss_G: 0.7497 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.7267 Loss_G: 0.7414 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6640 Loss_G: 0.7189 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6754 Loss_G: 0.7263 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6992 Loss_G: 0.7014 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.7229 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7111 Loss_G: 0.7170 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6748 Loss_G: 0.7089 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.7309 Loss_G: 0.7225 acc: 79.7%\n",
      "[BATCH 109/149] Loss_D: 0.6705 Loss_G: 0.7357 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7313 Loss_G: 0.7347 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7071 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7054 Loss_G: 0.7606 acc: 95.3%\n",
      "[BATCH 113/149] Loss_D: 0.7033 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.6786 Loss_G: 0.7206 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.6896 Loss_G: 0.7055 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.7091 Loss_G: 0.7236 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7134 Loss_G: 0.7191 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6980 Loss_G: 0.7200 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7289 Loss_G: 0.7509 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.7130 Loss_G: 0.7653 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7463 Loss_G: 0.7610 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7492 Loss_G: 0.7631 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7533 Loss_G: 0.7628 acc: 87.5%\n",
      "[EPOCH 3550] TEST ACC is : 74.8%\n",
      "[BATCH 124/149] Loss_D: 0.7436 Loss_G: 0.7898 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.7373 Loss_G: 0.7537 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.7213 Loss_G: 0.7411 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7496 Loss_G: 0.7519 acc: 78.1%\n",
      "[BATCH 128/149] Loss_D: 0.7204 Loss_G: 0.7517 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6899 Loss_G: 0.7431 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7057 Loss_G: 0.7424 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7008 Loss_G: 0.7509 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.7201 Loss_G: 0.7605 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6878 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6781 Loss_G: 0.7275 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6736 Loss_G: 0.7149 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7448 Loss_G: 0.7537 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7174 Loss_G: 0.7418 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7142 Loss_G: 0.7357 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7281 Loss_G: 0.7337 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6889 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6994 Loss_G: 0.7369 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.7012 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6775 Loss_G: 0.7338 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.7042 Loss_G: 0.7502 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7434 Loss_G: 0.7537 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6746 Loss_G: 0.7270 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.7186 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6935 Loss_G: 0.7396 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6914 Loss_G: 0.7344 acc: 89.1%\n",
      "-----THE [24/50] epoch end-----\n",
      "-----THE [25/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7021 Loss_G: 0.7324 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6940 Loss_G: 0.7353 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6970 Loss_G: 0.7288 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.6935 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7315 Loss_G: 0.7305 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7051 Loss_G: 0.7409 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.7300 Loss_G: 0.7785 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6910 Loss_G: 0.7735 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7181 Loss_G: 0.7912 acc: 95.3%\n",
      "[BATCH 10/149] Loss_D: 0.7188 Loss_G: 0.7623 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.7343 Loss_G: 0.7507 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.7146 Loss_G: 0.7549 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6944 Loss_G: 0.7524 acc: 96.9%\n",
      "[BATCH 14/149] Loss_D: 0.6673 Loss_G: 0.7437 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6995 Loss_G: 0.7279 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6944 Loss_G: 0.7344 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.7251 Loss_G: 0.7349 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6905 Loss_G: 0.7364 acc: 96.9%\n",
      "[BATCH 19/149] Loss_D: 0.7330 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.7053 Loss_G: 0.7163 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7159 Loss_G: 0.7310 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7175 Loss_G: 0.7480 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6795 Loss_G: 0.7464 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7128 Loss_G: 0.7238 acc: 92.2%\n",
      "[EPOCH 3600] TEST ACC is : 76.2%\n",
      "[BATCH 25/149] Loss_D: 0.7102 Loss_G: 0.7543 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6869 Loss_G: 0.7259 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6954 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7179 Loss_G: 0.7362 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6968 Loss_G: 0.7320 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6886 Loss_G: 0.7295 acc: 95.3%\n",
      "[BATCH 31/149] Loss_D: 0.7108 Loss_G: 0.7373 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.7196 Loss_G: 0.7260 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7294 Loss_G: 0.7268 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.7037 Loss_G: 0.7226 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7164 Loss_G: 0.7383 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6693 Loss_G: 0.7205 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7171 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6965 Loss_G: 0.7253 acc: 81.2%\n",
      "[BATCH 39/149] Loss_D: 0.6939 Loss_G: 0.7265 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6702 Loss_G: 0.7303 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6956 Loss_G: 0.7198 acc: 95.3%\n",
      "[BATCH 42/149] Loss_D: 0.7116 Loss_G: 0.7155 acc: 93.8%\n",
      "[BATCH 43/149] Loss_D: 0.7087 Loss_G: 0.7498 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.7346 Loss_G: 0.7497 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6919 Loss_G: 0.7325 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7013 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7053 Loss_G: 0.7305 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7216 Loss_G: 0.7286 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7129 Loss_G: 0.7313 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7280 Loss_G: 0.7479 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6816 Loss_G: 0.7304 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6987 Loss_G: 0.7301 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6938 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6838 Loss_G: 0.7220 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6958 Loss_G: 0.7479 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.7098 Loss_G: 0.7364 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7073 Loss_G: 0.7322 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7049 Loss_G: 0.7189 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6684 Loss_G: 0.7093 acc: 82.8%\n",
      "[BATCH 60/149] Loss_D: 0.6601 Loss_G: 0.7093 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6897 Loss_G: 0.7074 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6667 Loss_G: 0.7020 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7091 Loss_G: 0.7056 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6884 Loss_G: 0.7278 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7031 Loss_G: 0.7324 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7314 Loss_G: 0.7619 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.7565 Loss_G: 0.7595 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.6865 Loss_G: 0.7333 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.6959 Loss_G: 0.7397 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.6623 Loss_G: 0.7249 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7231 Loss_G: 0.7228 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7061 Loss_G: 0.7350 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7184 Loss_G: 0.7356 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6653 Loss_G: 0.7327 acc: 90.6%\n",
      "[EPOCH 3650] TEST ACC is : 75.0%\n",
      "[BATCH 75/149] Loss_D: 0.7120 Loss_G: 0.7383 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6739 Loss_G: 0.7109 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6790 Loss_G: 0.7342 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.7191 Loss_G: 0.7440 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7231 Loss_G: 0.7471 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.7192 Loss_G: 0.7419 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6781 Loss_G: 0.7301 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7214 Loss_G: 0.7388 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6811 Loss_G: 0.7268 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.7122 Loss_G: 0.7402 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6802 Loss_G: 0.7276 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.7152 Loss_G: 0.7336 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.6882 Loss_G: 0.7308 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6693 Loss_G: 0.7228 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7471 Loss_G: 0.7563 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7038 Loss_G: 0.7497 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.7164 Loss_G: 0.7333 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7050 Loss_G: 0.7315 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7304 Loss_G: 0.7237 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6975 Loss_G: 0.7532 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.7369 Loss_G: 0.7460 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6935 Loss_G: 0.7423 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.7194 Loss_G: 0.7721 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.7182 Loss_G: 0.7621 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6996 Loss_G: 0.7479 acc: 100.0%\n",
      "[BATCH 100/149] Loss_D: 0.7090 Loss_G: 0.7730 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.7574 Loss_G: 0.7538 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.7320 Loss_G: 0.7559 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7145 Loss_G: 0.7366 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.7143 Loss_G: 0.7450 acc: 79.7%\n",
      "[BATCH 105/149] Loss_D: 0.7157 Loss_G: 0.7393 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.7227 Loss_G: 0.7512 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7263 Loss_G: 0.7469 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7449 Loss_G: 0.7495 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7223 Loss_G: 0.7628 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.7391 Loss_G: 0.7654 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.7177 Loss_G: 0.7403 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6841 Loss_G: 0.7291 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7271 Loss_G: 0.7389 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6692 Loss_G: 0.7099 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7250 Loss_G: 0.7167 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7012 Loss_G: 0.7201 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.7363 Loss_G: 0.7263 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.6708 Loss_G: 0.7163 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7279 Loss_G: 0.7287 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7090 Loss_G: 0.7316 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7197 Loss_G: 0.7438 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6849 Loss_G: 0.7328 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7056 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.7114 Loss_G: 0.7608 acc: 89.1%\n",
      "[EPOCH 3700] TEST ACC is : 75.4%\n",
      "[BATCH 125/149] Loss_D: 0.7100 Loss_G: 0.7501 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6727 Loss_G: 0.7201 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.7608 Loss_G: 0.7487 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7340 Loss_G: 0.7687 acc: 81.2%\n",
      "[BATCH 129/149] Loss_D: 0.7105 Loss_G: 0.7658 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7075 Loss_G: 0.7546 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.7122 Loss_G: 0.7525 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7010 Loss_G: 0.7504 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.7335 Loss_G: 0.7543 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.7197 Loss_G: 0.7586 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6551 Loss_G: 0.7395 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.7408 Loss_G: 0.7377 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.7444 Loss_G: 0.7394 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6814 Loss_G: 0.7239 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.7039 Loss_G: 0.7204 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6945 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6784 Loss_G: 0.7216 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7270 Loss_G: 0.7269 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6970 Loss_G: 0.7282 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.6856 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6920 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7801 Loss_G: 0.7546 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.7046 Loss_G: 0.7405 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7564 Loss_G: 0.7353 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.7268 Loss_G: 0.7418 acc: 90.6%\n",
      "-----THE [25/50] epoch end-----\n",
      "-----THE [26/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7168 Loss_G: 0.7299 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7995 Loss_G: 0.8033 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7008 Loss_G: 0.7314 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.7131 Loss_G: 0.7383 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.7325 Loss_G: 0.7287 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.7155 Loss_G: 0.7440 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.6947 Loss_G: 0.7374 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6958 Loss_G: 0.7593 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6990 Loss_G: 0.7358 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.7335 Loss_G: 0.7377 acc: 79.7%\n",
      "[BATCH 11/149] Loss_D: 0.7299 Loss_G: 0.7507 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6820 Loss_G: 0.7339 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.7043 Loss_G: 0.7364 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.7028 Loss_G: 0.7466 acc: 96.9%\n",
      "[BATCH 15/149] Loss_D: 0.7524 Loss_G: 0.7629 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.7030 Loss_G: 0.7534 acc: 81.2%\n",
      "[BATCH 17/149] Loss_D: 0.6896 Loss_G: 0.7459 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7131 Loss_G: 0.7380 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.6656 Loss_G: 0.7225 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.7154 Loss_G: 0.7260 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.7157 Loss_G: 0.7266 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.6937 Loss_G: 0.7458 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7247 Loss_G: 0.7485 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.6847 Loss_G: 0.7421 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.6837 Loss_G: 0.7234 acc: 84.4%\n",
      "[EPOCH 3750] TEST ACC is : 75.6%\n",
      "[BATCH 26/149] Loss_D: 0.7214 Loss_G: 0.7348 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7278 Loss_G: 0.7470 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6620 Loss_G: 0.7429 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.7316 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7146 Loss_G: 0.7481 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6831 Loss_G: 0.7331 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7287 Loss_G: 0.7511 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.7014 Loss_G: 0.7510 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6688 Loss_G: 0.7322 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.6730 Loss_G: 0.7265 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.7269 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7142 Loss_G: 0.7275 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.7197 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6767 Loss_G: 0.7401 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.6821 Loss_G: 0.7234 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.7027 Loss_G: 0.7501 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6859 Loss_G: 0.7308 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7498 Loss_G: 0.7336 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.7737 Loss_G: 0.7263 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6919 Loss_G: 0.7342 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6863 Loss_G: 0.7327 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7603 Loss_G: 0.7398 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6971 Loss_G: 0.7337 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.6856 Loss_G: 0.7261 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7523 Loss_G: 0.7412 acc: 82.8%\n",
      "[BATCH 51/149] Loss_D: 0.6837 Loss_G: 0.7342 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.6789 Loss_G: 0.7276 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6978 Loss_G: 0.7206 acc: 79.7%\n",
      "[BATCH 54/149] Loss_D: 0.7487 Loss_G: 0.7567 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6972 Loss_G: 0.7318 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6918 Loss_G: 0.7326 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7101 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6855 Loss_G: 0.7071 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7025 Loss_G: 0.7183 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6916 Loss_G: 0.7340 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7231 Loss_G: 0.7410 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7034 Loss_G: 0.7297 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7011 Loss_G: 0.7369 acc: 95.3%\n",
      "[BATCH 64/149] Loss_D: 0.7028 Loss_G: 0.7234 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.6993 Loss_G: 0.7410 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7119 Loss_G: 0.7501 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6853 Loss_G: 0.7460 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7197 Loss_G: 0.7329 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.7164 Loss_G: 0.7517 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.6804 Loss_G: 0.7186 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7149 Loss_G: 0.7238 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.7061 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6990 Loss_G: 0.7154 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.7242 Loss_G: 0.7240 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6920 Loss_G: 0.7136 acc: 87.5%\n",
      "[EPOCH 3800] TEST ACC is : 76.2%\n",
      "[BATCH 76/149] Loss_D: 0.7728 Loss_G: 0.7443 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6785 Loss_G: 0.7453 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7070 Loss_G: 0.7328 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6735 Loss_G: 0.7449 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6677 Loss_G: 0.7453 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.7422 Loss_G: 0.7599 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7094 Loss_G: 0.7440 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7201 Loss_G: 0.7447 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.7126 Loss_G: 0.7534 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.7254 Loss_G: 0.7631 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6943 Loss_G: 0.7356 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7462 Loss_G: 0.7350 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7525 Loss_G: 0.7486 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.7012 Loss_G: 0.7636 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7053 Loss_G: 0.7444 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.6653 Loss_G: 0.7346 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6985 Loss_G: 0.7137 acc: 96.9%\n",
      "[BATCH 93/149] Loss_D: 0.6642 Loss_G: 0.7385 acc: 95.3%\n",
      "[BATCH 94/149] Loss_D: 0.6733 Loss_G: 0.7277 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7270 Loss_G: 0.7390 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.7310 Loss_G: 0.7557 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6706 Loss_G: 0.7411 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.7374 Loss_G: 0.7306 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.7067 Loss_G: 0.7403 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7046 Loss_G: 0.7335 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6734 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.7132 Loss_G: 0.7368 acc: 84.4%\n",
      "[BATCH 103/149] Loss_D: 0.6886 Loss_G: 0.7165 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6902 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7159 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6949 Loss_G: 0.7360 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7045 Loss_G: 0.7160 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.6817 Loss_G: 0.7308 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6942 Loss_G: 0.7439 acc: 96.9%\n",
      "[BATCH 110/149] Loss_D: 0.7158 Loss_G: 0.7329 acc: 81.2%\n",
      "[BATCH 111/149] Loss_D: 0.7542 Loss_G: 0.7551 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.6887 Loss_G: 0.7311 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7149 Loss_G: 0.7101 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.7265 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6795 Loss_G: 0.7213 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.6952 Loss_G: 0.7243 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.6727 Loss_G: 0.7239 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7057 Loss_G: 0.7273 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6797 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7400 Loss_G: 0.7411 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7292 Loss_G: 0.7321 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7207 Loss_G: 0.7330 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7116 Loss_G: 0.7457 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.6641 Loss_G: 0.7345 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.6856 Loss_G: 0.7200 acc: 81.2%\n",
      "[EPOCH 3850] TEST ACC is : 76.4%\n",
      "[BATCH 126/149] Loss_D: 0.6745 Loss_G: 0.7121 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.6984 Loss_G: 0.7381 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6964 Loss_G: 0.7271 acc: 81.2%\n",
      "[BATCH 129/149] Loss_D: 0.7250 Loss_G: 0.7194 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.6946 Loss_G: 0.7289 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.7200 Loss_G: 0.7371 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6852 Loss_G: 0.7371 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6878 Loss_G: 0.7384 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7214 Loss_G: 0.7314 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.7299 Loss_G: 0.7413 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6977 Loss_G: 0.7488 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.7444 Loss_G: 0.7549 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.7041 Loss_G: 0.7329 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7235 Loss_G: 0.7410 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.7256 Loss_G: 0.7418 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.7012 Loss_G: 0.7247 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.7491 Loss_G: 0.7418 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.7213 Loss_G: 0.7448 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7650 Loss_G: 0.7813 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6928 Loss_G: 0.7729 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7068 Loss_G: 0.7416 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.7079 Loss_G: 0.7337 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7196 Loss_G: 0.7298 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7215 Loss_G: 0.7622 acc: 92.2%\n",
      "-----THE [26/50] epoch end-----\n",
      "-----THE [27/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7151 Loss_G: 0.7296 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7310 Loss_G: 0.7411 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.7491 Loss_G: 0.7784 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.8019 Loss_G: 0.7783 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7487 Loss_G: 0.7709 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7001 Loss_G: 0.7596 acc: 79.7%\n",
      "[BATCH 7/149] Loss_D: 0.6928 Loss_G: 0.7507 acc: 95.3%\n",
      "[BATCH 8/149] Loss_D: 0.6860 Loss_G: 0.7357 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7121 Loss_G: 0.7340 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.7401 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6929 Loss_G: 0.7312 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6917 Loss_G: 0.7169 acc: 81.2%\n",
      "[BATCH 13/149] Loss_D: 0.7115 Loss_G: 0.7314 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.7077 Loss_G: 0.7321 acc: 81.2%\n",
      "[BATCH 15/149] Loss_D: 0.6644 Loss_G: 0.7252 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7129 Loss_G: 0.7204 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6787 Loss_G: 0.7107 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6914 Loss_G: 0.7125 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.7377 Loss_G: 0.7241 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6851 Loss_G: 0.7258 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7094 Loss_G: 0.7283 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6789 Loss_G: 0.7179 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.7029 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.7164 Loss_G: 0.7521 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.7235 Loss_G: 0.7380 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7359 Loss_G: 0.7504 acc: 90.6%\n",
      "[EPOCH 3900] TEST ACC is : 77.0%\n",
      "[BATCH 27/149] Loss_D: 0.7253 Loss_G: 0.7515 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.7075 Loss_G: 0.7441 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.7049 Loss_G: 0.7443 acc: 93.8%\n",
      "[BATCH 30/149] Loss_D: 0.6925 Loss_G: 0.7397 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7169 Loss_G: 0.7385 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7471 Loss_G: 0.7477 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6909 Loss_G: 0.7378 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.7154 Loss_G: 0.7300 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6788 Loss_G: 0.7174 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7381 Loss_G: 0.7329 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6970 Loss_G: 0.7336 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.7165 Loss_G: 0.7309 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6814 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6926 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6635 Loss_G: 0.6970 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6769 Loss_G: 0.7102 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7084 Loss_G: 0.7252 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6953 Loss_G: 0.7266 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6966 Loss_G: 0.7403 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.7676 Loss_G: 0.7586 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7406 Loss_G: 0.7575 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6718 Loss_G: 0.7385 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7044 Loss_G: 0.7311 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7024 Loss_G: 0.7418 acc: 82.8%\n",
      "[BATCH 51/149] Loss_D: 0.6679 Loss_G: 0.7225 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.7241 Loss_G: 0.7475 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7015 Loss_G: 0.7513 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7356 Loss_G: 0.7314 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.7002 Loss_G: 0.7404 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6961 Loss_G: 0.7343 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6856 Loss_G: 0.7567 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7063 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7179 Loss_G: 0.7589 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6666 Loss_G: 0.7291 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7106 Loss_G: 0.7280 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6973 Loss_G: 0.7337 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7444 Loss_G: 0.7275 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7134 Loss_G: 0.7244 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7317 Loss_G: 0.7369 acc: 79.7%\n",
      "[BATCH 66/149] Loss_D: 0.7132 Loss_G: 0.7299 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6831 Loss_G: 0.7251 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7528 Loss_G: 0.7671 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.7244 Loss_G: 0.7525 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7140 Loss_G: 0.7417 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6616 Loss_G: 0.7282 acc: 93.8%\n",
      "[BATCH 72/149] Loss_D: 0.6794 Loss_G: 0.7365 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6890 Loss_G: 0.7205 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7059 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7130 Loss_G: 0.7403 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6842 Loss_G: 0.7272 acc: 85.9%\n",
      "[EPOCH 3950] TEST ACC is : 76.8%\n",
      "[BATCH 77/149] Loss_D: 0.7105 Loss_G: 0.7403 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7101 Loss_G: 0.7292 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6874 Loss_G: 0.7052 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.7179 Loss_G: 0.7060 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7141 Loss_G: 0.7275 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.6843 Loss_G: 0.7183 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6720 Loss_G: 0.7102 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7080 Loss_G: 0.7542 acc: 98.4%\n",
      "[BATCH 85/149] Loss_D: 0.6917 Loss_G: 0.7318 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6831 Loss_G: 0.7263 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6991 Loss_G: 0.7327 acc: 98.4%\n",
      "[BATCH 88/149] Loss_D: 0.6982 Loss_G: 0.7437 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.6895 Loss_G: 0.7662 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7416 Loss_G: 0.7517 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6987 Loss_G: 0.7283 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6889 Loss_G: 0.7422 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7168 Loss_G: 0.7259 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6838 Loss_G: 0.7468 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.7259 Loss_G: 0.7490 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6926 Loss_G: 0.7451 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6965 Loss_G: 0.7187 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.7261 Loss_G: 0.7429 acc: 78.1%\n",
      "[BATCH 99/149] Loss_D: 0.6998 Loss_G: 0.7262 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7153 Loss_G: 0.7476 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7852 Loss_G: 0.7630 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7465 Loss_G: 0.7643 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6916 Loss_G: 0.7442 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7197 Loss_G: 0.7463 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.7239 Loss_G: 0.7449 acc: 95.3%\n",
      "[BATCH 106/149] Loss_D: 0.7016 Loss_G: 0.7511 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.7245 Loss_G: 0.7323 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6976 Loss_G: 0.7355 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.7274 Loss_G: 0.7339 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.7307 Loss_G: 0.7306 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6869 Loss_G: 0.7137 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7235 Loss_G: 0.7320 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6782 Loss_G: 0.7380 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7208 Loss_G: 0.7606 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6799 Loss_G: 0.7706 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6897 Loss_G: 0.7677 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6618 Loss_G: 0.7382 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6880 Loss_G: 0.7402 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.7304 Loss_G: 0.7660 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.7349 Loss_G: 0.7267 acc: 79.7%\n",
      "[BATCH 121/149] Loss_D: 0.6916 Loss_G: 0.7138 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.7034 Loss_G: 0.7427 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.7263 Loss_G: 0.7526 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6880 Loss_G: 0.7481 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.7032 Loss_G: 0.7210 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.7528 Loss_G: 0.7451 acc: 92.2%\n",
      "[EPOCH 4000] TEST ACC is : 76.6%\n",
      "[BATCH 127/149] Loss_D: 0.7293 Loss_G: 0.7502 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7301 Loss_G: 0.7327 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7246 Loss_G: 0.7470 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.6820 Loss_G: 0.7394 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6853 Loss_G: 0.7196 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.7274 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6863 Loss_G: 0.7256 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7010 Loss_G: 0.7290 acc: 79.7%\n",
      "[BATCH 135/149] Loss_D: 0.7583 Loss_G: 0.7322 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6872 Loss_G: 0.7250 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7155 Loss_G: 0.7355 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.7093 Loss_G: 0.7386 acc: 84.4%\n",
      "[BATCH 139/149] Loss_D: 0.7418 Loss_G: 0.7408 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.6871 Loss_G: 0.7405 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6705 Loss_G: 0.7278 acc: 96.9%\n",
      "[BATCH 142/149] Loss_D: 0.7621 Loss_G: 0.7705 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.6968 Loss_G: 0.7357 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6521 Loss_G: 0.7324 acc: 96.9%\n",
      "[BATCH 145/149] Loss_D: 0.6955 Loss_G: 0.7312 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.7041 Loss_G: 0.7315 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.7180 Loss_G: 0.7681 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6813 Loss_G: 0.7453 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.6846 Loss_G: 0.7125 acc: 89.1%\n",
      "-----THE [27/50] epoch end-----\n",
      "-----THE [28/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6908 Loss_G: 0.7236 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7100 Loss_G: 0.7427 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7220 Loss_G: 0.7327 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7220 Loss_G: 0.7529 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7172 Loss_G: 0.7463 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6965 Loss_G: 0.7467 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6971 Loss_G: 0.7396 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6742 Loss_G: 0.7173 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6916 Loss_G: 0.7154 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.6916 Loss_G: 0.7102 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.7181 Loss_G: 0.7271 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6751 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6672 Loss_G: 0.7208 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.7159 Loss_G: 0.7312 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6979 Loss_G: 0.7348 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6840 Loss_G: 0.7214 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.7494 Loss_G: 0.7581 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6691 Loss_G: 0.7398 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.7355 Loss_G: 0.7367 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.7067 Loss_G: 0.7228 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6803 Loss_G: 0.7258 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.6687 Loss_G: 0.7280 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6669 Loss_G: 0.7349 acc: 95.3%\n",
      "[BATCH 24/149] Loss_D: 0.7307 Loss_G: 0.7823 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.7137 Loss_G: 0.7535 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.7475 Loss_G: 0.7443 acc: 78.1%\n",
      "[BATCH 27/149] Loss_D: 0.6853 Loss_G: 0.7254 acc: 89.1%\n",
      "[EPOCH 4050] TEST ACC is : 76.8%\n",
      "[BATCH 28/149] Loss_D: 0.7291 Loss_G: 0.7498 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6862 Loss_G: 0.7381 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6973 Loss_G: 0.7308 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6990 Loss_G: 0.7301 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.7264 Loss_G: 0.7228 acc: 81.2%\n",
      "[BATCH 33/149] Loss_D: 0.7223 Loss_G: 0.7375 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7376 Loss_G: 0.7261 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7114 Loss_G: 0.7459 acc: 78.1%\n",
      "[BATCH 36/149] Loss_D: 0.7069 Loss_G: 0.7330 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7159 Loss_G: 0.7455 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6918 Loss_G: 0.7435 acc: 96.9%\n",
      "[BATCH 39/149] Loss_D: 0.6901 Loss_G: 0.7361 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6937 Loss_G: 0.7129 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.6911 Loss_G: 0.7115 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.6642 Loss_G: 0.7183 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.7445 Loss_G: 0.7367 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.6834 Loss_G: 0.7354 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6626 Loss_G: 0.7179 acc: 93.8%\n",
      "[BATCH 46/149] Loss_D: 0.7107 Loss_G: 0.7204 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7121 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6478 Loss_G: 0.7128 acc: 93.8%\n",
      "[BATCH 49/149] Loss_D: 0.7345 Loss_G: 0.7484 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6928 Loss_G: 0.7515 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6725 Loss_G: 0.7276 acc: 81.2%\n",
      "[BATCH 52/149] Loss_D: 0.7199 Loss_G: 0.7245 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.6975 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6509 Loss_G: 0.7262 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.6915 Loss_G: 0.7356 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6933 Loss_G: 0.7293 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7016 Loss_G: 0.7385 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.7141 Loss_G: 0.7465 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.7313 Loss_G: 0.7450 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7120 Loss_G: 0.7376 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7147 Loss_G: 0.7576 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6897 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7108 Loss_G: 0.7141 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.7002 Loss_G: 0.7197 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7443 Loss_G: 0.7512 acc: 96.9%\n",
      "[BATCH 66/149] Loss_D: 0.6891 Loss_G: 0.7360 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7014 Loss_G: 0.7368 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6755 Loss_G: 0.7187 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6974 Loss_G: 0.7443 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7149 Loss_G: 0.7606 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7523 Loss_G: 0.7692 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.7186 Loss_G: 0.7597 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.6896 Loss_G: 0.7304 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7210 Loss_G: 0.7573 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7069 Loss_G: 0.7642 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7261 Loss_G: 0.7329 acc: 79.7%\n",
      "[BATCH 77/149] Loss_D: 0.7144 Loss_G: 0.7431 acc: 93.8%\n",
      "[EPOCH 4100] TEST ACC is : 76.8%\n",
      "[BATCH 78/149] Loss_D: 0.7127 Loss_G: 0.7459 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7502 Loss_G: 0.7473 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.6666 Loss_G: 0.7244 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6883 Loss_G: 0.7141 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6667 Loss_G: 0.7006 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.7614 Loss_G: 0.7321 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6830 Loss_G: 0.7266 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6858 Loss_G: 0.7305 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7133 Loss_G: 0.7350 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6877 Loss_G: 0.7264 acc: 95.3%\n",
      "[BATCH 88/149] Loss_D: 0.6736 Loss_G: 0.7136 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.7350 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7291 Loss_G: 0.7459 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7005 Loss_G: 0.7294 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.7040 Loss_G: 0.7236 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7128 Loss_G: 0.7334 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6793 Loss_G: 0.7287 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6667 Loss_G: 0.7359 acc: 95.3%\n",
      "[BATCH 96/149] Loss_D: 0.7249 Loss_G: 0.7376 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.7325 Loss_G: 0.7462 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6901 Loss_G: 0.7349 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7283 Loss_G: 0.7285 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7022 Loss_G: 0.7355 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6857 Loss_G: 0.7431 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.7613 Loss_G: 0.7451 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7323 Loss_G: 0.7395 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7529 Loss_G: 0.7540 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.7577 Loss_G: 0.7672 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6878 Loss_G: 0.7438 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.7239 Loss_G: 0.7417 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7330 Loss_G: 0.7433 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.7844 Loss_G: 0.7453 acc: 79.7%\n",
      "[BATCH 110/149] Loss_D: 0.7053 Loss_G: 0.7667 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6924 Loss_G: 0.7276 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7165 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6896 Loss_G: 0.7230 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6838 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6992 Loss_G: 0.7197 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6807 Loss_G: 0.7166 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6863 Loss_G: 0.7170 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7416 Loss_G: 0.7470 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7194 Loss_G: 0.7710 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.7092 Loss_G: 0.7410 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.6953 Loss_G: 0.7264 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6853 Loss_G: 0.7242 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.7027 Loss_G: 0.7218 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.7359 Loss_G: 0.7261 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.6896 Loss_G: 0.7382 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7207 Loss_G: 0.7134 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6742 Loss_G: 0.7160 acc: 85.9%\n",
      "[EPOCH 4150] TEST ACC is : 75.6%\n",
      "[BATCH 128/149] Loss_D: 0.7055 Loss_G: 0.7195 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.7153 Loss_G: 0.7379 acc: 96.9%\n",
      "[BATCH 130/149] Loss_D: 0.7193 Loss_G: 0.7296 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7101 Loss_G: 0.7435 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.7017 Loss_G: 0.7238 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6661 Loss_G: 0.7198 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7413 Loss_G: 0.7479 acc: 79.7%\n",
      "[BATCH 135/149] Loss_D: 0.7345 Loss_G: 0.7477 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7494 Loss_G: 0.7646 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7298 Loss_G: 0.7291 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6700 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.7216 Loss_G: 0.7363 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7059 Loss_G: 0.7424 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.7118 Loss_G: 0.7304 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7265 Loss_G: 0.7289 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.6697 Loss_G: 0.7449 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6991 Loss_G: 0.7528 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6882 Loss_G: 0.7263 acc: 93.8%\n",
      "[BATCH 146/149] Loss_D: 0.7026 Loss_G: 0.7442 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.7328 Loss_G: 0.7375 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.7111 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6776 Loss_G: 0.7411 acc: 90.6%\n",
      "-----THE [28/50] epoch end-----\n",
      "-----THE [29/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6949 Loss_G: 0.7227 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7275 Loss_G: 0.7328 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7146 Loss_G: 0.7455 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.7248 Loss_G: 0.7560 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6853 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7565 Loss_G: 0.7387 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6733 Loss_G: 0.7418 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6790 Loss_G: 0.7250 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6852 Loss_G: 0.7250 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7584 Loss_G: 0.7421 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7680 Loss_G: 0.7680 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7112 Loss_G: 0.7613 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7073 Loss_G: 0.7394 acc: 79.7%\n",
      "[BATCH 14/149] Loss_D: 0.7095 Loss_G: 0.7368 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7367 Loss_G: 0.7387 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7128 Loss_G: 0.7456 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7353 Loss_G: 0.7484 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6910 Loss_G: 0.7272 acc: 93.8%\n",
      "[BATCH 19/149] Loss_D: 0.6712 Loss_G: 0.7327 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.6805 Loss_G: 0.7234 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7186 Loss_G: 0.7208 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6939 Loss_G: 0.7548 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.7275 Loss_G: 0.7363 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6932 Loss_G: 0.7218 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6936 Loss_G: 0.7151 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.7317 Loss_G: 0.7299 acc: 95.3%\n",
      "[BATCH 27/149] Loss_D: 0.7040 Loss_G: 0.7336 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6919 Loss_G: 0.7280 acc: 87.5%\n",
      "[EPOCH 4200] TEST ACC is : 76.0%\n",
      "[BATCH 29/149] Loss_D: 0.7100 Loss_G: 0.7209 acc: 81.2%\n",
      "[BATCH 30/149] Loss_D: 0.7108 Loss_G: 0.7637 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.7164 Loss_G: 0.7471 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.7074 Loss_G: 0.7392 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6893 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7007 Loss_G: 0.7159 acc: 96.9%\n",
      "[BATCH 35/149] Loss_D: 0.6557 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6926 Loss_G: 0.6949 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7181 Loss_G: 0.7201 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6836 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7111 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6906 Loss_G: 0.7468 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.7072 Loss_G: 0.7783 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7397 Loss_G: 0.7516 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.7319 Loss_G: 0.7507 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7148 Loss_G: 0.7507 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.7098 Loss_G: 0.7376 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6976 Loss_G: 0.7425 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6908 Loss_G: 0.7314 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6950 Loss_G: 0.7267 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6551 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7218 Loss_G: 0.7504 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7346 Loss_G: 0.7504 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.6773 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6736 Loss_G: 0.7122 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7769 Loss_G: 0.7268 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6932 Loss_G: 0.7202 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.6940 Loss_G: 0.7451 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7456 Loss_G: 0.7476 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7097 Loss_G: 0.7360 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7089 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6828 Loss_G: 0.7369 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7239 Loss_G: 0.7472 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6764 Loss_G: 0.7228 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.7058 Loss_G: 0.7135 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7073 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7135 Loss_G: 0.7420 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.7394 Loss_G: 0.7290 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.7085 Loss_G: 0.7307 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7383 Loss_G: 0.7556 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7310 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7009 Loss_G: 0.7490 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6928 Loss_G: 0.7294 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.7062 Loss_G: 0.7213 acc: 79.7%\n",
      "[BATCH 73/149] Loss_D: 0.7074 Loss_G: 0.7326 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.7151 Loss_G: 0.7547 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6879 Loss_G: 0.7468 acc: 95.3%\n",
      "[BATCH 76/149] Loss_D: 0.6742 Loss_G: 0.7201 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6699 Loss_G: 0.7212 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7376 Loss_G: 0.7418 acc: 87.5%\n",
      "[EPOCH 4250] TEST ACC is : 76.2%\n",
      "[BATCH 79/149] Loss_D: 0.6929 Loss_G: 0.7265 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6592 Loss_G: 0.7210 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7215 Loss_G: 0.7295 acc: 81.2%\n",
      "[BATCH 82/149] Loss_D: 0.7479 Loss_G: 0.7392 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6782 Loss_G: 0.7226 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7062 Loss_G: 0.7224 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7048 Loss_G: 0.7398 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6864 Loss_G: 0.7379 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6995 Loss_G: 0.7772 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.7386 Loss_G: 0.7585 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.6801 Loss_G: 0.7402 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6975 Loss_G: 0.7186 acc: 79.7%\n",
      "[BATCH 91/149] Loss_D: 0.6873 Loss_G: 0.7067 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.7385 Loss_G: 0.7270 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7007 Loss_G: 0.7334 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7006 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6933 Loss_G: 0.7166 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7022 Loss_G: 0.7266 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6898 Loss_G: 0.7120 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6649 Loss_G: 0.7334 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6667 Loss_G: 0.7243 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7035 Loss_G: 0.7442 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6855 Loss_G: 0.7000 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6958 Loss_G: 0.7310 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.7219 Loss_G: 0.7205 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.6904 Loss_G: 0.7227 acc: 95.3%\n",
      "[BATCH 105/149] Loss_D: 0.6566 Loss_G: 0.7058 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7254 Loss_G: 0.7182 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.7171 Loss_G: 0.7281 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.6918 Loss_G: 0.7418 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7110 Loss_G: 0.7279 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6983 Loss_G: 0.7317 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.6935 Loss_G: 0.7237 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6762 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 113/149] Loss_D: 0.7327 Loss_G: 0.7546 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7140 Loss_G: 0.7746 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6872 Loss_G: 0.7410 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.7094 Loss_G: 0.7346 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6759 Loss_G: 0.7214 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7317 Loss_G: 0.7104 acc: 78.1%\n",
      "[BATCH 119/149] Loss_D: 0.7304 Loss_G: 0.7369 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.7236 Loss_G: 0.7409 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6877 Loss_G: 0.7306 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6907 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6943 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6976 Loss_G: 0.7295 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.7158 Loss_G: 0.7381 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7131 Loss_G: 0.7456 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7062 Loss_G: 0.7200 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.6827 Loss_G: 0.7218 acc: 90.6%\n",
      "[EPOCH 4300] TEST ACC is : 77.1%\n",
      "[BATCH 129/149] Loss_D: 0.7326 Loss_G: 0.7373 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7056 Loss_G: 0.7380 acc: 95.3%\n",
      "[BATCH 131/149] Loss_D: 0.7319 Loss_G: 0.7402 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.7041 Loss_G: 0.7381 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6776 Loss_G: 0.7243 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7237 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7033 Loss_G: 0.7398 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7099 Loss_G: 0.7324 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6881 Loss_G: 0.7369 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.7095 Loss_G: 0.7382 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6902 Loss_G: 0.7234 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7186 Loss_G: 0.7436 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.7036 Loss_G: 0.7376 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.7113 Loss_G: 0.7376 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6796 Loss_G: 0.7433 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.7071 Loss_G: 0.7468 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7265 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7516 Loss_G: 0.7408 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7196 Loss_G: 0.7517 acc: 95.3%\n",
      "[BATCH 148/149] Loss_D: 0.7076 Loss_G: 0.7518 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7186 Loss_G: 0.7367 acc: 81.2%\n",
      "-----THE [29/50] epoch end-----\n",
      "-----THE [30/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6618 Loss_G: 0.7194 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7078 Loss_G: 0.7326 acc: 79.7%\n",
      "[BATCH 3/149] Loss_D: 0.7071 Loss_G: 0.7407 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7192 Loss_G: 0.7518 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7018 Loss_G: 0.7447 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7127 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.7637 Loss_G: 0.7333 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.6994 Loss_G: 0.7201 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.7459 Loss_G: 0.7361 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7177 Loss_G: 0.7505 acc: 93.8%\n",
      "[BATCH 11/149] Loss_D: 0.7058 Loss_G: 0.7282 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7163 Loss_G: 0.7296 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.7654 Loss_G: 0.7582 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6905 Loss_G: 0.7323 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.7030 Loss_G: 0.7377 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7283 Loss_G: 0.7508 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6552 Loss_G: 0.7179 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7077 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.7121 Loss_G: 0.7357 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6691 Loss_G: 0.7367 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6913 Loss_G: 0.7190 acc: 95.3%\n",
      "[BATCH 22/149] Loss_D: 0.6870 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7199 Loss_G: 0.7279 acc: 95.3%\n",
      "[BATCH 24/149] Loss_D: 0.7142 Loss_G: 0.7494 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.6790 Loss_G: 0.7438 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6888 Loss_G: 0.7380 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7056 Loss_G: 0.7241 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7229 Loss_G: 0.7125 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6983 Loss_G: 0.7066 acc: 78.1%\n",
      "[EPOCH 4350] TEST ACC is : 75.4%\n",
      "[BATCH 30/149] Loss_D: 0.6970 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6882 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6555 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.7389 Loss_G: 0.7240 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.7059 Loss_G: 0.7279 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.7137 Loss_G: 0.7226 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.7150 Loss_G: 0.7219 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.7150 Loss_G: 0.7237 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.7194 Loss_G: 0.7532 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6954 Loss_G: 0.7309 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6980 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.7022 Loss_G: 0.7434 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.7005 Loss_G: 0.7492 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6948 Loss_G: 0.7465 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.7194 Loss_G: 0.7304 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.7036 Loss_G: 0.7505 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7163 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6704 Loss_G: 0.7269 acc: 93.8%\n",
      "[BATCH 48/149] Loss_D: 0.7776 Loss_G: 0.7720 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.7282 Loss_G: 0.7762 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6594 Loss_G: 0.7365 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7307 Loss_G: 0.7364 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.6763 Loss_G: 0.7339 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6919 Loss_G: 0.7347 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7069 Loss_G: 0.7303 acc: 95.3%\n",
      "[BATCH 55/149] Loss_D: 0.7498 Loss_G: 0.7400 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7251 Loss_G: 0.7464 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6791 Loss_G: 0.7244 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7595 Loss_G: 0.7470 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7017 Loss_G: 0.7221 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6982 Loss_G: 0.7254 acc: 95.3%\n",
      "[BATCH 61/149] Loss_D: 0.7222 Loss_G: 0.7311 acc: 95.3%\n",
      "[BATCH 62/149] Loss_D: 0.6990 Loss_G: 0.7311 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7092 Loss_G: 0.7378 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7138 Loss_G: 0.7444 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7011 Loss_G: 0.7598 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6738 Loss_G: 0.7366 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.6890 Loss_G: 0.7358 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6841 Loss_G: 0.7088 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.7008 Loss_G: 0.7132 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.7005 Loss_G: 0.7045 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7096 Loss_G: 0.7137 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6925 Loss_G: 0.7234 acc: 82.8%\n",
      "[BATCH 73/149] Loss_D: 0.7690 Loss_G: 0.7621 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7130 Loss_G: 0.7365 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.7303 Loss_G: 0.7651 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6841 Loss_G: 0.7468 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.7217 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6799 Loss_G: 0.7125 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.6902 Loss_G: 0.6994 acc: 93.8%\n",
      "[EPOCH 4400] TEST ACC is : 75.4%\n",
      "[BATCH 80/149] Loss_D: 0.6911 Loss_G: 0.7189 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.6639 Loss_G: 0.7280 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6850 Loss_G: 0.6978 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7034 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6988 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6670 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7015 Loss_G: 0.7145 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6725 Loss_G: 0.7248 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6697 Loss_G: 0.7034 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7091 Loss_G: 0.7339 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6514 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7166 Loss_G: 0.7274 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6706 Loss_G: 0.7103 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7243 Loss_G: 0.7289 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7100 Loss_G: 0.7313 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6741 Loss_G: 0.7127 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.6976 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6911 Loss_G: 0.7215 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7411 Loss_G: 0.7564 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6922 Loss_G: 0.7307 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.6953 Loss_G: 0.7327 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7358 Loss_G: 0.7531 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6914 Loss_G: 0.7303 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7177 Loss_G: 0.7397 acc: 95.3%\n",
      "[BATCH 104/149] Loss_D: 0.7278 Loss_G: 0.7452 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.7161 Loss_G: 0.7337 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6960 Loss_G: 0.7313 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7245 Loss_G: 0.7430 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7037 Loss_G: 0.7503 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7278 Loss_G: 0.7575 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.7112 Loss_G: 0.7401 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7068 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.7493 Loss_G: 0.7322 acc: 82.8%\n",
      "[BATCH 113/149] Loss_D: 0.7092 Loss_G: 0.7291 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7275 Loss_G: 0.7328 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.7321 Loss_G: 0.7289 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7102 Loss_G: 0.7689 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7063 Loss_G: 0.7306 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.7618 Loss_G: 0.7497 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6868 Loss_G: 0.7385 acc: 95.3%\n",
      "[BATCH 120/149] Loss_D: 0.7019 Loss_G: 0.7232 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6842 Loss_G: 0.7308 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.7079 Loss_G: 0.7327 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6802 Loss_G: 0.7437 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.7070 Loss_G: 0.7531 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6945 Loss_G: 0.7323 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.7118 Loss_G: 0.7384 acc: 78.1%\n",
      "[BATCH 127/149] Loss_D: 0.6973 Loss_G: 0.7312 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7219 Loss_G: 0.7205 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.7151 Loss_G: 0.7338 acc: 90.6%\n",
      "[EPOCH 4450] TEST ACC is : 74.8%\n",
      "[BATCH 130/149] Loss_D: 0.6936 Loss_G: 0.7299 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6921 Loss_G: 0.7286 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.7115 Loss_G: 0.7470 acc: 81.2%\n",
      "[BATCH 133/149] Loss_D: 0.6886 Loss_G: 0.7319 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6740 Loss_G: 0.7171 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7281 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6847 Loss_G: 0.7223 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6957 Loss_G: 0.7313 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.6849 Loss_G: 0.7505 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6840 Loss_G: 0.7499 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7382 Loss_G: 0.7454 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7368 Loss_G: 0.7802 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6964 Loss_G: 0.7249 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6881 Loss_G: 0.7270 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.7108 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7232 Loss_G: 0.7307 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7053 Loss_G: 0.7173 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6754 Loss_G: 0.7191 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6878 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.7404 Loss_G: 0.7583 acc: 87.5%\n",
      "-----THE [30/50] epoch end-----\n",
      "-----THE [31/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7092 Loss_G: 0.7403 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7588 Loss_G: 0.7519 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.6711 Loss_G: 0.7125 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7414 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7321 Loss_G: 0.7479 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.7131 Loss_G: 0.7406 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6919 Loss_G: 0.7194 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.7394 Loss_G: 0.7277 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.7368 Loss_G: 0.7386 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.7424 Loss_G: 0.7479 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6972 Loss_G: 0.7490 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7002 Loss_G: 0.7628 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7025 Loss_G: 0.7426 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6732 Loss_G: 0.7363 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.6966 Loss_G: 0.7396 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.7325 Loss_G: 0.7484 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6743 Loss_G: 0.7444 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7248 Loss_G: 0.7262 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7123 Loss_G: 0.7278 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7206 Loss_G: 0.7406 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7262 Loss_G: 0.7739 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.7359 Loss_G: 0.7480 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7192 Loss_G: 0.7459 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6803 Loss_G: 0.7620 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7195 Loss_G: 0.7561 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7137 Loss_G: 0.7364 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6778 Loss_G: 0.7355 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.7115 Loss_G: 0.7235 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.7084 Loss_G: 0.7243 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.7042 Loss_G: 0.7454 acc: 93.8%\n",
      "[EPOCH 4500] TEST ACC is : 76.0%\n",
      "[BATCH 31/149] Loss_D: 0.6918 Loss_G: 0.7253 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7163 Loss_G: 0.7382 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.6983 Loss_G: 0.7357 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6637 Loss_G: 0.7202 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.7091 Loss_G: 0.7119 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.7240 Loss_G: 0.7134 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7178 Loss_G: 0.7288 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.7173 Loss_G: 0.7418 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6744 Loss_G: 0.7391 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.7065 Loss_G: 0.7330 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.6977 Loss_G: 0.7315 acc: 95.3%\n",
      "[BATCH 42/149] Loss_D: 0.7593 Loss_G: 0.7520 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7120 Loss_G: 0.7430 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7012 Loss_G: 0.7360 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7475 Loss_G: 0.7547 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7313 Loss_G: 0.7541 acc: 95.3%\n",
      "[BATCH 47/149] Loss_D: 0.6795 Loss_G: 0.7374 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7029 Loss_G: 0.7416 acc: 96.9%\n",
      "[BATCH 49/149] Loss_D: 0.7255 Loss_G: 0.7360 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7038 Loss_G: 0.7292 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.6839 Loss_G: 0.7250 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.7176 Loss_G: 0.7273 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7111 Loss_G: 0.7151 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.7282 Loss_G: 0.7267 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.7176 Loss_G: 0.7330 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.7099 Loss_G: 0.7235 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.7063 Loss_G: 0.7231 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.7008 Loss_G: 0.7057 acc: 78.1%\n",
      "[BATCH 59/149] Loss_D: 0.6499 Loss_G: 0.7035 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.6865 Loss_G: 0.7156 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.7827 Loss_G: 0.7501 acc: 81.2%\n",
      "[BATCH 62/149] Loss_D: 0.6950 Loss_G: 0.7324 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7066 Loss_G: 0.7176 acc: 79.7%\n",
      "[BATCH 64/149] Loss_D: 0.7009 Loss_G: 0.7150 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6951 Loss_G: 0.7333 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7082 Loss_G: 0.7417 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.6983 Loss_G: 0.7492 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6859 Loss_G: 0.7605 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6849 Loss_G: 0.7422 acc: 93.8%\n",
      "[BATCH 70/149] Loss_D: 0.7101 Loss_G: 0.7335 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6942 Loss_G: 0.7288 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7006 Loss_G: 0.7167 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7422 Loss_G: 0.7431 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7087 Loss_G: 0.7477 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.6957 Loss_G: 0.7455 acc: 82.8%\n",
      "[BATCH 76/149] Loss_D: 0.6570 Loss_G: 0.7101 acc: 96.9%\n",
      "[BATCH 77/149] Loss_D: 0.7105 Loss_G: 0.7103 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.6971 Loss_G: 0.7125 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.6894 Loss_G: 0.7369 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6978 Loss_G: 0.7154 acc: 87.5%\n",
      "[EPOCH 4550] TEST ACC is : 75.6%\n",
      "[BATCH 81/149] Loss_D: 0.7035 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7115 Loss_G: 0.7490 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7161 Loss_G: 0.7444 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6939 Loss_G: 0.7269 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.6977 Loss_G: 0.7301 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6946 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6983 Loss_G: 0.7058 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6906 Loss_G: 0.7143 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7027 Loss_G: 0.7216 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6753 Loss_G: 0.7169 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6737 Loss_G: 0.7194 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7116 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6964 Loss_G: 0.7349 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.7277 Loss_G: 0.7486 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7272 Loss_G: 0.7669 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7031 Loss_G: 0.7385 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7101 Loss_G: 0.7484 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7860 Loss_G: 0.7462 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6877 Loss_G: 0.7229 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7281 Loss_G: 0.7383 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.6922 Loss_G: 0.7317 acc: 84.4%\n",
      "[BATCH 102/149] Loss_D: 0.7381 Loss_G: 0.7386 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7076 Loss_G: 0.7373 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.7003 Loss_G: 0.7350 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7191 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7149 Loss_G: 0.7748 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6851 Loss_G: 0.7432 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.7206 Loss_G: 0.7636 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6773 Loss_G: 0.7254 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.7963 Loss_G: 0.7517 acc: 95.3%\n",
      "[BATCH 111/149] Loss_D: 0.7105 Loss_G: 0.7390 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6688 Loss_G: 0.7342 acc: 92.2%\n",
      "[BATCH 113/149] Loss_D: 0.6727 Loss_G: 0.7391 acc: 93.8%\n",
      "[BATCH 114/149] Loss_D: 0.7255 Loss_G: 0.7279 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.7230 Loss_G: 0.7365 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6730 Loss_G: 0.7158 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.7497 Loss_G: 0.7367 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6873 Loss_G: 0.7235 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.7051 Loss_G: 0.7386 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.7209 Loss_G: 0.7456 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6798 Loss_G: 0.7363 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.7180 Loss_G: 0.7355 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7226 Loss_G: 0.7287 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.6979 Loss_G: 0.7318 acc: 93.8%\n",
      "[BATCH 125/149] Loss_D: 0.6836 Loss_G: 0.7356 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6783 Loss_G: 0.7076 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7166 Loss_G: 0.7217 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6755 Loss_G: 0.7141 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6940 Loss_G: 0.7231 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6528 Loss_G: 0.7052 acc: 89.1%\n",
      "[EPOCH 4600] TEST ACC is : 76.8%\n",
      "[BATCH 131/149] Loss_D: 0.6869 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6642 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6980 Loss_G: 0.7094 acc: 93.8%\n",
      "[BATCH 134/149] Loss_D: 0.6828 Loss_G: 0.7167 acc: 93.8%\n",
      "[BATCH 135/149] Loss_D: 0.7397 Loss_G: 0.7387 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6992 Loss_G: 0.7370 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6528 Loss_G: 0.7121 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.6996 Loss_G: 0.7122 acc: 78.1%\n",
      "[BATCH 139/149] Loss_D: 0.6746 Loss_G: 0.7088 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6912 Loss_G: 0.7397 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6659 Loss_G: 0.7160 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.7040 Loss_G: 0.7273 acc: 79.7%\n",
      "[BATCH 143/149] Loss_D: 0.7323 Loss_G: 0.7405 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.7246 Loss_G: 0.7342 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6894 Loss_G: 0.7254 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6918 Loss_G: 0.7240 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6831 Loss_G: 0.7109 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6953 Loss_G: 0.7329 acc: 95.3%\n",
      "[BATCH 149/149] Loss_D: 0.6898 Loss_G: 0.7220 acc: 85.9%\n",
      "-----THE [31/50] epoch end-----\n",
      "-----THE [32/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6862 Loss_G: 0.7097 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6809 Loss_G: 0.7142 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6965 Loss_G: 0.7317 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.6962 Loss_G: 0.7139 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6987 Loss_G: 0.7355 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.7203 Loss_G: 0.7332 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7338 Loss_G: 0.7427 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.7210 Loss_G: 0.7347 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6835 Loss_G: 0.7419 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6989 Loss_G: 0.7377 acc: 93.8%\n",
      "[BATCH 11/149] Loss_D: 0.7097 Loss_G: 0.7412 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6849 Loss_G: 0.7343 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.7107 Loss_G: 0.7377 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6872 Loss_G: 0.7273 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6943 Loss_G: 0.7348 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.7186 Loss_G: 0.7252 acc: 96.9%\n",
      "[BATCH 17/149] Loss_D: 0.7009 Loss_G: 0.7189 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6754 Loss_G: 0.7288 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.6760 Loss_G: 0.7218 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7212 Loss_G: 0.7073 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6984 Loss_G: 0.7189 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6798 Loss_G: 0.7177 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7257 Loss_G: 0.7142 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6836 Loss_G: 0.7130 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6901 Loss_G: 0.7364 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.7260 Loss_G: 0.7435 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7456 Loss_G: 0.7523 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6790 Loss_G: 0.7242 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6829 Loss_G: 0.6987 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6793 Loss_G: 0.7093 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.7209 Loss_G: 0.7270 acc: 87.5%\n",
      "[EPOCH 4650] TEST ACC is : 75.6%\n",
      "[BATCH 32/149] Loss_D: 0.7058 Loss_G: 0.7166 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6677 Loss_G: 0.7330 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.7131 Loss_G: 0.7414 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7037 Loss_G: 0.7504 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.7504 Loss_G: 0.7336 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7057 Loss_G: 0.7474 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.7746 Loss_G: 0.7449 acc: 93.8%\n",
      "[BATCH 39/149] Loss_D: 0.6626 Loss_G: 0.7184 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6993 Loss_G: 0.7199 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.7234 Loss_G: 0.7304 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6837 Loss_G: 0.7461 acc: 93.8%\n",
      "[BATCH 43/149] Loss_D: 0.6602 Loss_G: 0.7430 acc: 95.3%\n",
      "[BATCH 44/149] Loss_D: 0.6819 Loss_G: 0.7353 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7373 Loss_G: 0.7404 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.6584 Loss_G: 0.7461 acc: 95.3%\n",
      "[BATCH 47/149] Loss_D: 0.6914 Loss_G: 0.7213 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7340 Loss_G: 0.7367 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6906 Loss_G: 0.7453 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6875 Loss_G: 0.7039 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.6835 Loss_G: 0.7149 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.6772 Loss_G: 0.7108 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7106 Loss_G: 0.7393 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6954 Loss_G: 0.7383 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7506 Loss_G: 0.7330 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.6774 Loss_G: 0.7244 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.7018 Loss_G: 0.7296 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.7075 Loss_G: 0.7349 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6536 Loss_G: 0.7127 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7361 Loss_G: 0.7260 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.7096 Loss_G: 0.7322 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6588 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.6826 Loss_G: 0.7256 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.6730 Loss_G: 0.7311 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6678 Loss_G: 0.7256 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7200 Loss_G: 0.7517 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7298 Loss_G: 0.7367 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7701 Loss_G: 0.7434 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.7197 Loss_G: 0.7483 acc: 76.6%\n",
      "[BATCH 70/149] Loss_D: 0.7347 Loss_G: 0.7498 acc: 81.2%\n",
      "[BATCH 71/149] Loss_D: 0.6759 Loss_G: 0.7177 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.7267 Loss_G: 0.7462 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6962 Loss_G: 0.7363 acc: 95.3%\n",
      "[BATCH 74/149] Loss_D: 0.6946 Loss_G: 0.7081 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.6897 Loss_G: 0.7248 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6864 Loss_G: 0.7127 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.7039 Loss_G: 0.7078 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.6885 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.7163 Loss_G: 0.7058 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6914 Loss_G: 0.7151 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.7077 Loss_G: 0.7170 acc: 87.5%\n",
      "[EPOCH 4700] TEST ACC is : 76.8%\n",
      "[BATCH 82/149] Loss_D: 0.6940 Loss_G: 0.7170 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6790 Loss_G: 0.7477 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6897 Loss_G: 0.7365 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.6654 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6956 Loss_G: 0.7311 acc: 95.3%\n",
      "[BATCH 87/149] Loss_D: 0.7392 Loss_G: 0.7336 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7185 Loss_G: 0.7329 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6887 Loss_G: 0.7461 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6667 Loss_G: 0.7099 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.6936 Loss_G: 0.7252 acc: 95.3%\n",
      "[BATCH 92/149] Loss_D: 0.7223 Loss_G: 0.7267 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7150 Loss_G: 0.7184 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.7442 Loss_G: 0.7428 acc: 78.1%\n",
      "[BATCH 95/149] Loss_D: 0.7293 Loss_G: 0.7422 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.6702 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7246 Loss_G: 0.7299 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.6991 Loss_G: 0.7369 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6869 Loss_G: 0.7232 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.7378 Loss_G: 0.7464 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.6936 Loss_G: 0.7451 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7270 Loss_G: 0.7387 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7022 Loss_G: 0.7174 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6990 Loss_G: 0.7313 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7098 Loss_G: 0.7273 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7174 Loss_G: 0.7679 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.7345 Loss_G: 0.7799 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7296 Loss_G: 0.7535 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6955 Loss_G: 0.7295 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.6909 Loss_G: 0.7141 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6865 Loss_G: 0.7386 acc: 95.3%\n",
      "[BATCH 112/149] Loss_D: 0.7133 Loss_G: 0.7363 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7616 Loss_G: 0.7760 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6770 Loss_G: 0.7485 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.7146 Loss_G: 0.7289 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6946 Loss_G: 0.7287 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7090 Loss_G: 0.7421 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6937 Loss_G: 0.7218 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6909 Loss_G: 0.7332 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7605 Loss_G: 0.7446 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7101 Loss_G: 0.7327 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7263 Loss_G: 0.7306 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6746 Loss_G: 0.7183 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6754 Loss_G: 0.7132 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.7137 Loss_G: 0.7232 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.7097 Loss_G: 0.7253 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7562 Loss_G: 0.7276 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7101 Loss_G: 0.7304 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6744 Loss_G: 0.7200 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7233 Loss_G: 0.7453 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7245 Loss_G: 0.7573 acc: 85.9%\n",
      "[EPOCH 4750] TEST ACC is : 77.1%\n",
      "[BATCH 132/149] Loss_D: 0.7446 Loss_G: 0.7402 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7505 Loss_G: 0.7477 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.6879 Loss_G: 0.7548 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6811 Loss_G: 0.7324 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7294 Loss_G: 0.7446 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.7228 Loss_G: 0.7293 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7046 Loss_G: 0.7205 acc: 98.4%\n",
      "[BATCH 139/149] Loss_D: 0.6919 Loss_G: 0.7453 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7259 Loss_G: 0.7507 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.6861 Loss_G: 0.7404 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6965 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.7161 Loss_G: 0.7210 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6857 Loss_G: 0.7402 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6839 Loss_G: 0.7303 acc: 96.9%\n",
      "[BATCH 146/149] Loss_D: 0.6769 Loss_G: 0.7143 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7478 Loss_G: 0.7334 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.7544 Loss_G: 0.7596 acc: 79.7%\n",
      "[BATCH 149/149] Loss_D: 0.7022 Loss_G: 0.7396 acc: 85.9%\n",
      "-----THE [32/50] epoch end-----\n",
      "-----THE [33/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7076 Loss_G: 0.7385 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7069 Loss_G: 0.7417 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6553 Loss_G: 0.7244 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7377 Loss_G: 0.7454 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6931 Loss_G: 0.7407 acc: 96.9%\n",
      "[BATCH 6/149] Loss_D: 0.7250 Loss_G: 0.7760 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7002 Loss_G: 0.7440 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7002 Loss_G: 0.7438 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6819 Loss_G: 0.7182 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.6973 Loss_G: 0.7447 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.6921 Loss_G: 0.7309 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6932 Loss_G: 0.7270 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.6801 Loss_G: 0.7353 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6797 Loss_G: 0.7196 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.7312 Loss_G: 0.7253 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7065 Loss_G: 0.7123 acc: 79.7%\n",
      "[BATCH 17/149] Loss_D: 0.7157 Loss_G: 0.7391 acc: 93.8%\n",
      "[BATCH 18/149] Loss_D: 0.6641 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.7269 Loss_G: 0.7418 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.7076 Loss_G: 0.7306 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7274 Loss_G: 0.7362 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6873 Loss_G: 0.7285 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6929 Loss_G: 0.7025 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.6950 Loss_G: 0.7174 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.7269 Loss_G: 0.7449 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7580 Loss_G: 0.7419 acc: 76.6%\n",
      "[BATCH 27/149] Loss_D: 0.6800 Loss_G: 0.7312 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.6709 Loss_G: 0.7092 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7290 Loss_G: 0.7182 acc: 79.7%\n",
      "[BATCH 30/149] Loss_D: 0.6812 Loss_G: 0.7215 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7076 Loss_G: 0.7280 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7039 Loss_G: 0.7066 acc: 82.8%\n",
      "[EPOCH 4800] TEST ACC is : 75.6%\n",
      "[BATCH 33/149] Loss_D: 0.7069 Loss_G: 0.7290 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.6977 Loss_G: 0.7462 acc: 95.3%\n",
      "[BATCH 35/149] Loss_D: 0.6644 Loss_G: 0.7320 acc: 95.3%\n",
      "[BATCH 36/149] Loss_D: 0.7210 Loss_G: 0.7396 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7056 Loss_G: 0.7360 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.7031 Loss_G: 0.7471 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.7229 Loss_G: 0.7525 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7196 Loss_G: 0.7324 acc: 93.8%\n",
      "[BATCH 41/149] Loss_D: 0.7053 Loss_G: 0.7346 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.7547 Loss_G: 0.7393 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6925 Loss_G: 0.7233 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6948 Loss_G: 0.7330 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6764 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7285 Loss_G: 0.7266 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6734 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.7006 Loss_G: 0.7479 acc: 95.3%\n",
      "[BATCH 49/149] Loss_D: 0.6873 Loss_G: 0.7337 acc: 81.2%\n",
      "[BATCH 50/149] Loss_D: 0.6884 Loss_G: 0.7135 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.6785 Loss_G: 0.7291 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.6976 Loss_G: 0.7331 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7359 Loss_G: 0.7398 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.7625 Loss_G: 0.7573 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.6983 Loss_G: 0.7457 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7166 Loss_G: 0.7283 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.6901 Loss_G: 0.7168 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.6899 Loss_G: 0.7383 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7263 Loss_G: 0.7607 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7300 Loss_G: 0.7527 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7663 Loss_G: 0.7738 acc: 93.8%\n",
      "[BATCH 62/149] Loss_D: 0.7171 Loss_G: 0.7688 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6944 Loss_G: 0.7376 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.6917 Loss_G: 0.7416 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7624 Loss_G: 0.7486 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.6817 Loss_G: 0.7516 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.6770 Loss_G: 0.7334 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7409 Loss_G: 0.7187 acc: 79.7%\n",
      "[BATCH 69/149] Loss_D: 0.7148 Loss_G: 0.7305 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6862 Loss_G: 0.7163 acc: 79.7%\n",
      "[BATCH 71/149] Loss_D: 0.6891 Loss_G: 0.7137 acc: 93.8%\n",
      "[BATCH 72/149] Loss_D: 0.6919 Loss_G: 0.7165 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.6745 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.7168 Loss_G: 0.7370 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.6743 Loss_G: 0.7150 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7232 Loss_G: 0.7344 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6802 Loss_G: 0.7270 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6756 Loss_G: 0.7347 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.7123 Loss_G: 0.7422 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6976 Loss_G: 0.7407 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.7155 Loss_G: 0.7373 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6956 Loss_G: 0.7293 acc: 89.1%\n",
      "[EPOCH 4850] TEST ACC is : 75.8%\n",
      "[BATCH 83/149] Loss_D: 0.7083 Loss_G: 0.7245 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7250 Loss_G: 0.7340 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7251 Loss_G: 0.7424 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6902 Loss_G: 0.7542 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6791 Loss_G: 0.7329 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7828 Loss_G: 0.7456 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.6759 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7139 Loss_G: 0.7458 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6923 Loss_G: 0.7195 acc: 81.2%\n",
      "[BATCH 92/149] Loss_D: 0.7510 Loss_G: 0.7354 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.6927 Loss_G: 0.7376 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6842 Loss_G: 0.7264 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6890 Loss_G: 0.7209 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6997 Loss_G: 0.7281 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.7219 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.7039 Loss_G: 0.7326 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7043 Loss_G: 0.7719 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.7201 Loss_G: 0.7447 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7294 Loss_G: 0.7391 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6816 Loss_G: 0.7219 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6758 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7023 Loss_G: 0.7250 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6966 Loss_G: 0.7166 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7161 Loss_G: 0.7391 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6882 Loss_G: 0.7534 acc: 95.3%\n",
      "[BATCH 108/149] Loss_D: 0.7298 Loss_G: 0.7746 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6802 Loss_G: 0.7186 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.7161 Loss_G: 0.7275 acc: 81.2%\n",
      "[BATCH 111/149] Loss_D: 0.7143 Loss_G: 0.7415 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7261 Loss_G: 0.7414 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7036 Loss_G: 0.7347 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7150 Loss_G: 0.7264 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6864 Loss_G: 0.7226 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6726 Loss_G: 0.7039 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6967 Loss_G: 0.7130 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7229 Loss_G: 0.7352 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.6837 Loss_G: 0.7375 acc: 96.9%\n",
      "[BATCH 120/149] Loss_D: 0.7153 Loss_G: 0.7203 acc: 79.7%\n",
      "[BATCH 121/149] Loss_D: 0.6948 Loss_G: 0.7334 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.7006 Loss_G: 0.7268 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6742 Loss_G: 0.7274 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.7124 Loss_G: 0.7277 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6866 Loss_G: 0.7396 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6865 Loss_G: 0.7152 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.7154 Loss_G: 0.7276 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7182 Loss_G: 0.7258 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.6866 Loss_G: 0.7190 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7064 Loss_G: 0.7198 acc: 93.8%\n",
      "[BATCH 131/149] Loss_D: 0.7392 Loss_G: 0.7493 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6833 Loss_G: 0.7339 acc: 93.8%\n",
      "[EPOCH 4900] TEST ACC is : 75.8%\n",
      "[BATCH 133/149] Loss_D: 0.6797 Loss_G: 0.7102 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7002 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.7528 Loss_G: 0.7628 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7285 Loss_G: 0.7328 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6655 Loss_G: 0.7323 acc: 95.3%\n",
      "[BATCH 138/149] Loss_D: 0.6967 Loss_G: 0.7316 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.7162 Loss_G: 0.7362 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6832 Loss_G: 0.7034 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.7011 Loss_G: 0.7038 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7189 Loss_G: 0.7160 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.7669 Loss_G: 0.7384 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.6901 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6787 Loss_G: 0.7085 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7075 Loss_G: 0.7283 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7203 Loss_G: 0.7179 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.7056 Loss_G: 0.7752 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.7037 Loss_G: 0.7291 acc: 93.8%\n",
      "-----THE [33/50] epoch end-----\n",
      "-----THE [34/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6881 Loss_G: 0.7196 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6903 Loss_G: 0.7078 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.7329 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6642 Loss_G: 0.7035 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6796 Loss_G: 0.7003 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.7506 Loss_G: 0.7118 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7067 Loss_G: 0.7253 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.6808 Loss_G: 0.7135 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6925 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6941 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7369 Loss_G: 0.7618 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.7396 Loss_G: 0.7464 acc: 79.7%\n",
      "[BATCH 13/149] Loss_D: 0.7257 Loss_G: 0.7404 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.7690 Loss_G: 0.7647 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7062 Loss_G: 0.7764 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7631 Loss_G: 0.7469 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7038 Loss_G: 0.7213 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.7270 Loss_G: 0.7137 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.6867 Loss_G: 0.7303 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.7242 Loss_G: 0.7779 acc: 95.3%\n",
      "[BATCH 21/149] Loss_D: 0.7060 Loss_G: 0.7389 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6915 Loss_G: 0.7587 acc: 95.3%\n",
      "[BATCH 23/149] Loss_D: 0.6956 Loss_G: 0.7343 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6903 Loss_G: 0.7075 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7256 Loss_G: 0.7257 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6945 Loss_G: 0.7214 acc: 82.8%\n",
      "[BATCH 27/149] Loss_D: 0.7069 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6794 Loss_G: 0.7274 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.6709 Loss_G: 0.7093 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6757 Loss_G: 0.7173 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7035 Loss_G: 0.7182 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.7171 Loss_G: 0.7275 acc: 79.7%\n",
      "[BATCH 33/149] Loss_D: 0.6790 Loss_G: 0.7167 acc: 84.4%\n",
      "[EPOCH 4950] TEST ACC is : 75.8%\n",
      "[BATCH 34/149] Loss_D: 0.6679 Loss_G: 0.7212 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7077 Loss_G: 0.7277 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6515 Loss_G: 0.6934 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7078 Loss_G: 0.7075 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6767 Loss_G: 0.7259 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6802 Loss_G: 0.7153 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6586 Loss_G: 0.7015 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6848 Loss_G: 0.6954 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.7122 Loss_G: 0.7271 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7247 Loss_G: 0.7242 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6858 Loss_G: 0.7121 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.7124 Loss_G: 0.7376 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6955 Loss_G: 0.7374 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6714 Loss_G: 0.7236 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7019 Loss_G: 0.7367 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6673 Loss_G: 0.7240 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7080 Loss_G: 0.7372 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7076 Loss_G: 0.7451 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.7366 Loss_G: 0.7398 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6880 Loss_G: 0.7186 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6942 Loss_G: 0.7200 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6792 Loss_G: 0.7212 acc: 81.2%\n",
      "[BATCH 56/149] Loss_D: 0.7152 Loss_G: 0.7300 acc: 81.2%\n",
      "[BATCH 57/149] Loss_D: 0.7071 Loss_G: 0.7307 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6670 Loss_G: 0.7239 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7384 Loss_G: 0.7259 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.7116 Loss_G: 0.7383 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6787 Loss_G: 0.7353 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7110 Loss_G: 0.7256 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7189 Loss_G: 0.7263 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7325 Loss_G: 0.7542 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7026 Loss_G: 0.7429 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6746 Loss_G: 0.7446 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6707 Loss_G: 0.7166 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6816 Loss_G: 0.7272 acc: 96.9%\n",
      "[BATCH 69/149] Loss_D: 0.6967 Loss_G: 0.7284 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7336 Loss_G: 0.7361 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6907 Loss_G: 0.7160 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7019 Loss_G: 0.7436 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6582 Loss_G: 0.7257 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6773 Loss_G: 0.7272 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7186 Loss_G: 0.7331 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6952 Loss_G: 0.7240 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.6807 Loss_G: 0.7073 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7003 Loss_G: 0.7085 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6810 Loss_G: 0.7110 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.7202 Loss_G: 0.7313 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7055 Loss_G: 0.7458 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.7178 Loss_G: 0.7354 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6911 Loss_G: 0.7388 acc: 93.8%\n",
      "[EPOCH 5000] TEST ACC is : 76.2%\n",
      "[BATCH 84/149] Loss_D: 0.7245 Loss_G: 0.7423 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.7062 Loss_G: 0.7589 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.6800 Loss_G: 0.7271 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7161 Loss_G: 0.7142 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.7234 Loss_G: 0.7270 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6786 Loss_G: 0.7457 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.7148 Loss_G: 0.7342 acc: 93.8%\n",
      "[BATCH 91/149] Loss_D: 0.6666 Loss_G: 0.7715 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.7106 Loss_G: 0.7353 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.6811 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7574 Loss_G: 0.7321 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7198 Loss_G: 0.7238 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6886 Loss_G: 0.7268 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.6896 Loss_G: 0.7054 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7072 Loss_G: 0.7203 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7064 Loss_G: 0.7323 acc: 82.8%\n",
      "[BATCH 100/149] Loss_D: 0.7426 Loss_G: 0.7230 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7722 Loss_G: 0.7672 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6533 Loss_G: 0.7410 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6783 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6967 Loss_G: 0.7405 acc: 96.9%\n",
      "[BATCH 105/149] Loss_D: 0.6867 Loss_G: 0.7443 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7090 Loss_G: 0.7221 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.7092 Loss_G: 0.7309 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7082 Loss_G: 0.7424 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6971 Loss_G: 0.7517 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7284 Loss_G: 0.7587 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6957 Loss_G: 0.7386 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6893 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7378 Loss_G: 0.7375 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7171 Loss_G: 0.7320 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6977 Loss_G: 0.7456 acc: 95.3%\n",
      "[BATCH 116/149] Loss_D: 0.6829 Loss_G: 0.7202 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.7003 Loss_G: 0.7243 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.7315 Loss_G: 0.7848 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7297 Loss_G: 0.7453 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7302 Loss_G: 0.7703 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6919 Loss_G: 0.7380 acc: 96.9%\n",
      "[BATCH 122/149] Loss_D: 0.6809 Loss_G: 0.7263 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7201 Loss_G: 0.7410 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7184 Loss_G: 0.7614 acc: 95.3%\n",
      "[BATCH 125/149] Loss_D: 0.7286 Loss_G: 0.7548 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7229 Loss_G: 0.7411 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7685 Loss_G: 0.7495 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.7535 Loss_G: 0.7433 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6676 Loss_G: 0.7432 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7142 Loss_G: 0.7282 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6643 Loss_G: 0.7282 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7155 Loss_G: 0.7293 acc: 95.3%\n",
      "[BATCH 133/149] Loss_D: 0.6932 Loss_G: 0.7340 acc: 84.4%\n",
      "[EPOCH 5050] TEST ACC is : 75.6%\n",
      "[BATCH 134/149] Loss_D: 0.7144 Loss_G: 0.7283 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6863 Loss_G: 0.7328 acc: 96.9%\n",
      "[BATCH 136/149] Loss_D: 0.6836 Loss_G: 0.7250 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.7194 Loss_G: 0.7327 acc: 95.3%\n",
      "[BATCH 138/149] Loss_D: 0.7196 Loss_G: 0.7331 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.7269 Loss_G: 0.7523 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.6958 Loss_G: 0.7488 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.6619 Loss_G: 0.7019 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6966 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7226 Loss_G: 0.7071 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.6808 Loss_G: 0.7165 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7044 Loss_G: 0.7320 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.7080 Loss_G: 0.7423 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.7336 Loss_G: 0.7507 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6810 Loss_G: 0.7322 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6874 Loss_G: 0.7113 acc: 90.6%\n",
      "-----THE [34/50] epoch end-----\n",
      "-----THE [35/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6869 Loss_G: 0.7202 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7113 Loss_G: 0.7265 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.6804 Loss_G: 0.7307 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.6852 Loss_G: 0.7084 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7139 Loss_G: 0.7144 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.7175 Loss_G: 0.7415 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7430 Loss_G: 0.7280 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7103 Loss_G: 0.7288 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6619 Loss_G: 0.7032 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.7072 Loss_G: 0.7032 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6880 Loss_G: 0.7319 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.7213 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.7008 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6791 Loss_G: 0.7336 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.7090 Loss_G: 0.7316 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.6700 Loss_G: 0.7364 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.7147 Loss_G: 0.7477 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6904 Loss_G: 0.7188 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7167 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7329 Loss_G: 0.7380 acc: 81.2%\n",
      "[BATCH 21/149] Loss_D: 0.6778 Loss_G: 0.7429 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.7170 Loss_G: 0.7357 acc: 81.2%\n",
      "[BATCH 23/149] Loss_D: 0.6903 Loss_G: 0.7247 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7075 Loss_G: 0.7426 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.7383 Loss_G: 0.7618 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.7404 Loss_G: 0.7557 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7129 Loss_G: 0.7867 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6821 Loss_G: 0.7375 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.7073 Loss_G: 0.7433 acc: 93.8%\n",
      "[BATCH 30/149] Loss_D: 0.6879 Loss_G: 0.7371 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6868 Loss_G: 0.7228 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6838 Loss_G: 0.7167 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.6753 Loss_G: 0.7036 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.7054 Loss_G: 0.7130 acc: 85.9%\n",
      "[EPOCH 5100] TEST ACC is : 75.4%\n",
      "[BATCH 35/149] Loss_D: 0.7460 Loss_G: 0.7649 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6664 Loss_G: 0.7383 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7265 Loss_G: 0.7395 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6801 Loss_G: 0.7190 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7302 Loss_G: 0.7369 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.6814 Loss_G: 0.7160 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.7032 Loss_G: 0.7295 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6653 Loss_G: 0.7269 acc: 93.8%\n",
      "[BATCH 43/149] Loss_D: 0.6543 Loss_G: 0.7196 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.7108 Loss_G: 0.7062 acc: 82.8%\n",
      "[BATCH 45/149] Loss_D: 0.7547 Loss_G: 0.7688 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.7403 Loss_G: 0.7489 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.7194 Loss_G: 0.7465 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6721 Loss_G: 0.7175 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7395 Loss_G: 0.7322 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7083 Loss_G: 0.7416 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6807 Loss_G: 0.7472 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.7344 Loss_G: 0.7536 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6623 Loss_G: 0.7180 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.6577 Loss_G: 0.7404 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.6558 Loss_G: 0.7207 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.7022 Loss_G: 0.7206 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.6639 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6993 Loss_G: 0.7120 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.6978 Loss_G: 0.7542 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.7621 Loss_G: 0.7479 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7264 Loss_G: 0.7406 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7225 Loss_G: 0.7311 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7333 Loss_G: 0.7370 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.6595 Loss_G: 0.7163 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6870 Loss_G: 0.7185 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6965 Loss_G: 0.7467 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6961 Loss_G: 0.7320 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7113 Loss_G: 0.7165 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.7178 Loss_G: 0.7259 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6881 Loss_G: 0.7080 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7823 Loss_G: 0.7509 acc: 95.3%\n",
      "[BATCH 72/149] Loss_D: 0.7292 Loss_G: 0.7468 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7250 Loss_G: 0.7496 acc: 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.7088 Loss_G: 0.7445 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.7264 Loss_G: 0.7345 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6805 Loss_G: 0.7252 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.7154 Loss_G: 0.7180 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6685 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.7215 Loss_G: 0.7414 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.7144 Loss_G: 0.7400 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7093 Loss_G: 0.7203 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7257 Loss_G: 0.7374 acc: 79.7%\n",
      "[BATCH 83/149] Loss_D: 0.6828 Loss_G: 0.7278 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6891 Loss_G: 0.7156 acc: 89.1%\n",
      "[EPOCH 5150] TEST ACC is : 75.4%\n",
      "[BATCH 85/149] Loss_D: 0.7074 Loss_G: 0.7197 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7220 Loss_G: 0.7289 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7081 Loss_G: 0.7501 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6654 Loss_G: 0.7359 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7262 Loss_G: 0.7604 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.7187 Loss_G: 0.7435 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7116 Loss_G: 0.7348 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.7044 Loss_G: 0.7096 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7191 Loss_G: 0.7208 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.7311 Loss_G: 0.7467 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7259 Loss_G: 0.7700 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.7160 Loss_G: 0.7296 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7231 Loss_G: 0.7462 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6938 Loss_G: 0.7311 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7425 Loss_G: 0.7510 acc: 93.8%\n",
      "[BATCH 100/149] Loss_D: 0.6944 Loss_G: 0.7432 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.7243 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6728 Loss_G: 0.7342 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6834 Loss_G: 0.7315 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6740 Loss_G: 0.7461 acc: 96.9%\n",
      "[BATCH 105/149] Loss_D: 0.7000 Loss_G: 0.7282 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7204 Loss_G: 0.7270 acc: 81.2%\n",
      "[BATCH 107/149] Loss_D: 0.7294 Loss_G: 0.7420 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7002 Loss_G: 0.7444 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6647 Loss_G: 0.7380 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6751 Loss_G: 0.7201 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.7502 Loss_G: 0.7235 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7381 Loss_G: 0.7369 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6967 Loss_G: 0.7275 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6732 Loss_G: 0.7220 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6950 Loss_G: 0.7122 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6620 Loss_G: 0.7122 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6982 Loss_G: 0.7213 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6734 Loss_G: 0.7279 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6959 Loss_G: 0.7334 acc: 98.4%\n",
      "[BATCH 120/149] Loss_D: 0.7118 Loss_G: 0.7363 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7100 Loss_G: 0.7186 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6977 Loss_G: 0.7310 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.7117 Loss_G: 0.7367 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.7139 Loss_G: 0.7347 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6925 Loss_G: 0.7196 acc: 95.3%\n",
      "[BATCH 126/149] Loss_D: 0.7136 Loss_G: 0.7244 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7265 Loss_G: 0.7266 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.7175 Loss_G: 0.7262 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.7196 Loss_G: 0.7430 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7050 Loss_G: 0.7144 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6903 Loss_G: 0.7043 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7223 Loss_G: 0.7177 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6949 Loss_G: 0.7198 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7256 Loss_G: 0.7356 acc: 87.5%\n",
      "[EPOCH 5200] TEST ACC is : 75.8%\n",
      "[BATCH 135/149] Loss_D: 0.6697 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6676 Loss_G: 0.7130 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.7008 Loss_G: 0.6963 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.7165 Loss_G: 0.7131 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6502 Loss_G: 0.7193 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6996 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6867 Loss_G: 0.7376 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6861 Loss_G: 0.7308 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.6906 Loss_G: 0.7351 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.6863 Loss_G: 0.7340 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7038 Loss_G: 0.7285 acc: 95.3%\n",
      "[BATCH 146/149] Loss_D: 0.7257 Loss_G: 0.7393 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6898 Loss_G: 0.7487 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6957 Loss_G: 0.7561 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6797 Loss_G: 0.7377 acc: 92.2%\n",
      "-----THE [35/50] epoch end-----\n",
      "-----THE [36/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6979 Loss_G: 0.7348 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.7236 Loss_G: 0.7389 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7146 Loss_G: 0.7342 acc: 79.7%\n",
      "[BATCH 4/149] Loss_D: 0.6669 Loss_G: 0.7176 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.6586 Loss_G: 0.6897 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.7392 Loss_G: 0.7231 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7326 Loss_G: 0.7334 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.7536 Loss_G: 0.7484 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6820 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.7253 Loss_G: 0.7288 acc: 93.8%\n",
      "[BATCH 11/149] Loss_D: 0.6592 Loss_G: 0.7148 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7512 Loss_G: 0.7218 acc: 81.2%\n",
      "[BATCH 13/149] Loss_D: 0.7183 Loss_G: 0.7465 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.7692 Loss_G: 0.7377 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.7082 Loss_G: 0.7318 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7134 Loss_G: 0.7153 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6995 Loss_G: 0.7376 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7285 Loss_G: 0.7533 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6986 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6595 Loss_G: 0.7107 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.7039 Loss_G: 0.7299 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.7136 Loss_G: 0.7368 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7127 Loss_G: 0.7441 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6942 Loss_G: 0.7340 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7109 Loss_G: 0.7278 acc: 82.8%\n",
      "[BATCH 26/149] Loss_D: 0.6713 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7137 Loss_G: 0.7220 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7139 Loss_G: 0.7649 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.7115 Loss_G: 0.7441 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.6862 Loss_G: 0.7369 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6824 Loss_G: 0.7188 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7854 Loss_G: 0.7788 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.7248 Loss_G: 0.7411 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6760 Loss_G: 0.7374 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.7109 Loss_G: 0.7514 acc: 85.9%\n",
      "[EPOCH 5250] TEST ACC is : 76.4%\n",
      "[BATCH 36/149] Loss_D: 0.7221 Loss_G: 0.7444 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6770 Loss_G: 0.7140 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.7049 Loss_G: 0.7077 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.6769 Loss_G: 0.7224 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6821 Loss_G: 0.7186 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6633 Loss_G: 0.7064 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6968 Loss_G: 0.7006 acc: 76.6%\n",
      "[BATCH 43/149] Loss_D: 0.7283 Loss_G: 0.7105 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.7530 Loss_G: 0.7340 acc: 82.8%\n",
      "[BATCH 45/149] Loss_D: 0.7108 Loss_G: 0.7545 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6998 Loss_G: 0.7523 acc: 96.9%\n",
      "[BATCH 47/149] Loss_D: 0.6827 Loss_G: 0.7313 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7085 Loss_G: 0.7327 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7124 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.7359 Loss_G: 0.7340 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.7241 Loss_G: 0.7610 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6977 Loss_G: 0.7307 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6990 Loss_G: 0.7376 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.7088 Loss_G: 0.7388 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6844 Loss_G: 0.7122 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.6836 Loss_G: 0.7281 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.6446 Loss_G: 0.7119 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6718 Loss_G: 0.7055 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6870 Loss_G: 0.7127 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6690 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6784 Loss_G: 0.7142 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6659 Loss_G: 0.7163 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6575 Loss_G: 0.7217 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.7123 Loss_G: 0.7347 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7017 Loss_G: 0.7177 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7164 Loss_G: 0.7358 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6972 Loss_G: 0.7257 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.7268 Loss_G: 0.7426 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6881 Loss_G: 0.7345 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7209 Loss_G: 0.7178 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7404 Loss_G: 0.7294 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7130 Loss_G: 0.7555 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7180 Loss_G: 0.7490 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6884 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.7238 Loss_G: 0.7374 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6734 Loss_G: 0.7217 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.7269 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7239 Loss_G: 0.7405 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.7303 Loss_G: 0.7342 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6885 Loss_G: 0.7299 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7031 Loss_G: 0.7222 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.7047 Loss_G: 0.7403 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7003 Loss_G: 0.7309 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6882 Loss_G: 0.7403 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7185 Loss_G: 0.7327 acc: 90.6%\n",
      "[EPOCH 5300] TEST ACC is : 76.4%\n",
      "[BATCH 86/149] Loss_D: 0.6781 Loss_G: 0.7316 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.7198 Loss_G: 0.7305 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.6428 Loss_G: 0.7372 acc: 95.3%\n",
      "[BATCH 89/149] Loss_D: 0.6885 Loss_G: 0.7250 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7214 Loss_G: 0.7217 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6848 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7011 Loss_G: 0.7126 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.7095 Loss_G: 0.7249 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.7029 Loss_G: 0.7441 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6598 Loss_G: 0.7074 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.7097 Loss_G: 0.7145 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6764 Loss_G: 0.7120 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6916 Loss_G: 0.7087 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6862 Loss_G: 0.6988 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6953 Loss_G: 0.6978 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7058 Loss_G: 0.7285 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.7293 Loss_G: 0.7273 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6974 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7016 Loss_G: 0.7365 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6725 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7249 Loss_G: 0.7688 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7089 Loss_G: 0.7975 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7570 Loss_G: 0.7841 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6701 Loss_G: 0.7399 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6979 Loss_G: 0.7257 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7079 Loss_G: 0.7354 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6971 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6683 Loss_G: 0.7150 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6982 Loss_G: 0.7246 acc: 96.9%\n",
      "[BATCH 115/149] Loss_D: 0.6756 Loss_G: 0.7319 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.7202 Loss_G: 0.7425 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7177 Loss_G: 0.7286 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6753 Loss_G: 0.7384 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7124 Loss_G: 0.7343 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7006 Loss_G: 0.7257 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.7177 Loss_G: 0.7288 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6795 Loss_G: 0.7309 acc: 95.3%\n",
      "[BATCH 123/149] Loss_D: 0.6985 Loss_G: 0.7197 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.7177 Loss_G: 0.7260 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.6946 Loss_G: 0.7262 acc: 95.3%\n",
      "[BATCH 126/149] Loss_D: 0.7046 Loss_G: 0.7379 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6721 Loss_G: 0.7224 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7252 Loss_G: 0.7483 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.7029 Loss_G: 0.7143 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.6942 Loss_G: 0.7386 acc: 95.3%\n",
      "[BATCH 131/149] Loss_D: 0.6958 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6910 Loss_G: 0.7129 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.7312 Loss_G: 0.7241 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7304 Loss_G: 0.7351 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.7216 Loss_G: 0.7512 acc: 87.5%\n",
      "[EPOCH 5350] TEST ACC is : 76.6%\n",
      "[BATCH 136/149] Loss_D: 0.7064 Loss_G: 0.7360 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6973 Loss_G: 0.7550 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.6842 Loss_G: 0.7482 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7572 Loss_G: 0.7563 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7257 Loss_G: 0.7556 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.7118 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6678 Loss_G: 0.7142 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.6782 Loss_G: 0.6991 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.7081 Loss_G: 0.7220 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6644 Loss_G: 0.7275 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.7033 Loss_G: 0.7465 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7055 Loss_G: 0.6984 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6814 Loss_G: 0.7224 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7339 Loss_G: 0.7245 acc: 89.1%\n",
      "-----THE [36/50] epoch end-----\n",
      "-----THE [37/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6721 Loss_G: 0.7050 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.6686 Loss_G: 0.7050 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6924 Loss_G: 0.6991 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.7306 Loss_G: 0.7118 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6985 Loss_G: 0.7643 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6668 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7040 Loss_G: 0.7491 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6928 Loss_G: 0.7311 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6814 Loss_G: 0.7055 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6850 Loss_G: 0.7268 acc: 95.3%\n",
      "[BATCH 11/149] Loss_D: 0.7337 Loss_G: 0.7418 acc: 79.7%\n",
      "[BATCH 12/149] Loss_D: 0.7098 Loss_G: 0.7570 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6908 Loss_G: 0.7564 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.7242 Loss_G: 0.7379 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.7304 Loss_G: 0.7235 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7456 Loss_G: 0.7267 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6903 Loss_G: 0.7256 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6827 Loss_G: 0.7181 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.6673 Loss_G: 0.7068 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7025 Loss_G: 0.7220 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6844 Loss_G: 0.7334 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7202 Loss_G: 0.7250 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.6916 Loss_G: 0.7413 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.6979 Loss_G: 0.7759 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6999 Loss_G: 0.7650 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.7131 Loss_G: 0.7481 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.6957 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6933 Loss_G: 0.7164 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.7238 Loss_G: 0.7191 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.6751 Loss_G: 0.7257 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7103 Loss_G: 0.7124 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6899 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7274 Loss_G: 0.7442 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.7425 Loss_G: 0.7398 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7133 Loss_G: 0.7493 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6966 Loss_G: 0.7230 acc: 89.1%\n",
      "[EPOCH 5400] TEST ACC is : 76.6%\n",
      "[BATCH 37/149] Loss_D: 0.6984 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.7098 Loss_G: 0.7253 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7276 Loss_G: 0.7282 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.7483 Loss_G: 0.7514 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6961 Loss_G: 0.7394 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.7490 Loss_G: 0.7512 acc: 79.7%\n",
      "[BATCH 43/149] Loss_D: 0.7267 Loss_G: 0.7449 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6623 Loss_G: 0.7612 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7425 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6659 Loss_G: 0.7225 acc: 95.3%\n",
      "[BATCH 47/149] Loss_D: 0.6752 Loss_G: 0.7239 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7346 Loss_G: 0.7261 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7050 Loss_G: 0.7120 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6961 Loss_G: 0.7320 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6992 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7195 Loss_G: 0.7351 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6855 Loss_G: 0.7427 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7014 Loss_G: 0.7272 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6771 Loss_G: 0.7061 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.7060 Loss_G: 0.7184 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6880 Loss_G: 0.7205 acc: 95.3%\n",
      "[BATCH 58/149] Loss_D: 0.6699 Loss_G: 0.7179 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.7002 Loss_G: 0.7168 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6924 Loss_G: 0.7086 acc: 81.2%\n",
      "[BATCH 61/149] Loss_D: 0.7040 Loss_G: 0.7109 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7218 Loss_G: 0.7323 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6841 Loss_G: 0.7354 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7182 Loss_G: 0.7454 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6716 Loss_G: 0.7471 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.7122 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7195 Loss_G: 0.7523 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7156 Loss_G: 0.7425 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7270 Loss_G: 0.7412 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7014 Loss_G: 0.7403 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7088 Loss_G: 0.7293 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.6959 Loss_G: 0.7163 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6869 Loss_G: 0.7226 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6788 Loss_G: 0.6981 acc: 82.8%\n",
      "[BATCH 75/149] Loss_D: 0.7184 Loss_G: 0.7267 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7144 Loss_G: 0.7373 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7248 Loss_G: 0.7393 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.6946 Loss_G: 0.7446 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6967 Loss_G: 0.7194 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7181 Loss_G: 0.7179 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7182 Loss_G: 0.7342 acc: 81.2%\n",
      "[BATCH 82/149] Loss_D: 0.7146 Loss_G: 0.7105 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7181 Loss_G: 0.7259 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.7038 Loss_G: 0.7396 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7214 Loss_G: 0.7420 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.7385 Loss_G: 0.7495 acc: 95.3%\n",
      "[EPOCH 5450] TEST ACC is : 76.2%\n",
      "[BATCH 87/149] Loss_D: 0.7120 Loss_G: 0.7291 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6855 Loss_G: 0.7100 acc: 78.1%\n",
      "[BATCH 89/149] Loss_D: 0.7011 Loss_G: 0.7244 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6975 Loss_G: 0.7310 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7408 Loss_G: 0.7454 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6757 Loss_G: 0.7382 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.7047 Loss_G: 0.7172 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6980 Loss_G: 0.7089 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.7200 Loss_G: 0.7234 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.7010 Loss_G: 0.7404 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6849 Loss_G: 0.7530 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.7264 Loss_G: 0.7604 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6922 Loss_G: 0.7402 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7028 Loss_G: 0.7402 acc: 96.9%\n",
      "[BATCH 101/149] Loss_D: 0.6862 Loss_G: 0.7192 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6837 Loss_G: 0.7146 acc: 84.4%\n",
      "[BATCH 103/149] Loss_D: 0.6911 Loss_G: 0.7157 acc: 93.8%\n",
      "[BATCH 104/149] Loss_D: 0.6668 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6774 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7160 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.6830 Loss_G: 0.7325 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6726 Loss_G: 0.7121 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6912 Loss_G: 0.7116 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.6724 Loss_G: 0.7202 acc: 95.3%\n",
      "[BATCH 111/149] Loss_D: 0.7586 Loss_G: 0.7353 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7251 Loss_G: 0.7381 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7323 Loss_G: 0.7370 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.6714 Loss_G: 0.7465 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.7161 Loss_G: 0.7376 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6664 Loss_G: 0.7209 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.6941 Loss_G: 0.7160 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7276 Loss_G: 0.7399 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6943 Loss_G: 0.7639 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.7002 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7156 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6923 Loss_G: 0.7152 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.7097 Loss_G: 0.7244 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.7038 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6997 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6999 Loss_G: 0.7142 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7104 Loss_G: 0.7143 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6593 Loss_G: 0.6952 acc: 95.3%\n",
      "[BATCH 129/149] Loss_D: 0.6938 Loss_G: 0.7238 acc: 95.3%\n",
      "[BATCH 130/149] Loss_D: 0.7021 Loss_G: 0.7283 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7363 Loss_G: 0.7468 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6908 Loss_G: 0.7360 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7246 Loss_G: 0.7237 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7351 Loss_G: 0.7236 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6792 Loss_G: 0.7224 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.7029 Loss_G: 0.7265 acc: 84.4%\n",
      "[EPOCH 5500] TEST ACC is : 75.2%\n",
      "[BATCH 137/149] Loss_D: 0.6999 Loss_G: 0.7445 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7123 Loss_G: 0.7401 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.7241 Loss_G: 0.7209 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7139 Loss_G: 0.7206 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6933 Loss_G: 0.7431 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6858 Loss_G: 0.7307 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.7181 Loss_G: 0.7237 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.7221 Loss_G: 0.7268 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6770 Loss_G: 0.7325 acc: 95.3%\n",
      "[BATCH 146/149] Loss_D: 0.6756 Loss_G: 0.7132 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.7057 Loss_G: 0.7052 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.7094 Loss_G: 0.7347 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6724 Loss_G: 0.7148 acc: 87.5%\n",
      "-----THE [37/50] epoch end-----\n",
      "-----THE [38/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7119 Loss_G: 0.7284 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.6784 Loss_G: 0.7335 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7216 Loss_G: 0.7353 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.6917 Loss_G: 0.7313 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7018 Loss_G: 0.7473 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7018 Loss_G: 0.7396 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.7095 Loss_G: 0.7454 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.6839 Loss_G: 0.7440 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.6792 Loss_G: 0.7302 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.6770 Loss_G: 0.7137 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.6893 Loss_G: 0.7197 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.7154 Loss_G: 0.7156 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6845 Loss_G: 0.7144 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6949 Loss_G: 0.7020 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6889 Loss_G: 0.7248 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.7024 Loss_G: 0.7455 acc: 95.3%\n",
      "[BATCH 17/149] Loss_D: 0.7230 Loss_G: 0.7499 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6850 Loss_G: 0.7234 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6876 Loss_G: 0.7218 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.7388 Loss_G: 0.7422 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6814 Loss_G: 0.7219 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6591 Loss_G: 0.7337 acc: 96.9%\n",
      "[BATCH 23/149] Loss_D: 0.7213 Loss_G: 0.7570 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.6849 Loss_G: 0.7131 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6980 Loss_G: 0.7285 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7400 Loss_G: 0.7277 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6736 Loss_G: 0.7335 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6932 Loss_G: 0.7170 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6619 Loss_G: 0.7075 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7149 Loss_G: 0.7277 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6799 Loss_G: 0.7196 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.6679 Loss_G: 0.7034 acc: 76.6%\n",
      "[BATCH 33/149] Loss_D: 0.7001 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6848 Loss_G: 0.7401 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.6705 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6541 Loss_G: 0.7071 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6734 Loss_G: 0.7103 acc: 95.3%\n",
      "[EPOCH 5550] TEST ACC is : 77.0%\n",
      "[BATCH 38/149] Loss_D: 0.7186 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7100 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6819 Loss_G: 0.7277 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.6964 Loss_G: 0.7287 acc: 98.4%\n",
      "[BATCH 42/149] Loss_D: 0.7516 Loss_G: 0.7638 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6765 Loss_G: 0.7607 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7161 Loss_G: 0.7881 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6886 Loss_G: 0.7454 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.7165 Loss_G: 0.7515 acc: 93.8%\n",
      "[BATCH 47/149] Loss_D: 0.6879 Loss_G: 0.7379 acc: 93.8%\n",
      "[BATCH 48/149] Loss_D: 0.7113 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7099 Loss_G: 0.7264 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6963 Loss_G: 0.7171 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6806 Loss_G: 0.7073 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.7081 Loss_G: 0.7286 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7323 Loss_G: 0.7394 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.7170 Loss_G: 0.7473 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6912 Loss_G: 0.7266 acc: 96.9%\n",
      "[BATCH 56/149] Loss_D: 0.7022 Loss_G: 0.7218 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6923 Loss_G: 0.7230 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.7017 Loss_G: 0.7340 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.7327 Loss_G: 0.7359 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7178 Loss_G: 0.7447 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6876 Loss_G: 0.7506 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7092 Loss_G: 0.7202 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7127 Loss_G: 0.7390 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6637 Loss_G: 0.7090 acc: 81.2%\n",
      "[BATCH 65/149] Loss_D: 0.7236 Loss_G: 0.7229 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7312 Loss_G: 0.7327 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.7137 Loss_G: 0.7262 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6712 Loss_G: 0.7112 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7055 Loss_G: 0.7117 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6738 Loss_G: 0.7149 acc: 93.8%\n",
      "[BATCH 71/149] Loss_D: 0.7019 Loss_G: 0.7221 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.7462 Loss_G: 0.7301 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7175 Loss_G: 0.7147 acc: 79.7%\n",
      "[BATCH 74/149] Loss_D: 0.6783 Loss_G: 0.7200 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7268 Loss_G: 0.7145 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.6653 Loss_G: 0.7458 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7124 Loss_G: 0.7511 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.7042 Loss_G: 0.7375 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6796 Loss_G: 0.7212 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6863 Loss_G: 0.7294 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.7333 Loss_G: 0.7510 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.6791 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6680 Loss_G: 0.7068 acc: 95.3%\n",
      "[BATCH 84/149] Loss_D: 0.7367 Loss_G: 0.7103 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7036 Loss_G: 0.7235 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.6802 Loss_G: 0.7212 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.7068 Loss_G: 0.7406 acc: 93.8%\n",
      "[EPOCH 5600] TEST ACC is : 75.6%\n",
      "[BATCH 88/149] Loss_D: 0.6924 Loss_G: 0.7368 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7079 Loss_G: 0.7518 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.7372 Loss_G: 0.7428 acc: 82.8%\n",
      "[BATCH 91/149] Loss_D: 0.6693 Loss_G: 0.7004 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.7184 Loss_G: 0.7295 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7349 Loss_G: 0.7275 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.7192 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.7005 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7177 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7034 Loss_G: 0.7423 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.7080 Loss_G: 0.7461 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.7292 Loss_G: 0.7794 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.7109 Loss_G: 0.7544 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6835 Loss_G: 0.7404 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6800 Loss_G: 0.7290 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6739 Loss_G: 0.7284 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6864 Loss_G: 0.7219 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.7152 Loss_G: 0.7225 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7174 Loss_G: 0.7368 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.6983 Loss_G: 0.7302 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.7193 Loss_G: 0.7329 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6812 Loss_G: 0.7165 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6622 Loss_G: 0.7159 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6865 Loss_G: 0.7149 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7187 Loss_G: 0.7259 acc: 95.3%\n",
      "[BATCH 113/149] Loss_D: 0.7023 Loss_G: 0.7393 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7589 Loss_G: 0.7468 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7050 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6778 Loss_G: 0.7082 acc: 79.7%\n",
      "[BATCH 117/149] Loss_D: 0.7117 Loss_G: 0.7283 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6618 Loss_G: 0.7220 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.6938 Loss_G: 0.7346 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.7375 Loss_G: 0.7408 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7032 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7130 Loss_G: 0.7261 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7103 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6814 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.7194 Loss_G: 0.7312 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.7283 Loss_G: 0.7279 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6951 Loss_G: 0.7325 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7321 Loss_G: 0.7514 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.6710 Loss_G: 0.7315 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7168 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6859 Loss_G: 0.7244 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.7181 Loss_G: 0.7248 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7228 Loss_G: 0.7235 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6850 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7346 Loss_G: 0.7460 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6878 Loss_G: 0.7294 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7139 Loss_G: 0.7349 acc: 92.2%\n",
      "[EPOCH 5650] TEST ACC is : 75.8%\n",
      "[BATCH 138/149] Loss_D: 0.7286 Loss_G: 0.7405 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.7155 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7110 Loss_G: 0.7298 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.7785 Loss_G: 0.7577 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.7183 Loss_G: 0.7395 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6875 Loss_G: 0.7334 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.7085 Loss_G: 0.7491 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6807 Loss_G: 0.7291 acc: 96.9%\n",
      "[BATCH 146/149] Loss_D: 0.7362 Loss_G: 0.7452 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.6646 Loss_G: 0.7235 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6940 Loss_G: 0.7186 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.6617 Loss_G: 0.6925 acc: 90.6%\n",
      "-----THE [38/50] epoch end-----\n",
      "-----THE [39/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6764 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7126 Loss_G: 0.7150 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.7008 Loss_G: 0.7064 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.7053 Loss_G: 0.7355 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7273 Loss_G: 0.7427 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.6928 Loss_G: 0.7391 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6922 Loss_G: 0.7220 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7009 Loss_G: 0.7211 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.6755 Loss_G: 0.7151 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6932 Loss_G: 0.7103 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7162 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.7332 Loss_G: 0.7518 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6934 Loss_G: 0.7281 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.7477 Loss_G: 0.7410 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.6883 Loss_G: 0.7224 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6961 Loss_G: 0.7480 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.7042 Loss_G: 0.7534 acc: 95.3%\n",
      "[BATCH 18/149] Loss_D: 0.7210 Loss_G: 0.7369 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.6885 Loss_G: 0.7375 acc: 81.2%\n",
      "[BATCH 20/149] Loss_D: 0.7306 Loss_G: 0.7311 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6761 Loss_G: 0.7144 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7386 Loss_G: 0.7566 acc: 82.8%\n",
      "[BATCH 23/149] Loss_D: 0.6706 Loss_G: 0.7157 acc: 93.8%\n",
      "[BATCH 24/149] Loss_D: 0.7335 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6973 Loss_G: 0.7283 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.7332 Loss_G: 0.7291 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7373 Loss_G: 0.7451 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7071 Loss_G: 0.7349 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.7091 Loss_G: 0.7344 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7115 Loss_G: 0.7361 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7319 Loss_G: 0.7354 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.6789 Loss_G: 0.7150 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7067 Loss_G: 0.7263 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.7289 Loss_G: 0.7503 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.7500 Loss_G: 0.7296 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6910 Loss_G: 0.7308 acc: 95.3%\n",
      "[BATCH 37/149] Loss_D: 0.6780 Loss_G: 0.7277 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7042 Loss_G: 0.7240 acc: 82.8%\n",
      "[EPOCH 5700] TEST ACC is : 76.2%\n",
      "[BATCH 39/149] Loss_D: 0.6979 Loss_G: 0.7536 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6877 Loss_G: 0.7133 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7594 Loss_G: 0.7402 acc: 81.2%\n",
      "[BATCH 42/149] Loss_D: 0.6849 Loss_G: 0.7162 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6654 Loss_G: 0.7110 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6981 Loss_G: 0.7051 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6597 Loss_G: 0.7035 acc: 93.8%\n",
      "[BATCH 46/149] Loss_D: 0.6576 Loss_G: 0.6944 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7156 Loss_G: 0.7073 acc: 79.7%\n",
      "[BATCH 48/149] Loss_D: 0.6863 Loss_G: 0.7078 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7237 Loss_G: 0.7100 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.6464 Loss_G: 0.7027 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.6717 Loss_G: 0.7103 acc: 96.9%\n",
      "[BATCH 52/149] Loss_D: 0.7132 Loss_G: 0.7219 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.6782 Loss_G: 0.7116 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.7073 Loss_G: 0.7315 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6820 Loss_G: 0.7174 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.7203 Loss_G: 0.7174 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7194 Loss_G: 0.7333 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6801 Loss_G: 0.7248 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6955 Loss_G: 0.7340 acc: 95.3%\n",
      "[BATCH 60/149] Loss_D: 0.7195 Loss_G: 0.7312 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.7061 Loss_G: 0.7150 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6611 Loss_G: 0.7006 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.7213 Loss_G: 0.7417 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6885 Loss_G: 0.7263 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.7143 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7253 Loss_G: 0.7325 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7486 Loss_G: 0.7419 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6867 Loss_G: 0.7295 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7002 Loss_G: 0.7529 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6646 Loss_G: 0.7293 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6847 Loss_G: 0.7154 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6949 Loss_G: 0.7036 acc: 81.2%\n",
      "[BATCH 73/149] Loss_D: 0.6912 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7057 Loss_G: 0.7268 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6610 Loss_G: 0.7069 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6942 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6631 Loss_G: 0.7068 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6751 Loss_G: 0.7126 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6754 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7318 Loss_G: 0.7533 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6915 Loss_G: 0.7299 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7375 Loss_G: 0.7401 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7243 Loss_G: 0.7266 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6720 Loss_G: 0.7378 acc: 96.9%\n",
      "[BATCH 85/149] Loss_D: 0.6970 Loss_G: 0.7355 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7116 Loss_G: 0.7243 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.7015 Loss_G: 0.7275 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.6850 Loss_G: 0.7313 acc: 90.6%\n",
      "[EPOCH 5750] TEST ACC is : 77.0%\n",
      "[BATCH 89/149] Loss_D: 0.6787 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6703 Loss_G: 0.7038 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.7169 Loss_G: 0.7103 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6857 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6987 Loss_G: 0.7438 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7415 Loss_G: 0.7604 acc: 95.3%\n",
      "[BATCH 95/149] Loss_D: 0.7629 Loss_G: 0.7715 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6908 Loss_G: 0.7530 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6893 Loss_G: 0.7134 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7126 Loss_G: 0.7222 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7174 Loss_G: 0.7333 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.6970 Loss_G: 0.7323 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6546 Loss_G: 0.7001 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7120 Loss_G: 0.6873 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6832 Loss_G: 0.7148 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.7213 Loss_G: 0.7412 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.6998 Loss_G: 0.7331 acc: 82.8%\n",
      "[BATCH 106/149] Loss_D: 0.6729 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6756 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7150 Loss_G: 0.7379 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6585 Loss_G: 0.7424 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6900 Loss_G: 0.7268 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.7237 Loss_G: 0.7303 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.6886 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7154 Loss_G: 0.7270 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6969 Loss_G: 0.7142 acc: 76.6%\n",
      "[BATCH 115/149] Loss_D: 0.7065 Loss_G: 0.7346 acc: 82.8%\n",
      "[BATCH 116/149] Loss_D: 0.7032 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6840 Loss_G: 0.7255 acc: 95.3%\n",
      "[BATCH 118/149] Loss_D: 0.6534 Loss_G: 0.6979 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7026 Loss_G: 0.7192 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7610 Loss_G: 0.7363 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.6910 Loss_G: 0.7305 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6973 Loss_G: 0.7493 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.6932 Loss_G: 0.7276 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.7284 Loss_G: 0.7363 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7091 Loss_G: 0.7205 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.6653 Loss_G: 0.7273 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7618 Loss_G: 0.7721 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6871 Loss_G: 0.7879 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7233 Loss_G: 0.7390 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6822 Loss_G: 0.7430 acc: 93.8%\n",
      "[BATCH 131/149] Loss_D: 0.6824 Loss_G: 0.7339 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6978 Loss_G: 0.7156 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.7179 Loss_G: 0.7224 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7239 Loss_G: 0.7384 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6800 Loss_G: 0.7311 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7115 Loss_G: 0.7301 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6800 Loss_G: 0.7340 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.7096 Loss_G: 0.7363 acc: 92.2%\n",
      "[EPOCH 5800] TEST ACC is : 76.8%\n",
      "[BATCH 139/149] Loss_D: 0.7020 Loss_G: 0.7530 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6983 Loss_G: 0.7408 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6910 Loss_G: 0.7182 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7256 Loss_G: 0.7345 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7154 Loss_G: 0.7222 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6969 Loss_G: 0.7288 acc: 82.8%\n",
      "[BATCH 145/149] Loss_D: 0.7235 Loss_G: 0.7347 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7518 Loss_G: 0.7521 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6939 Loss_G: 0.7321 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.7180 Loss_G: 0.7233 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7208 Loss_G: 0.7558 acc: 96.9%\n",
      "-----THE [39/50] epoch end-----\n",
      "-----THE [40/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7225 Loss_G: 0.7266 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.6650 Loss_G: 0.7133 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6750 Loss_G: 0.7195 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6534 Loss_G: 0.7126 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6853 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.7032 Loss_G: 0.7236 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7239 Loss_G: 0.7336 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7439 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.7263 Loss_G: 0.7301 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6580 Loss_G: 0.7209 acc: 93.8%\n",
      "[BATCH 11/149] Loss_D: 0.6777 Loss_G: 0.7198 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.7138 Loss_G: 0.7440 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6811 Loss_G: 0.7118 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.7110 Loss_G: 0.7472 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6932 Loss_G: 0.7330 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7450 Loss_G: 0.7387 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.7191 Loss_G: 0.7284 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.6857 Loss_G: 0.7161 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7052 Loss_G: 0.7051 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6893 Loss_G: 0.7171 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7078 Loss_G: 0.7229 acc: 81.2%\n",
      "[BATCH 22/149] Loss_D: 0.6682 Loss_G: 0.7009 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.6895 Loss_G: 0.7315 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6973 Loss_G: 0.7395 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7227 Loss_G: 0.7542 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6640 Loss_G: 0.7378 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7748 Loss_G: 0.7278 acc: 76.6%\n",
      "[BATCH 28/149] Loss_D: 0.6872 Loss_G: 0.7353 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.7261 Loss_G: 0.7354 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.7229 Loss_G: 0.7514 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.6818 Loss_G: 0.7511 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6848 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7159 Loss_G: 0.7241 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7350 Loss_G: 0.7139 acc: 79.7%\n",
      "[BATCH 35/149] Loss_D: 0.7216 Loss_G: 0.7299 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.7474 Loss_G: 0.7549 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6903 Loss_G: 0.7643 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7224 Loss_G: 0.7911 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7605 Loss_G: 0.7628 acc: 89.1%\n",
      "[EPOCH 5850] TEST ACC is : 75.4%\n",
      "[BATCH 40/149] Loss_D: 0.7094 Loss_G: 0.7190 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6813 Loss_G: 0.7345 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.7169 Loss_G: 0.7354 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7129 Loss_G: 0.7383 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6988 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.7161 Loss_G: 0.7336 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7078 Loss_G: 0.7570 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6754 Loss_G: 0.7352 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7033 Loss_G: 0.7351 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6989 Loss_G: 0.7332 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.7052 Loss_G: 0.7441 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6810 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6418 Loss_G: 0.7120 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.7127 Loss_G: 0.7177 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.7139 Loss_G: 0.7252 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6820 Loss_G: 0.7235 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7369 Loss_G: 0.7431 acc: 81.2%\n",
      "[BATCH 57/149] Loss_D: 0.6766 Loss_G: 0.7303 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6701 Loss_G: 0.7144 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6707 Loss_G: 0.7034 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6993 Loss_G: 0.7239 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.6880 Loss_G: 0.7080 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7180 Loss_G: 0.7233 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.7261 Loss_G: 0.7192 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7439 Loss_G: 0.7567 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6980 Loss_G: 0.7336 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6939 Loss_G: 0.7297 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.6953 Loss_G: 0.7219 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7333 Loss_G: 0.7491 acc: 96.9%\n",
      "[BATCH 69/149] Loss_D: 0.6888 Loss_G: 0.7329 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.7485 Loss_G: 0.7385 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6866 Loss_G: 0.7147 acc: 79.7%\n",
      "[BATCH 72/149] Loss_D: 0.7228 Loss_G: 0.7169 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6926 Loss_G: 0.7362 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7253 Loss_G: 0.7606 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6632 Loss_G: 0.7406 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7084 Loss_G: 0.7405 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7397 Loss_G: 0.7561 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7262 Loss_G: 0.7649 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6998 Loss_G: 0.7324 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7252 Loss_G: 0.7288 acc: 79.7%\n",
      "[BATCH 81/149] Loss_D: 0.7208 Loss_G: 0.7447 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.6736 Loss_G: 0.7315 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.6953 Loss_G: 0.7220 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.6955 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6735 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6896 Loss_G: 0.7281 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6788 Loss_G: 0.7378 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.7052 Loss_G: 0.7303 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.7376 Loss_G: 0.7575 acc: 90.6%\n",
      "[EPOCH 5900] TEST ACC is : 77.3%\n",
      "[BATCH 90/149] Loss_D: 0.6883 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.7192 Loss_G: 0.7070 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6958 Loss_G: 0.7111 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7777 Loss_G: 0.7399 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.6860 Loss_G: 0.7500 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7085 Loss_G: 0.7415 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7093 Loss_G: 0.7229 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6783 Loss_G: 0.7138 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.7020 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7320 Loss_G: 0.7271 acc: 82.8%\n",
      "[BATCH 100/149] Loss_D: 0.6929 Loss_G: 0.7254 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6985 Loss_G: 0.7276 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7022 Loss_G: 0.7137 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6820 Loss_G: 0.7204 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7113 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.7037 Loss_G: 0.7068 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.6625 Loss_G: 0.7105 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.7069 Loss_G: 0.7147 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6850 Loss_G: 0.7153 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6948 Loss_G: 0.7426 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.6572 Loss_G: 0.7236 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6709 Loss_G: 0.7015 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6812 Loss_G: 0.7313 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6914 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6889 Loss_G: 0.7098 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.7101 Loss_G: 0.7320 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7016 Loss_G: 0.7362 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6763 Loss_G: 0.7120 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6830 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6842 Loss_G: 0.7121 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.7440 Loss_G: 0.7289 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7104 Loss_G: 0.7466 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6960 Loss_G: 0.7398 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7175 Loss_G: 0.7253 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6784 Loss_G: 0.7233 acc: 81.2%\n",
      "[BATCH 125/149] Loss_D: 0.6754 Loss_G: 0.7068 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6635 Loss_G: 0.6926 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.6871 Loss_G: 0.7005 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6962 Loss_G: 0.7148 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6797 Loss_G: 0.7059 acc: 81.2%\n",
      "[BATCH 130/149] Loss_D: 0.7085 Loss_G: 0.7326 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6860 Loss_G: 0.7103 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7150 Loss_G: 0.7382 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.7114 Loss_G: 0.7338 acc: 93.8%\n",
      "[BATCH 134/149] Loss_D: 0.6804 Loss_G: 0.7179 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.6944 Loss_G: 0.7115 acc: 93.8%\n",
      "[BATCH 136/149] Loss_D: 0.6992 Loss_G: 0.7181 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.7572 Loss_G: 0.7550 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6999 Loss_G: 0.7318 acc: 82.8%\n",
      "[BATCH 139/149] Loss_D: 0.7083 Loss_G: 0.7370 acc: 90.6%\n",
      "[EPOCH 5950] TEST ACC is : 76.8%\n",
      "[BATCH 140/149] Loss_D: 0.6985 Loss_G: 0.7063 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6719 Loss_G: 0.7010 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6737 Loss_G: 0.6953 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.7227 Loss_G: 0.7062 acc: 81.2%\n",
      "[BATCH 144/149] Loss_D: 0.7006 Loss_G: 0.7147 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.7184 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6715 Loss_G: 0.7064 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.7003 Loss_G: 0.7200 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6841 Loss_G: 0.7328 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.6834 Loss_G: 0.7233 acc: 95.3%\n",
      "-----THE [40/50] epoch end-----\n",
      "-----THE [41/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6468 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6997 Loss_G: 0.7262 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.6833 Loss_G: 0.6963 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.7420 Loss_G: 0.7160 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7114 Loss_G: 0.7287 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7309 Loss_G: 0.7487 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.7234 Loss_G: 0.7455 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7603 Loss_G: 0.7643 acc: 79.7%\n",
      "[BATCH 9/149] Loss_D: 0.7650 Loss_G: 0.7534 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.6953 Loss_G: 0.7485 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7058 Loss_G: 0.7467 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6921 Loss_G: 0.7305 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.7074 Loss_G: 0.7282 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.7231 Loss_G: 0.7413 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6703 Loss_G: 0.7372 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6970 Loss_G: 0.7203 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6863 Loss_G: 0.7203 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6624 Loss_G: 0.7149 acc: 93.8%\n",
      "[BATCH 19/149] Loss_D: 0.6739 Loss_G: 0.7112 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6974 Loss_G: 0.7145 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6991 Loss_G: 0.7101 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6956 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6891 Loss_G: 0.7161 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6927 Loss_G: 0.7050 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.6727 Loss_G: 0.7071 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6636 Loss_G: 0.7009 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6714 Loss_G: 0.7015 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6790 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6831 Loss_G: 0.7059 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6715 Loss_G: 0.7374 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.7041 Loss_G: 0.7373 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6791 Loss_G: 0.7039 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.7017 Loss_G: 0.7440 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6726 Loss_G: 0.7165 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.6893 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6599 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.6786 Loss_G: 0.7257 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6943 Loss_G: 0.7245 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7188 Loss_G: 0.7294 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6828 Loss_G: 0.7284 acc: 84.4%\n",
      "[EPOCH 6000] TEST ACC is : 76.0%\n",
      "[BATCH 41/149] Loss_D: 0.7225 Loss_G: 0.7234 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.6861 Loss_G: 0.7247 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7357 Loss_G: 0.7328 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6766 Loss_G: 0.7252 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7155 Loss_G: 0.7520 acc: 93.8%\n",
      "[BATCH 46/149] Loss_D: 0.7284 Loss_G: 0.7445 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7379 Loss_G: 0.7342 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6577 Loss_G: 0.7117 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6763 Loss_G: 0.7065 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7239 Loss_G: 0.7248 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6842 Loss_G: 0.7242 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.7062 Loss_G: 0.7513 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.7081 Loss_G: 0.7330 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.7193 Loss_G: 0.7174 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.7138 Loss_G: 0.7435 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.7004 Loss_G: 0.7230 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.7008 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7467 Loss_G: 0.7237 acc: 79.7%\n",
      "[BATCH 59/149] Loss_D: 0.6979 Loss_G: 0.7063 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7110 Loss_G: 0.7170 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6893 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6919 Loss_G: 0.7160 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6908 Loss_G: 0.7007 acc: 78.1%\n",
      "[BATCH 64/149] Loss_D: 0.7190 Loss_G: 0.7159 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7048 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6966 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7319 Loss_G: 0.7356 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6915 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7009 Loss_G: 0.7211 acc: 95.3%\n",
      "[BATCH 70/149] Loss_D: 0.7140 Loss_G: 0.7371 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.7048 Loss_G: 0.7183 acc: 81.2%\n",
      "[BATCH 72/149] Loss_D: 0.6937 Loss_G: 0.7262 acc: 82.8%\n",
      "[BATCH 73/149] Loss_D: 0.7760 Loss_G: 0.7470 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.7214 Loss_G: 0.7314 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7079 Loss_G: 0.7230 acc: 81.2%\n",
      "[BATCH 76/149] Loss_D: 0.6578 Loss_G: 0.7259 acc: 96.9%\n",
      "[BATCH 77/149] Loss_D: 0.6735 Loss_G: 0.7149 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7096 Loss_G: 0.7279 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6639 Loss_G: 0.6995 acc: 93.8%\n",
      "[BATCH 80/149] Loss_D: 0.7421 Loss_G: 0.7434 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.7627 Loss_G: 0.8096 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6769 Loss_G: 0.7470 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.7260 Loss_G: 0.7238 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.7314 Loss_G: 0.7432 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6807 Loss_G: 0.7285 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.6591 Loss_G: 0.7084 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7186 Loss_G: 0.7111 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6963 Loss_G: 0.7223 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7360 Loss_G: 0.7385 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7056 Loss_G: 0.7156 acc: 82.8%\n",
      "[EPOCH 6050] TEST ACC is : 76.4%\n",
      "[BATCH 91/149] Loss_D: 0.7052 Loss_G: 0.7112 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.6397 Loss_G: 0.6894 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6578 Loss_G: 0.6995 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6654 Loss_G: 0.7028 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6941 Loss_G: 0.7031 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6695 Loss_G: 0.7133 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6694 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6719 Loss_G: 0.7135 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6488 Loss_G: 0.6991 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.7328 Loss_G: 0.7197 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6792 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6676 Loss_G: 0.7175 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6776 Loss_G: 0.7311 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6822 Loss_G: 0.7385 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6869 Loss_G: 0.7177 acc: 96.9%\n",
      "[BATCH 106/149] Loss_D: 0.6891 Loss_G: 0.7250 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6505 Loss_G: 0.7148 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7612 Loss_G: 0.7570 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6912 Loss_G: 0.7250 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.7169 Loss_G: 0.7472 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6855 Loss_G: 0.7225 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.6920 Loss_G: 0.7242 acc: 96.9%\n",
      "[BATCH 113/149] Loss_D: 0.6725 Loss_G: 0.7286 acc: 95.3%\n",
      "[BATCH 114/149] Loss_D: 0.7184 Loss_G: 0.7357 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7027 Loss_G: 0.7153 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6962 Loss_G: 0.7337 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6996 Loss_G: 0.7406 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6760 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7451 Loss_G: 0.7420 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7240 Loss_G: 0.7344 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6511 Loss_G: 0.7187 acc: 96.9%\n",
      "[BATCH 122/149] Loss_D: 0.6905 Loss_G: 0.7305 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.7176 Loss_G: 0.7270 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.7118 Loss_G: 0.7264 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.7503 Loss_G: 0.7491 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6820 Loss_G: 0.7256 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.7458 Loss_G: 0.7412 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7039 Loss_G: 0.7284 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6855 Loss_G: 0.7128 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.7202 Loss_G: 0.7361 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6911 Loss_G: 0.7204 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6996 Loss_G: 0.7372 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6905 Loss_G: 0.7301 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7355 Loss_G: 0.7221 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7001 Loss_G: 0.7159 acc: 93.8%\n",
      "[BATCH 136/149] Loss_D: 0.6871 Loss_G: 0.7057 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.7438 Loss_G: 0.7481 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7010 Loss_G: 0.7367 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6953 Loss_G: 0.7359 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.7082 Loss_G: 0.7281 acc: 85.9%\n",
      "[EPOCH 6100] TEST ACC is : 75.8%\n",
      "[BATCH 141/149] Loss_D: 0.7396 Loss_G: 0.7423 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.7178 Loss_G: 0.7497 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.7210 Loss_G: 0.7500 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.7440 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6960 Loss_G: 0.7402 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6919 Loss_G: 0.7339 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6991 Loss_G: 0.7394 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7513 Loss_G: 0.7731 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7447 Loss_G: 0.7644 acc: 87.5%\n",
      "-----THE [41/50] epoch end-----\n",
      "-----THE [42/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7139 Loss_G: 0.7657 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7073 Loss_G: 0.7446 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7015 Loss_G: 0.7109 acc: 81.2%\n",
      "[BATCH 4/149] Loss_D: 0.7003 Loss_G: 0.7154 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.6942 Loss_G: 0.7209 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.7116 Loss_G: 0.7117 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6831 Loss_G: 0.7087 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6856 Loss_G: 0.7163 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6731 Loss_G: 0.7372 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6643 Loss_G: 0.7102 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6711 Loss_G: 0.7125 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6766 Loss_G: 0.7538 acc: 95.3%\n",
      "[BATCH 13/149] Loss_D: 0.6889 Loss_G: 0.7323 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6892 Loss_G: 0.7273 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.7319 Loss_G: 0.7485 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7041 Loss_G: 0.7373 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6979 Loss_G: 0.7161 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6455 Loss_G: 0.7252 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6616 Loss_G: 0.7163 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6790 Loss_G: 0.7050 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6644 Loss_G: 0.7111 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6900 Loss_G: 0.7153 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7328 Loss_G: 0.7181 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.7079 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.7008 Loss_G: 0.7253 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.7037 Loss_G: 0.7433 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7038 Loss_G: 0.7419 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7113 Loss_G: 0.7600 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6980 Loss_G: 0.7599 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.7417 Loss_G: 0.7381 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7250 Loss_G: 0.7389 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7034 Loss_G: 0.7400 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7085 Loss_G: 0.7362 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.7063 Loss_G: 0.7268 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6989 Loss_G: 0.7336 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6877 Loss_G: 0.7118 acc: 95.3%\n",
      "[BATCH 37/149] Loss_D: 0.7005 Loss_G: 0.7087 acc: 81.2%\n",
      "[BATCH 38/149] Loss_D: 0.6933 Loss_G: 0.7223 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7102 Loss_G: 0.7400 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7341 Loss_G: 0.7513 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6804 Loss_G: 0.7282 acc: 81.2%\n",
      "[EPOCH 6150] TEST ACC is : 76.8%\n",
      "[BATCH 42/149] Loss_D: 0.7063 Loss_G: 0.7257 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6764 Loss_G: 0.7277 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7006 Loss_G: 0.7295 acc: 82.8%\n",
      "[BATCH 45/149] Loss_D: 0.7709 Loss_G: 0.7532 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6951 Loss_G: 0.7330 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6848 Loss_G: 0.7403 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6896 Loss_G: 0.7277 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7004 Loss_G: 0.7345 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.7001 Loss_G: 0.7383 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6863 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7383 Loss_G: 0.7495 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.7042 Loss_G: 0.7176 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.6891 Loss_G: 0.7132 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.7258 Loss_G: 0.7337 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.6690 Loss_G: 0.7205 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7030 Loss_G: 0.7330 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7045 Loss_G: 0.7236 acc: 95.3%\n",
      "[BATCH 59/149] Loss_D: 0.7425 Loss_G: 0.7279 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6982 Loss_G: 0.7239 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7136 Loss_G: 0.7251 acc: 78.1%\n",
      "[BATCH 62/149] Loss_D: 0.7251 Loss_G: 0.7200 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7095 Loss_G: 0.7123 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6857 Loss_G: 0.7230 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7053 Loss_G: 0.7352 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.6805 Loss_G: 0.7137 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6757 Loss_G: 0.6982 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7101 Loss_G: 0.7182 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6742 Loss_G: 0.7173 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.7016 Loss_G: 0.7207 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6976 Loss_G: 0.7614 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6975 Loss_G: 0.7535 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6779 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.6947 Loss_G: 0.7296 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.7192 Loss_G: 0.7527 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.7362 Loss_G: 0.7422 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.7049 Loss_G: 0.7551 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6841 Loss_G: 0.7156 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.6829 Loss_G: 0.7157 acc: 95.3%\n",
      "[BATCH 80/149] Loss_D: 0.7826 Loss_G: 0.7506 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.6968 Loss_G: 0.7558 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6639 Loss_G: 0.7204 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.7085 Loss_G: 0.7326 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.7541 Loss_G: 0.7355 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6889 Loss_G: 0.7132 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6544 Loss_G: 0.7071 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6986 Loss_G: 0.7224 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7020 Loss_G: 0.7397 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6867 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6982 Loss_G: 0.7173 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7192 Loss_G: 0.7396 acc: 93.8%\n",
      "[EPOCH 6200] TEST ACC is : 75.6%\n",
      "[BATCH 92/149] Loss_D: 0.7015 Loss_G: 0.7270 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6902 Loss_G: 0.7197 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.7439 Loss_G: 0.7235 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7512 Loss_G: 0.7260 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6915 Loss_G: 0.7559 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7082 Loss_G: 0.7421 acc: 81.2%\n",
      "[BATCH 98/149] Loss_D: 0.7177 Loss_G: 0.7370 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7104 Loss_G: 0.7299 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.6841 Loss_G: 0.7396 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.6750 Loss_G: 0.7229 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6503 Loss_G: 0.7047 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.7806 Loss_G: 0.7414 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7081 Loss_G: 0.7347 acc: 78.1%\n",
      "[BATCH 105/149] Loss_D: 0.6984 Loss_G: 0.7291 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6999 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6654 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6956 Loss_G: 0.7353 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.7200 Loss_G: 0.7321 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7089 Loss_G: 0.7285 acc: 81.2%\n",
      "[BATCH 111/149] Loss_D: 0.6899 Loss_G: 0.7423 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7445 Loss_G: 0.7354 acc: 96.9%\n",
      "[BATCH 113/149] Loss_D: 0.7056 Loss_G: 0.7300 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7274 Loss_G: 0.7360 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.7002 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6930 Loss_G: 0.7275 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.7061 Loss_G: 0.7492 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6596 Loss_G: 0.7327 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7088 Loss_G: 0.7317 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7083 Loss_G: 0.7259 acc: 93.8%\n",
      "[BATCH 121/149] Loss_D: 0.7240 Loss_G: 0.7511 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6909 Loss_G: 0.7256 acc: 81.2%\n",
      "[BATCH 123/149] Loss_D: 0.6700 Loss_G: 0.7114 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7120 Loss_G: 0.7225 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.6824 Loss_G: 0.7285 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6699 Loss_G: 0.7118 acc: 96.9%\n",
      "[BATCH 127/149] Loss_D: 0.6739 Loss_G: 0.7342 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7073 Loss_G: 0.7020 acc: 79.7%\n",
      "[BATCH 129/149] Loss_D: 0.6925 Loss_G: 0.6860 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7131 Loss_G: 0.7187 acc: 93.8%\n",
      "[BATCH 131/149] Loss_D: 0.6911 Loss_G: 0.7248 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6816 Loss_G: 0.7056 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.7209 Loss_G: 0.7267 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.7070 Loss_G: 0.7283 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7038 Loss_G: 0.7319 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6487 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6722 Loss_G: 0.6997 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.7534 Loss_G: 0.7351 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7060 Loss_G: 0.7453 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6779 Loss_G: 0.7205 acc: 95.3%\n",
      "[BATCH 141/149] Loss_D: 0.6849 Loss_G: 0.7107 acc: 92.2%\n",
      "[EPOCH 6250] TEST ACC is : 76.0%\n",
      "[BATCH 142/149] Loss_D: 0.6747 Loss_G: 0.7009 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.6982 Loss_G: 0.7146 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.7001 Loss_G: 0.7381 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7199 Loss_G: 0.7355 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.7066 Loss_G: 0.7235 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.7121 Loss_G: 0.7409 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.7007 Loss_G: 0.7252 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6896 Loss_G: 0.7105 acc: 93.8%\n",
      "-----THE [42/50] epoch end-----\n",
      "-----THE [43/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6821 Loss_G: 0.7106 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.6938 Loss_G: 0.7212 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.7269 Loss_G: 0.7441 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.7057 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6657 Loss_G: 0.7111 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6824 Loss_G: 0.7142 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.6870 Loss_G: 0.7010 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7222 Loss_G: 0.7096 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.7043 Loss_G: 0.7259 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7158 Loss_G: 0.7344 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7120 Loss_G: 0.7238 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6982 Loss_G: 0.7156 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6666 Loss_G: 0.7219 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6929 Loss_G: 0.7147 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7155 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7367 Loss_G: 0.7275 acc: 79.7%\n",
      "[BATCH 17/149] Loss_D: 0.7025 Loss_G: 0.7279 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7000 Loss_G: 0.7160 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.7082 Loss_G: 0.7190 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7249 Loss_G: 0.7248 acc: 93.8%\n",
      "[BATCH 21/149] Loss_D: 0.7109 Loss_G: 0.7413 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7113 Loss_G: 0.7444 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.7064 Loss_G: 0.7390 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.6934 Loss_G: 0.7218 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6866 Loss_G: 0.7159 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6723 Loss_G: 0.7151 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6911 Loss_G: 0.7146 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.7002 Loss_G: 0.7045 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6611 Loss_G: 0.7075 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.7076 Loss_G: 0.7084 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.7474 Loss_G: 0.7162 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7147 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.7126 Loss_G: 0.7123 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7007 Loss_G: 0.7142 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7089 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7346 Loss_G: 0.7285 acc: 79.7%\n",
      "[BATCH 37/149] Loss_D: 0.7121 Loss_G: 0.7385 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6788 Loss_G: 0.7459 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7319 Loss_G: 0.7477 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7217 Loss_G: 0.7521 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.7147 Loss_G: 0.7472 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.6801 Loss_G: 0.7185 acc: 84.4%\n",
      "[EPOCH 6300] TEST ACC is : 76.2%\n",
      "[BATCH 43/149] Loss_D: 0.6796 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6935 Loss_G: 0.7325 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6979 Loss_G: 0.7339 acc: 95.3%\n",
      "[BATCH 46/149] Loss_D: 0.7118 Loss_G: 0.7442 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6963 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6657 Loss_G: 0.7253 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7312 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7004 Loss_G: 0.7220 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6709 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.7256 Loss_G: 0.7445 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.7024 Loss_G: 0.7440 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.6747 Loss_G: 0.7326 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.7181 Loss_G: 0.7174 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7524 Loss_G: 0.7228 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.6952 Loss_G: 0.7457 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7313 Loss_G: 0.7310 acc: 79.7%\n",
      "[BATCH 59/149] Loss_D: 0.6831 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6642 Loss_G: 0.7174 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6806 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6790 Loss_G: 0.7283 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7010 Loss_G: 0.7422 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6862 Loss_G: 0.7218 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7058 Loss_G: 0.7304 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7028 Loss_G: 0.7187 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7294 Loss_G: 0.7294 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.7107 Loss_G: 0.7282 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.6654 Loss_G: 0.7130 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6819 Loss_G: 0.7264 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6991 Loss_G: 0.7485 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6642 Loss_G: 0.7341 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.7689 Loss_G: 0.7344 acc: 79.7%\n",
      "[BATCH 74/149] Loss_D: 0.6979 Loss_G: 0.7297 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7176 Loss_G: 0.7302 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6933 Loss_G: 0.7284 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6988 Loss_G: 0.7198 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6745 Loss_G: 0.7439 acc: 95.3%\n",
      "[BATCH 79/149] Loss_D: 0.7016 Loss_G: 0.7208 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6737 Loss_G: 0.7376 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.7139 Loss_G: 0.7363 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.6992 Loss_G: 0.7211 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6902 Loss_G: 0.7031 acc: 95.3%\n",
      "[BATCH 84/149] Loss_D: 0.7113 Loss_G: 0.7263 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7043 Loss_G: 0.7345 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6803 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6686 Loss_G: 0.7170 acc: 95.3%\n",
      "[BATCH 88/149] Loss_D: 0.7016 Loss_G: 0.7441 acc: 98.4%\n",
      "[BATCH 89/149] Loss_D: 0.7175 Loss_G: 0.7399 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6953 Loss_G: 0.7265 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.7067 Loss_G: 0.7202 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6826 Loss_G: 0.7310 acc: 93.8%\n",
      "[EPOCH 6350] TEST ACC is : 77.0%\n",
      "[BATCH 93/149] Loss_D: 0.7155 Loss_G: 0.7396 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7087 Loss_G: 0.7767 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6815 Loss_G: 0.7300 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.7140 Loss_G: 0.7308 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7255 Loss_G: 0.7216 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.6651 Loss_G: 0.7157 acc: 93.8%\n",
      "[BATCH 99/149] Loss_D: 0.6933 Loss_G: 0.7388 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.7195 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7108 Loss_G: 0.7246 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7119 Loss_G: 0.7340 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6697 Loss_G: 0.7081 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6483 Loss_G: 0.6949 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6875 Loss_G: 0.7032 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7078 Loss_G: 0.7311 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6823 Loss_G: 0.7179 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6953 Loss_G: 0.7283 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6687 Loss_G: 0.7035 acc: 96.9%\n",
      "[BATCH 110/149] Loss_D: 0.7209 Loss_G: 0.7123 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.6740 Loss_G: 0.6907 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6822 Loss_G: 0.7160 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6899 Loss_G: 0.7187 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6675 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7279 Loss_G: 0.7374 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6983 Loss_G: 0.7459 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.7157 Loss_G: 0.7680 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7385 Loss_G: 0.7495 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6573 Loss_G: 0.7220 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.7154 Loss_G: 0.7269 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.6903 Loss_G: 0.7101 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6874 Loss_G: 0.7822 acc: 95.3%\n",
      "[BATCH 123/149] Loss_D: 0.7078 Loss_G: 0.7134 acc: 81.2%\n",
      "[BATCH 124/149] Loss_D: 0.6780 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7303 Loss_G: 0.7383 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7009 Loss_G: 0.7407 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6943 Loss_G: 0.7428 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.6964 Loss_G: 0.7208 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.6803 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6731 Loss_G: 0.6989 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7004 Loss_G: 0.7166 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6820 Loss_G: 0.7163 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6991 Loss_G: 0.7035 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.6778 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6908 Loss_G: 0.7008 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7063 Loss_G: 0.7340 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.7015 Loss_G: 0.7401 acc: 81.2%\n",
      "[BATCH 138/149] Loss_D: 0.6523 Loss_G: 0.7086 acc: 96.9%\n",
      "[BATCH 139/149] Loss_D: 0.7236 Loss_G: 0.7166 acc: 78.1%\n",
      "[BATCH 140/149] Loss_D: 0.7120 Loss_G: 0.7337 acc: 95.3%\n",
      "[BATCH 141/149] Loss_D: 0.7134 Loss_G: 0.7398 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7150 Loss_G: 0.7476 acc: 84.4%\n",
      "[EPOCH 6400] TEST ACC is : 77.0%\n",
      "[BATCH 143/149] Loss_D: 0.6647 Loss_G: 0.7265 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6967 Loss_G: 0.7309 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6862 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7590 Loss_G: 0.7524 acc: 81.2%\n",
      "[BATCH 147/149] Loss_D: 0.7182 Loss_G: 0.7513 acc: 81.2%\n",
      "[BATCH 148/149] Loss_D: 0.7016 Loss_G: 0.7311 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7116 Loss_G: 0.7375 acc: 89.1%\n",
      "-----THE [43/50] epoch end-----\n",
      "-----THE [44/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6804 Loss_G: 0.7588 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6826 Loss_G: 0.7438 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7137 Loss_G: 0.7278 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.7169 Loss_G: 0.7105 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6963 Loss_G: 0.7177 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7048 Loss_G: 0.7430 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.7054 Loss_G: 0.7170 acc: 79.7%\n",
      "[BATCH 8/149] Loss_D: 0.7124 Loss_G: 0.7250 acc: 96.9%\n",
      "[BATCH 9/149] Loss_D: 0.7080 Loss_G: 0.7379 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.7005 Loss_G: 0.7352 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.7167 Loss_G: 0.7215 acc: 79.7%\n",
      "[BATCH 12/149] Loss_D: 0.7346 Loss_G: 0.7525 acc: 95.3%\n",
      "[BATCH 13/149] Loss_D: 0.6914 Loss_G: 0.7253 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6827 Loss_G: 0.7362 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7114 Loss_G: 0.7289 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.6989 Loss_G: 0.7332 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6737 Loss_G: 0.7222 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6966 Loss_G: 0.7176 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7256 Loss_G: 0.7221 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7044 Loss_G: 0.7222 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.7248 Loss_G: 0.7639 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.7047 Loss_G: 0.7548 acc: 95.3%\n",
      "[BATCH 23/149] Loss_D: 0.7049 Loss_G: 0.7254 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7026 Loss_G: 0.7230 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6950 Loss_G: 0.7024 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.6997 Loss_G: 0.7123 acc: 82.8%\n",
      "[BATCH 27/149] Loss_D: 0.7086 Loss_G: 0.7442 acc: 95.3%\n",
      "[BATCH 28/149] Loss_D: 0.6914 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.7564 Loss_G: 0.7468 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6884 Loss_G: 0.7475 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6899 Loss_G: 0.7159 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6858 Loss_G: 0.7154 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.7480 Loss_G: 0.7262 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6932 Loss_G: 0.7344 acc: 78.1%\n",
      "[BATCH 35/149] Loss_D: 0.6896 Loss_G: 0.7173 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6769 Loss_G: 0.7237 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6873 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7195 Loss_G: 0.7471 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.7148 Loss_G: 0.7347 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6865 Loss_G: 0.7452 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6715 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.6850 Loss_G: 0.7055 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6771 Loss_G: 0.7137 acc: 90.6%\n",
      "[EPOCH 6450] TEST ACC is : 74.8%\n",
      "[BATCH 44/149] Loss_D: 0.7179 Loss_G: 0.7276 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.7182 Loss_G: 0.7415 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7232 Loss_G: 0.7308 acc: 81.2%\n",
      "[BATCH 47/149] Loss_D: 0.6905 Loss_G: 0.7284 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.7165 Loss_G: 0.7297 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6934 Loss_G: 0.7285 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7228 Loss_G: 0.7416 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6774 Loss_G: 0.7295 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.6900 Loss_G: 0.7153 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7287 Loss_G: 0.7434 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7136 Loss_G: 0.7237 acc: 79.7%\n",
      "[BATCH 55/149] Loss_D: 0.6958 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.7087 Loss_G: 0.7333 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6605 Loss_G: 0.7214 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.7249 Loss_G: 0.7215 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7226 Loss_G: 0.7392 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6694 Loss_G: 0.7285 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6907 Loss_G: 0.7364 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.7274 Loss_G: 0.7367 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6877 Loss_G: 0.7241 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6754 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6792 Loss_G: 0.7127 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6977 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6778 Loss_G: 0.7174 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6898 Loss_G: 0.7136 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6841 Loss_G: 0.7090 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6810 Loss_G: 0.7160 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.7345 Loss_G: 0.7414 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6943 Loss_G: 0.7385 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6745 Loss_G: 0.7413 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.7517 Loss_G: 0.7631 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6757 Loss_G: 0.7371 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6974 Loss_G: 0.7145 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7508 Loss_G: 0.7250 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.7003 Loss_G: 0.7348 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.7329 Loss_G: 0.7485 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6937 Loss_G: 0.7442 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.7197 Loss_G: 0.7291 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.7219 Loss_G: 0.7330 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.7066 Loss_G: 0.7195 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.7552 Loss_G: 0.7385 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.7390 Loss_G: 0.7502 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7026 Loss_G: 0.7263 acc: 95.3%\n",
      "[BATCH 87/149] Loss_D: 0.7049 Loss_G: 0.7198 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6779 Loss_G: 0.7196 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6770 Loss_G: 0.7210 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.7079 Loss_G: 0.7132 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7065 Loss_G: 0.7144 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.7124 Loss_G: 0.7332 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.6712 Loss_G: 0.7127 acc: 93.8%\n",
      "[EPOCH 6500] TEST ACC is : 76.6%\n",
      "[BATCH 94/149] Loss_D: 0.6920 Loss_G: 0.7058 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6932 Loss_G: 0.7207 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.6958 Loss_G: 0.6958 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6670 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6908 Loss_G: 0.7247 acc: 95.3%\n",
      "[BATCH 99/149] Loss_D: 0.6759 Loss_G: 0.7305 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6840 Loss_G: 0.7307 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6926 Loss_G: 0.7355 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7053 Loss_G: 0.7368 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.6730 Loss_G: 0.7400 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.7100 Loss_G: 0.7489 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7165 Loss_G: 0.7471 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.6999 Loss_G: 0.7360 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6953 Loss_G: 0.7353 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6780 Loss_G: 0.7088 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7271 Loss_G: 0.7247 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6749 Loss_G: 0.7214 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.7735 Loss_G: 0.7302 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.6842 Loss_G: 0.7084 acc: 79.7%\n",
      "[BATCH 113/149] Loss_D: 0.6967 Loss_G: 0.7245 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.7077 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.7112 Loss_G: 0.7272 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6955 Loss_G: 0.7553 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.6969 Loss_G: 0.7430 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6678 Loss_G: 0.7119 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6904 Loss_G: 0.7189 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6942 Loss_G: 0.7271 acc: 96.9%\n",
      "[BATCH 121/149] Loss_D: 0.7052 Loss_G: 0.7263 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6746 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6860 Loss_G: 0.7341 acc: 96.9%\n",
      "[BATCH 124/149] Loss_D: 0.6654 Loss_G: 0.7257 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7014 Loss_G: 0.7078 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7346 Loss_G: 0.7347 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.6746 Loss_G: 0.7162 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.7238 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.6857 Loss_G: 0.7401 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7409 Loss_G: 0.7318 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7004 Loss_G: 0.7223 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6802 Loss_G: 0.7204 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.7028 Loss_G: 0.7117 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.6993 Loss_G: 0.7303 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.7263 Loss_G: 0.7184 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6673 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6783 Loss_G: 0.6975 acc: 96.9%\n",
      "[BATCH 138/149] Loss_D: 0.7202 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7178 Loss_G: 0.7171 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.6698 Loss_G: 0.7154 acc: 78.1%\n",
      "[BATCH 141/149] Loss_D: 0.7118 Loss_G: 0.6977 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.7032 Loss_G: 0.7103 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6933 Loss_G: 0.7232 acc: 92.2%\n",
      "[EPOCH 6550] TEST ACC is : 77.0%\n",
      "[BATCH 144/149] Loss_D: 0.6522 Loss_G: 0.7121 acc: 95.3%\n",
      "[BATCH 145/149] Loss_D: 0.7137 Loss_G: 0.7128 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.6764 Loss_G: 0.7120 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.7044 Loss_G: 0.7325 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6776 Loss_G: 0.7591 acc: 96.9%\n",
      "[BATCH 149/149] Loss_D: 0.6983 Loss_G: 0.7121 acc: 85.9%\n",
      "-----THE [44/50] epoch end-----\n",
      "-----THE [45/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7230 Loss_G: 0.7242 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6877 Loss_G: 0.7145 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6816 Loss_G: 0.7058 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6534 Loss_G: 0.7073 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6824 Loss_G: 0.7242 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6840 Loss_G: 0.7234 acc: 95.3%\n",
      "[BATCH 7/149] Loss_D: 0.6903 Loss_G: 0.7208 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6838 Loss_G: 0.7147 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.7100 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7027 Loss_G: 0.7198 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7371 Loss_G: 0.7349 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.7050 Loss_G: 0.7119 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.7452 Loss_G: 0.7187 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.7175 Loss_G: 0.7158 acc: 75.0%\n",
      "[BATCH 15/149] Loss_D: 0.6800 Loss_G: 0.7155 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.7283 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7434 Loss_G: 0.7304 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7059 Loss_G: 0.7556 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6561 Loss_G: 0.7436 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.6897 Loss_G: 0.7252 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6940 Loss_G: 0.7279 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.7200 Loss_G: 0.7418 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6888 Loss_G: 0.7512 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6867 Loss_G: 0.7444 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6836 Loss_G: 0.7453 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.7278 Loss_G: 0.7736 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7194 Loss_G: 0.7469 acc: 95.3%\n",
      "[BATCH 28/149] Loss_D: 0.7001 Loss_G: 0.7317 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7355 Loss_G: 0.7563 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7292 Loss_G: 0.7923 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7603 Loss_G: 0.7556 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6563 Loss_G: 0.7319 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6716 Loss_G: 0.7171 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6713 Loss_G: 0.7110 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6846 Loss_G: 0.7170 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6819 Loss_G: 0.7103 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6928 Loss_G: 0.7174 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6894 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7424 Loss_G: 0.7289 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6836 Loss_G: 0.7155 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7175 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6892 Loss_G: 0.7328 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7365 Loss_G: 0.7226 acc: 96.9%\n",
      "[BATCH 44/149] Loss_D: 0.6742 Loss_G: 0.7118 acc: 90.6%\n",
      "[EPOCH 6600] TEST ACC is : 76.8%\n",
      "[BATCH 45/149] Loss_D: 0.7406 Loss_G: 0.7477 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6968 Loss_G: 0.7299 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7304 Loss_G: 0.7311 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6776 Loss_G: 0.7232 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6962 Loss_G: 0.7257 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6910 Loss_G: 0.7484 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6942 Loss_G: 0.7299 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7107 Loss_G: 0.7279 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7069 Loss_G: 0.7250 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6676 Loss_G: 0.7374 acc: 95.3%\n",
      "[BATCH 55/149] Loss_D: 0.7590 Loss_G: 0.7507 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.7157 Loss_G: 0.7489 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6926 Loss_G: 0.7280 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6791 Loss_G: 0.7267 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6740 Loss_G: 0.7253 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7076 Loss_G: 0.7204 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.6931 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7255 Loss_G: 0.7132 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6995 Loss_G: 0.7295 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.7227 Loss_G: 0.7427 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7478 Loss_G: 0.7356 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6598 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7177 Loss_G: 0.7052 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.7004 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6973 Loss_G: 0.7611 acc: 96.9%\n",
      "[BATCH 70/149] Loss_D: 0.7090 Loss_G: 0.7308 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.7186 Loss_G: 0.7408 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6862 Loss_G: 0.7104 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.7120 Loss_G: 0.7162 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7071 Loss_G: 0.7288 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.7177 Loss_G: 0.7198 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.6880 Loss_G: 0.7158 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6867 Loss_G: 0.7144 acc: 81.2%\n",
      "[BATCH 78/149] Loss_D: 0.6871 Loss_G: 0.7123 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.7251 Loss_G: 0.7308 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6817 Loss_G: 0.7318 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.7105 Loss_G: 0.7348 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7110 Loss_G: 0.7287 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6690 Loss_G: 0.7133 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6660 Loss_G: 0.7012 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.7040 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7197 Loss_G: 0.7374 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7175 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6948 Loss_G: 0.7328 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6817 Loss_G: 0.7162 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.6660 Loss_G: 0.7276 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6973 Loss_G: 0.7241 acc: 98.4%\n",
      "[BATCH 92/149] Loss_D: 0.7054 Loss_G: 0.7433 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6737 Loss_G: 0.7117 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6913 Loss_G: 0.7395 acc: 89.1%\n",
      "[EPOCH 6650] TEST ACC is : 76.0%\n",
      "[BATCH 95/149] Loss_D: 0.6912 Loss_G: 0.7087 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.6717 Loss_G: 0.7004 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.7101 Loss_G: 0.7193 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7032 Loss_G: 0.7319 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7072 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.7009 Loss_G: 0.7065 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7118 Loss_G: 0.7235 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6843 Loss_G: 0.7122 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6668 Loss_G: 0.7082 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6927 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6580 Loss_G: 0.7344 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.7186 Loss_G: 0.7245 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6825 Loss_G: 0.7004 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6692 Loss_G: 0.7033 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7336 Loss_G: 0.7229 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7043 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7260 Loss_G: 0.7195 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.7083 Loss_G: 0.7181 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6717 Loss_G: 0.7084 acc: 95.3%\n",
      "[BATCH 114/149] Loss_D: 0.7126 Loss_G: 0.7292 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.7186 Loss_G: 0.7500 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7133 Loss_G: 0.7272 acc: 81.2%\n",
      "[BATCH 117/149] Loss_D: 0.6584 Loss_G: 0.7327 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6684 Loss_G: 0.6982 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.7294 Loss_G: 0.7055 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.6831 Loss_G: 0.7026 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.7166 Loss_G: 0.7361 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6916 Loss_G: 0.7242 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.7009 Loss_G: 0.7312 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.7152 Loss_G: 0.7315 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.7302 Loss_G: 0.7375 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.7391 Loss_G: 0.7483 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.6537 Loss_G: 0.7282 acc: 95.3%\n",
      "[BATCH 128/149] Loss_D: 0.6918 Loss_G: 0.7008 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7556 Loss_G: 0.7287 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.6999 Loss_G: 0.7370 acc: 82.8%\n",
      "[BATCH 131/149] Loss_D: 0.6703 Loss_G: 0.7235 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.7170 Loss_G: 0.7336 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6744 Loss_G: 0.7252 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7182 Loss_G: 0.7268 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7048 Loss_G: 0.7196 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6843 Loss_G: 0.7306 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6977 Loss_G: 0.7354 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6718 Loss_G: 0.7013 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6615 Loss_G: 0.6934 acc: 95.3%\n",
      "[BATCH 140/149] Loss_D: 0.6725 Loss_G: 0.7024 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7280 Loss_G: 0.7558 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6699 Loss_G: 0.7530 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6638 Loss_G: 0.7218 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6862 Loss_G: 0.7231 acc: 89.1%\n",
      "[EPOCH 6700] TEST ACC is : 76.4%\n",
      "[BATCH 145/149] Loss_D: 0.6942 Loss_G: 0.6954 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7274 Loss_G: 0.7228 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.6767 Loss_G: 0.7025 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6821 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.7024 Loss_G: 0.7243 acc: 93.8%\n",
      "-----THE [45/50] epoch end-----\n",
      "-----THE [46/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6896 Loss_G: 0.7430 acc: 95.3%\n",
      "[BATCH 2/149] Loss_D: 0.7049 Loss_G: 0.6971 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.6822 Loss_G: 0.7051 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6686 Loss_G: 0.6989 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.7245 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6854 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6563 Loss_G: 0.7061 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6877 Loss_G: 0.7115 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.7105 Loss_G: 0.7070 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7007 Loss_G: 0.7494 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6915 Loss_G: 0.7418 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6964 Loss_G: 0.7182 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.7145 Loss_G: 0.7297 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6885 Loss_G: 0.7098 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6760 Loss_G: 0.7047 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.7113 Loss_G: 0.7184 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.6981 Loss_G: 0.7268 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7019 Loss_G: 0.7359 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7396 Loss_G: 0.7453 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7113 Loss_G: 0.7599 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.7228 Loss_G: 0.7533 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7093 Loss_G: 0.7356 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7018 Loss_G: 0.7522 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7082 Loss_G: 0.7324 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6924 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6689 Loss_G: 0.6985 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6912 Loss_G: 0.7206 acc: 95.3%\n",
      "[BATCH 28/149] Loss_D: 0.6871 Loss_G: 0.7219 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6635 Loss_G: 0.7181 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6862 Loss_G: 0.7171 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6790 Loss_G: 0.7205 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.7360 Loss_G: 0.7387 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6596 Loss_G: 0.7136 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.6806 Loss_G: 0.7069 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7115 Loss_G: 0.6987 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7125 Loss_G: 0.7303 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7362 Loss_G: 0.7236 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6843 Loss_G: 0.7282 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.6738 Loss_G: 0.7209 acc: 96.9%\n",
      "[BATCH 40/149] Loss_D: 0.6949 Loss_G: 0.7198 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7410 Loss_G: 0.7460 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.7072 Loss_G: 0.7526 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6936 Loss_G: 0.7186 acc: 81.2%\n",
      "[BATCH 44/149] Loss_D: 0.6799 Loss_G: 0.7184 acc: 96.9%\n",
      "[BATCH 45/149] Loss_D: 0.7305 Loss_G: 0.7196 acc: 81.2%\n",
      "[EPOCH 6750] TEST ACC is : 76.6%\n",
      "[BATCH 46/149] Loss_D: 0.6972 Loss_G: 0.7052 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6904 Loss_G: 0.6942 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7288 Loss_G: 0.7088 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7289 Loss_G: 0.7420 acc: 96.9%\n",
      "[BATCH 50/149] Loss_D: 0.7122 Loss_G: 0.7361 acc: 82.8%\n",
      "[BATCH 51/149] Loss_D: 0.7223 Loss_G: 0.7285 acc: 81.2%\n",
      "[BATCH 52/149] Loss_D: 0.7006 Loss_G: 0.7241 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7097 Loss_G: 0.7301 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6987 Loss_G: 0.7374 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6929 Loss_G: 0.7195 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6880 Loss_G: 0.7259 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7185 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.7131 Loss_G: 0.7227 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.7022 Loss_G: 0.7412 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7129 Loss_G: 0.7295 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.6575 Loss_G: 0.7117 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6870 Loss_G: 0.7250 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7288 Loss_G: 0.7217 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.6791 Loss_G: 0.7126 acc: 93.8%\n",
      "[BATCH 65/149] Loss_D: 0.7028 Loss_G: 0.7172 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7292 Loss_G: 0.7357 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.7025 Loss_G: 0.7190 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7109 Loss_G: 0.7415 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6625 Loss_G: 0.7479 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7336 Loss_G: 0.7256 acc: 81.2%\n",
      "[BATCH 71/149] Loss_D: 0.6680 Loss_G: 0.7212 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.7034 Loss_G: 0.7181 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6995 Loss_G: 0.7179 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.7124 Loss_G: 0.7431 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6585 Loss_G: 0.7372 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7093 Loss_G: 0.7299 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7069 Loss_G: 0.7328 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7141 Loss_G: 0.7328 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.7051 Loss_G: 0.7149 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.7401 Loss_G: 0.7273 acc: 81.2%\n",
      "[BATCH 81/149] Loss_D: 0.7078 Loss_G: 0.7369 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7284 Loss_G: 0.7327 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6863 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6932 Loss_G: 0.7424 acc: 76.6%\n",
      "[BATCH 85/149] Loss_D: 0.6858 Loss_G: 0.7238 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.7109 Loss_G: 0.7307 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6894 Loss_G: 0.7257 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7513 Loss_G: 0.7182 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6690 Loss_G: 0.7239 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6870 Loss_G: 0.7307 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7183 Loss_G: 0.7298 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.6911 Loss_G: 0.7245 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7136 Loss_G: 0.7330 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.7574 Loss_G: 0.7257 acc: 78.1%\n",
      "[BATCH 95/149] Loss_D: 0.6668 Loss_G: 0.7050 acc: 90.6%\n",
      "[EPOCH 6800] TEST ACC is : 76.6%\n",
      "[BATCH 96/149] Loss_D: 0.7057 Loss_G: 0.7042 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.6779 Loss_G: 0.7226 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.7298 Loss_G: 0.7334 acc: 93.8%\n",
      "[BATCH 99/149] Loss_D: 0.6992 Loss_G: 0.7125 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7134 Loss_G: 0.7153 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6782 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.6907 Loss_G: 0.7338 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.7145 Loss_G: 0.7214 acc: 73.4%\n",
      "[BATCH 104/149] Loss_D: 0.7345 Loss_G: 0.7242 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6635 Loss_G: 0.7215 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7018 Loss_G: 0.7159 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.6619 Loss_G: 0.7142 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.6708 Loss_G: 0.7248 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6785 Loss_G: 0.7090 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7083 Loss_G: 0.7299 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7018 Loss_G: 0.7380 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6717 Loss_G: 0.7224 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6989 Loss_G: 0.7308 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7006 Loss_G: 0.7403 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.7013 Loss_G: 0.7296 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6765 Loss_G: 0.7371 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6640 Loss_G: 0.7316 acc: 96.9%\n",
      "[BATCH 118/149] Loss_D: 0.7382 Loss_G: 0.7159 acc: 81.2%\n",
      "[BATCH 119/149] Loss_D: 0.7024 Loss_G: 0.7478 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6644 Loss_G: 0.7299 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.7371 Loss_G: 0.7279 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6737 Loss_G: 0.7285 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.7132 Loss_G: 0.7185 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.7001 Loss_G: 0.7395 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6939 Loss_G: 0.7218 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.7111 Loss_G: 0.7211 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6570 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7212 Loss_G: 0.7234 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6642 Loss_G: 0.7128 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7011 Loss_G: 0.7199 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6763 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.7245 Loss_G: 0.7252 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7086 Loss_G: 0.7340 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7224 Loss_G: 0.7295 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6793 Loss_G: 0.7252 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6802 Loss_G: 0.7072 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.6965 Loss_G: 0.7256 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6724 Loss_G: 0.7127 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6950 Loss_G: 0.7119 acc: 95.3%\n",
      "[BATCH 140/149] Loss_D: 0.7148 Loss_G: 0.7226 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6665 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6763 Loss_G: 0.7172 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6791 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7044 Loss_G: 0.7233 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7702 Loss_G: 0.7414 acc: 87.5%\n",
      "[EPOCH 6850] TEST ACC is : 76.4%\n",
      "[BATCH 146/149] Loss_D: 0.6915 Loss_G: 0.7264 acc: 95.3%\n",
      "[BATCH 147/149] Loss_D: 0.6782 Loss_G: 0.7401 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6681 Loss_G: 0.7416 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.7305 Loss_G: 0.7313 acc: 87.5%\n",
      "-----THE [46/50] epoch end-----\n",
      "-----THE [47/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6841 Loss_G: 0.7055 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6836 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7052 Loss_G: 0.7066 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.7133 Loss_G: 0.7311 acc: 96.9%\n",
      "[BATCH 5/149] Loss_D: 0.6680 Loss_G: 0.7329 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6776 Loss_G: 0.7156 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6956 Loss_G: 0.7138 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6792 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6978 Loss_G: 0.7064 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.6977 Loss_G: 0.7184 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7162 Loss_G: 0.7391 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6634 Loss_G: 0.7170 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.7255 Loss_G: 0.7181 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6687 Loss_G: 0.7041 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.6720 Loss_G: 0.7108 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.7386 Loss_G: 0.7500 acc: 93.8%\n",
      "[BATCH 17/149] Loss_D: 0.7161 Loss_G: 0.7480 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6780 Loss_G: 0.7199 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7280 Loss_G: 0.7253 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.7360 Loss_G: 0.7741 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6914 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6818 Loss_G: 0.7276 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.7044 Loss_G: 0.7265 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7051 Loss_G: 0.7235 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.7206 Loss_G: 0.7201 acc: 81.2%\n",
      "[BATCH 26/149] Loss_D: 0.7028 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.7259 Loss_G: 0.7285 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7287 Loss_G: 0.7332 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6957 Loss_G: 0.7170 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6904 Loss_G: 0.7124 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6616 Loss_G: 0.6956 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.7444 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7099 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7258 Loss_G: 0.7198 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6964 Loss_G: 0.7120 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6489 Loss_G: 0.7111 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6991 Loss_G: 0.7275 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.7104 Loss_G: 0.7369 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.6770 Loss_G: 0.7271 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6784 Loss_G: 0.7211 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6761 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6955 Loss_G: 0.7060 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6837 Loss_G: 0.7306 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.6970 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6570 Loss_G: 0.7156 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6952 Loss_G: 0.7103 acc: 85.9%\n",
      "[EPOCH 6900] TEST ACC is : 77.0%\n",
      "[BATCH 47/149] Loss_D: 0.7459 Loss_G: 0.7382 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6755 Loss_G: 0.7363 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.6712 Loss_G: 0.7199 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.6716 Loss_G: 0.7024 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6733 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6972 Loss_G: 0.7127 acc: 81.2%\n",
      "[BATCH 53/149] Loss_D: 0.6585 Loss_G: 0.7112 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7069 Loss_G: 0.7310 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.6886 Loss_G: 0.7213 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6792 Loss_G: 0.7148 acc: 81.2%\n",
      "[BATCH 57/149] Loss_D: 0.7177 Loss_G: 0.7333 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7035 Loss_G: 0.7283 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6867 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6964 Loss_G: 0.7173 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.6667 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6912 Loss_G: 0.7283 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.6907 Loss_G: 0.7113 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6921 Loss_G: 0.7151 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6896 Loss_G: 0.7078 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7152 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6893 Loss_G: 0.7316 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.7101 Loss_G: 0.7393 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6742 Loss_G: 0.7253 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6795 Loss_G: 0.7161 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6655 Loss_G: 0.7004 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6547 Loss_G: 0.6903 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7361 Loss_G: 0.7131 acc: 76.6%\n",
      "[BATCH 74/149] Loss_D: 0.6886 Loss_G: 0.7280 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6593 Loss_G: 0.7194 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6936 Loss_G: 0.7225 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6808 Loss_G: 0.7299 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7076 Loss_G: 0.7195 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.7165 Loss_G: 0.7304 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7174 Loss_G: 0.7358 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.7414 Loss_G: 0.7630 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7209 Loss_G: 0.7534 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6696 Loss_G: 0.7361 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.7128 Loss_G: 0.7296 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.6731 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7058 Loss_G: 0.7287 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7340 Loss_G: 0.7342 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.6875 Loss_G: 0.7220 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.7048 Loss_G: 0.7604 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7084 Loss_G: 0.7298 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6808 Loss_G: 0.7034 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.6770 Loss_G: 0.7142 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.7205 Loss_G: 0.7219 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7330 Loss_G: 0.7249 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6628 Loss_G: 0.7102 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7073 Loss_G: 0.7174 acc: 92.2%\n",
      "[EPOCH 6950] TEST ACC is : 77.0%\n",
      "[BATCH 97/149] Loss_D: 0.7030 Loss_G: 0.7465 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.6657 Loss_G: 0.7358 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6947 Loss_G: 0.7234 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6906 Loss_G: 0.7217 acc: 96.9%\n",
      "[BATCH 101/149] Loss_D: 0.7127 Loss_G: 0.7320 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7273 Loss_G: 0.7231 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.6920 Loss_G: 0.7536 acc: 96.9%\n",
      "[BATCH 104/149] Loss_D: 0.7008 Loss_G: 0.7203 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7500 Loss_G: 0.7233 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6872 Loss_G: 0.7099 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7555 Loss_G: 0.7303 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.7200 Loss_G: 0.7350 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6756 Loss_G: 0.7141 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7217 Loss_G: 0.7223 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.7174 Loss_G: 0.7316 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7046 Loss_G: 0.7208 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.7170 Loss_G: 0.7329 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7005 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6746 Loss_G: 0.7151 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7300 Loss_G: 0.7343 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6866 Loss_G: 0.7473 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6826 Loss_G: 0.7330 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7213 Loss_G: 0.7391 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7310 Loss_G: 0.7216 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.7169 Loss_G: 0.7255 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.7607 Loss_G: 0.7513 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.6989 Loss_G: 0.7574 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7223 Loss_G: 0.7449 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6757 Loss_G: 0.7454 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6913 Loss_G: 0.7317 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6904 Loss_G: 0.7185 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7013 Loss_G: 0.7215 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6868 Loss_G: 0.6980 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.7039 Loss_G: 0.7084 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.7034 Loss_G: 0.7266 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6942 Loss_G: 0.7167 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6656 Loss_G: 0.7307 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6948 Loss_G: 0.7115 acc: 81.2%\n",
      "[BATCH 135/149] Loss_D: 0.7017 Loss_G: 0.7320 acc: 92.2%\n",
      "[BATCH 136/149] Loss_D: 0.7058 Loss_G: 0.7151 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.6753 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7046 Loss_G: 0.7133 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.7215 Loss_G: 0.7332 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.7089 Loss_G: 0.7342 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.7113 Loss_G: 0.7653 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.7107 Loss_G: 0.7394 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.7131 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.7232 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7040 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7002 Loss_G: 0.7333 acc: 89.1%\n",
      "[EPOCH 7000] TEST ACC is : 76.8%\n",
      "[BATCH 147/149] Loss_D: 0.6698 Loss_G: 0.7212 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.6707 Loss_G: 0.7013 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7304 Loss_G: 0.7090 acc: 81.2%\n",
      "-----THE [47/50] epoch end-----\n",
      "-----THE [48/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6903 Loss_G: 0.7263 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6861 Loss_G: 0.7263 acc: 95.3%\n",
      "[BATCH 3/149] Loss_D: 0.6920 Loss_G: 0.7126 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6897 Loss_G: 0.7252 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6855 Loss_G: 0.7010 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6684 Loss_G: 0.7032 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6948 Loss_G: 0.7091 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6827 Loss_G: 0.7096 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.6699 Loss_G: 0.6944 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6984 Loss_G: 0.6969 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7183 Loss_G: 0.7128 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.7788 Loss_G: 0.7431 acc: 78.1%\n",
      "[BATCH 13/149] Loss_D: 0.6600 Loss_G: 0.7277 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6943 Loss_G: 0.7260 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.7251 Loss_G: 0.7596 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6654 Loss_G: 0.7273 acc: 93.8%\n",
      "[BATCH 17/149] Loss_D: 0.6767 Loss_G: 0.7369 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6849 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6901 Loss_G: 0.7511 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.6805 Loss_G: 0.7400 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6811 Loss_G: 0.7359 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6845 Loss_G: 0.7455 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6694 Loss_G: 0.7048 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7032 Loss_G: 0.7025 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7372 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6877 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.6536 Loss_G: 0.7044 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6771 Loss_G: 0.6929 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6683 Loss_G: 0.6992 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7294 Loss_G: 0.7143 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7018 Loss_G: 0.7317 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6651 Loss_G: 0.7116 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6779 Loss_G: 0.7052 acc: 75.0%\n",
      "[BATCH 34/149] Loss_D: 0.6775 Loss_G: 0.7143 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7528 Loss_G: 0.7344 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6930 Loss_G: 0.7192 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7270 Loss_G: 0.7258 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.7111 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7035 Loss_G: 0.7518 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.7375 Loss_G: 0.7776 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6764 Loss_G: 0.7613 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7066 Loss_G: 0.7133 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7272 Loss_G: 0.7211 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7157 Loss_G: 0.7330 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.7075 Loss_G: 0.7333 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7027 Loss_G: 0.7099 acc: 82.8%\n",
      "[BATCH 47/149] Loss_D: 0.6919 Loss_G: 0.7098 acc: 85.9%\n",
      "[EPOCH 7050] TEST ACC is : 75.8%\n",
      "[BATCH 48/149] Loss_D: 0.6945 Loss_G: 0.7382 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6824 Loss_G: 0.7518 acc: 98.4%\n",
      "[BATCH 50/149] Loss_D: 0.7046 Loss_G: 0.7279 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.6816 Loss_G: 0.7214 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.7169 Loss_G: 0.7272 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6814 Loss_G: 0.7234 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6669 Loss_G: 0.7130 acc: 95.3%\n",
      "[BATCH 55/149] Loss_D: 0.7470 Loss_G: 0.7222 acc: 73.4%\n",
      "[BATCH 56/149] Loss_D: 0.6928 Loss_G: 0.7327 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7531 Loss_G: 0.7686 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.6843 Loss_G: 0.7456 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7125 Loss_G: 0.7235 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.7325 Loss_G: 0.7437 acc: 81.2%\n",
      "[BATCH 61/149] Loss_D: 0.6782 Loss_G: 0.7192 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6641 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7334 Loss_G: 0.7243 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.7103 Loss_G: 0.7255 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6990 Loss_G: 0.7161 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6932 Loss_G: 0.7128 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6941 Loss_G: 0.7297 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7027 Loss_G: 0.7087 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.7272 Loss_G: 0.7055 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7169 Loss_G: 0.7139 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6662 Loss_G: 0.7384 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6795 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6631 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6625 Loss_G: 0.7115 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7227 Loss_G: 0.7162 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6879 Loss_G: 0.7190 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6786 Loss_G: 0.6956 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7311 Loss_G: 0.7090 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.6979 Loss_G: 0.7553 acc: 93.8%\n",
      "[BATCH 80/149] Loss_D: 0.6904 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.7431 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6698 Loss_G: 0.7162 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6917 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.6880 Loss_G: 0.7281 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6811 Loss_G: 0.7099 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6786 Loss_G: 0.6838 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6838 Loss_G: 0.7216 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6904 Loss_G: 0.7199 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.6689 Loss_G: 0.7126 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7233 Loss_G: 0.7030 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7125 Loss_G: 0.7112 acc: 81.2%\n",
      "[BATCH 92/149] Loss_D: 0.6791 Loss_G: 0.7185 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.6758 Loss_G: 0.7222 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7056 Loss_G: 0.7369 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.7134 Loss_G: 0.7236 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7038 Loss_G: 0.7312 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.7341 Loss_G: 0.7357 acc: 84.4%\n",
      "[EPOCH 7100] TEST ACC is : 76.0%\n",
      "[BATCH 98/149] Loss_D: 0.7568 Loss_G: 0.7570 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7128 Loss_G: 0.7476 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7419 Loss_G: 0.7371 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7390 Loss_G: 0.7338 acc: 95.3%\n",
      "[BATCH 102/149] Loss_D: 0.6974 Loss_G: 0.7226 acc: 84.4%\n",
      "[BATCH 103/149] Loss_D: 0.7575 Loss_G: 0.7352 acc: 78.1%\n",
      "[BATCH 104/149] Loss_D: 0.7034 Loss_G: 0.7470 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7102 Loss_G: 0.7261 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6620 Loss_G: 0.7269 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6370 Loss_G: 0.7427 acc: 95.3%\n",
      "[BATCH 108/149] Loss_D: 0.6823 Loss_G: 0.7224 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6832 Loss_G: 0.7271 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.7033 Loss_G: 0.7426 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7451 Loss_G: 0.7359 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7105 Loss_G: 0.7301 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6991 Loss_G: 0.7620 acc: 95.3%\n",
      "[BATCH 114/149] Loss_D: 0.6883 Loss_G: 0.7265 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6866 Loss_G: 0.7222 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7058 Loss_G: 0.7377 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7189 Loss_G: 0.7438 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7273 Loss_G: 0.7213 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.6754 Loss_G: 0.7037 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.7001 Loss_G: 0.6995 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6659 Loss_G: 0.7032 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.7033 Loss_G: 0.7132 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6999 Loss_G: 0.7295 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6753 Loss_G: 0.7019 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.7157 Loss_G: 0.6997 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7031 Loss_G: 0.7224 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.6856 Loss_G: 0.7122 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.7073 Loss_G: 0.7237 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.7205 Loss_G: 0.7486 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.7004 Loss_G: 0.7327 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6989 Loss_G: 0.7248 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.7179 Loss_G: 0.7452 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.7256 Loss_G: 0.7406 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7235 Loss_G: 0.7426 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6940 Loss_G: 0.7241 acc: 78.1%\n",
      "[BATCH 136/149] Loss_D: 0.6808 Loss_G: 0.7128 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6999 Loss_G: 0.7090 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6726 Loss_G: 0.7127 acc: 82.8%\n",
      "[BATCH 139/149] Loss_D: 0.7091 Loss_G: 0.7050 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7109 Loss_G: 0.7160 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6579 Loss_G: 0.7143 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.7181 Loss_G: 0.7214 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.6881 Loss_G: 0.7335 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6941 Loss_G: 0.7349 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7015 Loss_G: 0.7193 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7231 Loss_G: 0.7496 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6648 Loss_G: 0.7178 acc: 92.2%\n",
      "[EPOCH 7150] TEST ACC is : 75.2%\n",
      "[BATCH 148/149] Loss_D: 0.7150 Loss_G: 0.7518 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.6999 Loss_G: 0.7466 acc: 92.2%\n",
      "-----THE [48/50] epoch end-----\n",
      "-----THE [49/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7213 Loss_G: 0.7617 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7362 Loss_G: 0.7628 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.6571 Loss_G: 0.7447 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.7145 Loss_G: 0.7343 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6204 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6815 Loss_G: 0.7171 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6947 Loss_G: 0.7351 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.7200 Loss_G: 0.7369 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.7382 Loss_G: 0.7381 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6949 Loss_G: 0.7596 acc: 95.3%\n",
      "[BATCH 11/149] Loss_D: 0.6965 Loss_G: 0.7289 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.6796 Loss_G: 0.7177 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6964 Loss_G: 0.7206 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.7111 Loss_G: 0.7399 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6850 Loss_G: 0.7419 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6702 Loss_G: 0.6964 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.7069 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6829 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6552 Loss_G: 0.7053 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6836 Loss_G: 0.7048 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.7589 Loss_G: 0.7406 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7138 Loss_G: 0.7273 acc: 79.7%\n",
      "[BATCH 23/149] Loss_D: 0.7450 Loss_G: 0.7330 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6858 Loss_G: 0.7128 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.7020 Loss_G: 0.7171 acc: 95.3%\n",
      "[BATCH 26/149] Loss_D: 0.7058 Loss_G: 0.7165 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7144 Loss_G: 0.7348 acc: 95.3%\n",
      "[BATCH 28/149] Loss_D: 0.7074 Loss_G: 0.7527 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6642 Loss_G: 0.7087 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6839 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7097 Loss_G: 0.7299 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.7003 Loss_G: 0.7224 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6998 Loss_G: 0.7266 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6847 Loss_G: 0.7306 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7000 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6746 Loss_G: 0.7436 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6964 Loss_G: 0.7220 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6974 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7198 Loss_G: 0.7349 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6858 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6863 Loss_G: 0.7144 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.7161 Loss_G: 0.7217 acc: 95.3%\n",
      "[BATCH 43/149] Loss_D: 0.7004 Loss_G: 0.7285 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6763 Loss_G: 0.7008 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.6836 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6578 Loss_G: 0.7045 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6862 Loss_G: 0.7165 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6969 Loss_G: 0.7023 acc: 79.7%\n",
      "[EPOCH 7200] TEST ACC is : 75.0%\n",
      "[BATCH 49/149] Loss_D: 0.7484 Loss_G: 0.7260 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6584 Loss_G: 0.7221 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7136 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6928 Loss_G: 0.7126 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.7355 Loss_G: 0.7409 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.7097 Loss_G: 0.7317 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.7287 Loss_G: 0.7191 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6704 Loss_G: 0.6948 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.7595 Loss_G: 0.7082 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7341 Loss_G: 0.7241 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6536 Loss_G: 0.7393 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.7093 Loss_G: 0.7435 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7303 Loss_G: 0.7626 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6860 Loss_G: 0.7201 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.7040 Loss_G: 0.7290 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6925 Loss_G: 0.7223 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6624 Loss_G: 0.7054 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.7078 Loss_G: 0.7160 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7043 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6740 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7348 Loss_G: 0.7325 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.7216 Loss_G: 0.7301 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6678 Loss_G: 0.7226 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.7013 Loss_G: 0.7071 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7066 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7739 Loss_G: 0.7226 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.7086 Loss_G: 0.7283 acc: 79.7%\n",
      "[BATCH 76/149] Loss_D: 0.6856 Loss_G: 0.7218 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.6912 Loss_G: 0.6870 acc: 75.0%\n",
      "[BATCH 78/149] Loss_D: 0.6884 Loss_G: 0.7007 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6543 Loss_G: 0.7190 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.6829 Loss_G: 0.7134 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6779 Loss_G: 0.7304 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7175 Loss_G: 0.7403 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.7256 Loss_G: 0.7452 acc: 96.9%\n",
      "[BATCH 84/149] Loss_D: 0.6835 Loss_G: 0.7319 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6850 Loss_G: 0.7410 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6866 Loss_G: 0.7316 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7081 Loss_G: 0.7205 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.7079 Loss_G: 0.7201 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.7419 Loss_G: 0.7347 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6936 Loss_G: 0.7292 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7063 Loss_G: 0.7747 acc: 93.8%\n",
      "[BATCH 92/149] Loss_D: 0.7244 Loss_G: 0.7566 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7402 Loss_G: 0.7251 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6762 Loss_G: 0.7349 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7085 Loss_G: 0.7246 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.7245 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.7291 Loss_G: 0.7474 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.6923 Loss_G: 0.7318 acc: 84.4%\n",
      "[EPOCH 7250] TEST ACC is : 76.2%\n",
      "[BATCH 99/149] Loss_D: 0.6567 Loss_G: 0.7063 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.7002 Loss_G: 0.6950 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7223 Loss_G: 0.7277 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6852 Loss_G: 0.7322 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6700 Loss_G: 0.7179 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.6963 Loss_G: 0.7155 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6821 Loss_G: 0.7042 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7013 Loss_G: 0.7128 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6798 Loss_G: 0.7041 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7079 Loss_G: 0.7197 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6980 Loss_G: 0.7198 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.7105 Loss_G: 0.7282 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6984 Loss_G: 0.7199 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7182 Loss_G: 0.7491 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6870 Loss_G: 0.7361 acc: 93.8%\n",
      "[BATCH 114/149] Loss_D: 0.7442 Loss_G: 0.7461 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6498 Loss_G: 0.7075 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7041 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.6809 Loss_G: 0.7111 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7176 Loss_G: 0.7169 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6980 Loss_G: 0.7143 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.6861 Loss_G: 0.7368 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7481 Loss_G: 0.7535 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.6764 Loss_G: 0.7229 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7080 Loss_G: 0.7023 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.7342 Loss_G: 0.7238 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6914 Loss_G: 0.7081 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.6705 Loss_G: 0.7160 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.6788 Loss_G: 0.7247 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6376 Loss_G: 0.6992 acc: 95.3%\n",
      "[BATCH 129/149] Loss_D: 0.7303 Loss_G: 0.7287 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.6461 Loss_G: 0.7013 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6788 Loss_G: 0.7023 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6506 Loss_G: 0.6967 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6975 Loss_G: 0.7112 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7006 Loss_G: 0.7385 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6722 Loss_G: 0.7351 acc: 92.2%\n",
      "[BATCH 136/149] Loss_D: 0.7022 Loss_G: 0.7164 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6972 Loss_G: 0.7266 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6891 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.7029 Loss_G: 0.7394 acc: 95.3%\n",
      "[BATCH 140/149] Loss_D: 0.7131 Loss_G: 0.7596 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6881 Loss_G: 0.7369 acc: 95.3%\n",
      "[BATCH 142/149] Loss_D: 0.7005 Loss_G: 0.7211 acc: 81.2%\n",
      "[BATCH 143/149] Loss_D: 0.6859 Loss_G: 0.7100 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.7029 Loss_G: 0.7350 acc: 95.3%\n",
      "[BATCH 145/149] Loss_D: 0.6871 Loss_G: 0.7208 acc: 81.2%\n",
      "[BATCH 146/149] Loss_D: 0.6965 Loss_G: 0.7202 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.7086 Loss_G: 0.7232 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.7251 Loss_G: 0.7361 acc: 89.1%\n",
      "[EPOCH 7300] TEST ACC is : 77.1%\n",
      "[BATCH 149/149] Loss_D: 0.6771 Loss_G: 0.7406 acc: 92.2%\n",
      "-----THE [49/50] epoch end-----\n",
      "-----THE [50/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6853 Loss_G: 0.7138 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6995 Loss_G: 0.7091 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.6773 Loss_G: 0.7418 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7298 Loss_G: 0.7195 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7013 Loss_G: 0.7152 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.7680 Loss_G: 0.7500 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7249 Loss_G: 0.7328 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6901 Loss_G: 0.7217 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.7256 Loss_G: 0.7400 acc: 79.7%\n",
      "[BATCH 10/149] Loss_D: 0.6964 Loss_G: 0.7305 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6548 Loss_G: 0.7081 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6643 Loss_G: 0.7167 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6992 Loss_G: 0.7112 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6749 Loss_G: 0.7152 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.6909 Loss_G: 0.7198 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6810 Loss_G: 0.6987 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.7248 Loss_G: 0.7205 acc: 81.2%\n",
      "[BATCH 18/149] Loss_D: 0.6499 Loss_G: 0.7027 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6662 Loss_G: 0.7072 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6791 Loss_G: 0.7103 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6778 Loss_G: 0.7137 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.7501 Loss_G: 0.7391 acc: 81.2%\n",
      "[BATCH 23/149] Loss_D: 0.6870 Loss_G: 0.7244 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6931 Loss_G: 0.7394 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7108 Loss_G: 0.7206 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7352 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.6897 Loss_G: 0.7324 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6904 Loss_G: 0.7303 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6909 Loss_G: 0.7214 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7270 Loss_G: 0.7329 acc: 95.3%\n",
      "[BATCH 31/149] Loss_D: 0.6854 Loss_G: 0.7442 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.7518 Loss_G: 0.7320 acc: 79.7%\n",
      "[BATCH 33/149] Loss_D: 0.6830 Loss_G: 0.7191 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.6819 Loss_G: 0.7195 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.7181 Loss_G: 0.7134 acc: 81.2%\n",
      "[BATCH 36/149] Loss_D: 0.6931 Loss_G: 0.7278 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6834 Loss_G: 0.7193 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6868 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6740 Loss_G: 0.7083 acc: 78.1%\n",
      "[BATCH 40/149] Loss_D: 0.6901 Loss_G: 0.7046 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.6696 Loss_G: 0.7011 acc: 95.3%\n",
      "[BATCH 42/149] Loss_D: 0.7286 Loss_G: 0.7181 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.7259 Loss_G: 0.7398 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.7319 Loss_G: 0.7448 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.7189 Loss_G: 0.7276 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.7165 Loss_G: 0.7664 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.6768 Loss_G: 0.7241 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7770 Loss_G: 0.7604 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7335 Loss_G: 0.7457 acc: 87.5%\n",
      "[EPOCH 7350] TEST ACC is : 77.5%\n",
      "[BATCH 50/149] Loss_D: 0.7103 Loss_G: 0.7453 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6562 Loss_G: 0.7200 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6799 Loss_G: 0.7250 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6962 Loss_G: 0.7245 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6653 Loss_G: 0.7105 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6766 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6751 Loss_G: 0.7233 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6965 Loss_G: 0.7119 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6842 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6860 Loss_G: 0.7067 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.7176 Loss_G: 0.7084 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6631 Loss_G: 0.6989 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6750 Loss_G: 0.7101 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7130 Loss_G: 0.7179 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.6873 Loss_G: 0.7311 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.7179 Loss_G: 0.7364 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6942 Loss_G: 0.7296 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6970 Loss_G: 0.7359 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.7083 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6739 Loss_G: 0.7127 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7122 Loss_G: 0.7130 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7074 Loss_G: 0.7207 acc: 82.8%\n",
      "[BATCH 72/149] Loss_D: 0.7195 Loss_G: 0.7442 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6978 Loss_G: 0.7443 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6810 Loss_G: 0.7341 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.7017 Loss_G: 0.7407 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6785 Loss_G: 0.7262 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6964 Loss_G: 0.7427 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6981 Loss_G: 0.7448 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6986 Loss_G: 0.7403 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7152 Loss_G: 0.7406 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7378 Loss_G: 0.7332 acc: 81.2%\n",
      "[BATCH 82/149] Loss_D: 0.7572 Loss_G: 0.7377 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6627 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.7129 Loss_G: 0.7038 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.6843 Loss_G: 0.7267 acc: 95.3%\n",
      "[BATCH 86/149] Loss_D: 0.7077 Loss_G: 0.7264 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7056 Loss_G: 0.7211 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7060 Loss_G: 0.7484 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6758 Loss_G: 0.7009 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6805 Loss_G: 0.7029 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7078 Loss_G: 0.7140 acc: 93.8%\n",
      "[BATCH 92/149] Loss_D: 0.6648 Loss_G: 0.7072 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.6913 Loss_G: 0.6896 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6993 Loss_G: 0.7186 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7259 Loss_G: 0.7283 acc: 81.2%\n",
      "[BATCH 96/149] Loss_D: 0.6436 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7249 Loss_G: 0.7133 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.6914 Loss_G: 0.7288 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6809 Loss_G: 0.7080 acc: 92.2%\n",
      "[EPOCH 7400] TEST ACC is : 76.0%\n",
      "[BATCH 100/149] Loss_D: 0.7005 Loss_G: 0.7082 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6871 Loss_G: 0.7293 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6798 Loss_G: 0.7264 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.7262 Loss_G: 0.7264 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.6958 Loss_G: 0.7324 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7132 Loss_G: 0.7214 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.7131 Loss_G: 0.7339 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7020 Loss_G: 0.7382 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7094 Loss_G: 0.7091 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.6899 Loss_G: 0.7083 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.6602 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6961 Loss_G: 0.6997 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7017 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7086 Loss_G: 0.7239 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.6985 Loss_G: 0.7152 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.7086 Loss_G: 0.7465 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6637 Loss_G: 0.7266 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7091 Loss_G: 0.7211 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6930 Loss_G: 0.7375 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6829 Loss_G: 0.7436 acc: 96.9%\n",
      "[BATCH 120/149] Loss_D: 0.6924 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6559 Loss_G: 0.7131 acc: 100.0%\n",
      "[BATCH 122/149] Loss_D: 0.6844 Loss_G: 0.7225 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.6795 Loss_G: 0.7210 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.7118 Loss_G: 0.7433 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6800 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6845 Loss_G: 0.7149 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7213 Loss_G: 0.7359 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6843 Loss_G: 0.7085 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.6959 Loss_G: 0.7047 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6889 Loss_G: 0.6975 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.7278 Loss_G: 0.6989 acc: 78.1%\n",
      "[BATCH 132/149] Loss_D: 0.7265 Loss_G: 0.7115 acc: 81.2%\n",
      "[BATCH 133/149] Loss_D: 0.6807 Loss_G: 0.7111 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.7137 Loss_G: 0.7145 acc: 95.3%\n",
      "[BATCH 135/149] Loss_D: 0.6766 Loss_G: 0.7102 acc: 79.7%\n",
      "[BATCH 136/149] Loss_D: 0.7027 Loss_G: 0.7211 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6817 Loss_G: 0.7234 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6919 Loss_G: 0.7068 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.7119 Loss_G: 0.7295 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6949 Loss_G: 0.7370 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.7491 Loss_G: 0.7499 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7061 Loss_G: 0.7449 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6780 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6994 Loss_G: 0.7214 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7452 Loss_G: 0.7240 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7088 Loss_G: 0.7193 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6835 Loss_G: 0.7068 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.7154 Loss_G: 0.7239 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7103 Loss_G: 0.7622 acc: 85.9%\n",
      "[EPOCH 7450] TEST ACC is : 77.0%\n",
      "-----THE [50/50] epoch end-----\n",
      "The 5 * 50 epochs train starts:\n",
      "-----THE [1/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7031 Loss_G: 0.7205 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6934 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7150 Loss_G: 0.7413 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6997 Loss_G: 0.7409 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6839 Loss_G: 0.7153 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7274 Loss_G: 0.7571 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7115 Loss_G: 0.7371 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.7403 Loss_G: 0.7434 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7445 Loss_G: 0.7584 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7098 Loss_G: 0.7434 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7243 Loss_G: 0.7449 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6772 Loss_G: 0.7293 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.6911 Loss_G: 0.7300 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.7305 Loss_G: 0.7476 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.6884 Loss_G: 0.7331 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6454 Loss_G: 0.6817 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.7083 Loss_G: 0.7009 acc: 81.2%\n",
      "[BATCH 18/149] Loss_D: 0.6569 Loss_G: 0.7146 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.6834 Loss_G: 0.7264 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6946 Loss_G: 0.7107 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6635 Loss_G: 0.7316 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6934 Loss_G: 0.7204 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.6995 Loss_G: 0.7086 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7337 Loss_G: 0.7448 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6814 Loss_G: 0.7267 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6834 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7201 Loss_G: 0.7217 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6964 Loss_G: 0.7246 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7214 Loss_G: 0.7387 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.7368 Loss_G: 0.7305 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.7056 Loss_G: 0.7235 acc: 81.2%\n",
      "[BATCH 32/149] Loss_D: 0.6932 Loss_G: 0.7219 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6427 Loss_G: 0.7119 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.6668 Loss_G: 0.7081 acc: 95.3%\n",
      "[BATCH 35/149] Loss_D: 0.7237 Loss_G: 0.7351 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.7240 Loss_G: 0.7399 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7001 Loss_G: 0.7443 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6903 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6778 Loss_G: 0.7161 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6742 Loss_G: 0.7154 acc: 95.3%\n",
      "[BATCH 41/149] Loss_D: 0.6799 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7087 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7399 Loss_G: 0.7111 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.7116 Loss_G: 0.7349 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7243 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6914 Loss_G: 0.7211 acc: 82.8%\n",
      "[BATCH 47/149] Loss_D: 0.6931 Loss_G: 0.7340 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6537 Loss_G: 0.7136 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6783 Loss_G: 0.7270 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6956 Loss_G: 0.7193 acc: 87.5%\n",
      "[EPOCH 50] TEST ACC is : 75.8%\n",
      "[BATCH 51/149] Loss_D: 0.6609 Loss_G: 0.6965 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.6970 Loss_G: 0.7356 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.6738 Loss_G: 0.7096 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.6710 Loss_G: 0.6995 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6873 Loss_G: 0.7236 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.6959 Loss_G: 0.7276 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6822 Loss_G: 0.7151 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.7134 Loss_G: 0.7138 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.7155 Loss_G: 0.7400 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6953 Loss_G: 0.7085 acc: 81.2%\n",
      "[BATCH 61/149] Loss_D: 0.6660 Loss_G: 0.7111 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6721 Loss_G: 0.6913 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6868 Loss_G: 0.6826 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6742 Loss_G: 0.6975 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7180 Loss_G: 0.7128 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7264 Loss_G: 0.7267 acc: 95.3%\n",
      "[BATCH 67/149] Loss_D: 0.7367 Loss_G: 0.7418 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6633 Loss_G: 0.7119 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7333 Loss_G: 0.7225 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.7150 Loss_G: 0.7316 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7490 Loss_G: 0.7487 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7058 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7061 Loss_G: 0.7173 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6634 Loss_G: 0.7128 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.7273 Loss_G: 0.7236 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6799 Loss_G: 0.7261 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.7284 Loss_G: 0.7739 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.6884 Loss_G: 0.7465 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6991 Loss_G: 0.7053 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6952 Loss_G: 0.7317 acc: 78.1%\n",
      "[BATCH 81/149] Loss_D: 0.6806 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6580 Loss_G: 0.7176 acc: 95.3%\n",
      "[BATCH 83/149] Loss_D: 0.6784 Loss_G: 0.7126 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6711 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6909 Loss_G: 0.7279 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.7399 Loss_G: 0.7248 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.7064 Loss_G: 0.7321 acc: 96.9%\n",
      "[BATCH 88/149] Loss_D: 0.6988 Loss_G: 0.7176 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6994 Loss_G: 0.6887 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.6927 Loss_G: 0.7153 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6915 Loss_G: 0.7265 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7347 Loss_G: 0.7687 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7206 Loss_G: 0.7600 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.7084 Loss_G: 0.7367 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.7090 Loss_G: 0.7484 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7122 Loss_G: 0.7508 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6796 Loss_G: 0.7417 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6587 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7194 Loss_G: 0.7084 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7133 Loss_G: 0.7062 acc: 87.5%\n",
      "[EPOCH 100] TEST ACC is : 76.8%\n",
      "[BATCH 101/149] Loss_D: 0.7078 Loss_G: 0.7213 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7127 Loss_G: 0.7276 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6526 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7019 Loss_G: 0.7030 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.7311 Loss_G: 0.7321 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6761 Loss_G: 0.7567 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.7019 Loss_G: 0.7261 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6807 Loss_G: 0.7015 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6899 Loss_G: 0.7201 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6908 Loss_G: 0.7225 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6815 Loss_G: 0.7185 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7500 Loss_G: 0.7422 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6922 Loss_G: 0.7457 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7057 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.7028 Loss_G: 0.7246 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6989 Loss_G: 0.7077 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7117 Loss_G: 0.7186 acc: 81.2%\n",
      "[BATCH 118/149] Loss_D: 0.6800 Loss_G: 0.7269 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7173 Loss_G: 0.7235 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.6815 Loss_G: 0.7210 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6930 Loss_G: 0.7304 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.7183 Loss_G: 0.7289 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6793 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6825 Loss_G: 0.7029 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.6601 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7339 Loss_G: 0.7114 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.6865 Loss_G: 0.7122 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.7055 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7065 Loss_G: 0.7218 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7395 Loss_G: 0.7349 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6851 Loss_G: 0.7227 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6747 Loss_G: 0.7033 acc: 78.1%\n",
      "[BATCH 133/149] Loss_D: 0.6955 Loss_G: 0.7269 acc: 95.3%\n",
      "[BATCH 134/149] Loss_D: 0.7143 Loss_G: 0.7202 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.6772 Loss_G: 0.7063 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6933 Loss_G: 0.7065 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.7158 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7203 Loss_G: 0.7360 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.7072 Loss_G: 0.7230 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.6884 Loss_G: 0.7247 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6905 Loss_G: 0.7293 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6775 Loss_G: 0.6975 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6680 Loss_G: 0.7033 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6751 Loss_G: 0.7170 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7279 Loss_G: 0.7329 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.6532 Loss_G: 0.7404 acc: 95.3%\n",
      "[BATCH 147/149] Loss_D: 0.7201 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6868 Loss_G: 0.7202 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.7174 Loss_G: 0.7359 acc: 85.9%\n",
      "-----THE [1/50] epoch end-----\n",
      "-----THE [2/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6803 Loss_G: 0.7035 acc: 90.6%\n",
      "[EPOCH 150] TEST ACC is : 75.2%\n",
      "[BATCH 2/149] Loss_D: 0.7284 Loss_G: 0.7267 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7027 Loss_G: 0.7086 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6775 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7188 Loss_G: 0.7346 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.7078 Loss_G: 0.7711 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.7186 Loss_G: 0.7573 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.7089 Loss_G: 0.7360 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6815 Loss_G: 0.7069 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6553 Loss_G: 0.6948 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6870 Loss_G: 0.7019 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.7234 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6829 Loss_G: 0.7017 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.7037 Loss_G: 0.7056 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7015 Loss_G: 0.7209 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6815 Loss_G: 0.7122 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6806 Loss_G: 0.7019 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7174 Loss_G: 0.7117 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6972 Loss_G: 0.6903 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.7289 Loss_G: 0.7451 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6649 Loss_G: 0.7201 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.7049 Loss_G: 0.7196 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7011 Loss_G: 0.7382 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6835 Loss_G: 0.7165 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.7139 Loss_G: 0.7048 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.6965 Loss_G: 0.7229 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6610 Loss_G: 0.7121 acc: 95.3%\n",
      "[BATCH 28/149] Loss_D: 0.7065 Loss_G: 0.7302 acc: 95.3%\n",
      "[BATCH 29/149] Loss_D: 0.7210 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7296 Loss_G: 0.7044 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7304 Loss_G: 0.7315 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6921 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.7098 Loss_G: 0.7066 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7111 Loss_G: 0.7046 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.7145 Loss_G: 0.7072 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6881 Loss_G: 0.7172 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6917 Loss_G: 0.7281 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.6785 Loss_G: 0.7376 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.6440 Loss_G: 0.7218 acc: 96.9%\n",
      "[BATCH 40/149] Loss_D: 0.7045 Loss_G: 0.7179 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7093 Loss_G: 0.7278 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6918 Loss_G: 0.7351 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7531 Loss_G: 0.7380 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.7015 Loss_G: 0.7303 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6776 Loss_G: 0.7082 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.7027 Loss_G: 0.7218 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6716 Loss_G: 0.7229 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6860 Loss_G: 0.7264 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.7111 Loss_G: 0.7188 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6819 Loss_G: 0.7152 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6867 Loss_G: 0.7288 acc: 92.2%\n",
      "[EPOCH 200] TEST ACC is : 77.0%\n",
      "[BATCH 52/149] Loss_D: 0.6904 Loss_G: 0.7219 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6732 Loss_G: 0.7039 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6931 Loss_G: 0.7022 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.6721 Loss_G: 0.6973 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6979 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.7093 Loss_G: 0.7155 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7195 Loss_G: 0.7082 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6708 Loss_G: 0.7100 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7448 Loss_G: 0.7293 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.7118 Loss_G: 0.7338 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6698 Loss_G: 0.7281 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.7178 Loss_G: 0.7129 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7317 Loss_G: 0.7390 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7098 Loss_G: 0.7242 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6879 Loss_G: 0.7234 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.6844 Loss_G: 0.7297 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6526 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.6683 Loss_G: 0.7068 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7151 Loss_G: 0.7166 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6953 Loss_G: 0.7396 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6888 Loss_G: 0.7102 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6658 Loss_G: 0.7156 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.7274 Loss_G: 0.7089 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7325 Loss_G: 0.7373 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.7161 Loss_G: 0.7311 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.6751 Loss_G: 0.7304 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.7437 Loss_G: 0.7314 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.6768 Loss_G: 0.7124 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.7323 Loss_G: 0.7238 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6996 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6965 Loss_G: 0.7344 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6659 Loss_G: 0.7127 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6671 Loss_G: 0.7188 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.6957 Loss_G: 0.7247 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6608 Loss_G: 0.6944 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6608 Loss_G: 0.7127 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.6748 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6973 Loss_G: 0.7101 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.7061 Loss_G: 0.7345 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6679 Loss_G: 0.7167 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6866 Loss_G: 0.7184 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6976 Loss_G: 0.7433 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7115 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.6728 Loss_G: 0.7286 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7031 Loss_G: 0.7282 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6949 Loss_G: 0.7074 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.7094 Loss_G: 0.7183 acc: 81.2%\n",
      "[BATCH 99/149] Loss_D: 0.6995 Loss_G: 0.7281 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7167 Loss_G: 0.7370 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6598 Loss_G: 0.7288 acc: 92.2%\n",
      "[EPOCH 250] TEST ACC is : 76.6%\n",
      "[BATCH 102/149] Loss_D: 0.6920 Loss_G: 0.7069 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7154 Loss_G: 0.7177 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.6750 Loss_G: 0.7315 acc: 95.3%\n",
      "[BATCH 105/149] Loss_D: 0.6776 Loss_G: 0.7389 acc: 95.3%\n",
      "[BATCH 106/149] Loss_D: 0.6739 Loss_G: 0.7276 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6706 Loss_G: 0.7083 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7069 Loss_G: 0.7372 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6828 Loss_G: 0.7276 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.6961 Loss_G: 0.7306 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7086 Loss_G: 0.7250 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7400 Loss_G: 0.7501 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6975 Loss_G: 0.7570 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6524 Loss_G: 0.7006 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.7380 Loss_G: 0.7174 acc: 78.1%\n",
      "[BATCH 116/149] Loss_D: 0.6854 Loss_G: 0.7054 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.6725 Loss_G: 0.6998 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7149 Loss_G: 0.7231 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7233 Loss_G: 0.7291 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.7182 Loss_G: 0.7242 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.7076 Loss_G: 0.7275 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6879 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6567 Loss_G: 0.7107 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.7062 Loss_G: 0.7085 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.6895 Loss_G: 0.7254 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6864 Loss_G: 0.7363 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6897 Loss_G: 0.7387 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.6974 Loss_G: 0.7214 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7398 Loss_G: 0.7516 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7558 Loss_G: 0.7995 acc: 82.8%\n",
      "[BATCH 131/149] Loss_D: 0.6985 Loss_G: 0.7791 acc: 96.9%\n",
      "[BATCH 132/149] Loss_D: 0.7063 Loss_G: 0.7488 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7251 Loss_G: 0.7368 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.6819 Loss_G: 0.7025 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.7078 Loss_G: 0.7040 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7051 Loss_G: 0.7154 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.6849 Loss_G: 0.7201 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6911 Loss_G: 0.7110 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6730 Loss_G: 0.7007 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.6971 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6996 Loss_G: 0.7133 acc: 79.7%\n",
      "[BATCH 142/149] Loss_D: 0.7235 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6736 Loss_G: 0.7198 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7435 Loss_G: 0.7268 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6783 Loss_G: 0.7121 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.6797 Loss_G: 0.7030 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6692 Loss_G: 0.6996 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.7491 Loss_G: 0.7334 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.7302 Loss_G: 0.7315 acc: 84.4%\n",
      "-----THE [2/50] epoch end-----\n",
      "-----THE [3/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6823 Loss_G: 0.7205 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7247 Loss_G: 0.7349 acc: 89.1%\n",
      "[EPOCH 300] TEST ACC is : 76.2%\n",
      "[BATCH 3/149] Loss_D: 0.6919 Loss_G: 0.7489 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.7322 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7274 Loss_G: 0.7749 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.7025 Loss_G: 0.7458 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.7293 Loss_G: 0.7534 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7477 Loss_G: 0.7711 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6892 Loss_G: 0.7338 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.7518 Loss_G: 0.7547 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6808 Loss_G: 0.7280 acc: 78.1%\n",
      "[BATCH 12/149] Loss_D: 0.6860 Loss_G: 0.7137 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6904 Loss_G: 0.7242 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6778 Loss_G: 0.7245 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.6845 Loss_G: 0.7120 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7360 Loss_G: 0.7213 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6825 Loss_G: 0.7195 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7332 Loss_G: 0.7174 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.6840 Loss_G: 0.7102 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6810 Loss_G: 0.7248 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6690 Loss_G: 0.7109 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7216 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6905 Loss_G: 0.7309 acc: 82.8%\n",
      "[BATCH 24/149] Loss_D: 0.6570 Loss_G: 0.7057 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.7859 Loss_G: 0.7409 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6974 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7079 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6562 Loss_G: 0.6991 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6900 Loss_G: 0.7141 acc: 93.8%\n",
      "[BATCH 30/149] Loss_D: 0.6837 Loss_G: 0.7010 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.7120 Loss_G: 0.7098 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.7005 Loss_G: 0.7197 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6771 Loss_G: 0.7508 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6989 Loss_G: 0.7263 acc: 78.1%\n",
      "[BATCH 35/149] Loss_D: 0.7069 Loss_G: 0.7591 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6793 Loss_G: 0.7171 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6805 Loss_G: 0.7248 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6994 Loss_G: 0.7143 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6562 Loss_G: 0.7062 acc: 95.3%\n",
      "[BATCH 40/149] Loss_D: 0.6761 Loss_G: 0.7105 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7136 Loss_G: 0.7149 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.7224 Loss_G: 0.7219 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6663 Loss_G: 0.7353 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.7141 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6970 Loss_G: 0.7363 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7075 Loss_G: 0.7218 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7003 Loss_G: 0.7155 acc: 93.8%\n",
      "[BATCH 48/149] Loss_D: 0.6993 Loss_G: 0.7380 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6687 Loss_G: 0.7116 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7209 Loss_G: 0.7174 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.7072 Loss_G: 0.7216 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6587 Loss_G: 0.7050 acc: 95.3%\n",
      "[EPOCH 350] TEST ACC is : 76.8%\n",
      "[BATCH 53/149] Loss_D: 0.7053 Loss_G: 0.7098 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6871 Loss_G: 0.7355 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.7112 Loss_G: 0.7420 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.6807 Loss_G: 0.7232 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7217 Loss_G: 0.7536 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.7078 Loss_G: 0.7322 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.6838 Loss_G: 0.7106 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6978 Loss_G: 0.7209 acc: 95.3%\n",
      "[BATCH 61/149] Loss_D: 0.7103 Loss_G: 0.7285 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6908 Loss_G: 0.7309 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7282 Loss_G: 0.7362 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7182 Loss_G: 0.7351 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7038 Loss_G: 0.7041 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.6761 Loss_G: 0.7084 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6847 Loss_G: 0.7345 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7128 Loss_G: 0.7257 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6778 Loss_G: 0.7143 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.7121 Loss_G: 0.7238 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6908 Loss_G: 0.7098 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6560 Loss_G: 0.6955 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.7042 Loss_G: 0.7016 acc: 93.8%\n",
      "[BATCH 74/149] Loss_D: 0.6636 Loss_G: 0.7065 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.6759 Loss_G: 0.6892 acc: 95.3%\n",
      "[BATCH 76/149] Loss_D: 0.7065 Loss_G: 0.7207 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7091 Loss_G: 0.7305 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6838 Loss_G: 0.7283 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.7079 Loss_G: 0.7153 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7104 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7033 Loss_G: 0.7276 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6608 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6615 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7077 Loss_G: 0.7181 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7264 Loss_G: 0.7374 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6895 Loss_G: 0.7164 acc: 81.2%\n",
      "[BATCH 87/149] Loss_D: 0.6644 Loss_G: 0.7179 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.7081 Loss_G: 0.7216 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6952 Loss_G: 0.7149 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.7266 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6804 Loss_G: 0.7267 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7130 Loss_G: 0.7301 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.6994 Loss_G: 0.7137 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6840 Loss_G: 0.7207 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6707 Loss_G: 0.7066 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7163 Loss_G: 0.7296 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6993 Loss_G: 0.7220 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6753 Loss_G: 0.7163 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.7239 Loss_G: 0.7260 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7133 Loss_G: 0.7312 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.7052 Loss_G: 0.7317 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6903 Loss_G: 0.7068 acc: 93.8%\n",
      "[EPOCH 400] TEST ACC is : 77.1%\n",
      "[BATCH 103/149] Loss_D: 0.6583 Loss_G: 0.6919 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7233 Loss_G: 0.7262 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.7110 Loss_G: 0.7474 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.7030 Loss_G: 0.7419 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7090 Loss_G: 0.7111 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6900 Loss_G: 0.7144 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6663 Loss_G: 0.7089 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7057 Loss_G: 0.6993 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.6735 Loss_G: 0.7035 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6726 Loss_G: 0.7083 acc: 92.2%\n",
      "[BATCH 113/149] Loss_D: 0.6752 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6853 Loss_G: 0.7081 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7128 Loss_G: 0.7036 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.7093 Loss_G: 0.7025 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6879 Loss_G: 0.6943 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.7230 Loss_G: 0.7065 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6876 Loss_G: 0.7057 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7110 Loss_G: 0.7159 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6927 Loss_G: 0.7242 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6578 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6892 Loss_G: 0.7276 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.7111 Loss_G: 0.7168 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.6598 Loss_G: 0.7220 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6961 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6685 Loss_G: 0.7190 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6608 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.6976 Loss_G: 0.7278 acc: 93.8%\n",
      "[BATCH 130/149] Loss_D: 0.6843 Loss_G: 0.7376 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7361 Loss_G: 0.7492 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.6819 Loss_G: 0.7181 acc: 95.3%\n",
      "[BATCH 133/149] Loss_D: 0.6991 Loss_G: 0.7111 acc: 79.7%\n",
      "[BATCH 134/149] Loss_D: 0.7029 Loss_G: 0.7313 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.7105 Loss_G: 0.7195 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6864 Loss_G: 0.7423 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6817 Loss_G: 0.7166 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6968 Loss_G: 0.7197 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7215 Loss_G: 0.7284 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7373 Loss_G: 0.7550 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.6943 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6922 Loss_G: 0.7292 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.7651 Loss_G: 0.7360 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6886 Loss_G: 0.7210 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7049 Loss_G: 0.7235 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6843 Loss_G: 0.7204 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6647 Loss_G: 0.7252 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6802 Loss_G: 0.7040 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.6958 Loss_G: 0.7102 acc: 85.9%\n",
      "-----THE [3/50] epoch end-----\n",
      "-----THE [4/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7153 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7080 Loss_G: 0.7138 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7049 Loss_G: 0.7048 acc: 87.5%\n",
      "[EPOCH 450] TEST ACC is : 75.8%\n",
      "[BATCH 4/149] Loss_D: 0.6791 Loss_G: 0.7150 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.6768 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6806 Loss_G: 0.7185 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6718 Loss_G: 0.7254 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7334 Loss_G: 0.7541 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6816 Loss_G: 0.7547 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6769 Loss_G: 0.7433 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.6919 Loss_G: 0.7126 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6691 Loss_G: 0.7097 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7013 Loss_G: 0.7186 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6733 Loss_G: 0.6922 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7073 Loss_G: 0.6994 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7043 Loss_G: 0.7138 acc: 96.9%\n",
      "[BATCH 17/149] Loss_D: 0.6882 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7123 Loss_G: 0.7254 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6893 Loss_G: 0.7191 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.7082 Loss_G: 0.7350 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6717 Loss_G: 0.7515 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.7043 Loss_G: 0.7474 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.7049 Loss_G: 0.7275 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.7142 Loss_G: 0.7553 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6984 Loss_G: 0.7172 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6865 Loss_G: 0.7119 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6665 Loss_G: 0.6988 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7249 Loss_G: 0.7055 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.7080 Loss_G: 0.7142 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7390 Loss_G: 0.7261 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.7181 Loss_G: 0.7205 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7073 Loss_G: 0.7345 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.6899 Loss_G: 0.7196 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6842 Loss_G: 0.7148 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6860 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7079 Loss_G: 0.7314 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7380 Loss_G: 0.7356 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.7153 Loss_G: 0.7395 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6477 Loss_G: 0.7100 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.7010 Loss_G: 0.7098 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.7525 Loss_G: 0.7197 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.6920 Loss_G: 0.7078 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7046 Loss_G: 0.7181 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6850 Loss_G: 0.7242 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.6703 Loss_G: 0.7091 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.7395 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6804 Loss_G: 0.7280 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7073 Loss_G: 0.7352 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.6631 Loss_G: 0.7286 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7015 Loss_G: 0.7185 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6499 Loss_G: 0.7185 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6633 Loss_G: 0.7080 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6737 Loss_G: 0.7381 acc: 93.8%\n",
      "[EPOCH 500] TEST ACC is : 77.5%\n",
      "[BATCH 54/149] Loss_D: 0.7307 Loss_G: 0.7525 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.6861 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6655 Loss_G: 0.7131 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6812 Loss_G: 0.7063 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6820 Loss_G: 0.7286 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6912 Loss_G: 0.7063 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7366 Loss_G: 0.7323 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.6967 Loss_G: 0.7577 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6908 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6674 Loss_G: 0.7113 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7033 Loss_G: 0.7158 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6840 Loss_G: 0.7153 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7160 Loss_G: 0.7388 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6668 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7163 Loss_G: 0.7039 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7280 Loss_G: 0.7258 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6720 Loss_G: 0.7203 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6982 Loss_G: 0.7159 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.7134 Loss_G: 0.7153 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6938 Loss_G: 0.7241 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.6817 Loss_G: 0.7187 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6748 Loss_G: 0.7043 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7081 Loss_G: 0.7173 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.6621 Loss_G: 0.7336 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.7043 Loss_G: 0.7402 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.7277 Loss_G: 0.7311 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7113 Loss_G: 0.7154 acc: 81.2%\n",
      "[BATCH 81/149] Loss_D: 0.7355 Loss_G: 0.7295 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7090 Loss_G: 0.7308 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.7251 Loss_G: 0.7582 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6960 Loss_G: 0.7602 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.7158 Loss_G: 0.7428 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7109 Loss_G: 0.7379 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6684 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7127 Loss_G: 0.7192 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7093 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7203 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7027 Loss_G: 0.7116 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6927 Loss_G: 0.6982 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6869 Loss_G: 0.6949 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6685 Loss_G: 0.6882 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.6891 Loss_G: 0.7184 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7244 Loss_G: 0.7400 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7230 Loss_G: 0.7128 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6887 Loss_G: 0.7062 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7268 Loss_G: 0.7381 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.7172 Loss_G: 0.7405 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.6825 Loss_G: 0.7071 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6981 Loss_G: 0.7051 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7001 Loss_G: 0.7053 acc: 82.8%\n",
      "[EPOCH 550] TEST ACC is : 77.1%\n",
      "[BATCH 104/149] Loss_D: 0.7252 Loss_G: 0.7249 acc: 79.7%\n",
      "[BATCH 105/149] Loss_D: 0.6866 Loss_G: 0.7110 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.6579 Loss_G: 0.7024 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6660 Loss_G: 0.6986 acc: 81.2%\n",
      "[BATCH 108/149] Loss_D: 0.6880 Loss_G: 0.7013 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6895 Loss_G: 0.7039 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.7183 Loss_G: 0.7196 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6756 Loss_G: 0.7210 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6924 Loss_G: 0.7050 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6797 Loss_G: 0.6899 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.6919 Loss_G: 0.7013 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.6728 Loss_G: 0.7157 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6950 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6723 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.7194 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6795 Loss_G: 0.6934 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.6844 Loss_G: 0.6934 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6852 Loss_G: 0.7080 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6991 Loss_G: 0.7045 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.7127 Loss_G: 0.7485 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6973 Loss_G: 0.7378 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.7285 Loss_G: 0.7390 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6690 Loss_G: 0.7443 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6730 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7019 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6822 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6801 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6963 Loss_G: 0.7190 acc: 95.3%\n",
      "[BATCH 132/149] Loss_D: 0.7115 Loss_G: 0.7168 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.7043 Loss_G: 0.7268 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.7068 Loss_G: 0.7116 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7293 Loss_G: 0.7275 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6924 Loss_G: 0.7281 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.6642 Loss_G: 0.7105 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7007 Loss_G: 0.7230 acc: 84.4%\n",
      "[BATCH 139/149] Loss_D: 0.6956 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7090 Loss_G: 0.7079 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.7214 Loss_G: 0.7254 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6858 Loss_G: 0.7323 acc: 96.9%\n",
      "[BATCH 143/149] Loss_D: 0.6805 Loss_G: 0.7078 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6899 Loss_G: 0.7317 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6665 Loss_G: 0.7189 acc: 93.8%\n",
      "[BATCH 146/149] Loss_D: 0.7134 Loss_G: 0.7329 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6915 Loss_G: 0.7340 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.7577 Loss_G: 0.7447 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7095 Loss_G: 0.7499 acc: 92.2%\n",
      "-----THE [4/50] epoch end-----\n",
      "-----THE [5/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6962 Loss_G: 0.7346 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6851 Loss_G: 0.7273 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.7104 Loss_G: 0.7121 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6735 Loss_G: 0.7081 acc: 89.1%\n",
      "[EPOCH 600] TEST ACC is : 76.2%\n",
      "[BATCH 5/149] Loss_D: 0.7181 Loss_G: 0.7240 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.6812 Loss_G: 0.7179 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6678 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.7406 Loss_G: 0.7246 acc: 95.3%\n",
      "[BATCH 9/149] Loss_D: 0.7488 Loss_G: 0.7497 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6942 Loss_G: 0.7392 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6723 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7014 Loss_G: 0.7186 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6816 Loss_G: 0.7018 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6764 Loss_G: 0.7184 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6913 Loss_G: 0.6969 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7245 Loss_G: 0.7154 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.7365 Loss_G: 0.7264 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7308 Loss_G: 0.7230 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6625 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6749 Loss_G: 0.7211 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6818 Loss_G: 0.7219 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6986 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6957 Loss_G: 0.7297 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.6843 Loss_G: 0.7090 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6764 Loss_G: 0.6958 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.7177 Loss_G: 0.7252 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6617 Loss_G: 0.7240 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.7276 Loss_G: 0.7101 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.6578 Loss_G: 0.7166 acc: 82.8%\n",
      "[BATCH 30/149] Loss_D: 0.7248 Loss_G: 0.7267 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.7172 Loss_G: 0.7350 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7118 Loss_G: 0.7560 acc: 95.3%\n",
      "[BATCH 33/149] Loss_D: 0.7075 Loss_G: 0.7746 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6791 Loss_G: 0.7243 acc: 81.2%\n",
      "[BATCH 35/149] Loss_D: 0.6715 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6947 Loss_G: 0.7048 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.6989 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6659 Loss_G: 0.7236 acc: 96.9%\n",
      "[BATCH 39/149] Loss_D: 0.6980 Loss_G: 0.7150 acc: 78.1%\n",
      "[BATCH 40/149] Loss_D: 0.6492 Loss_G: 0.7169 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7148 Loss_G: 0.7313 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6822 Loss_G: 0.6991 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.6765 Loss_G: 0.7050 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6785 Loss_G: 0.7135 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7589 Loss_G: 0.7343 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6574 Loss_G: 0.7184 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.7247 Loss_G: 0.7141 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6891 Loss_G: 0.7036 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.6839 Loss_G: 0.6943 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7160 Loss_G: 0.6997 acc: 82.8%\n",
      "[BATCH 51/149] Loss_D: 0.6731 Loss_G: 0.7056 acc: 96.9%\n",
      "[BATCH 52/149] Loss_D: 0.7352 Loss_G: 0.7304 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6841 Loss_G: 0.7355 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6537 Loss_G: 0.7062 acc: 87.5%\n",
      "[EPOCH 650] TEST ACC is : 76.2%\n",
      "[BATCH 55/149] Loss_D: 0.7183 Loss_G: 0.7074 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.6837 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6834 Loss_G: 0.7284 acc: 81.2%\n",
      "[BATCH 58/149] Loss_D: 0.7224 Loss_G: 0.7493 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6812 Loss_G: 0.7397 acc: 79.7%\n",
      "[BATCH 60/149] Loss_D: 0.7280 Loss_G: 0.7596 acc: 95.3%\n",
      "[BATCH 61/149] Loss_D: 0.6699 Loss_G: 0.7172 acc: 93.8%\n",
      "[BATCH 62/149] Loss_D: 0.7139 Loss_G: 0.7057 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6907 Loss_G: 0.7335 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.7138 Loss_G: 0.7136 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.7294 Loss_G: 0.7341 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6699 Loss_G: 0.7339 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6823 Loss_G: 0.6964 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6860 Loss_G: 0.7065 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7241 Loss_G: 0.7190 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6929 Loss_G: 0.7501 acc: 93.8%\n",
      "[BATCH 71/149] Loss_D: 0.7540 Loss_G: 0.7494 acc: 78.1%\n",
      "[BATCH 72/149] Loss_D: 0.7041 Loss_G: 0.7190 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7198 Loss_G: 0.7110 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6702 Loss_G: 0.7055 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.6898 Loss_G: 0.7168 acc: 81.2%\n",
      "[BATCH 76/149] Loss_D: 0.7027 Loss_G: 0.7223 acc: 95.3%\n",
      "[BATCH 77/149] Loss_D: 0.7464 Loss_G: 0.7597 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.7001 Loss_G: 0.7255 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7153 Loss_G: 0.7382 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6792 Loss_G: 0.7191 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7359 Loss_G: 0.7319 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.6966 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6950 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.7095 Loss_G: 0.7166 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.6966 Loss_G: 0.7387 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7232 Loss_G: 0.7342 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.6661 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6774 Loss_G: 0.7140 acc: 95.3%\n",
      "[BATCH 89/149] Loss_D: 0.6787 Loss_G: 0.7239 acc: 96.9%\n",
      "[BATCH 90/149] Loss_D: 0.6753 Loss_G: 0.7164 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.7094 Loss_G: 0.7257 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7143 Loss_G: 0.7225 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7144 Loss_G: 0.7445 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6946 Loss_G: 0.7726 acc: 95.3%\n",
      "[BATCH 95/149] Loss_D: 0.7188 Loss_G: 0.7377 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6959 Loss_G: 0.7188 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6791 Loss_G: 0.7263 acc: 96.9%\n",
      "[BATCH 98/149] Loss_D: 0.7054 Loss_G: 0.7207 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6611 Loss_G: 0.7077 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.7024 Loss_G: 0.7099 acc: 81.2%\n",
      "[BATCH 101/149] Loss_D: 0.6906 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6656 Loss_G: 0.7141 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.6852 Loss_G: 0.7032 acc: 93.8%\n",
      "[BATCH 104/149] Loss_D: 0.7070 Loss_G: 0.7230 acc: 85.9%\n",
      "[EPOCH 700] TEST ACC is : 76.8%\n",
      "[BATCH 105/149] Loss_D: 0.6821 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6953 Loss_G: 0.7236 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7131 Loss_G: 0.7323 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6496 Loss_G: 0.7176 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6572 Loss_G: 0.6957 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6633 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6899 Loss_G: 0.7030 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6698 Loss_G: 0.7241 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6671 Loss_G: 0.7059 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7211 Loss_G: 0.7330 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7246 Loss_G: 0.7302 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.7082 Loss_G: 0.7418 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.7005 Loss_G: 0.7217 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7088 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7182 Loss_G: 0.7148 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7075 Loss_G: 0.7244 acc: 81.2%\n",
      "[BATCH 121/149] Loss_D: 0.6536 Loss_G: 0.7127 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.6925 Loss_G: 0.7079 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.6556 Loss_G: 0.7052 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6843 Loss_G: 0.7023 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.6925 Loss_G: 0.7241 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7198 Loss_G: 0.7286 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.7204 Loss_G: 0.7366 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.7103 Loss_G: 0.7415 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.7034 Loss_G: 0.7236 acc: 79.7%\n",
      "[BATCH 130/149] Loss_D: 0.7055 Loss_G: 0.6950 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6837 Loss_G: 0.6997 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6853 Loss_G: 0.6961 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7069 Loss_G: 0.7332 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7221 Loss_G: 0.7420 acc: 79.7%\n",
      "[BATCH 135/149] Loss_D: 0.6910 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6665 Loss_G: 0.7152 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.7227 Loss_G: 0.7278 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.7052 Loss_G: 0.7236 acc: 82.8%\n",
      "[BATCH 139/149] Loss_D: 0.6972 Loss_G: 0.7539 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6935 Loss_G: 0.7368 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.7142 Loss_G: 0.7337 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7030 Loss_G: 0.7454 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6822 Loss_G: 0.7234 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7044 Loss_G: 0.7303 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6960 Loss_G: 0.7076 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7177 Loss_G: 0.7121 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.7006 Loss_G: 0.7207 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.7306 Loss_G: 0.7184 acc: 81.2%\n",
      "[BATCH 149/149] Loss_D: 0.6839 Loss_G: 0.7039 acc: 89.1%\n",
      "-----THE [5/50] epoch end-----\n",
      "-----THE [6/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7021 Loss_G: 0.7059 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.7385 Loss_G: 0.7101 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6439 Loss_G: 0.7098 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.7109 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6943 Loss_G: 0.7025 acc: 78.1%\n",
      "[EPOCH 750] TEST ACC is : 75.6%\n",
      "[BATCH 6/149] Loss_D: 0.7320 Loss_G: 0.7224 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7140 Loss_G: 0.7491 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6654 Loss_G: 0.7214 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6735 Loss_G: 0.7124 acc: 81.2%\n",
      "[BATCH 10/149] Loss_D: 0.6548 Loss_G: 0.7152 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.7215 Loss_G: 0.7419 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6755 Loss_G: 0.7311 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6956 Loss_G: 0.7529 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.7101 Loss_G: 0.7347 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6714 Loss_G: 0.7150 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7050 Loss_G: 0.7157 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6847 Loss_G: 0.7232 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6637 Loss_G: 0.7102 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6815 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.6956 Loss_G: 0.7119 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.7101 Loss_G: 0.7153 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.7058 Loss_G: 0.7338 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7073 Loss_G: 0.7217 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6829 Loss_G: 0.7228 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6930 Loss_G: 0.7094 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7282 Loss_G: 0.7203 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.6878 Loss_G: 0.7381 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6736 Loss_G: 0.7105 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6804 Loss_G: 0.7181 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.6736 Loss_G: 0.7249 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6999 Loss_G: 0.7261 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.6758 Loss_G: 0.7180 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6624 Loss_G: 0.7131 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.7146 Loss_G: 0.7100 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.7213 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6940 Loss_G: 0.7259 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.7178 Loss_G: 0.7067 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.7221 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6966 Loss_G: 0.7272 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.6902 Loss_G: 0.7184 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.6629 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7852 Loss_G: 0.7466 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6882 Loss_G: 0.7138 acc: 82.8%\n",
      "[BATCH 44/149] Loss_D: 0.7176 Loss_G: 0.7054 acc: 78.1%\n",
      "[BATCH 45/149] Loss_D: 0.6885 Loss_G: 0.7057 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.6531 Loss_G: 0.6929 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.6937 Loss_G: 0.7025 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6831 Loss_G: 0.7102 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6807 Loss_G: 0.6919 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6741 Loss_G: 0.6963 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.7263 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6586 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6976 Loss_G: 0.6977 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.6732 Loss_G: 0.7014 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.7151 Loss_G: 0.7321 acc: 89.1%\n",
      "[EPOCH 800] TEST ACC is : 77.0%\n",
      "[BATCH 56/149] Loss_D: 0.6727 Loss_G: 0.7141 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.7023 Loss_G: 0.7220 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7063 Loss_G: 0.7469 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.6904 Loss_G: 0.7205 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7226 Loss_G: 0.7260 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.7507 Loss_G: 0.7417 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6852 Loss_G: 0.7194 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6947 Loss_G: 0.7324 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7148 Loss_G: 0.7640 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.6895 Loss_G: 0.7148 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.6825 Loss_G: 0.7062 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6919 Loss_G: 0.7218 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6937 Loss_G: 0.7053 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7045 Loss_G: 0.7298 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6646 Loss_G: 0.7027 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7032 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6901 Loss_G: 0.7255 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6677 Loss_G: 0.7018 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6677 Loss_G: 0.7137 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6555 Loss_G: 0.7139 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6914 Loss_G: 0.7236 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7018 Loss_G: 0.7227 acc: 95.3%\n",
      "[BATCH 78/149] Loss_D: 0.6874 Loss_G: 0.7121 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7078 Loss_G: 0.7195 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6982 Loss_G: 0.7343 acc: 96.9%\n",
      "[BATCH 81/149] Loss_D: 0.6965 Loss_G: 0.7312 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6902 Loss_G: 0.7312 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6702 Loss_G: 0.7269 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6790 Loss_G: 0.7323 acc: 100.0%\n",
      "[BATCH 85/149] Loss_D: 0.7178 Loss_G: 0.7250 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7098 Loss_G: 0.7384 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6812 Loss_G: 0.7106 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6939 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7081 Loss_G: 0.7181 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6995 Loss_G: 0.7189 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6990 Loss_G: 0.7352 acc: 95.3%\n",
      "[BATCH 92/149] Loss_D: 0.6806 Loss_G: 0.7098 acc: 78.1%\n",
      "[BATCH 93/149] Loss_D: 0.6721 Loss_G: 0.7132 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.7055 Loss_G: 0.7121 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6855 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.7206 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6814 Loss_G: 0.7058 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.6856 Loss_G: 0.7128 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7059 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6888 Loss_G: 0.7044 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6994 Loss_G: 0.7249 acc: 78.1%\n",
      "[BATCH 102/149] Loss_D: 0.6940 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6629 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6812 Loss_G: 0.7214 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6708 Loss_G: 0.7281 acc: 95.3%\n",
      "[EPOCH 850] TEST ACC is : 75.6%\n",
      "[BATCH 106/149] Loss_D: 0.7232 Loss_G: 0.7622 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6998 Loss_G: 0.7320 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6873 Loss_G: 0.7238 acc: 81.2%\n",
      "[BATCH 109/149] Loss_D: 0.7187 Loss_G: 0.7534 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7320 Loss_G: 0.7614 acc: 81.2%\n",
      "[BATCH 111/149] Loss_D: 0.6868 Loss_G: 0.7381 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6938 Loss_G: 0.7443 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7044 Loss_G: 0.7372 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7223 Loss_G: 0.7478 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6912 Loss_G: 0.7373 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6894 Loss_G: 0.7091 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6994 Loss_G: 0.7099 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.7141 Loss_G: 0.7177 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6746 Loss_G: 0.7129 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6475 Loss_G: 0.6995 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.7583 Loss_G: 0.7297 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6976 Loss_G: 0.7304 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6971 Loss_G: 0.7150 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6872 Loss_G: 0.7171 acc: 81.2%\n",
      "[BATCH 125/149] Loss_D: 0.6732 Loss_G: 0.7005 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6811 Loss_G: 0.7114 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.7010 Loss_G: 0.7372 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6637 Loss_G: 0.7321 acc: 95.3%\n",
      "[BATCH 129/149] Loss_D: 0.7151 Loss_G: 0.7238 acc: 95.3%\n",
      "[BATCH 130/149] Loss_D: 0.7347 Loss_G: 0.7417 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6886 Loss_G: 0.7454 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7352 Loss_G: 0.7292 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.7010 Loss_G: 0.7203 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6575 Loss_G: 0.7342 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6980 Loss_G: 0.7235 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6967 Loss_G: 0.7118 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7550 Loss_G: 0.7384 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.6960 Loss_G: 0.7210 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.6985 Loss_G: 0.7250 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.7348 Loss_G: 0.7440 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6933 Loss_G: 0.7165 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6865 Loss_G: 0.7142 acc: 81.2%\n",
      "[BATCH 143/149] Loss_D: 0.6720 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7407 Loss_G: 0.7586 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7130 Loss_G: 0.7271 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7155 Loss_G: 0.7316 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.7016 Loss_G: 0.7147 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.7250 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6995 Loss_G: 0.7253 acc: 89.1%\n",
      "-----THE [6/50] epoch end-----\n",
      "-----THE [7/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7309 Loss_G: 0.7200 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6573 Loss_G: 0.7077 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7226 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6947 Loss_G: 0.7374 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.7114 Loss_G: 0.7262 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7238 Loss_G: 0.7438 acc: 92.2%\n",
      "[EPOCH 900] TEST ACC is : 77.0%\n",
      "[BATCH 7/149] Loss_D: 0.6669 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7101 Loss_G: 0.7146 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6875 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6846 Loss_G: 0.7080 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6547 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7265 Loss_G: 0.7214 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6651 Loss_G: 0.7100 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.7453 Loss_G: 0.7436 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.7058 Loss_G: 0.7286 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6906 Loss_G: 0.7097 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.6996 Loss_G: 0.7185 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6587 Loss_G: 0.7132 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.6963 Loss_G: 0.7190 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.7306 Loss_G: 0.7321 acc: 79.7%\n",
      "[BATCH 21/149] Loss_D: 0.6897 Loss_G: 0.7336 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6776 Loss_G: 0.7201 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6880 Loss_G: 0.7114 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6757 Loss_G: 0.7109 acc: 95.3%\n",
      "[BATCH 25/149] Loss_D: 0.6630 Loss_G: 0.7081 acc: 95.3%\n",
      "[BATCH 26/149] Loss_D: 0.7459 Loss_G: 0.7354 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.6944 Loss_G: 0.7165 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.7277 Loss_G: 0.7195 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6964 Loss_G: 0.7234 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6936 Loss_G: 0.7237 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7297 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.6919 Loss_G: 0.7264 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6617 Loss_G: 0.7198 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6876 Loss_G: 0.7148 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.6856 Loss_G: 0.7266 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7680 Loss_G: 0.7502 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7201 Loss_G: 0.7508 acc: 81.2%\n",
      "[BATCH 38/149] Loss_D: 0.7246 Loss_G: 0.7536 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6971 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.7045 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7007 Loss_G: 0.7027 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.6836 Loss_G: 0.7002 acc: 81.2%\n",
      "[BATCH 43/149] Loss_D: 0.6842 Loss_G: 0.7266 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7212 Loss_G: 0.7273 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6939 Loss_G: 0.7208 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6774 Loss_G: 0.7064 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.7238 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6965 Loss_G: 0.6925 acc: 78.1%\n",
      "[BATCH 49/149] Loss_D: 0.7121 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6756 Loss_G: 0.6972 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6582 Loss_G: 0.7004 acc: 95.3%\n",
      "[BATCH 52/149] Loss_D: 0.6867 Loss_G: 0.7183 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6878 Loss_G: 0.7398 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7052 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6678 Loss_G: 0.7229 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6999 Loss_G: 0.7122 acc: 90.6%\n",
      "[EPOCH 950] TEST ACC is : 76.8%\n",
      "[BATCH 57/149] Loss_D: 0.7057 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6861 Loss_G: 0.6899 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6919 Loss_G: 0.7011 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7123 Loss_G: 0.7317 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6828 Loss_G: 0.7294 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.7660 Loss_G: 0.7539 acc: 79.7%\n",
      "[BATCH 63/149] Loss_D: 0.6800 Loss_G: 0.7747 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7047 Loss_G: 0.7340 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6907 Loss_G: 0.7164 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.7119 Loss_G: 0.7237 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7230 Loss_G: 0.7301 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6851 Loss_G: 0.7350 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7056 Loss_G: 0.7379 acc: 79.7%\n",
      "[BATCH 70/149] Loss_D: 0.6751 Loss_G: 0.7085 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6880 Loss_G: 0.7070 acc: 82.8%\n",
      "[BATCH 72/149] Loss_D: 0.7089 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6857 Loss_G: 0.7105 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6656 Loss_G: 0.7057 acc: 95.3%\n",
      "[BATCH 75/149] Loss_D: 0.7095 Loss_G: 0.7194 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7278 Loss_G: 0.7182 acc: 79.7%\n",
      "[BATCH 77/149] Loss_D: 0.6784 Loss_G: 0.7155 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6662 Loss_G: 0.7382 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.6738 Loss_G: 0.7363 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6989 Loss_G: 0.7595 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6921 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7127 Loss_G: 0.7223 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6990 Loss_G: 0.7151 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.7141 Loss_G: 0.7232 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.7334 Loss_G: 0.7273 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6714 Loss_G: 0.7267 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6642 Loss_G: 0.7086 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.7172 Loss_G: 0.7125 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7148 Loss_G: 0.7258 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7462 Loss_G: 0.7202 acc: 78.1%\n",
      "[BATCH 91/149] Loss_D: 0.6810 Loss_G: 0.7218 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6636 Loss_G: 0.7176 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7303 Loss_G: 0.7392 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6664 Loss_G: 0.7225 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6977 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6811 Loss_G: 0.7307 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.7543 Loss_G: 0.7429 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.7691 Loss_G: 0.7507 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7187 Loss_G: 0.7608 acc: 82.8%\n",
      "[BATCH 100/149] Loss_D: 0.6909 Loss_G: 0.7139 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6874 Loss_G: 0.7059 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6523 Loss_G: 0.7056 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6796 Loss_G: 0.7096 acc: 93.8%\n",
      "[BATCH 104/149] Loss_D: 0.6788 Loss_G: 0.7372 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7122 Loss_G: 0.7364 acc: 95.3%\n",
      "[BATCH 106/149] Loss_D: 0.6726 Loss_G: 0.7246 acc: 92.2%\n",
      "[EPOCH 1000] TEST ACC is : 77.5%\n",
      "[BATCH 107/149] Loss_D: 0.6840 Loss_G: 0.7253 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7402 Loss_G: 0.7075 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.6624 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6957 Loss_G: 0.7150 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6558 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7019 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6825 Loss_G: 0.7122 acc: 93.8%\n",
      "[BATCH 114/149] Loss_D: 0.6631 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.7315 Loss_G: 0.7454 acc: 95.3%\n",
      "[BATCH 116/149] Loss_D: 0.7028 Loss_G: 0.7382 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6888 Loss_G: 0.7052 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6953 Loss_G: 0.7132 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7066 Loss_G: 0.7224 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.6953 Loss_G: 0.7355 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6541 Loss_G: 0.7085 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.7076 Loss_G: 0.7220 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7058 Loss_G: 0.7017 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6872 Loss_G: 0.7050 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7050 Loss_G: 0.7082 acc: 95.3%\n",
      "[BATCH 126/149] Loss_D: 0.6922 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6862 Loss_G: 0.7260 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6598 Loss_G: 0.6984 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6619 Loss_G: 0.7080 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6972 Loss_G: 0.7031 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6876 Loss_G: 0.7275 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7180 Loss_G: 0.7514 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6991 Loss_G: 0.7237 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6877 Loss_G: 0.7179 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6823 Loss_G: 0.7055 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6965 Loss_G: 0.7208 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.6754 Loss_G: 0.7257 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6974 Loss_G: 0.7202 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.6712 Loss_G: 0.7282 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6897 Loss_G: 0.7046 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.6863 Loss_G: 0.6981 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6738 Loss_G: 0.7120 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.7131 Loss_G: 0.7286 acc: 95.3%\n",
      "[BATCH 144/149] Loss_D: 0.6869 Loss_G: 0.7249 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6563 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.7193 Loss_G: 0.7116 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.7064 Loss_G: 0.7217 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7149 Loss_G: 0.7113 acc: 81.2%\n",
      "[BATCH 149/149] Loss_D: 0.6983 Loss_G: 0.7432 acc: 89.1%\n",
      "-----THE [7/50] epoch end-----\n",
      "-----THE [8/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7032 Loss_G: 0.7362 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6689 Loss_G: 0.7170 acc: 98.4%\n",
      "[BATCH 3/149] Loss_D: 0.6796 Loss_G: 0.7103 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7386 Loss_G: 0.7396 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7047 Loss_G: 0.7577 acc: 95.3%\n",
      "[BATCH 6/149] Loss_D: 0.6818 Loss_G: 0.7372 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.6955 Loss_G: 0.7547 acc: 87.5%\n",
      "[EPOCH 1050] TEST ACC is : 77.5%\n",
      "[BATCH 8/149] Loss_D: 0.7364 Loss_G: 0.7208 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7549 Loss_G: 0.7269 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.7038 Loss_G: 0.7348 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6838 Loss_G: 0.7277 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6608 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6934 Loss_G: 0.7299 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6879 Loss_G: 0.7195 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7234 Loss_G: 0.7229 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6723 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6656 Loss_G: 0.7127 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6921 Loss_G: 0.7093 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6706 Loss_G: 0.7050 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6582 Loss_G: 0.6942 acc: 93.8%\n",
      "[BATCH 21/149] Loss_D: 0.6749 Loss_G: 0.7129 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6738 Loss_G: 0.7098 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6918 Loss_G: 0.7127 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.7101 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7258 Loss_G: 0.7271 acc: 98.4%\n",
      "[BATCH 26/149] Loss_D: 0.6669 Loss_G: 0.7200 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6882 Loss_G: 0.7119 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6805 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6588 Loss_G: 0.7400 acc: 96.9%\n",
      "[BATCH 30/149] Loss_D: 0.7078 Loss_G: 0.7321 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.6793 Loss_G: 0.7207 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.7052 Loss_G: 0.7168 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.7050 Loss_G: 0.7034 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.6799 Loss_G: 0.6984 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.6946 Loss_G: 0.7055 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.7029 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6666 Loss_G: 0.6968 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6628 Loss_G: 0.6916 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.6637 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6990 Loss_G: 0.7184 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6952 Loss_G: 0.7068 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6881 Loss_G: 0.7123 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6839 Loss_G: 0.7058 acc: 82.8%\n",
      "[BATCH 44/149] Loss_D: 0.6628 Loss_G: 0.6930 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.6759 Loss_G: 0.6995 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6862 Loss_G: 0.7073 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6911 Loss_G: 0.7336 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6675 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7481 Loss_G: 0.7469 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.6874 Loss_G: 0.7476 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6758 Loss_G: 0.7403 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.7312 Loss_G: 0.7471 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6769 Loss_G: 0.7275 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6726 Loss_G: 0.7099 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.7026 Loss_G: 0.7089 acc: 78.1%\n",
      "[BATCH 56/149] Loss_D: 0.7186 Loss_G: 0.7240 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.6948 Loss_G: 0.7266 acc: 89.1%\n",
      "[EPOCH 1100] TEST ACC is : 76.0%\n",
      "[BATCH 58/149] Loss_D: 0.7199 Loss_G: 0.7261 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6727 Loss_G: 0.7080 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7144 Loss_G: 0.7126 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7137 Loss_G: 0.7337 acc: 93.8%\n",
      "[BATCH 62/149] Loss_D: 0.6645 Loss_G: 0.7288 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7073 Loss_G: 0.7200 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6841 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7033 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.7042 Loss_G: 0.7107 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.6939 Loss_G: 0.7070 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6918 Loss_G: 0.7264 acc: 95.3%\n",
      "[BATCH 69/149] Loss_D: 0.6963 Loss_G: 0.7238 acc: 95.3%\n",
      "[BATCH 70/149] Loss_D: 0.7367 Loss_G: 0.7161 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6706 Loss_G: 0.7184 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6712 Loss_G: 0.7039 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7091 Loss_G: 0.7283 acc: 95.3%\n",
      "[BATCH 74/149] Loss_D: 0.6456 Loss_G: 0.7123 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7305 Loss_G: 0.7097 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7562 Loss_G: 0.7628 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.7027 Loss_G: 0.7471 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6658 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.7135 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6863 Loss_G: 0.7265 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6782 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.7024 Loss_G: 0.7187 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.7579 Loss_G: 0.7661 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7045 Loss_G: 0.7513 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.7166 Loss_G: 0.7322 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.6896 Loss_G: 0.7201 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7374 Loss_G: 0.7335 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.7085 Loss_G: 0.7213 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.6884 Loss_G: 0.7164 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6983 Loss_G: 0.7422 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6768 Loss_G: 0.7247 acc: 93.8%\n",
      "[BATCH 92/149] Loss_D: 0.7221 Loss_G: 0.7265 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7077 Loss_G: 0.7599 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7206 Loss_G: 0.7600 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.7051 Loss_G: 0.7382 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7205 Loss_G: 0.7398 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.7072 Loss_G: 0.7357 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.7160 Loss_G: 0.7244 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7011 Loss_G: 0.7304 acc: 95.3%\n",
      "[BATCH 100/149] Loss_D: 0.6914 Loss_G: 0.7242 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6778 Loss_G: 0.6974 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6784 Loss_G: 0.6953 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6913 Loss_G: 0.6948 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.7114 Loss_G: 0.7021 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6952 Loss_G: 0.7365 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.6939 Loss_G: 0.7207 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.6959 Loss_G: 0.7140 acc: 84.4%\n",
      "[EPOCH 1150] TEST ACC is : 77.0%\n",
      "[BATCH 108/149] Loss_D: 0.6890 Loss_G: 0.7083 acc: 84.4%\n",
      "[BATCH 109/149] Loss_D: 0.6933 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.6813 Loss_G: 0.7005 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6884 Loss_G: 0.7103 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7081 Loss_G: 0.7287 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6970 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7204 Loss_G: 0.7339 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6594 Loss_G: 0.7237 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6972 Loss_G: 0.7294 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6901 Loss_G: 0.7328 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7235 Loss_G: 0.7392 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.6953 Loss_G: 0.7291 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.6801 Loss_G: 0.7221 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7369 Loss_G: 0.7240 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.8004 Loss_G: 0.7416 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6915 Loss_G: 0.7296 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.7303 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6879 Loss_G: 0.7009 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6841 Loss_G: 0.7178 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.6822 Loss_G: 0.7258 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6734 Loss_G: 0.7111 acc: 93.8%\n",
      "[BATCH 129/149] Loss_D: 0.7092 Loss_G: 0.7199 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6623 Loss_G: 0.6888 acc: 82.8%\n",
      "[BATCH 131/149] Loss_D: 0.7064 Loss_G: 0.6911 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6903 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6881 Loss_G: 0.6927 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.7056 Loss_G: 0.7001 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6780 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7116 Loss_G: 0.7354 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7151 Loss_G: 0.7296 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6646 Loss_G: 0.7083 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7314 Loss_G: 0.7303 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6845 Loss_G: 0.7330 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6579 Loss_G: 0.7181 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6663 Loss_G: 0.7012 acc: 79.7%\n",
      "[BATCH 143/149] Loss_D: 0.7579 Loss_G: 0.7345 acc: 79.7%\n",
      "[BATCH 144/149] Loss_D: 0.6901 Loss_G: 0.7234 acc: 82.8%\n",
      "[BATCH 145/149] Loss_D: 0.6841 Loss_G: 0.7290 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7111 Loss_G: 0.7239 acc: 79.7%\n",
      "[BATCH 147/149] Loss_D: 0.6767 Loss_G: 0.7028 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6781 Loss_G: 0.7097 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6576 Loss_G: 0.6952 acc: 79.7%\n",
      "-----THE [8/50] epoch end-----\n",
      "-----THE [9/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7291 Loss_G: 0.7054 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6770 Loss_G: 0.7198 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7084 Loss_G: 0.7150 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.6858 Loss_G: 0.7148 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.6646 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7053 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7331 Loss_G: 0.7288 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6650 Loss_G: 0.7246 acc: 95.3%\n",
      "[EPOCH 1200] TEST ACC is : 77.1%\n",
      "[BATCH 9/149] Loss_D: 0.6951 Loss_G: 0.7250 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.6756 Loss_G: 0.7172 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7017 Loss_G: 0.7175 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6993 Loss_G: 0.7309 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.7404 Loss_G: 0.7569 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6870 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6960 Loss_G: 0.7311 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6765 Loss_G: 0.7118 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6741 Loss_G: 0.7062 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6856 Loss_G: 0.7077 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6659 Loss_G: 0.7152 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.7215 Loss_G: 0.7201 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6883 Loss_G: 0.7204 acc: 95.3%\n",
      "[BATCH 22/149] Loss_D: 0.7007 Loss_G: 0.7114 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6961 Loss_G: 0.7131 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.6848 Loss_G: 0.7338 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7371 Loss_G: 0.7200 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6756 Loss_G: 0.7128 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6828 Loss_G: 0.7189 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.7215 Loss_G: 0.7435 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6922 Loss_G: 0.7441 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7067 Loss_G: 0.7288 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6864 Loss_G: 0.7181 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6823 Loss_G: 0.7123 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6930 Loss_G: 0.7024 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.7066 Loss_G: 0.7115 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7092 Loss_G: 0.7148 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6753 Loss_G: 0.7152 acc: 81.2%\n",
      "[BATCH 37/149] Loss_D: 0.7365 Loss_G: 0.7528 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.7032 Loss_G: 0.7399 acc: 81.2%\n",
      "[BATCH 39/149] Loss_D: 0.6999 Loss_G: 0.7447 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7346 Loss_G: 0.7373 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.7168 Loss_G: 0.7340 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6921 Loss_G: 0.7182 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.7164 Loss_G: 0.7247 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6907 Loss_G: 0.7214 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6833 Loss_G: 0.7091 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6718 Loss_G: 0.6960 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6505 Loss_G: 0.7022 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6580 Loss_G: 0.7189 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6622 Loss_G: 0.7007 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6914 Loss_G: 0.6904 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6973 Loss_G: 0.7177 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.6551 Loss_G: 0.7408 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6925 Loss_G: 0.7281 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.6915 Loss_G: 0.7297 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7299 Loss_G: 0.7189 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6870 Loss_G: 0.7190 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7057 Loss_G: 0.7184 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7229 Loss_G: 0.7156 acc: 82.8%\n",
      "[EPOCH 1250] TEST ACC is : 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.7497 Loss_G: 0.7415 acc: 96.9%\n",
      "[BATCH 60/149] Loss_D: 0.7193 Loss_G: 0.7211 acc: 95.3%\n",
      "[BATCH 61/149] Loss_D: 0.6579 Loss_G: 0.6900 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7184 Loss_G: 0.7307 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6854 Loss_G: 0.7192 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.6948 Loss_G: 0.7092 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6784 Loss_G: 0.6959 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6793 Loss_G: 0.7201 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.6798 Loss_G: 0.7466 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.6969 Loss_G: 0.7519 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.6679 Loss_G: 0.7507 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6936 Loss_G: 0.7179 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.7294 Loss_G: 0.7350 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6568 Loss_G: 0.7271 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.7049 Loss_G: 0.7277 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.6854 Loss_G: 0.7152 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6768 Loss_G: 0.7086 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6821 Loss_G: 0.7051 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.7214 Loss_G: 0.7118 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6596 Loss_G: 0.7013 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.7049 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6820 Loss_G: 0.6964 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7108 Loss_G: 0.7063 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7411 Loss_G: 0.7528 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6960 Loss_G: 0.7687 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6945 Loss_G: 0.7349 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.7047 Loss_G: 0.7089 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.7108 Loss_G: 0.7329 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.6689 Loss_G: 0.7268 acc: 96.9%\n",
      "[BATCH 88/149] Loss_D: 0.6788 Loss_G: 0.7258 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6668 Loss_G: 0.7005 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6586 Loss_G: 0.7278 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6784 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6764 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7192 Loss_G: 0.7407 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6703 Loss_G: 0.7210 acc: 93.8%\n",
      "[BATCH 95/149] Loss_D: 0.7213 Loss_G: 0.7314 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6620 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6959 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6988 Loss_G: 0.7232 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7330 Loss_G: 0.7229 acc: 78.1%\n",
      "[BATCH 100/149] Loss_D: 0.7225 Loss_G: 0.7421 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.6885 Loss_G: 0.7071 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6761 Loss_G: 0.7219 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7010 Loss_G: 0.7263 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.6940 Loss_G: 0.7297 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.7010 Loss_G: 0.7225 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7442 Loss_G: 0.7268 acc: 79.7%\n",
      "[BATCH 107/149] Loss_D: 0.6868 Loss_G: 0.6967 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.6882 Loss_G: 0.7230 acc: 85.9%\n",
      "[EPOCH 1300] TEST ACC is : 76.0%\n",
      "[BATCH 109/149] Loss_D: 0.7245 Loss_G: 0.7369 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6511 Loss_G: 0.7247 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6883 Loss_G: 0.7022 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6687 Loss_G: 0.7271 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6841 Loss_G: 0.7071 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7677 Loss_G: 0.7163 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.6999 Loss_G: 0.7241 acc: 82.8%\n",
      "[BATCH 116/149] Loss_D: 0.7019 Loss_G: 0.7384 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6850 Loss_G: 0.7252 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7251 Loss_G: 0.7374 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7032 Loss_G: 0.7302 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7211 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6956 Loss_G: 0.7114 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7354 Loss_G: 0.7051 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.7055 Loss_G: 0.7337 acc: 95.3%\n",
      "[BATCH 124/149] Loss_D: 0.7435 Loss_G: 0.7742 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6709 Loss_G: 0.7226 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7005 Loss_G: 0.7240 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.7123 Loss_G: 0.7431 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.7042 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.7031 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6873 Loss_G: 0.7241 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6672 Loss_G: 0.7219 acc: 95.3%\n",
      "[BATCH 132/149] Loss_D: 0.6984 Loss_G: 0.7180 acc: 79.7%\n",
      "[BATCH 133/149] Loss_D: 0.6991 Loss_G: 0.7068 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6563 Loss_G: 0.6996 acc: 95.3%\n",
      "[BATCH 135/149] Loss_D: 0.7234 Loss_G: 0.6968 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6750 Loss_G: 0.7068 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6470 Loss_G: 0.6798 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.6989 Loss_G: 0.6950 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6726 Loss_G: 0.7189 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7066 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7073 Loss_G: 0.7169 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.7003 Loss_G: 0.7327 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.7102 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.7245 Loss_G: 0.7559 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7205 Loss_G: 0.7335 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6577 Loss_G: 0.7040 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6971 Loss_G: 0.7017 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.7007 Loss_G: 0.6995 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6881 Loss_G: 0.7049 acc: 84.4%\n",
      "-----THE [9/50] epoch end-----\n",
      "-----THE [10/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6698 Loss_G: 0.6919 acc: 82.8%\n",
      "[BATCH 2/149] Loss_D: 0.6680 Loss_G: 0.6953 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6973 Loss_G: 0.7190 acc: 81.2%\n",
      "[BATCH 4/149] Loss_D: 0.6955 Loss_G: 0.7208 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6811 Loss_G: 0.7357 acc: 93.8%\n",
      "[BATCH 6/149] Loss_D: 0.6855 Loss_G: 0.7057 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6910 Loss_G: 0.6982 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6997 Loss_G: 0.7078 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.6822 Loss_G: 0.7166 acc: 93.8%\n",
      "[EPOCH 1350] TEST ACC is : 76.2%\n",
      "[BATCH 10/149] Loss_D: 0.6717 Loss_G: 0.7132 acc: 95.3%\n",
      "[BATCH 11/149] Loss_D: 0.6892 Loss_G: 0.7161 acc: 79.7%\n",
      "[BATCH 12/149] Loss_D: 0.7203 Loss_G: 0.7340 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6785 Loss_G: 0.7313 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.7010 Loss_G: 0.7392 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6492 Loss_G: 0.7085 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7142 Loss_G: 0.7140 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.7321 Loss_G: 0.7357 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6795 Loss_G: 0.7150 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6860 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6871 Loss_G: 0.7122 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7018 Loss_G: 0.7123 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6892 Loss_G: 0.7098 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.7052 Loss_G: 0.7274 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6692 Loss_G: 0.7139 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.6971 Loss_G: 0.7166 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6614 Loss_G: 0.7026 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7084 Loss_G: 0.7010 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6853 Loss_G: 0.7271 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7480 Loss_G: 0.7589 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6808 Loss_G: 0.7609 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6760 Loss_G: 0.7398 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6678 Loss_G: 0.7182 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.7061 Loss_G: 0.7086 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7178 Loss_G: 0.7146 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7040 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7238 Loss_G: 0.7303 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6689 Loss_G: 0.6902 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.6736 Loss_G: 0.7168 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6982 Loss_G: 0.7438 acc: 95.3%\n",
      "[BATCH 40/149] Loss_D: 0.6798 Loss_G: 0.7165 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6876 Loss_G: 0.7184 acc: 79.7%\n",
      "[BATCH 42/149] Loss_D: 0.7010 Loss_G: 0.7154 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7455 Loss_G: 0.7514 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6888 Loss_G: 0.7471 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.7402 Loss_G: 0.7408 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6881 Loss_G: 0.7221 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.7218 Loss_G: 0.7256 acc: 82.8%\n",
      "[BATCH 48/149] Loss_D: 0.7010 Loss_G: 0.7318 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7062 Loss_G: 0.7199 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6873 Loss_G: 0.7266 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.7443 Loss_G: 0.7361 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6541 Loss_G: 0.7175 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6436 Loss_G: 0.7075 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.7272 Loss_G: 0.7499 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6875 Loss_G: 0.7314 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6740 Loss_G: 0.7236 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6841 Loss_G: 0.7134 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.7358 Loss_G: 0.7090 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.6717 Loss_G: 0.6983 acc: 82.8%\n",
      "[EPOCH 1400] TEST ACC is : 76.8%\n",
      "[BATCH 60/149] Loss_D: 0.6583 Loss_G: 0.6859 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7147 Loss_G: 0.7046 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6777 Loss_G: 0.7256 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6935 Loss_G: 0.7423 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.6988 Loss_G: 0.7184 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7125 Loss_G: 0.7154 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7147 Loss_G: 0.7350 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.7124 Loss_G: 0.7200 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.6716 Loss_G: 0.7200 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.7108 Loss_G: 0.7293 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.7063 Loss_G: 0.7384 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6962 Loss_G: 0.7350 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6855 Loss_G: 0.7248 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6698 Loss_G: 0.7206 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6756 Loss_G: 0.7028 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6662 Loss_G: 0.7064 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7002 Loss_G: 0.6986 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6933 Loss_G: 0.7110 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.6899 Loss_G: 0.7204 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6679 Loss_G: 0.7058 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7440 Loss_G: 0.7094 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7494 Loss_G: 0.7375 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.6661 Loss_G: 0.7130 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6783 Loss_G: 0.7195 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.7368 Loss_G: 0.7492 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7144 Loss_G: 0.7388 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6993 Loss_G: 0.7341 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6734 Loss_G: 0.7173 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7021 Loss_G: 0.7323 acc: 95.3%\n",
      "[BATCH 89/149] Loss_D: 0.7458 Loss_G: 0.7415 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6659 Loss_G: 0.7060 acc: 81.2%\n",
      "[BATCH 91/149] Loss_D: 0.6625 Loss_G: 0.7162 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6888 Loss_G: 0.7252 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6779 Loss_G: 0.7201 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6811 Loss_G: 0.7286 acc: 95.3%\n",
      "[BATCH 95/149] Loss_D: 0.7488 Loss_G: 0.7326 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.7213 Loss_G: 0.7182 acc: 79.7%\n",
      "[BATCH 97/149] Loss_D: 0.6863 Loss_G: 0.7151 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6743 Loss_G: 0.7139 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7277 Loss_G: 0.7436 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7050 Loss_G: 0.7478 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7436 Loss_G: 0.7513 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6589 Loss_G: 0.7329 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.7384 Loss_G: 0.7481 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7149 Loss_G: 0.7605 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6534 Loss_G: 0.7219 acc: 93.8%\n",
      "[BATCH 106/149] Loss_D: 0.6863 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7160 Loss_G: 0.7302 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7400 Loss_G: 0.7325 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.6622 Loss_G: 0.7292 acc: 90.6%\n",
      "[EPOCH 1450] TEST ACC is : 76.2%\n",
      "[BATCH 110/149] Loss_D: 0.6703 Loss_G: 0.7066 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.7016 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7285 Loss_G: 0.7119 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7149 Loss_G: 0.7259 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.6848 Loss_G: 0.7105 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6644 Loss_G: 0.7217 acc: 93.8%\n",
      "[BATCH 116/149] Loss_D: 0.6846 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7039 Loss_G: 0.7269 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.6706 Loss_G: 0.7075 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7008 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6987 Loss_G: 0.7206 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6751 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6433 Loss_G: 0.7033 acc: 95.3%\n",
      "[BATCH 123/149] Loss_D: 0.6981 Loss_G: 0.7083 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.6799 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6910 Loss_G: 0.7130 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6715 Loss_G: 0.7001 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7118 Loss_G: 0.7124 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6947 Loss_G: 0.7059 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6964 Loss_G: 0.7178 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6612 Loss_G: 0.7204 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6396 Loss_G: 0.6765 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6874 Loss_G: 0.6969 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7118 Loss_G: 0.7066 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6794 Loss_G: 0.7042 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6874 Loss_G: 0.7070 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6709 Loss_G: 0.7244 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.7251 Loss_G: 0.7218 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.7192 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7117 Loss_G: 0.7247 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6983 Loss_G: 0.7379 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.7086 Loss_G: 0.7309 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6938 Loss_G: 0.7340 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6862 Loss_G: 0.7004 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.7165 Loss_G: 0.7304 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.7331 Loss_G: 0.7429 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.7126 Loss_G: 0.7425 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6796 Loss_G: 0.7096 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6861 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.6726 Loss_G: 0.7060 acc: 90.6%\n",
      "-----THE [10/50] epoch end-----\n",
      "-----THE [11/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6855 Loss_G: 0.6920 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6719 Loss_G: 0.7024 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6963 Loss_G: 0.7142 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6430 Loss_G: 0.7031 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7024 Loss_G: 0.7381 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.7103 Loss_G: 0.7466 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6587 Loss_G: 0.7075 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7123 Loss_G: 0.7114 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7022 Loss_G: 0.7317 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7130 Loss_G: 0.7254 acc: 89.1%\n",
      "[EPOCH 1500] TEST ACC is : 76.6%\n",
      "[BATCH 11/149] Loss_D: 0.6628 Loss_G: 0.7122 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6741 Loss_G: 0.7245 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6750 Loss_G: 0.7084 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.7128 Loss_G: 0.7295 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7011 Loss_G: 0.7220 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6829 Loss_G: 0.7152 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7165 Loss_G: 0.7393 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7109 Loss_G: 0.7305 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7053 Loss_G: 0.7347 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.6704 Loss_G: 0.7203 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6631 Loss_G: 0.7014 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.7032 Loss_G: 0.7069 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7040 Loss_G: 0.7100 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6841 Loss_G: 0.7069 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6658 Loss_G: 0.7099 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6926 Loss_G: 0.7062 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6847 Loss_G: 0.7055 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6885 Loss_G: 0.7028 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6557 Loss_G: 0.7038 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.6843 Loss_G: 0.7111 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7236 Loss_G: 0.7431 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6828 Loss_G: 0.7450 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.7349 Loss_G: 0.7309 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6892 Loss_G: 0.7273 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7027 Loss_G: 0.7185 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6430 Loss_G: 0.7230 acc: 93.8%\n",
      "[BATCH 37/149] Loss_D: 0.7097 Loss_G: 0.7227 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6725 Loss_G: 0.7136 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.7129 Loss_G: 0.7167 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.7202 Loss_G: 0.7356 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.6929 Loss_G: 0.7284 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.7232 Loss_G: 0.7444 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.7031 Loss_G: 0.7629 acc: 95.3%\n",
      "[BATCH 44/149] Loss_D: 0.7099 Loss_G: 0.7270 acc: 81.2%\n",
      "[BATCH 45/149] Loss_D: 0.7228 Loss_G: 0.7416 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6689 Loss_G: 0.7080 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6886 Loss_G: 0.6970 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6916 Loss_G: 0.7064 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.7200 Loss_G: 0.6962 acc: 81.2%\n",
      "[BATCH 50/149] Loss_D: 0.6819 Loss_G: 0.6958 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7111 Loss_G: 0.7001 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.6810 Loss_G: 0.7154 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6745 Loss_G: 0.7082 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6750 Loss_G: 0.7237 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6518 Loss_G: 0.7414 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6700 Loss_G: 0.7306 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6960 Loss_G: 0.7117 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.6679 Loss_G: 0.6951 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.7685 Loss_G: 0.7172 acc: 79.7%\n",
      "[BATCH 60/149] Loss_D: 0.6749 Loss_G: 0.7115 acc: 90.6%\n",
      "[EPOCH 1550] TEST ACC is : 76.4%\n",
      "[BATCH 61/149] Loss_D: 0.7030 Loss_G: 0.7443 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6846 Loss_G: 0.7273 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6783 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7520 Loss_G: 0.7580 acc: 93.8%\n",
      "[BATCH 65/149] Loss_D: 0.6955 Loss_G: 0.7744 acc: 93.8%\n",
      "[BATCH 66/149] Loss_D: 0.6928 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7064 Loss_G: 0.7181 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7073 Loss_G: 0.7205 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6903 Loss_G: 0.7104 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6892 Loss_G: 0.7044 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7108 Loss_G: 0.7134 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6825 Loss_G: 0.7088 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6953 Loss_G: 0.7181 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7348 Loss_G: 0.7260 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7458 Loss_G: 0.7564 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6886 Loss_G: 0.7458 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6806 Loss_G: 0.7172 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6948 Loss_G: 0.7307 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.6997 Loss_G: 0.7267 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.7001 Loss_G: 0.7195 acc: 79.7%\n",
      "[BATCH 81/149] Loss_D: 0.7211 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6934 Loss_G: 0.7594 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6998 Loss_G: 0.7198 acc: 82.8%\n",
      "[BATCH 84/149] Loss_D: 0.6838 Loss_G: 0.7211 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6522 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6550 Loss_G: 0.7122 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7009 Loss_G: 0.7135 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6964 Loss_G: 0.7283 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.7102 Loss_G: 0.7225 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.6844 Loss_G: 0.7262 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6910 Loss_G: 0.7289 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7014 Loss_G: 0.7391 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7046 Loss_G: 0.7113 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7177 Loss_G: 0.7124 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6749 Loss_G: 0.7341 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.6914 Loss_G: 0.7361 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6843 Loss_G: 0.7319 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7136 Loss_G: 0.7180 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7000 Loss_G: 0.7203 acc: 95.3%\n",
      "[BATCH 100/149] Loss_D: 0.7054 Loss_G: 0.7406 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6528 Loss_G: 0.7264 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7114 Loss_G: 0.7242 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6944 Loss_G: 0.7167 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7130 Loss_G: 0.7137 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6887 Loss_G: 0.7173 acc: 96.9%\n",
      "[BATCH 106/149] Loss_D: 0.7173 Loss_G: 0.7220 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6739 Loss_G: 0.6942 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6756 Loss_G: 0.6853 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7214 Loss_G: 0.6982 acc: 78.1%\n",
      "[BATCH 110/149] Loss_D: 0.8052 Loss_G: 0.7244 acc: 90.6%\n",
      "[EPOCH 1600] TEST ACC is : 76.8%\n",
      "[BATCH 111/149] Loss_D: 0.7044 Loss_G: 0.7323 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7149 Loss_G: 0.7390 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.7323 Loss_G: 0.7496 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7257 Loss_G: 0.7399 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6805 Loss_G: 0.7185 acc: 93.8%\n",
      "[BATCH 116/149] Loss_D: 0.6811 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6857 Loss_G: 0.7083 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6846 Loss_G: 0.7134 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7113 Loss_G: 0.7242 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6939 Loss_G: 0.7505 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6938 Loss_G: 0.7077 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.7003 Loss_G: 0.6919 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.6878 Loss_G: 0.7261 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6809 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7632 Loss_G: 0.7392 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6585 Loss_G: 0.7162 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.6569 Loss_G: 0.6841 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7085 Loss_G: 0.7177 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7192 Loss_G: 0.7069 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7163 Loss_G: 0.7155 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6784 Loss_G: 0.7252 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6915 Loss_G: 0.7152 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7490 Loss_G: 0.7087 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6928 Loss_G: 0.7076 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.6581 Loss_G: 0.7263 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6642 Loss_G: 0.7185 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6844 Loss_G: 0.7061 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6983 Loss_G: 0.7057 acc: 81.2%\n",
      "[BATCH 139/149] Loss_D: 0.7173 Loss_G: 0.7059 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.6800 Loss_G: 0.6982 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.7068 Loss_G: 0.7340 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6765 Loss_G: 0.7196 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6767 Loss_G: 0.7092 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.7095 Loss_G: 0.7116 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6759 Loss_G: 0.7050 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.6711 Loss_G: 0.7052 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6904 Loss_G: 0.7431 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6807 Loss_G: 0.7295 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6981 Loss_G: 0.7228 acc: 87.5%\n",
      "-----THE [11/50] epoch end-----\n",
      "-----THE [12/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6832 Loss_G: 0.7112 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6738 Loss_G: 0.6863 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.7009 Loss_G: 0.7004 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.7187 Loss_G: 0.7215 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7060 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6752 Loss_G: 0.7153 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.7028 Loss_G: 0.7026 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6871 Loss_G: 0.7041 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.7355 Loss_G: 0.7403 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.7040 Loss_G: 0.7338 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.7103 Loss_G: 0.7440 acc: 90.6%\n",
      "[EPOCH 1650] TEST ACC is : 75.6%\n",
      "[BATCH 12/149] Loss_D: 0.6597 Loss_G: 0.7190 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7118 Loss_G: 0.7269 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6807 Loss_G: 0.7261 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.7148 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6988 Loss_G: 0.7291 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6673 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7134 Loss_G: 0.7319 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7035 Loss_G: 0.7333 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7182 Loss_G: 0.7272 acc: 78.1%\n",
      "[BATCH 21/149] Loss_D: 0.6924 Loss_G: 0.7326 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6629 Loss_G: 0.7079 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.7147 Loss_G: 0.7146 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7110 Loss_G: 0.7336 acc: 98.4%\n",
      "[BATCH 25/149] Loss_D: 0.7868 Loss_G: 0.7695 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6589 Loss_G: 0.7218 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7272 Loss_G: 0.7280 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7482 Loss_G: 0.7388 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6825 Loss_G: 0.7178 acc: 93.8%\n",
      "[BATCH 30/149] Loss_D: 0.7293 Loss_G: 0.7179 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6939 Loss_G: 0.7143 acc: 79.7%\n",
      "[BATCH 32/149] Loss_D: 0.7039 Loss_G: 0.7232 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6680 Loss_G: 0.7016 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7055 Loss_G: 0.7156 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7175 Loss_G: 0.7286 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6526 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6660 Loss_G: 0.7123 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6842 Loss_G: 0.7093 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.7249 Loss_G: 0.7127 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6674 Loss_G: 0.6943 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.7163 Loss_G: 0.6976 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6999 Loss_G: 0.7274 acc: 93.8%\n",
      "[BATCH 43/149] Loss_D: 0.7053 Loss_G: 0.7119 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6852 Loss_G: 0.7157 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6989 Loss_G: 0.7195 acc: 82.8%\n",
      "[BATCH 46/149] Loss_D: 0.7132 Loss_G: 0.7198 acc: 82.8%\n",
      "[BATCH 47/149] Loss_D: 0.6813 Loss_G: 0.7135 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6650 Loss_G: 0.7108 acc: 93.8%\n",
      "[BATCH 49/149] Loss_D: 0.6824 Loss_G: 0.6976 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7003 Loss_G: 0.7299 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6949 Loss_G: 0.7141 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.6471 Loss_G: 0.6869 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7224 Loss_G: 0.7115 acc: 81.2%\n",
      "[BATCH 54/149] Loss_D: 0.7004 Loss_G: 0.7024 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6855 Loss_G: 0.7276 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6841 Loss_G: 0.7185 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6960 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6778 Loss_G: 0.7225 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7208 Loss_G: 0.7112 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7002 Loss_G: 0.7055 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7178 Loss_G: 0.7273 acc: 84.4%\n",
      "[EPOCH 1700] TEST ACC is : 75.6%\n",
      "[BATCH 62/149] Loss_D: 0.7047 Loss_G: 0.7272 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6666 Loss_G: 0.7003 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6850 Loss_G: 0.7123 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6894 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7103 Loss_G: 0.7210 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6728 Loss_G: 0.7099 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6647 Loss_G: 0.7320 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.6928 Loss_G: 0.7176 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6834 Loss_G: 0.7495 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7284 Loss_G: 0.7419 acc: 93.8%\n",
      "[BATCH 72/149] Loss_D: 0.7052 Loss_G: 0.7633 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.6850 Loss_G: 0.7284 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.7138 Loss_G: 0.7323 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.6711 Loss_G: 0.7150 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7194 Loss_G: 0.7015 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.7034 Loss_G: 0.6949 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7039 Loss_G: 0.7121 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.6917 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6898 Loss_G: 0.7086 acc: 81.2%\n",
      "[BATCH 81/149] Loss_D: 0.7093 Loss_G: 0.7109 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6951 Loss_G: 0.7138 acc: 93.8%\n",
      "[BATCH 83/149] Loss_D: 0.7157 Loss_G: 0.7133 acc: 82.8%\n",
      "[BATCH 84/149] Loss_D: 0.7109 Loss_G: 0.7088 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.6881 Loss_G: 0.7117 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7155 Loss_G: 0.7236 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.6746 Loss_G: 0.7195 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6692 Loss_G: 0.7014 acc: 81.2%\n",
      "[BATCH 89/149] Loss_D: 0.6569 Loss_G: 0.6960 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6678 Loss_G: 0.7289 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6655 Loss_G: 0.7241 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7254 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.6928 Loss_G: 0.7211 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6740 Loss_G: 0.6937 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7296 Loss_G: 0.6945 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.6863 Loss_G: 0.7083 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7138 Loss_G: 0.7073 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6563 Loss_G: 0.7055 acc: 93.8%\n",
      "[BATCH 99/149] Loss_D: 0.6820 Loss_G: 0.7024 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7411 Loss_G: 0.7366 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.7123 Loss_G: 0.7637 acc: 84.4%\n",
      "[BATCH 102/149] Loss_D: 0.7192 Loss_G: 0.7382 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6941 Loss_G: 0.7256 acc: 95.3%\n",
      "[BATCH 104/149] Loss_D: 0.7035 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7197 Loss_G: 0.7253 acc: 82.8%\n",
      "[BATCH 106/149] Loss_D: 0.6631 Loss_G: 0.7300 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6844 Loss_G: 0.7107 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6974 Loss_G: 0.7106 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6642 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7082 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6899 Loss_G: 0.7331 acc: 89.1%\n",
      "[EPOCH 1750] TEST ACC is : 76.8%\n",
      "[BATCH 112/149] Loss_D: 0.6866 Loss_G: 0.7197 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7055 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6873 Loss_G: 0.7149 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7234 Loss_G: 0.7429 acc: 96.9%\n",
      "[BATCH 116/149] Loss_D: 0.6475 Loss_G: 0.7264 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6775 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6555 Loss_G: 0.7144 acc: 95.3%\n",
      "[BATCH 119/149] Loss_D: 0.6520 Loss_G: 0.7011 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.6814 Loss_G: 0.6888 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6973 Loss_G: 0.6908 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6764 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6713 Loss_G: 0.6932 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6985 Loss_G: 0.7122 acc: 79.7%\n",
      "[BATCH 125/149] Loss_D: 0.6617 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6608 Loss_G: 0.7094 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6870 Loss_G: 0.7208 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6814 Loss_G: 0.7446 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7359 Loss_G: 0.7398 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7042 Loss_G: 0.7252 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7008 Loss_G: 0.7125 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6844 Loss_G: 0.6931 acc: 81.2%\n",
      "[BATCH 133/149] Loss_D: 0.7462 Loss_G: 0.7227 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6857 Loss_G: 0.7314 acc: 93.8%\n",
      "[BATCH 135/149] Loss_D: 0.6943 Loss_G: 0.7246 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7326 Loss_G: 0.7334 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6488 Loss_G: 0.7180 acc: 95.3%\n",
      "[BATCH 138/149] Loss_D: 0.7022 Loss_G: 0.7249 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6669 Loss_G: 0.7377 acc: 95.3%\n",
      "[BATCH 140/149] Loss_D: 0.7187 Loss_G: 0.7448 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6789 Loss_G: 0.7132 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7075 Loss_G: 0.7287 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6726 Loss_G: 0.7087 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6692 Loss_G: 0.7203 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.6847 Loss_G: 0.7168 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6954 Loss_G: 0.7310 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6888 Loss_G: 0.7326 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6796 Loss_G: 0.7122 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7183 Loss_G: 0.7631 acc: 95.3%\n",
      "-----THE [12/50] epoch end-----\n",
      "-----THE [13/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6886 Loss_G: 0.7340 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6910 Loss_G: 0.7195 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7164 Loss_G: 0.7370 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6998 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.6717 Loss_G: 0.7048 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6725 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6896 Loss_G: 0.7174 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.7083 Loss_G: 0.7140 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6895 Loss_G: 0.6980 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.6850 Loss_G: 0.6982 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6826 Loss_G: 0.7071 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6771 Loss_G: 0.7203 acc: 96.9%\n",
      "[EPOCH 1800] TEST ACC is : 77.0%\n",
      "[BATCH 13/149] Loss_D: 0.6709 Loss_G: 0.7127 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6915 Loss_G: 0.7097 acc: 81.2%\n",
      "[BATCH 15/149] Loss_D: 0.7153 Loss_G: 0.7248 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6621 Loss_G: 0.7269 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6772 Loss_G: 0.7176 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6896 Loss_G: 0.7221 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6512 Loss_G: 0.6927 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6778 Loss_G: 0.7113 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6918 Loss_G: 0.7069 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.7168 Loss_G: 0.7035 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.6524 Loss_G: 0.7212 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.6765 Loss_G: 0.7209 acc: 95.3%\n",
      "[BATCH 25/149] Loss_D: 0.7266 Loss_G: 0.7528 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6679 Loss_G: 0.6963 acc: 82.8%\n",
      "[BATCH 27/149] Loss_D: 0.6656 Loss_G: 0.7028 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.7199 Loss_G: 0.7087 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.6954 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7022 Loss_G: 0.7441 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6867 Loss_G: 0.7262 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6821 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6870 Loss_G: 0.7065 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6986 Loss_G: 0.7230 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6774 Loss_G: 0.7267 acc: 93.8%\n",
      "[BATCH 36/149] Loss_D: 0.6859 Loss_G: 0.7357 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.7167 Loss_G: 0.7351 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6608 Loss_G: 0.7014 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.6753 Loss_G: 0.6978 acc: 95.3%\n",
      "[BATCH 40/149] Loss_D: 0.7206 Loss_G: 0.7089 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.6777 Loss_G: 0.7315 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.7000 Loss_G: 0.7305 acc: 93.8%\n",
      "[BATCH 43/149] Loss_D: 0.7025 Loss_G: 0.7512 acc: 95.3%\n",
      "[BATCH 44/149] Loss_D: 0.7577 Loss_G: 0.7538 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6997 Loss_G: 0.7166 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.7114 Loss_G: 0.7187 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6885 Loss_G: 0.7138 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.7014 Loss_G: 0.7253 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7217 Loss_G: 0.7273 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6793 Loss_G: 0.7128 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6797 Loss_G: 0.7054 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6852 Loss_G: 0.7378 acc: 100.0%\n",
      "[BATCH 53/149] Loss_D: 0.7469 Loss_G: 0.7433 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.7098 Loss_G: 0.7324 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6824 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6781 Loss_G: 0.7235 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.6668 Loss_G: 0.7218 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6835 Loss_G: 0.7390 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.7100 Loss_G: 0.7770 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6781 Loss_G: 0.7591 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7017 Loss_G: 0.7336 acc: 82.8%\n",
      "[BATCH 62/149] Loss_D: 0.7368 Loss_G: 0.7249 acc: 93.8%\n",
      "[EPOCH 1850] TEST ACC is : 77.5%\n",
      "[BATCH 63/149] Loss_D: 0.7122 Loss_G: 0.7215 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7025 Loss_G: 0.7337 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7007 Loss_G: 0.7223 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6652 Loss_G: 0.7092 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6911 Loss_G: 0.7207 acc: 95.3%\n",
      "[BATCH 68/149] Loss_D: 0.6455 Loss_G: 0.7014 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6981 Loss_G: 0.7046 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6974 Loss_G: 0.7003 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6629 Loss_G: 0.7064 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6859 Loss_G: 0.6972 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6883 Loss_G: 0.6888 acc: 78.1%\n",
      "[BATCH 74/149] Loss_D: 0.6775 Loss_G: 0.7204 acc: 96.9%\n",
      "[BATCH 75/149] Loss_D: 0.6764 Loss_G: 0.7218 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6823 Loss_G: 0.7164 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6880 Loss_G: 0.7165 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6983 Loss_G: 0.7054 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.6776 Loss_G: 0.7343 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.6770 Loss_G: 0.7266 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6696 Loss_G: 0.7019 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7272 Loss_G: 0.7116 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.6933 Loss_G: 0.7583 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7100 Loss_G: 0.7398 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7246 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6889 Loss_G: 0.7367 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7164 Loss_G: 0.7340 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6934 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6822 Loss_G: 0.6974 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.7226 Loss_G: 0.7412 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7291 Loss_G: 0.7525 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.6969 Loss_G: 0.7125 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7045 Loss_G: 0.7309 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.7195 Loss_G: 0.7412 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7075 Loss_G: 0.7241 acc: 81.2%\n",
      "[BATCH 96/149] Loss_D: 0.6905 Loss_G: 0.7136 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6802 Loss_G: 0.7122 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.7398 Loss_G: 0.7395 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6689 Loss_G: 0.7093 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.6885 Loss_G: 0.7035 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6869 Loss_G: 0.6867 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6917 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7183 Loss_G: 0.7201 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7113 Loss_G: 0.7279 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.6494 Loss_G: 0.6919 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6599 Loss_G: 0.7155 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7029 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6964 Loss_G: 0.7192 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6814 Loss_G: 0.7074 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.6911 Loss_G: 0.7074 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.7149 Loss_G: 0.7382 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.6747 Loss_G: 0.7230 acc: 85.9%\n",
      "[EPOCH 1900] TEST ACC is : 76.8%\n",
      "[BATCH 113/149] Loss_D: 0.6876 Loss_G: 0.7135 acc: 95.3%\n",
      "[BATCH 114/149] Loss_D: 0.6719 Loss_G: 0.7139 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6859 Loss_G: 0.7417 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6807 Loss_G: 0.7214 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6927 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6911 Loss_G: 0.7235 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6956 Loss_G: 0.7229 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7108 Loss_G: 0.7522 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7143 Loss_G: 0.7509 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.7257 Loss_G: 0.7600 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6708 Loss_G: 0.7150 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.6902 Loss_G: 0.7048 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6752 Loss_G: 0.6915 acc: 79.7%\n",
      "[BATCH 126/149] Loss_D: 0.7484 Loss_G: 0.7131 acc: 79.7%\n",
      "[BATCH 127/149] Loss_D: 0.6942 Loss_G: 0.7229 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7016 Loss_G: 0.7242 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.7229 Loss_G: 0.7340 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.7361 Loss_G: 0.7407 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6729 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6644 Loss_G: 0.7011 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6720 Loss_G: 0.7029 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.6612 Loss_G: 0.7168 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7344 Loss_G: 0.7321 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6667 Loss_G: 0.7062 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.7280 Loss_G: 0.7224 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6989 Loss_G: 0.7244 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.7204 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7490 Loss_G: 0.7330 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.7322 Loss_G: 0.7382 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6531 Loss_G: 0.7283 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.7233 Loss_G: 0.7436 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.6931 Loss_G: 0.7235 acc: 81.2%\n",
      "[BATCH 145/149] Loss_D: 0.7024 Loss_G: 0.7360 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6835 Loss_G: 0.7074 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.7258 Loss_G: 0.7225 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.7158 Loss_G: 0.7392 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6888 Loss_G: 0.7065 acc: 87.5%\n",
      "-----THE [13/50] epoch end-----\n",
      "-----THE [14/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6936 Loss_G: 0.7015 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7123 Loss_G: 0.7178 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.6912 Loss_G: 0.7137 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6942 Loss_G: 0.7236 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.7301 Loss_G: 0.7291 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6791 Loss_G: 0.7087 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.6966 Loss_G: 0.7248 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6597 Loss_G: 0.7315 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6825 Loss_G: 0.7150 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.6490 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6895 Loss_G: 0.7054 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6714 Loss_G: 0.7235 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.6985 Loss_G: 0.7361 acc: 92.2%\n",
      "[EPOCH 1950] TEST ACC is : 76.6%\n",
      "[BATCH 14/149] Loss_D: 0.6723 Loss_G: 0.7273 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6707 Loss_G: 0.6919 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6845 Loss_G: 0.7048 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6546 Loss_G: 0.7213 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6745 Loss_G: 0.7096 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.7478 Loss_G: 0.6983 acc: 81.2%\n",
      "[BATCH 20/149] Loss_D: 0.6845 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7117 Loss_G: 0.6986 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6739 Loss_G: 0.7186 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6638 Loss_G: 0.6915 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6885 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6894 Loss_G: 0.7032 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6928 Loss_G: 0.7022 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6810 Loss_G: 0.6964 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.6893 Loss_G: 0.6947 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6898 Loss_G: 0.7268 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7231 Loss_G: 0.7166 acc: 78.1%\n",
      "[BATCH 31/149] Loss_D: 0.7138 Loss_G: 0.7285 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6603 Loss_G: 0.7012 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.7019 Loss_G: 0.7010 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7420 Loss_G: 0.7471 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7155 Loss_G: 0.7419 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6839 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6763 Loss_G: 0.7082 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.7592 Loss_G: 0.7245 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7027 Loss_G: 0.7155 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6453 Loss_G: 0.7107 acc: 98.4%\n",
      "[BATCH 41/149] Loss_D: 0.6718 Loss_G: 0.7098 acc: 95.3%\n",
      "[BATCH 42/149] Loss_D: 0.6948 Loss_G: 0.7158 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6909 Loss_G: 0.7144 acc: 82.8%\n",
      "[BATCH 44/149] Loss_D: 0.6845 Loss_G: 0.7240 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6865 Loss_G: 0.7106 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.7076 Loss_G: 0.7403 acc: 95.3%\n",
      "[BATCH 47/149] Loss_D: 0.7145 Loss_G: 0.7391 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7318 Loss_G: 0.7274 acc: 81.2%\n",
      "[BATCH 49/149] Loss_D: 0.6779 Loss_G: 0.7485 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6828 Loss_G: 0.7384 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.6760 Loss_G: 0.7160 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.6770 Loss_G: 0.7060 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.7474 Loss_G: 0.7288 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6981 Loss_G: 0.7199 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6954 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6932 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6988 Loss_G: 0.7265 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.7192 Loss_G: 0.7209 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6987 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6872 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.7212 Loss_G: 0.7159 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7171 Loss_G: 0.7177 acc: 95.3%\n",
      "[BATCH 63/149] Loss_D: 0.7636 Loss_G: 0.7321 acc: 85.9%\n",
      "[EPOCH 2000] TEST ACC is : 77.1%\n",
      "[BATCH 64/149] Loss_D: 0.7216 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6744 Loss_G: 0.7076 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6975 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6890 Loss_G: 0.7142 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6767 Loss_G: 0.7349 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6499 Loss_G: 0.7093 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6767 Loss_G: 0.7060 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7033 Loss_G: 0.7216 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.7062 Loss_G: 0.7242 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7087 Loss_G: 0.7240 acc: 93.8%\n",
      "[BATCH 74/149] Loss_D: 0.7015 Loss_G: 0.7229 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7206 Loss_G: 0.7366 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.7142 Loss_G: 0.7346 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.6966 Loss_G: 0.7171 acc: 81.2%\n",
      "[BATCH 78/149] Loss_D: 0.6568 Loss_G: 0.7167 acc: 95.3%\n",
      "[BATCH 79/149] Loss_D: 0.6579 Loss_G: 0.6992 acc: 95.3%\n",
      "[BATCH 80/149] Loss_D: 0.6901 Loss_G: 0.7064 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7178 Loss_G: 0.7198 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.6498 Loss_G: 0.7325 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.7018 Loss_G: 0.7335 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6982 Loss_G: 0.7333 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6921 Loss_G: 0.7246 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.7033 Loss_G: 0.7158 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7193 Loss_G: 0.7156 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.7060 Loss_G: 0.7152 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7063 Loss_G: 0.7379 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6873 Loss_G: 0.7286 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.7041 Loss_G: 0.7335 acc: 95.3%\n",
      "[BATCH 92/149] Loss_D: 0.6795 Loss_G: 0.7129 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7426 Loss_G: 0.7291 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6806 Loss_G: 0.7290 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.7438 Loss_G: 0.7300 acc: 78.1%\n",
      "[BATCH 96/149] Loss_D: 0.7060 Loss_G: 0.7151 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6837 Loss_G: 0.7194 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6698 Loss_G: 0.6997 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6610 Loss_G: 0.7181 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6920 Loss_G: 0.7298 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7234 Loss_G: 0.7335 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6990 Loss_G: 0.7456 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.7005 Loss_G: 0.7088 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.6757 Loss_G: 0.7048 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.7432 Loss_G: 0.7023 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7143 Loss_G: 0.7388 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6839 Loss_G: 0.7287 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6968 Loss_G: 0.7254 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.7074 Loss_G: 0.7174 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.7265 Loss_G: 0.7277 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7122 Loss_G: 0.7358 acc: 79.7%\n",
      "[BATCH 112/149] Loss_D: 0.6682 Loss_G: 0.7342 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6872 Loss_G: 0.7305 acc: 95.3%\n",
      "[EPOCH 2050] TEST ACC is : 77.5%\n",
      "[BATCH 114/149] Loss_D: 0.6999 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6802 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6866 Loss_G: 0.7055 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.6812 Loss_G: 0.7017 acc: 81.2%\n",
      "[BATCH 118/149] Loss_D: 0.7068 Loss_G: 0.7095 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6784 Loss_G: 0.7184 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6745 Loss_G: 0.7006 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6887 Loss_G: 0.7159 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6791 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6845 Loss_G: 0.7167 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.6983 Loss_G: 0.7070 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6967 Loss_G: 0.7099 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.7144 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.6627 Loss_G: 0.7111 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6645 Loss_G: 0.7106 acc: 95.3%\n",
      "[BATCH 129/149] Loss_D: 0.6882 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7210 Loss_G: 0.7281 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7066 Loss_G: 0.7302 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6643 Loss_G: 0.7091 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6710 Loss_G: 0.7191 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7275 Loss_G: 0.7266 acc: 79.7%\n",
      "[BATCH 135/149] Loss_D: 0.6749 Loss_G: 0.7146 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7043 Loss_G: 0.7384 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.7027 Loss_G: 0.7236 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.6576 Loss_G: 0.7209 acc: 96.9%\n",
      "[BATCH 139/149] Loss_D: 0.7036 Loss_G: 0.7157 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7363 Loss_G: 0.7101 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.7178 Loss_G: 0.7091 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7065 Loss_G: 0.7054 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6959 Loss_G: 0.6969 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6651 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6564 Loss_G: 0.7053 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6543 Loss_G: 0.6961 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6677 Loss_G: 0.7109 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.6830 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.6892 Loss_G: 0.7057 acc: 81.2%\n",
      "-----THE [14/50] epoch end-----\n",
      "-----THE [15/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6654 Loss_G: 0.7174 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6957 Loss_G: 0.7131 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.7254 Loss_G: 0.7428 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6853 Loss_G: 0.7263 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6995 Loss_G: 0.7126 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.6816 Loss_G: 0.7026 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7077 Loss_G: 0.7168 acc: 82.8%\n",
      "[BATCH 8/149] Loss_D: 0.6924 Loss_G: 0.7086 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7270 Loss_G: 0.7185 acc: 79.7%\n",
      "[BATCH 10/149] Loss_D: 0.6885 Loss_G: 0.7152 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6976 Loss_G: 0.7252 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6919 Loss_G: 0.7050 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6903 Loss_G: 0.7093 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6675 Loss_G: 0.7118 acc: 92.2%\n",
      "[EPOCH 2100] TEST ACC is : 77.3%\n",
      "[BATCH 15/149] Loss_D: 0.7289 Loss_G: 0.7324 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6880 Loss_G: 0.7198 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7073 Loss_G: 0.7126 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.6716 Loss_G: 0.7115 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.6940 Loss_G: 0.7112 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.6793 Loss_G: 0.7032 acc: 95.3%\n",
      "[BATCH 21/149] Loss_D: 0.7179 Loss_G: 0.7326 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7233 Loss_G: 0.7278 acc: 82.8%\n",
      "[BATCH 23/149] Loss_D: 0.6528 Loss_G: 0.7114 acc: 98.4%\n",
      "[BATCH 24/149] Loss_D: 0.6979 Loss_G: 0.7022 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7157 Loss_G: 0.7196 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6814 Loss_G: 0.7072 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7021 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7205 Loss_G: 0.7475 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.7081 Loss_G: 0.7507 acc: 82.8%\n",
      "[BATCH 30/149] Loss_D: 0.6976 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7272 Loss_G: 0.7281 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7202 Loss_G: 0.7673 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7276 Loss_G: 0.7607 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6846 Loss_G: 0.7337 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7000 Loss_G: 0.7258 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6820 Loss_G: 0.7187 acc: 93.8%\n",
      "[BATCH 37/149] Loss_D: 0.6804 Loss_G: 0.7084 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.7727 Loss_G: 0.7246 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7024 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7201 Loss_G: 0.7126 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6814 Loss_G: 0.7284 acc: 96.9%\n",
      "[BATCH 42/149] Loss_D: 0.6760 Loss_G: 0.7335 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6820 Loss_G: 0.7494 acc: 95.3%\n",
      "[BATCH 44/149] Loss_D: 0.6969 Loss_G: 0.7479 acc: 96.9%\n",
      "[BATCH 45/149] Loss_D: 0.7071 Loss_G: 0.7457 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.6688 Loss_G: 0.7042 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6846 Loss_G: 0.7068 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6949 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6896 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6853 Loss_G: 0.7265 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6773 Loss_G: 0.7275 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6856 Loss_G: 0.7069 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6544 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6868 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6714 Loss_G: 0.7197 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.6682 Loss_G: 0.6819 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7410 Loss_G: 0.7071 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.7068 Loss_G: 0.7284 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.6711 Loss_G: 0.6949 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.6874 Loss_G: 0.6788 acc: 79.7%\n",
      "[BATCH 61/149] Loss_D: 0.6693 Loss_G: 0.7048 acc: 93.8%\n",
      "[BATCH 62/149] Loss_D: 0.7064 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6735 Loss_G: 0.7146 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.7081 Loss_G: 0.7311 acc: 82.8%\n",
      "[EPOCH 2150] TEST ACC is : 76.4%\n",
      "[BATCH 65/149] Loss_D: 0.6856 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6877 Loss_G: 0.7052 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.6650 Loss_G: 0.7048 acc: 93.8%\n",
      "[BATCH 68/149] Loss_D: 0.7547 Loss_G: 0.7319 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6768 Loss_G: 0.7374 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7227 Loss_G: 0.7351 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6678 Loss_G: 0.7078 acc: 96.9%\n",
      "[BATCH 72/149] Loss_D: 0.7222 Loss_G: 0.7269 acc: 95.3%\n",
      "[BATCH 73/149] Loss_D: 0.7365 Loss_G: 0.7502 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7484 Loss_G: 0.7381 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7248 Loss_G: 0.7311 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.7056 Loss_G: 0.7161 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6897 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6891 Loss_G: 0.7323 acc: 95.3%\n",
      "[BATCH 79/149] Loss_D: 0.6782 Loss_G: 0.7288 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6739 Loss_G: 0.7270 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.6573 Loss_G: 0.7077 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6888 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.7162 Loss_G: 0.7150 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6659 Loss_G: 0.7192 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7181 Loss_G: 0.7177 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6741 Loss_G: 0.7177 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6830 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7038 Loss_G: 0.7083 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7036 Loss_G: 0.7184 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6608 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6935 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.7121 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7053 Loss_G: 0.7089 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.7159 Loss_G: 0.7051 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7403 Loss_G: 0.7250 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.6767 Loss_G: 0.7455 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.6907 Loss_G: 0.7123 acc: 79.7%\n",
      "[BATCH 98/149] Loss_D: 0.6748 Loss_G: 0.7121 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6569 Loss_G: 0.6890 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6755 Loss_G: 0.6779 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.6912 Loss_G: 0.6887 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6654 Loss_G: 0.7128 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7329 Loss_G: 0.7216 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.7026 Loss_G: 0.7316 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6915 Loss_G: 0.7046 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.7049 Loss_G: 0.7112 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.7212 Loss_G: 0.7218 acc: 82.8%\n",
      "[BATCH 108/149] Loss_D: 0.6966 Loss_G: 0.7351 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.7374 Loss_G: 0.7382 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.7252 Loss_G: 0.7421 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7049 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6761 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6952 Loss_G: 0.7090 acc: 81.2%\n",
      "[BATCH 114/149] Loss_D: 0.7116 Loss_G: 0.7474 acc: 89.1%\n",
      "[EPOCH 2200] TEST ACC is : 76.2%\n",
      "[BATCH 115/149] Loss_D: 0.6838 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.7040 Loss_G: 0.7153 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7160 Loss_G: 0.7370 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.7055 Loss_G: 0.7342 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.7019 Loss_G: 0.7453 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6840 Loss_G: 0.7382 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6800 Loss_G: 0.7191 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6796 Loss_G: 0.7222 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6968 Loss_G: 0.7092 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6703 Loss_G: 0.6864 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6622 Loss_G: 0.6947 acc: 93.8%\n",
      "[BATCH 126/149] Loss_D: 0.6883 Loss_G: 0.6962 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.6724 Loss_G: 0.7215 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6670 Loss_G: 0.6981 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6690 Loss_G: 0.7000 acc: 93.8%\n",
      "[BATCH 130/149] Loss_D: 0.7014 Loss_G: 0.7172 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.7044 Loss_G: 0.7488 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.6943 Loss_G: 0.7071 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6745 Loss_G: 0.6973 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7014 Loss_G: 0.7089 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7004 Loss_G: 0.7138 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7154 Loss_G: 0.7364 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6884 Loss_G: 0.7230 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.7074 Loss_G: 0.7005 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7159 Loss_G: 0.7064 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6683 Loss_G: 0.6963 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.6691 Loss_G: 0.7004 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7043 Loss_G: 0.7174 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6833 Loss_G: 0.7096 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6529 Loss_G: 0.6885 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7039 Loss_G: 0.7093 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6772 Loss_G: 0.7158 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6978 Loss_G: 0.7335 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6849 Loss_G: 0.7377 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6866 Loss_G: 0.7098 acc: 82.8%\n",
      "-----THE [15/50] epoch end-----\n",
      "-----THE [16/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7288 Loss_G: 0.7219 acc: 95.3%\n",
      "[BATCH 2/149] Loss_D: 0.6638 Loss_G: 0.7211 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7145 Loss_G: 0.7207 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6755 Loss_G: 0.7253 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.6820 Loss_G: 0.7420 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.6953 Loss_G: 0.7193 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6718 Loss_G: 0.7096 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6947 Loss_G: 0.7184 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6916 Loss_G: 0.7076 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.6394 Loss_G: 0.6963 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6600 Loss_G: 0.7096 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6698 Loss_G: 0.7021 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.6908 Loss_G: 0.7010 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.7256 Loss_G: 0.7101 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.7066 Loss_G: 0.7330 acc: 87.5%\n",
      "[EPOCH 2250] TEST ACC is : 76.8%\n",
      "[BATCH 16/149] Loss_D: 0.6810 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.7338 Loss_G: 0.7296 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6703 Loss_G: 0.7334 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.6738 Loss_G: 0.7119 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6393 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6948 Loss_G: 0.7066 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6830 Loss_G: 0.7134 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6998 Loss_G: 0.7219 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6979 Loss_G: 0.7149 acc: 96.9%\n",
      "[BATCH 25/149] Loss_D: 0.6690 Loss_G: 0.7090 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7155 Loss_G: 0.7483 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.6990 Loss_G: 0.7454 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6858 Loss_G: 0.7255 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.6746 Loss_G: 0.7202 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7280 Loss_G: 0.7156 acc: 78.1%\n",
      "[BATCH 31/149] Loss_D: 0.7196 Loss_G: 0.7161 acc: 79.7%\n",
      "[BATCH 32/149] Loss_D: 0.7036 Loss_G: 0.7184 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.6813 Loss_G: 0.7007 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6714 Loss_G: 0.7045 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.7098 Loss_G: 0.7220 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6572 Loss_G: 0.6901 acc: 81.2%\n",
      "[BATCH 37/149] Loss_D: 0.7046 Loss_G: 0.7199 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6825 Loss_G: 0.7188 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.6813 Loss_G: 0.7413 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.7100 Loss_G: 0.7512 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6665 Loss_G: 0.7155 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.7054 Loss_G: 0.7204 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.7122 Loss_G: 0.7259 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6757 Loss_G: 0.7220 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.7029 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6903 Loss_G: 0.7168 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6638 Loss_G: 0.7103 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6705 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6580 Loss_G: 0.7058 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7371 Loss_G: 0.7222 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.6590 Loss_G: 0.7049 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.7196 Loss_G: 0.7369 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.7025 Loss_G: 0.7379 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6586 Loss_G: 0.7037 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.7195 Loss_G: 0.7124 acc: 81.2%\n",
      "[BATCH 56/149] Loss_D: 0.6888 Loss_G: 0.7172 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6925 Loss_G: 0.7119 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6964 Loss_G: 0.7269 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.6890 Loss_G: 0.7076 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6623 Loss_G: 0.7172 acc: 95.3%\n",
      "[BATCH 61/149] Loss_D: 0.7131 Loss_G: 0.7267 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.7010 Loss_G: 0.7259 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7063 Loss_G: 0.7201 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7599 Loss_G: 0.7348 acc: 81.2%\n",
      "[BATCH 65/149] Loss_D: 0.6809 Loss_G: 0.7441 acc: 87.5%\n",
      "[EPOCH 2300] TEST ACC is : 76.6%\n",
      "[BATCH 66/149] Loss_D: 0.6996 Loss_G: 0.7253 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.7140 Loss_G: 0.7393 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6677 Loss_G: 0.7084 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6982 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7155 Loss_G: 0.7230 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.7048 Loss_G: 0.7161 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.7230 Loss_G: 0.7488 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7018 Loss_G: 0.7261 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6637 Loss_G: 0.7179 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.6753 Loss_G: 0.6954 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.6773 Loss_G: 0.7057 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6978 Loss_G: 0.7053 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6897 Loss_G: 0.6988 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.6709 Loss_G: 0.6980 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.7055 Loss_G: 0.7306 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6402 Loss_G: 0.7289 acc: 95.3%\n",
      "[BATCH 82/149] Loss_D: 0.6798 Loss_G: 0.7109 acc: 95.3%\n",
      "[BATCH 83/149] Loss_D: 0.6950 Loss_G: 0.7079 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6740 Loss_G: 0.6939 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.7304 Loss_G: 0.7098 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7104 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.7180 Loss_G: 0.7463 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7310 Loss_G: 0.7456 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6881 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.7287 Loss_G: 0.7199 acc: 81.2%\n",
      "[BATCH 91/149] Loss_D: 0.7002 Loss_G: 0.7118 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.6870 Loss_G: 0.7135 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.6925 Loss_G: 0.7265 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7474 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.7318 Loss_G: 0.7266 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.6742 Loss_G: 0.7289 acc: 96.9%\n",
      "[BATCH 97/149] Loss_D: 0.6605 Loss_G: 0.7104 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6887 Loss_G: 0.7066 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.7035 Loss_G: 0.7122 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6686 Loss_G: 0.7130 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7057 Loss_G: 0.7301 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.7006 Loss_G: 0.7317 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7122 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7198 Loss_G: 0.7406 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6978 Loss_G: 0.7218 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.6977 Loss_G: 0.6937 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6980 Loss_G: 0.7102 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7031 Loss_G: 0.7152 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.7032 Loss_G: 0.7082 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.6606 Loss_G: 0.7075 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.6821 Loss_G: 0.7168 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.6839 Loss_G: 0.7414 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6951 Loss_G: 0.7128 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6851 Loss_G: 0.7047 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6583 Loss_G: 0.6928 acc: 92.2%\n",
      "[EPOCH 2350] TEST ACC is : 77.0%\n",
      "[BATCH 116/149] Loss_D: 0.6869 Loss_G: 0.7111 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6772 Loss_G: 0.7036 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6798 Loss_G: 0.7511 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.7182 Loss_G: 0.7247 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6844 Loss_G: 0.7168 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6623 Loss_G: 0.7254 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.7274 Loss_G: 0.7037 acc: 78.1%\n",
      "[BATCH 123/149] Loss_D: 0.6722 Loss_G: 0.6964 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.7448 Loss_G: 0.7355 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6602 Loss_G: 0.7208 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6984 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6577 Loss_G: 0.7216 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.6913 Loss_G: 0.7103 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.6558 Loss_G: 0.6943 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7093 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6977 Loss_G: 0.7295 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.7171 Loss_G: 0.7390 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6714 Loss_G: 0.7504 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6643 Loss_G: 0.7153 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7393 Loss_G: 0.7255 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7397 Loss_G: 0.7259 acc: 81.2%\n",
      "[BATCH 137/149] Loss_D: 0.7000 Loss_G: 0.7526 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6987 Loss_G: 0.7498 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7044 Loss_G: 0.7464 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7127 Loss_G: 0.7191 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6823 Loss_G: 0.6983 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6718 Loss_G: 0.7093 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.7245 Loss_G: 0.7355 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6586 Loss_G: 0.7091 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7235 Loss_G: 0.7147 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.7002 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6978 Loss_G: 0.7076 acc: 82.8%\n",
      "[BATCH 148/149] Loss_D: 0.6910 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.6758 Loss_G: 0.7211 acc: 87.5%\n",
      "-----THE [16/50] epoch end-----\n",
      "-----THE [17/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6925 Loss_G: 0.7200 acc: 95.3%\n",
      "[BATCH 2/149] Loss_D: 0.6693 Loss_G: 0.6856 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7216 Loss_G: 0.6893 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6759 Loss_G: 0.6961 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6873 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7355 Loss_G: 0.7164 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6938 Loss_G: 0.7216 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7117 Loss_G: 0.7572 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6962 Loss_G: 0.7185 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6941 Loss_G: 0.7357 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6693 Loss_G: 0.7213 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7048 Loss_G: 0.7085 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.7012 Loss_G: 0.6954 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6974 Loss_G: 0.7149 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6886 Loss_G: 0.7178 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7020 Loss_G: 0.7350 acc: 82.8%\n",
      "[EPOCH 2400] TEST ACC is : 76.8%\n",
      "[BATCH 17/149] Loss_D: 0.7029 Loss_G: 0.7210 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6862 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6638 Loss_G: 0.7099 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.7090 Loss_G: 0.7175 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7131 Loss_G: 0.7232 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6875 Loss_G: 0.7230 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.6670 Loss_G: 0.7227 acc: 93.8%\n",
      "[BATCH 24/149] Loss_D: 0.6734 Loss_G: 0.7081 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.7223 Loss_G: 0.7180 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6750 Loss_G: 0.6990 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7265 Loss_G: 0.7127 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.7586 Loss_G: 0.7262 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.6815 Loss_G: 0.7279 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6737 Loss_G: 0.6987 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7216 Loss_G: 0.7184 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.6687 Loss_G: 0.7073 acc: 95.3%\n",
      "[BATCH 33/149] Loss_D: 0.7455 Loss_G: 0.7293 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6885 Loss_G: 0.7377 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6401 Loss_G: 0.7108 acc: 95.3%\n",
      "[BATCH 36/149] Loss_D: 0.6999 Loss_G: 0.7177 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7052 Loss_G: 0.7182 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6853 Loss_G: 0.7289 acc: 96.9%\n",
      "[BATCH 39/149] Loss_D: 0.6721 Loss_G: 0.7245 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6701 Loss_G: 0.6979 acc: 96.9%\n",
      "[BATCH 41/149] Loss_D: 0.7050 Loss_G: 0.7115 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7384 Loss_G: 0.7358 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7179 Loss_G: 0.7620 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6909 Loss_G: 0.7413 acc: 79.7%\n",
      "[BATCH 45/149] Loss_D: 0.6873 Loss_G: 0.7338 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7448 Loss_G: 0.7371 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.7556 Loss_G: 0.7433 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6650 Loss_G: 0.7093 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.6582 Loss_G: 0.6987 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6735 Loss_G: 0.6982 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6729 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6699 Loss_G: 0.7163 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7023 Loss_G: 0.7154 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7034 Loss_G: 0.7255 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6918 Loss_G: 0.7211 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6875 Loss_G: 0.7196 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7077 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.7018 Loss_G: 0.7229 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.7008 Loss_G: 0.7274 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6875 Loss_G: 0.7238 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.6970 Loss_G: 0.7567 acc: 79.7%\n",
      "[BATCH 62/149] Loss_D: 0.7132 Loss_G: 0.7225 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6545 Loss_G: 0.7309 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.7208 Loss_G: 0.7478 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6709 Loss_G: 0.7354 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6897 Loss_G: 0.7088 acc: 89.1%\n",
      "[EPOCH 2450] TEST ACC is : 77.3%\n",
      "[BATCH 67/149] Loss_D: 0.6717 Loss_G: 0.6978 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6845 Loss_G: 0.7098 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6608 Loss_G: 0.7034 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6963 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6716 Loss_G: 0.7225 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7371 Loss_G: 0.7316 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.6894 Loss_G: 0.7247 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.7062 Loss_G: 0.7224 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.7324 Loss_G: 0.7487 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7095 Loss_G: 0.7417 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7143 Loss_G: 0.7268 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6700 Loss_G: 0.6944 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6962 Loss_G: 0.7027 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.6701 Loss_G: 0.7053 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6740 Loss_G: 0.6943 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.7088 Loss_G: 0.7166 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6770 Loss_G: 0.7222 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7193 Loss_G: 0.6988 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6986 Loss_G: 0.6912 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7276 Loss_G: 0.6946 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6519 Loss_G: 0.6983 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6964 Loss_G: 0.7146 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.6805 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6788 Loss_G: 0.7091 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7293 Loss_G: 0.7176 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6803 Loss_G: 0.6996 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7334 Loss_G: 0.7085 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6911 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6904 Loss_G: 0.6972 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7065 Loss_G: 0.7108 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6725 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.7016 Loss_G: 0.7278 acc: 79.7%\n",
      "[BATCH 99/149] Loss_D: 0.6782 Loss_G: 0.7187 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.7104 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6874 Loss_G: 0.6929 acc: 79.7%\n",
      "[BATCH 102/149] Loss_D: 0.6664 Loss_G: 0.6856 acc: 81.2%\n",
      "[BATCH 103/149] Loss_D: 0.6584 Loss_G: 0.6922 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7112 Loss_G: 0.7034 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.6827 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.6803 Loss_G: 0.7151 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.7327 Loss_G: 0.7351 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.6862 Loss_G: 0.7569 acc: 96.9%\n",
      "[BATCH 109/149] Loss_D: 0.7013 Loss_G: 0.7385 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7097 Loss_G: 0.7294 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6978 Loss_G: 0.7207 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.6854 Loss_G: 0.7256 acc: 96.9%\n",
      "[BATCH 113/149] Loss_D: 0.6745 Loss_G: 0.7066 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6885 Loss_G: 0.7032 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6878 Loss_G: 0.7381 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.7137 Loss_G: 0.7124 acc: 87.5%\n",
      "[EPOCH 2500] TEST ACC is : 75.6%\n",
      "[BATCH 117/149] Loss_D: 0.6664 Loss_G: 0.6961 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7215 Loss_G: 0.7009 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6913 Loss_G: 0.7124 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.6774 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6951 Loss_G: 0.7443 acc: 95.3%\n",
      "[BATCH 122/149] Loss_D: 0.6973 Loss_G: 0.7235 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7082 Loss_G: 0.7125 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6748 Loss_G: 0.7065 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7288 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6965 Loss_G: 0.7300 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.6961 Loss_G: 0.7218 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6982 Loss_G: 0.7187 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6572 Loss_G: 0.7160 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7216 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6672 Loss_G: 0.7088 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.6455 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7292 Loss_G: 0.7060 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6835 Loss_G: 0.7387 acc: 93.8%\n",
      "[BATCH 135/149] Loss_D: 0.6699 Loss_G: 0.7096 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6818 Loss_G: 0.7332 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6992 Loss_G: 0.7190 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.6722 Loss_G: 0.7068 acc: 84.4%\n",
      "[BATCH 139/149] Loss_D: 0.7189 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6915 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6879 Loss_G: 0.7037 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6489 Loss_G: 0.7058 acc: 98.4%\n",
      "[BATCH 143/149] Loss_D: 0.7278 Loss_G: 0.7391 acc: 96.9%\n",
      "[BATCH 144/149] Loss_D: 0.6905 Loss_G: 0.7277 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6810 Loss_G: 0.7178 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7062 Loss_G: 0.7286 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6748 Loss_G: 0.7118 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.6910 Loss_G: 0.7175 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.7091 Loss_G: 0.7198 acc: 75.0%\n",
      "-----THE [17/50] epoch end-----\n",
      "-----THE [18/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6556 Loss_G: 0.6986 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.7171 Loss_G: 0.7016 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.6579 Loss_G: 0.6857 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7436 Loss_G: 0.7191 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6995 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6811 Loss_G: 0.7285 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.6951 Loss_G: 0.7264 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6826 Loss_G: 0.7185 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6894 Loss_G: 0.7035 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.6911 Loss_G: 0.7152 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.6939 Loss_G: 0.7219 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6936 Loss_G: 0.7176 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.7060 Loss_G: 0.7210 acc: 79.7%\n",
      "[BATCH 14/149] Loss_D: 0.6619 Loss_G: 0.7104 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7126 Loss_G: 0.7250 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6852 Loss_G: 0.7092 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6608 Loss_G: 0.6964 acc: 87.5%\n",
      "[EPOCH 2550] TEST ACC is : 76.2%\n",
      "[BATCH 18/149] Loss_D: 0.6910 Loss_G: 0.6983 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6658 Loss_G: 0.6906 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6800 Loss_G: 0.6961 acc: 81.2%\n",
      "[BATCH 21/149] Loss_D: 0.6853 Loss_G: 0.7268 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.7105 Loss_G: 0.7640 acc: 96.9%\n",
      "[BATCH 23/149] Loss_D: 0.6827 Loss_G: 0.7408 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6836 Loss_G: 0.7292 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6634 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7202 Loss_G: 0.7154 acc: 95.3%\n",
      "[BATCH 27/149] Loss_D: 0.6962 Loss_G: 0.7165 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.7088 Loss_G: 0.7305 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6762 Loss_G: 0.7211 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.6803 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7045 Loss_G: 0.7219 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.6863 Loss_G: 0.7216 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6629 Loss_G: 0.6980 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7004 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7288 Loss_G: 0.7401 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6884 Loss_G: 0.7331 acc: 95.3%\n",
      "[BATCH 37/149] Loss_D: 0.6928 Loss_G: 0.7070 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6906 Loss_G: 0.7196 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.7128 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6837 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.7111 Loss_G: 0.7285 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.6912 Loss_G: 0.7386 acc: 81.2%\n",
      "[BATCH 43/149] Loss_D: 0.6877 Loss_G: 0.7356 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6936 Loss_G: 0.7161 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.6633 Loss_G: 0.7102 acc: 93.8%\n",
      "[BATCH 46/149] Loss_D: 0.7061 Loss_G: 0.7186 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7174 Loss_G: 0.7162 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6806 Loss_G: 0.7044 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.7048 Loss_G: 0.7324 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6792 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6506 Loss_G: 0.6905 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6829 Loss_G: 0.6981 acc: 95.3%\n",
      "[BATCH 53/149] Loss_D: 0.6424 Loss_G: 0.6813 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.7517 Loss_G: 0.7071 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.7010 Loss_G: 0.7210 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.7326 Loss_G: 0.7318 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7170 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7010 Loss_G: 0.7331 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.7112 Loss_G: 0.7182 acc: 81.2%\n",
      "[BATCH 60/149] Loss_D: 0.6863 Loss_G: 0.6951 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6854 Loss_G: 0.6959 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7381 Loss_G: 0.7135 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6739 Loss_G: 0.7047 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6843 Loss_G: 0.6984 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.6763 Loss_G: 0.6826 acc: 79.7%\n",
      "[BATCH 66/149] Loss_D: 0.6966 Loss_G: 0.7094 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6984 Loss_G: 0.7379 acc: 93.8%\n",
      "[EPOCH 2600] TEST ACC is : 76.6%\n",
      "[BATCH 68/149] Loss_D: 0.7112 Loss_G: 0.7314 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.7148 Loss_G: 0.7210 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7055 Loss_G: 0.7297 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7211 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7123 Loss_G: 0.7356 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7147 Loss_G: 0.7220 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6970 Loss_G: 0.7117 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6797 Loss_G: 0.6945 acc: 81.2%\n",
      "[BATCH 76/149] Loss_D: 0.6935 Loss_G: 0.7208 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.6610 Loss_G: 0.7045 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.6844 Loss_G: 0.7066 acc: 78.1%\n",
      "[BATCH 79/149] Loss_D: 0.6586 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6776 Loss_G: 0.7239 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.6941 Loss_G: 0.7407 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6873 Loss_G: 0.6969 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6760 Loss_G: 0.7048 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.6623 Loss_G: 0.6745 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7144 Loss_G: 0.7079 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.6797 Loss_G: 0.7114 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6514 Loss_G: 0.7103 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.7291 Loss_G: 0.7359 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.6984 Loss_G: 0.7224 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6947 Loss_G: 0.7325 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6846 Loss_G: 0.7151 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6545 Loss_G: 0.7017 acc: 95.3%\n",
      "[BATCH 93/149] Loss_D: 0.6786 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6740 Loss_G: 0.7104 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.7059 Loss_G: 0.7067 acc: 81.2%\n",
      "[BATCH 96/149] Loss_D: 0.7149 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.7130 Loss_G: 0.7300 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6616 Loss_G: 0.7081 acc: 93.8%\n",
      "[BATCH 99/149] Loss_D: 0.6574 Loss_G: 0.7281 acc: 95.3%\n",
      "[BATCH 100/149] Loss_D: 0.6752 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6852 Loss_G: 0.7139 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7045 Loss_G: 0.7256 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6996 Loss_G: 0.7634 acc: 93.8%\n",
      "[BATCH 104/149] Loss_D: 0.6794 Loss_G: 0.7157 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.7363 Loss_G: 0.7266 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.6977 Loss_G: 0.7296 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7060 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6779 Loss_G: 0.7195 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7140 Loss_G: 0.7150 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.6649 Loss_G: 0.7336 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.7550 Loss_G: 0.7605 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7037 Loss_G: 0.7376 acc: 93.8%\n",
      "[BATCH 113/149] Loss_D: 0.6644 Loss_G: 0.7269 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6692 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6867 Loss_G: 0.6955 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.7133 Loss_G: 0.7133 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.7045 Loss_G: 0.7289 acc: 95.3%\n",
      "[EPOCH 2650] TEST ACC is : 76.6%\n",
      "[BATCH 118/149] Loss_D: 0.7297 Loss_G: 0.7284 acc: 78.1%\n",
      "[BATCH 119/149] Loss_D: 0.6825 Loss_G: 0.7133 acc: 95.3%\n",
      "[BATCH 120/149] Loss_D: 0.6785 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7882 Loss_G: 0.7605 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.7101 Loss_G: 0.7545 acc: 95.3%\n",
      "[BATCH 123/149] Loss_D: 0.6992 Loss_G: 0.7278 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6650 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6894 Loss_G: 0.7211 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6565 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.7163 Loss_G: 0.7465 acc: 82.8%\n",
      "[BATCH 128/149] Loss_D: 0.7300 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7051 Loss_G: 0.7364 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6826 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6836 Loss_G: 0.7136 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.7064 Loss_G: 0.7185 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6930 Loss_G: 0.7311 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.7330 Loss_G: 0.7393 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6831 Loss_G: 0.7365 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6637 Loss_G: 0.7030 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7386 Loss_G: 0.6952 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.7009 Loss_G: 0.7086 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6789 Loss_G: 0.6982 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6730 Loss_G: 0.6969 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6956 Loss_G: 0.7154 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7032 Loss_G: 0.7368 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6528 Loss_G: 0.7093 acc: 96.9%\n",
      "[BATCH 144/149] Loss_D: 0.7261 Loss_G: 0.7297 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7225 Loss_G: 0.7436 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6948 Loss_G: 0.7381 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6714 Loss_G: 0.7054 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6400 Loss_G: 0.6830 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6756 Loss_G: 0.6858 acc: 85.9%\n",
      "-----THE [18/50] epoch end-----\n",
      "-----THE [19/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6830 Loss_G: 0.6964 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.7089 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6894 Loss_G: 0.7067 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6725 Loss_G: 0.7031 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6926 Loss_G: 0.7189 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7247 Loss_G: 0.7317 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6968 Loss_G: 0.7382 acc: 79.7%\n",
      "[BATCH 8/149] Loss_D: 0.7068 Loss_G: 0.7338 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.6730 Loss_G: 0.7074 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.7158 Loss_G: 0.7247 acc: 96.9%\n",
      "[BATCH 11/149] Loss_D: 0.6992 Loss_G: 0.7139 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6925 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6560 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.7059 Loss_G: 0.7072 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7254 Loss_G: 0.7053 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7009 Loss_G: 0.7057 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7169 Loss_G: 0.7264 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6787 Loss_G: 0.7103 acc: 85.9%\n",
      "[EPOCH 2700] TEST ACC is : 76.2%\n",
      "[BATCH 19/149] Loss_D: 0.6871 Loss_G: 0.7178 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6626 Loss_G: 0.7154 acc: 95.3%\n",
      "[BATCH 21/149] Loss_D: 0.6802 Loss_G: 0.6970 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6673 Loss_G: 0.7084 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7020 Loss_G: 0.7301 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.6753 Loss_G: 0.7358 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7234 Loss_G: 0.7226 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6454 Loss_G: 0.7044 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.6896 Loss_G: 0.7192 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6707 Loss_G: 0.7075 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.6786 Loss_G: 0.7114 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6644 Loss_G: 0.7080 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6816 Loss_G: 0.6901 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6835 Loss_G: 0.7097 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6547 Loss_G: 0.7000 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.7319 Loss_G: 0.7130 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.6768 Loss_G: 0.6981 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6545 Loss_G: 0.7079 acc: 96.9%\n",
      "[BATCH 37/149] Loss_D: 0.7096 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6922 Loss_G: 0.7097 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7075 Loss_G: 0.7063 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7298 Loss_G: 0.7142 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.7031 Loss_G: 0.7253 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7284 Loss_G: 0.7486 acc: 95.3%\n",
      "[BATCH 43/149] Loss_D: 0.6908 Loss_G: 0.7255 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6977 Loss_G: 0.7034 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6793 Loss_G: 0.7237 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6950 Loss_G: 0.7118 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.7002 Loss_G: 0.7237 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6844 Loss_G: 0.7079 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.6820 Loss_G: 0.7084 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.7399 Loss_G: 0.7368 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7488 Loss_G: 0.7608 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7071 Loss_G: 0.7213 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6846 Loss_G: 0.7020 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.7245 Loss_G: 0.7081 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6779 Loss_G: 0.7190 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6930 Loss_G: 0.7345 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6759 Loss_G: 0.7063 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6609 Loss_G: 0.6935 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6798 Loss_G: 0.6965 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7113 Loss_G: 0.7059 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6615 Loss_G: 0.7125 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6820 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6842 Loss_G: 0.7185 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6741 Loss_G: 0.7060 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6528 Loss_G: 0.6973 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6860 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6748 Loss_G: 0.6924 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6972 Loss_G: 0.7036 acc: 84.4%\n",
      "[EPOCH 2750] TEST ACC is : 76.0%\n",
      "[BATCH 69/149] Loss_D: 0.7510 Loss_G: 0.7373 acc: 78.1%\n",
      "[BATCH 70/149] Loss_D: 0.6861 Loss_G: 0.7174 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6979 Loss_G: 0.7192 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7012 Loss_G: 0.7027 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6639 Loss_G: 0.7167 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7215 Loss_G: 0.7140 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.7120 Loss_G: 0.7605 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.7017 Loss_G: 0.7352 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.7046 Loss_G: 0.7560 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7166 Loss_G: 0.7653 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6921 Loss_G: 0.7066 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6558 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6879 Loss_G: 0.6999 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6870 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6793 Loss_G: 0.7137 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6599 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6588 Loss_G: 0.6989 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.7111 Loss_G: 0.7035 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7052 Loss_G: 0.7123 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6852 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6982 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6946 Loss_G: 0.7181 acc: 82.8%\n",
      "[BATCH 91/149] Loss_D: 0.6613 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6762 Loss_G: 0.7152 acc: 95.3%\n",
      "[BATCH 93/149] Loss_D: 0.6844 Loss_G: 0.7146 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.7246 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.6942 Loss_G: 0.7099 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6973 Loss_G: 0.6985 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.7081 Loss_G: 0.7353 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6812 Loss_G: 0.7511 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6628 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7008 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7000 Loss_G: 0.7215 acc: 81.2%\n",
      "[BATCH 102/149] Loss_D: 0.6941 Loss_G: 0.7151 acc: 79.7%\n",
      "[BATCH 103/149] Loss_D: 0.6893 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.7169 Loss_G: 0.7098 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.6875 Loss_G: 0.7161 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7223 Loss_G: 0.7382 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6766 Loss_G: 0.7442 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6715 Loss_G: 0.6925 acc: 81.2%\n",
      "[BATCH 109/149] Loss_D: 0.7123 Loss_G: 0.6990 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7454 Loss_G: 0.7207 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.7105 Loss_G: 0.7537 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7129 Loss_G: 0.7717 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7075 Loss_G: 0.7631 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6888 Loss_G: 0.7120 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6895 Loss_G: 0.7056 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6647 Loss_G: 0.7073 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7113 Loss_G: 0.7381 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.7132 Loss_G: 0.7500 acc: 95.3%\n",
      "[EPOCH 2800] TEST ACC is : 77.3%\n",
      "[BATCH 119/149] Loss_D: 0.6883 Loss_G: 0.7271 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6885 Loss_G: 0.7212 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7179 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6681 Loss_G: 0.7017 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.6836 Loss_G: 0.6985 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6779 Loss_G: 0.7032 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6792 Loss_G: 0.7236 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6818 Loss_G: 0.7131 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.6711 Loss_G: 0.7066 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6807 Loss_G: 0.7053 acc: 93.8%\n",
      "[BATCH 129/149] Loss_D: 0.6994 Loss_G: 0.7440 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7172 Loss_G: 0.7295 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6653 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6644 Loss_G: 0.7146 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6605 Loss_G: 0.7005 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7002 Loss_G: 0.7211 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6981 Loss_G: 0.7349 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7223 Loss_G: 0.7240 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6973 Loss_G: 0.7143 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.6915 Loss_G: 0.7033 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.7015 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6692 Loss_G: 0.7045 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6952 Loss_G: 0.7129 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6906 Loss_G: 0.7180 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7420 Loss_G: 0.7422 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.6779 Loss_G: 0.7348 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7025 Loss_G: 0.7272 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7178 Loss_G: 0.7267 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.7168 Loss_G: 0.7097 acc: 81.2%\n",
      "[BATCH 148/149] Loss_D: 0.6737 Loss_G: 0.7063 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7096 Loss_G: 0.7021 acc: 82.8%\n",
      "-----THE [19/50] epoch end-----\n",
      "-----THE [20/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7044 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7425 Loss_G: 0.7578 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.6923 Loss_G: 0.7309 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6943 Loss_G: 0.7370 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.6791 Loss_G: 0.7376 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6924 Loss_G: 0.7067 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6803 Loss_G: 0.7022 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.7304 Loss_G: 0.7184 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6892 Loss_G: 0.7117 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7096 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6655 Loss_G: 0.7358 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6906 Loss_G: 0.7149 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.7084 Loss_G: 0.7100 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6835 Loss_G: 0.7274 acc: 98.4%\n",
      "[BATCH 15/149] Loss_D: 0.7123 Loss_G: 0.7355 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7096 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6820 Loss_G: 0.7074 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6697 Loss_G: 0.7186 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.6689 Loss_G: 0.7098 acc: 85.9%\n",
      "[EPOCH 2850] TEST ACC is : 77.1%\n",
      "[BATCH 20/149] Loss_D: 0.6995 Loss_G: 0.7094 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6932 Loss_G: 0.7279 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.7021 Loss_G: 0.7243 acc: 96.9%\n",
      "[BATCH 23/149] Loss_D: 0.6671 Loss_G: 0.7051 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7552 Loss_G: 0.7431 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.6835 Loss_G: 0.7308 acc: 82.8%\n",
      "[BATCH 26/149] Loss_D: 0.6755 Loss_G: 0.7147 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7038 Loss_G: 0.7099 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.7155 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6835 Loss_G: 0.7251 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6992 Loss_G: 0.7314 acc: 93.8%\n",
      "[BATCH 31/149] Loss_D: 0.7009 Loss_G: 0.7115 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6590 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6434 Loss_G: 0.6846 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7270 Loss_G: 0.7078 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.6902 Loss_G: 0.7100 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6758 Loss_G: 0.7253 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7086 Loss_G: 0.7338 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.7033 Loss_G: 0.7306 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6859 Loss_G: 0.7131 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.7345 Loss_G: 0.7139 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.6822 Loss_G: 0.6893 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.6517 Loss_G: 0.7046 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.7028 Loss_G: 0.7329 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6972 Loss_G: 0.6978 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.7135 Loss_G: 0.7230 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6801 Loss_G: 0.7040 acc: 95.3%\n",
      "[BATCH 47/149] Loss_D: 0.6879 Loss_G: 0.7101 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6597 Loss_G: 0.7154 acc: 95.3%\n",
      "[BATCH 49/149] Loss_D: 0.6659 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6646 Loss_G: 0.6877 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6868 Loss_G: 0.6993 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.7438 Loss_G: 0.7315 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6574 Loss_G: 0.6905 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6914 Loss_G: 0.6908 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.6503 Loss_G: 0.6960 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.7049 Loss_G: 0.6948 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6986 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6697 Loss_G: 0.7324 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.6667 Loss_G: 0.7027 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7322 Loss_G: 0.7473 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7333 Loss_G: 0.7350 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6709 Loss_G: 0.7253 acc: 95.3%\n",
      "[BATCH 63/149] Loss_D: 0.6652 Loss_G: 0.7029 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.7148 Loss_G: 0.7320 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7707 Loss_G: 0.7286 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7168 Loss_G: 0.7701 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6670 Loss_G: 0.7349 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.7282 Loss_G: 0.7322 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.7166 Loss_G: 0.7335 acc: 89.1%\n",
      "[EPOCH 2900] TEST ACC is : 76.6%\n",
      "[BATCH 70/149] Loss_D: 0.6789 Loss_G: 0.7081 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6672 Loss_G: 0.7078 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6571 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.7221 Loss_G: 0.7462 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6784 Loss_G: 0.7303 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7023 Loss_G: 0.7234 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7086 Loss_G: 0.7169 acc: 93.8%\n",
      "[BATCH 77/149] Loss_D: 0.7034 Loss_G: 0.7423 acc: 96.9%\n",
      "[BATCH 78/149] Loss_D: 0.7267 Loss_G: 0.7235 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.7449 Loss_G: 0.7592 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6925 Loss_G: 0.7318 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6626 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.6858 Loss_G: 0.6979 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6557 Loss_G: 0.6908 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6969 Loss_G: 0.6972 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6829 Loss_G: 0.7089 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6709 Loss_G: 0.7047 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6578 Loss_G: 0.7046 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6994 Loss_G: 0.7112 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6839 Loss_G: 0.7179 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.6866 Loss_G: 0.7279 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.6576 Loss_G: 0.7155 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6747 Loss_G: 0.7129 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7218 Loss_G: 0.7291 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7135 Loss_G: 0.7221 acc: 79.7%\n",
      "[BATCH 95/149] Loss_D: 0.6801 Loss_G: 0.7135 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.7256 Loss_G: 0.7149 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7040 Loss_G: 0.7260 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.6911 Loss_G: 0.7401 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6752 Loss_G: 0.7224 acc: 82.8%\n",
      "[BATCH 100/149] Loss_D: 0.6781 Loss_G: 0.7039 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6871 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7190 Loss_G: 0.7048 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6870 Loss_G: 0.7014 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6822 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.6974 Loss_G: 0.7184 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.6620 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6801 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6714 Loss_G: 0.7438 acc: 96.9%\n",
      "[BATCH 109/149] Loss_D: 0.6878 Loss_G: 0.7038 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.6843 Loss_G: 0.7067 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.6982 Loss_G: 0.7050 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7103 Loss_G: 0.7195 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6682 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7121 Loss_G: 0.7111 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.6595 Loss_G: 0.6921 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.6536 Loss_G: 0.7048 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7459 Loss_G: 0.7421 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6595 Loss_G: 0.7312 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6809 Loss_G: 0.7033 acc: 85.9%\n",
      "[EPOCH 2950] TEST ACC is : 75.6%\n",
      "[BATCH 120/149] Loss_D: 0.6952 Loss_G: 0.7001 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6739 Loss_G: 0.6969 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6808 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7199 Loss_G: 0.7140 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6779 Loss_G: 0.7258 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6813 Loss_G: 0.7175 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7017 Loss_G: 0.6912 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7380 Loss_G: 0.7592 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7317 Loss_G: 0.7346 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7299 Loss_G: 0.7343 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6931 Loss_G: 0.7347 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6846 Loss_G: 0.7216 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.7134 Loss_G: 0.7402 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7277 Loss_G: 0.7327 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.6836 Loss_G: 0.7101 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6977 Loss_G: 0.7110 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6778 Loss_G: 0.7138 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6392 Loss_G: 0.7240 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.7142 Loss_G: 0.7182 acc: 84.4%\n",
      "[BATCH 139/149] Loss_D: 0.7096 Loss_G: 0.7040 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.6779 Loss_G: 0.6919 acc: 79.7%\n",
      "[BATCH 141/149] Loss_D: 0.7224 Loss_G: 0.7097 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.7230 Loss_G: 0.7110 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.7250 Loss_G: 0.7065 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.6273 Loss_G: 0.6833 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6987 Loss_G: 0.6988 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6717 Loss_G: 0.6915 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.7076 Loss_G: 0.7195 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.7221 Loss_G: 0.7282 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6677 Loss_G: 0.7152 acc: 90.6%\n",
      "-----THE [20/50] epoch end-----\n",
      "-----THE [21/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6583 Loss_G: 0.7113 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6520 Loss_G: 0.6955 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6792 Loss_G: 0.6995 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6914 Loss_G: 0.7094 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6930 Loss_G: 0.7135 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6962 Loss_G: 0.7199 acc: 96.9%\n",
      "[BATCH 7/149] Loss_D: 0.6667 Loss_G: 0.7071 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6602 Loss_G: 0.7122 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.7616 Loss_G: 0.7437 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.6734 Loss_G: 0.7440 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6692 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.7340 Loss_G: 0.7157 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6903 Loss_G: 0.7143 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6792 Loss_G: 0.7055 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.6769 Loss_G: 0.6942 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6987 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7324 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6903 Loss_G: 0.7164 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6647 Loss_G: 0.7042 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.7074 Loss_G: 0.7151 acc: 90.6%\n",
      "[EPOCH 3000] TEST ACC is : 77.0%\n",
      "[BATCH 21/149] Loss_D: 0.6878 Loss_G: 0.7467 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.7181 Loss_G: 0.7362 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7317 Loss_G: 0.7385 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7168 Loss_G: 0.7389 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7076 Loss_G: 0.7225 acc: 81.2%\n",
      "[BATCH 26/149] Loss_D: 0.6693 Loss_G: 0.7078 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.6952 Loss_G: 0.7469 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.6973 Loss_G: 0.7171 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6799 Loss_G: 0.7184 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.7116 Loss_G: 0.7316 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6934 Loss_G: 0.7369 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7010 Loss_G: 0.7256 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.7003 Loss_G: 0.7182 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.7133 Loss_G: 0.7226 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7312 Loss_G: 0.7326 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6913 Loss_G: 0.7360 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7267 Loss_G: 0.7400 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6935 Loss_G: 0.7172 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7292 Loss_G: 0.7205 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7027 Loss_G: 0.7211 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6699 Loss_G: 0.7060 acc: 81.2%\n",
      "[BATCH 42/149] Loss_D: 0.6762 Loss_G: 0.7053 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7104 Loss_G: 0.7237 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.7198 Loss_G: 0.7404 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6878 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.7125 Loss_G: 0.7344 acc: 82.8%\n",
      "[BATCH 47/149] Loss_D: 0.7114 Loss_G: 0.7184 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.7446 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7053 Loss_G: 0.7062 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6961 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6892 Loss_G: 0.7175 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7160 Loss_G: 0.7252 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6728 Loss_G: 0.7073 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.6546 Loss_G: 0.6892 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.7336 Loss_G: 0.7032 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.7070 Loss_G: 0.7225 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7084 Loss_G: 0.7336 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6716 Loss_G: 0.7153 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.6592 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6757 Loss_G: 0.7006 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6687 Loss_G: 0.7010 acc: 93.8%\n",
      "[BATCH 62/149] Loss_D: 0.6955 Loss_G: 0.7115 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7066 Loss_G: 0.7162 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.6717 Loss_G: 0.7329 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6608 Loss_G: 0.7152 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7238 Loss_G: 0.7272 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.7440 Loss_G: 0.7288 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6621 Loss_G: 0.7221 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.6822 Loss_G: 0.7151 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.7187 Loss_G: 0.7034 acc: 85.9%\n",
      "[EPOCH 3050] TEST ACC is : 77.0%\n",
      "[BATCH 71/149] Loss_D: 0.7030 Loss_G: 0.7017 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6983 Loss_G: 0.6916 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7114 Loss_G: 0.7005 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.6898 Loss_G: 0.7173 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6627 Loss_G: 0.6989 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6794 Loss_G: 0.6993 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6815 Loss_G: 0.7160 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6962 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6915 Loss_G: 0.7322 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6686 Loss_G: 0.7154 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7025 Loss_G: 0.7129 acc: 81.2%\n",
      "[BATCH 82/149] Loss_D: 0.6984 Loss_G: 0.7111 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7217 Loss_G: 0.7180 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6488 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6668 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6643 Loss_G: 0.6981 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.7303 Loss_G: 0.7072 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6811 Loss_G: 0.7006 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7022 Loss_G: 0.7280 acc: 95.3%\n",
      "[BATCH 90/149] Loss_D: 0.6758 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6541 Loss_G: 0.6952 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7135 Loss_G: 0.7228 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.6843 Loss_G: 0.7390 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.6880 Loss_G: 0.7322 acc: 93.8%\n",
      "[BATCH 95/149] Loss_D: 0.6732 Loss_G: 0.7114 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.6776 Loss_G: 0.7004 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6969 Loss_G: 0.6952 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6807 Loss_G: 0.6961 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6545 Loss_G: 0.6914 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6733 Loss_G: 0.7065 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6963 Loss_G: 0.6947 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6586 Loss_G: 0.7006 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.7134 Loss_G: 0.7228 acc: 81.2%\n",
      "[BATCH 104/149] Loss_D: 0.6761 Loss_G: 0.7094 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6798 Loss_G: 0.6987 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.6688 Loss_G: 0.6995 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.7194 Loss_G: 0.7226 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7244 Loss_G: 0.7137 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.6792 Loss_G: 0.7251 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.7048 Loss_G: 0.7222 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.6773 Loss_G: 0.6969 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7286 Loss_G: 0.7081 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.6583 Loss_G: 0.7036 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6945 Loss_G: 0.7047 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6650 Loss_G: 0.6911 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6998 Loss_G: 0.7037 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6905 Loss_G: 0.7086 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6588 Loss_G: 0.7066 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7004 Loss_G: 0.7214 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7085 Loss_G: 0.7432 acc: 93.8%\n",
      "[EPOCH 3100] TEST ACC is : 76.2%\n",
      "[BATCH 121/149] Loss_D: 0.6650 Loss_G: 0.7129 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7201 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.6641 Loss_G: 0.7106 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6798 Loss_G: 0.6984 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7122 Loss_G: 0.7006 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6725 Loss_G: 0.7022 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6663 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7166 Loss_G: 0.7539 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7081 Loss_G: 0.7509 acc: 96.9%\n",
      "[BATCH 130/149] Loss_D: 0.6695 Loss_G: 0.7365 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6923 Loss_G: 0.7336 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7076 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6900 Loss_G: 0.7303 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.6972 Loss_G: 0.7217 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6611 Loss_G: 0.7243 acc: 93.8%\n",
      "[BATCH 136/149] Loss_D: 0.6692 Loss_G: 0.7353 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.7157 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6572 Loss_G: 0.7301 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6876 Loss_G: 0.7123 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7195 Loss_G: 0.7102 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6662 Loss_G: 0.7085 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6984 Loss_G: 0.7177 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6978 Loss_G: 0.7356 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6688 Loss_G: 0.7112 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.7150 Loss_G: 0.7098 acc: 78.1%\n",
      "[BATCH 146/149] Loss_D: 0.6806 Loss_G: 0.6995 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6787 Loss_G: 0.7034 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6845 Loss_G: 0.7135 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.7085 Loss_G: 0.7333 acc: 84.4%\n",
      "-----THE [21/50] epoch end-----\n",
      "-----THE [22/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6821 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6717 Loss_G: 0.7076 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6864 Loss_G: 0.7016 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6798 Loss_G: 0.7002 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6643 Loss_G: 0.6910 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6836 Loss_G: 0.6935 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6715 Loss_G: 0.7099 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7180 Loss_G: 0.7158 acc: 95.3%\n",
      "[BATCH 9/149] Loss_D: 0.7153 Loss_G: 0.7219 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7085 Loss_G: 0.7187 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6951 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6723 Loss_G: 0.7261 acc: 95.3%\n",
      "[BATCH 13/149] Loss_D: 0.7060 Loss_G: 0.7235 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.6866 Loss_G: 0.7063 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.6705 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6946 Loss_G: 0.7052 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6735 Loss_G: 0.7301 acc: 95.3%\n",
      "[BATCH 18/149] Loss_D: 0.7004 Loss_G: 0.7285 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6991 Loss_G: 0.7312 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.6956 Loss_G: 0.7070 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7060 Loss_G: 0.7046 acc: 84.4%\n",
      "[EPOCH 3150] TEST ACC is : 76.6%\n",
      "[BATCH 22/149] Loss_D: 0.7244 Loss_G: 0.7327 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6981 Loss_G: 0.7182 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7080 Loss_G: 0.7340 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6724 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.6557 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6920 Loss_G: 0.6996 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6919 Loss_G: 0.6895 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.7061 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6543 Loss_G: 0.7144 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6937 Loss_G: 0.7234 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6870 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6537 Loss_G: 0.7054 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6892 Loss_G: 0.7128 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6715 Loss_G: 0.6926 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.6910 Loss_G: 0.7101 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.6497 Loss_G: 0.6887 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6903 Loss_G: 0.7012 acc: 95.3%\n",
      "[BATCH 39/149] Loss_D: 0.6412 Loss_G: 0.7046 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6996 Loss_G: 0.7209 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6963 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6813 Loss_G: 0.7002 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7081 Loss_G: 0.7194 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6895 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.7105 Loss_G: 0.7169 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6868 Loss_G: 0.6874 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7084 Loss_G: 0.7369 acc: 93.8%\n",
      "[BATCH 48/149] Loss_D: 0.6711 Loss_G: 0.7138 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7216 Loss_G: 0.7078 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7319 Loss_G: 0.7213 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.6874 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6979 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6977 Loss_G: 0.7396 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7039 Loss_G: 0.7267 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6944 Loss_G: 0.7073 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6868 Loss_G: 0.7244 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6727 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6949 Loss_G: 0.7195 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7017 Loss_G: 0.7331 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7412 Loss_G: 0.7494 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7076 Loss_G: 0.7494 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7324 Loss_G: 0.7261 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6803 Loss_G: 0.7044 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.6851 Loss_G: 0.7164 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6698 Loss_G: 0.7001 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6865 Loss_G: 0.7143 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6984 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6882 Loss_G: 0.7024 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6573 Loss_G: 0.6837 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6820 Loss_G: 0.6943 acc: 81.2%\n",
      "[BATCH 71/149] Loss_D: 0.7329 Loss_G: 0.7413 acc: 90.6%\n",
      "[EPOCH 3200] TEST ACC is : 76.8%\n",
      "[BATCH 72/149] Loss_D: 0.6880 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6845 Loss_G: 0.7140 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7241 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6553 Loss_G: 0.7082 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6892 Loss_G: 0.7249 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6640 Loss_G: 0.7139 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6672 Loss_G: 0.6981 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.7057 Loss_G: 0.6984 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6959 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6920 Loss_G: 0.7133 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.7024 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6639 Loss_G: 0.6850 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.6897 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6917 Loss_G: 0.7118 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6872 Loss_G: 0.7070 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6954 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6775 Loss_G: 0.6985 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7238 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6699 Loss_G: 0.7090 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6755 Loss_G: 0.7281 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.6835 Loss_G: 0.7049 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.6624 Loss_G: 0.7049 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7087 Loss_G: 0.7026 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.6647 Loss_G: 0.7054 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6887 Loss_G: 0.7054 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7268 Loss_G: 0.6905 acc: 81.2%\n",
      "[BATCH 98/149] Loss_D: 0.6920 Loss_G: 0.7232 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6965 Loss_G: 0.7047 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6660 Loss_G: 0.7035 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.6667 Loss_G: 0.6986 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6966 Loss_G: 0.7314 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.7006 Loss_G: 0.7199 acc: 79.7%\n",
      "[BATCH 104/149] Loss_D: 0.6962 Loss_G: 0.7126 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.7425 Loss_G: 0.7666 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.6820 Loss_G: 0.7124 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6769 Loss_G: 0.7091 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7137 Loss_G: 0.7271 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7249 Loss_G: 0.7674 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6900 Loss_G: 0.7610 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6983 Loss_G: 0.7351 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7229 Loss_G: 0.7215 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6992 Loss_G: 0.7110 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.6746 Loss_G: 0.7029 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6992 Loss_G: 0.7214 acc: 95.3%\n",
      "[BATCH 116/149] Loss_D: 0.6873 Loss_G: 0.7051 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.6534 Loss_G: 0.6974 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.7054 Loss_G: 0.7201 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.6714 Loss_G: 0.7174 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6698 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7141 Loss_G: 0.7257 acc: 93.8%\n",
      "[EPOCH 3250] TEST ACC is : 77.0%\n",
      "[BATCH 122/149] Loss_D: 0.6990 Loss_G: 0.7263 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.7150 Loss_G: 0.7243 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.7194 Loss_G: 0.7389 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7342 Loss_G: 0.7524 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6951 Loss_G: 0.7271 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.7060 Loss_G: 0.7148 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6623 Loss_G: 0.7200 acc: 93.8%\n",
      "[BATCH 129/149] Loss_D: 0.7068 Loss_G: 0.7329 acc: 96.9%\n",
      "[BATCH 130/149] Loss_D: 0.6982 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6812 Loss_G: 0.7058 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7092 Loss_G: 0.7005 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.6799 Loss_G: 0.6965 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.6886 Loss_G: 0.7549 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7019 Loss_G: 0.7009 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6947 Loss_G: 0.7197 acc: 81.2%\n",
      "[BATCH 137/149] Loss_D: 0.6804 Loss_G: 0.7103 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7061 Loss_G: 0.7057 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7247 Loss_G: 0.7153 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7259 Loss_G: 0.7460 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.7138 Loss_G: 0.7270 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6747 Loss_G: 0.7157 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6562 Loss_G: 0.7207 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7006 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7137 Loss_G: 0.7207 acc: 95.3%\n",
      "[BATCH 146/149] Loss_D: 0.6591 Loss_G: 0.7271 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6545 Loss_G: 0.7239 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6847 Loss_G: 0.7201 acc: 81.2%\n",
      "[BATCH 149/149] Loss_D: 0.7177 Loss_G: 0.7215 acc: 84.4%\n",
      "-----THE [22/50] epoch end-----\n",
      "-----THE [23/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7122 Loss_G: 0.7334 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6895 Loss_G: 0.7118 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.7499 Loss_G: 0.7416 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6858 Loss_G: 0.7417 acc: 96.9%\n",
      "[BATCH 5/149] Loss_D: 0.6761 Loss_G: 0.7190 acc: 95.3%\n",
      "[BATCH 6/149] Loss_D: 0.6875 Loss_G: 0.7128 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6719 Loss_G: 0.7275 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6629 Loss_G: 0.7046 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7156 Loss_G: 0.7141 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7061 Loss_G: 0.7310 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7201 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7067 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6948 Loss_G: 0.7155 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6971 Loss_G: 0.7098 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.6685 Loss_G: 0.7020 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.7166 Loss_G: 0.7060 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6861 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7132 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6915 Loss_G: 0.7197 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.7241 Loss_G: 0.7183 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.6510 Loss_G: 0.7078 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6745 Loss_G: 0.7047 acc: 87.5%\n",
      "[EPOCH 3300] TEST ACC is : 77.1%\n",
      "[BATCH 23/149] Loss_D: 0.6969 Loss_G: 0.7391 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6868 Loss_G: 0.7424 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6928 Loss_G: 0.7472 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.6847 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6586 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6850 Loss_G: 0.6952 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7278 Loss_G: 0.7159 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7558 Loss_G: 0.7586 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6919 Loss_G: 0.7290 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6874 Loss_G: 0.7212 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6900 Loss_G: 0.7065 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7007 Loss_G: 0.6981 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7233 Loss_G: 0.7286 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7130 Loss_G: 0.7510 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6641 Loss_G: 0.7049 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.6968 Loss_G: 0.7083 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6568 Loss_G: 0.7125 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.7227 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6669 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6902 Loss_G: 0.7193 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6834 Loss_G: 0.7223 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7014 Loss_G: 0.7177 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6893 Loss_G: 0.7130 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.7033 Loss_G: 0.7226 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6743 Loss_G: 0.7140 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.6561 Loss_G: 0.6860 acc: 81.2%\n",
      "[BATCH 49/149] Loss_D: 0.7091 Loss_G: 0.7013 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6414 Loss_G: 0.6953 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.6477 Loss_G: 0.6950 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.6757 Loss_G: 0.7078 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6881 Loss_G: 0.7042 acc: 78.1%\n",
      "[BATCH 54/149] Loss_D: 0.7028 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6823 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.7214 Loss_G: 0.7229 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6897 Loss_G: 0.7109 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.7283 Loss_G: 0.7127 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6624 Loss_G: 0.7391 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.7013 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6786 Loss_G: 0.7047 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6825 Loss_G: 0.7067 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6973 Loss_G: 0.7101 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6801 Loss_G: 0.6898 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.6833 Loss_G: 0.7077 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7021 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6855 Loss_G: 0.7117 acc: 81.2%\n",
      "[BATCH 68/149] Loss_D: 0.7078 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.7309 Loss_G: 0.7175 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6872 Loss_G: 0.7392 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6734 Loss_G: 0.7234 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.6816 Loss_G: 0.7286 acc: 82.8%\n",
      "[EPOCH 3350] TEST ACC is : 76.8%\n",
      "[BATCH 73/149] Loss_D: 0.7005 Loss_G: 0.7285 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.6807 Loss_G: 0.7262 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6787 Loss_G: 0.7211 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6693 Loss_G: 0.7301 acc: 93.8%\n",
      "[BATCH 77/149] Loss_D: 0.6610 Loss_G: 0.7093 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6724 Loss_G: 0.7184 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6622 Loss_G: 0.6989 acc: 82.8%\n",
      "[BATCH 80/149] Loss_D: 0.6583 Loss_G: 0.6822 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6804 Loss_G: 0.7121 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6998 Loss_G: 0.7154 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6902 Loss_G: 0.7026 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6806 Loss_G: 0.7093 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6922 Loss_G: 0.7290 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.6732 Loss_G: 0.7107 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6793 Loss_G: 0.6977 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6719 Loss_G: 0.7176 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6483 Loss_G: 0.7110 acc: 95.3%\n",
      "[BATCH 90/149] Loss_D: 0.6825 Loss_G: 0.7184 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6859 Loss_G: 0.7007 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.6814 Loss_G: 0.7079 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.6614 Loss_G: 0.6933 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6638 Loss_G: 0.6949 acc: 93.8%\n",
      "[BATCH 95/149] Loss_D: 0.6802 Loss_G: 0.6954 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7012 Loss_G: 0.7196 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.6716 Loss_G: 0.6996 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6723 Loss_G: 0.6892 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7231 Loss_G: 0.7054 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.6753 Loss_G: 0.7039 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7078 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7367 Loss_G: 0.7307 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6629 Loss_G: 0.7146 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7080 Loss_G: 0.7167 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6854 Loss_G: 0.6947 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.6947 Loss_G: 0.6968 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7260 Loss_G: 0.7239 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6906 Loss_G: 0.7302 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.7086 Loss_G: 0.7321 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6768 Loss_G: 0.7247 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.7100 Loss_G: 0.7416 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7067 Loss_G: 0.7342 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7456 Loss_G: 0.7422 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6940 Loss_G: 0.7203 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7200 Loss_G: 0.7313 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7054 Loss_G: 0.7212 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.6945 Loss_G: 0.7289 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.7223 Loss_G: 0.7168 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.7001 Loss_G: 0.7208 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6621 Loss_G: 0.7420 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7114 Loss_G: 0.7352 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7164 Loss_G: 0.7251 acc: 90.6%\n",
      "[EPOCH 3400] TEST ACC is : 77.0%\n",
      "[BATCH 123/149] Loss_D: 0.7039 Loss_G: 0.7156 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.7328 Loss_G: 0.7123 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6811 Loss_G: 0.7201 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7517 Loss_G: 0.7306 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7420 Loss_G: 0.7127 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.6797 Loss_G: 0.7118 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6826 Loss_G: 0.7111 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6738 Loss_G: 0.7031 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6937 Loss_G: 0.7030 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6779 Loss_G: 0.7008 acc: 93.8%\n",
      "[BATCH 133/149] Loss_D: 0.6325 Loss_G: 0.6949 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7193 Loss_G: 0.7315 acc: 93.8%\n",
      "[BATCH 135/149] Loss_D: 0.7231 Loss_G: 0.7528 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6927 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6901 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6920 Loss_G: 0.7247 acc: 95.3%\n",
      "[BATCH 139/149] Loss_D: 0.6886 Loss_G: 0.7243 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6614 Loss_G: 0.7122 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6513 Loss_G: 0.7060 acc: 93.8%\n",
      "[BATCH 142/149] Loss_D: 0.6242 Loss_G: 0.6790 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6975 Loss_G: 0.6985 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7396 Loss_G: 0.7162 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7349 Loss_G: 0.7180 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.6879 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.7049 Loss_G: 0.7097 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.7250 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6684 Loss_G: 0.7208 acc: 89.1%\n",
      "-----THE [23/50] epoch end-----\n",
      "-----THE [24/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6897 Loss_G: 0.7300 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6900 Loss_G: 0.7414 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.6867 Loss_G: 0.7327 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7162 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7022 Loss_G: 0.7164 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6721 Loss_G: 0.7203 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.7059 Loss_G: 0.7246 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6682 Loss_G: 0.7202 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6804 Loss_G: 0.7116 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7160 Loss_G: 0.7255 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7049 Loss_G: 0.7488 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6873 Loss_G: 0.7186 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.6832 Loss_G: 0.7367 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.7023 Loss_G: 0.7284 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.7019 Loss_G: 0.7433 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.7582 Loss_G: 0.7586 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.7295 Loss_G: 0.7355 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6957 Loss_G: 0.7245 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.7156 Loss_G: 0.7253 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.6826 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6571 Loss_G: 0.7143 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6802 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7412 Loss_G: 0.7192 acc: 82.8%\n",
      "[EPOCH 3450] TEST ACC is : 76.8%\n",
      "[BATCH 24/149] Loss_D: 0.6976 Loss_G: 0.7193 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7472 Loss_G: 0.7418 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6866 Loss_G: 0.7074 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6549 Loss_G: 0.6944 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6951 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7026 Loss_G: 0.7116 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6607 Loss_G: 0.6966 acc: 95.3%\n",
      "[BATCH 31/149] Loss_D: 0.7036 Loss_G: 0.7113 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6697 Loss_G: 0.7093 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.7213 Loss_G: 0.7123 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6871 Loss_G: 0.7012 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.6780 Loss_G: 0.7270 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6826 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6785 Loss_G: 0.7088 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6521 Loss_G: 0.7151 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.7131 Loss_G: 0.7319 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7048 Loss_G: 0.7045 acc: 81.2%\n",
      "[BATCH 41/149] Loss_D: 0.6662 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6973 Loss_G: 0.7009 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.7124 Loss_G: 0.7385 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.7272 Loss_G: 0.7582 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6808 Loss_G: 0.7255 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6582 Loss_G: 0.7016 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6669 Loss_G: 0.7055 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6868 Loss_G: 0.6948 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6740 Loss_G: 0.6922 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6442 Loss_G: 0.7048 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7127 Loss_G: 0.6856 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7028 Loss_G: 0.6970 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.7058 Loss_G: 0.7053 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7256 Loss_G: 0.7075 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.7224 Loss_G: 0.7357 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6855 Loss_G: 0.7499 acc: 96.9%\n",
      "[BATCH 57/149] Loss_D: 0.6747 Loss_G: 0.7051 acc: 81.2%\n",
      "[BATCH 58/149] Loss_D: 0.6896 Loss_G: 0.6864 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6528 Loss_G: 0.6801 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6706 Loss_G: 0.7000 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6759 Loss_G: 0.7112 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7133 Loss_G: 0.7460 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7045 Loss_G: 0.7152 acc: 95.3%\n",
      "[BATCH 64/149] Loss_D: 0.6770 Loss_G: 0.7061 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7011 Loss_G: 0.7311 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.6838 Loss_G: 0.7166 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.7071 Loss_G: 0.7174 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6986 Loss_G: 0.7343 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.7653 Loss_G: 0.7351 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6718 Loss_G: 0.7439 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.7066 Loss_G: 0.7145 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6869 Loss_G: 0.7245 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.6678 Loss_G: 0.7198 acc: 87.5%\n",
      "[EPOCH 3500] TEST ACC is : 77.1%\n",
      "[BATCH 74/149] Loss_D: 0.6767 Loss_G: 0.7004 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7131 Loss_G: 0.7250 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.7024 Loss_G: 0.7397 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6960 Loss_G: 0.7214 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6997 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.7160 Loss_G: 0.7191 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.7179 Loss_G: 0.7205 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6758 Loss_G: 0.7258 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6636 Loss_G: 0.7187 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.7014 Loss_G: 0.7107 acc: 79.7%\n",
      "[BATCH 84/149] Loss_D: 0.7198 Loss_G: 0.7080 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.7099 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6834 Loss_G: 0.7062 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6775 Loss_G: 0.7048 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.6986 Loss_G: 0.7128 acc: 96.9%\n",
      "[BATCH 89/149] Loss_D: 0.6744 Loss_G: 0.7044 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6792 Loss_G: 0.7007 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6618 Loss_G: 0.7034 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.7365 Loss_G: 0.7202 acc: 78.1%\n",
      "[BATCH 93/149] Loss_D: 0.7228 Loss_G: 0.7604 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6729 Loss_G: 0.7529 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.6678 Loss_G: 0.7290 acc: 95.3%\n",
      "[BATCH 96/149] Loss_D: 0.6891 Loss_G: 0.7037 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.7074 Loss_G: 0.7219 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6518 Loss_G: 0.7181 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7294 Loss_G: 0.7427 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.6596 Loss_G: 0.7176 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7259 Loss_G: 0.7212 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.7393 Loss_G: 0.7332 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6836 Loss_G: 0.7326 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6826 Loss_G: 0.7296 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6701 Loss_G: 0.7387 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7075 Loss_G: 0.7069 acc: 79.7%\n",
      "[BATCH 107/149] Loss_D: 0.6549 Loss_G: 0.6964 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6948 Loss_G: 0.7021 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.6594 Loss_G: 0.6988 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6903 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6657 Loss_G: 0.7216 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6996 Loss_G: 0.7178 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7260 Loss_G: 0.6990 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6821 Loss_G: 0.7018 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6418 Loss_G: 0.7067 acc: 95.3%\n",
      "[BATCH 116/149] Loss_D: 0.7115 Loss_G: 0.7080 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6641 Loss_G: 0.6969 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7388 Loss_G: 0.6938 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.6725 Loss_G: 0.6817 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6684 Loss_G: 0.6917 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6742 Loss_G: 0.7106 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6866 Loss_G: 0.7195 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.7145 Loss_G: 0.7277 acc: 85.9%\n",
      "[EPOCH 3550] TEST ACC is : 76.2%\n",
      "[BATCH 124/149] Loss_D: 0.6739 Loss_G: 0.7131 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6887 Loss_G: 0.7197 acc: 96.9%\n",
      "[BATCH 126/149] Loss_D: 0.6786 Loss_G: 0.7491 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6378 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6899 Loss_G: 0.6971 acc: 82.8%\n",
      "[BATCH 129/149] Loss_D: 0.7126 Loss_G: 0.7123 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6999 Loss_G: 0.6959 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.6744 Loss_G: 0.6802 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.6707 Loss_G: 0.6841 acc: 93.8%\n",
      "[BATCH 133/149] Loss_D: 0.6299 Loss_G: 0.6847 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7206 Loss_G: 0.7111 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6986 Loss_G: 0.7030 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6972 Loss_G: 0.6982 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.6816 Loss_G: 0.7142 acc: 95.3%\n",
      "[BATCH 138/149] Loss_D: 0.7242 Loss_G: 0.7331 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7309 Loss_G: 0.7644 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6958 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6734 Loss_G: 0.6960 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6911 Loss_G: 0.7094 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.6778 Loss_G: 0.6919 acc: 81.2%\n",
      "[BATCH 144/149] Loss_D: 0.6600 Loss_G: 0.6936 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7233 Loss_G: 0.7240 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6764 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6794 Loss_G: 0.7136 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7024 Loss_G: 0.7443 acc: 95.3%\n",
      "[BATCH 149/149] Loss_D: 0.6814 Loss_G: 0.7229 acc: 89.1%\n",
      "-----THE [24/50] epoch end-----\n",
      "-----THE [25/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7156 Loss_G: 0.7255 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6919 Loss_G: 0.7235 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7216 Loss_G: 0.7320 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6994 Loss_G: 0.7280 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6995 Loss_G: 0.7170 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.7000 Loss_G: 0.7302 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7034 Loss_G: 0.7296 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.6958 Loss_G: 0.7245 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6909 Loss_G: 0.7216 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7030 Loss_G: 0.7064 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7209 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6981 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.7046 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6750 Loss_G: 0.7163 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6739 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7245 Loss_G: 0.7368 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7034 Loss_G: 0.7336 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6996 Loss_G: 0.7122 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7229 Loss_G: 0.7233 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.7326 Loss_G: 0.7234 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6846 Loss_G: 0.7256 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6853 Loss_G: 0.7411 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6820 Loss_G: 0.7193 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7012 Loss_G: 0.7043 acc: 87.5%\n",
      "[EPOCH 3600] TEST ACC is : 77.3%\n",
      "[BATCH 25/149] Loss_D: 0.6813 Loss_G: 0.7143 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6696 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6559 Loss_G: 0.6928 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6985 Loss_G: 0.6906 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.6985 Loss_G: 0.7340 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.6711 Loss_G: 0.7219 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.7368 Loss_G: 0.7396 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6607 Loss_G: 0.7088 acc: 95.3%\n",
      "[BATCH 33/149] Loss_D: 0.6847 Loss_G: 0.6914 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7042 Loss_G: 0.7099 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6861 Loss_G: 0.7309 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6970 Loss_G: 0.7052 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6626 Loss_G: 0.6892 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6729 Loss_G: 0.6963 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6697 Loss_G: 0.7069 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.7070 Loss_G: 0.7183 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.6622 Loss_G: 0.7093 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.7017 Loss_G: 0.7001 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6600 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6975 Loss_G: 0.7029 acc: 82.8%\n",
      "[BATCH 45/149] Loss_D: 0.6789 Loss_G: 0.7100 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7211 Loss_G: 0.7320 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6900 Loss_G: 0.6968 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6811 Loss_G: 0.6857 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6741 Loss_G: 0.6919 acc: 81.2%\n",
      "[BATCH 50/149] Loss_D: 0.7091 Loss_G: 0.6953 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6766 Loss_G: 0.7242 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.7180 Loss_G: 0.7102 acc: 81.2%\n",
      "[BATCH 53/149] Loss_D: 0.6823 Loss_G: 0.7039 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6929 Loss_G: 0.7213 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.7316 Loss_G: 0.7541 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.6567 Loss_G: 0.7219 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6491 Loss_G: 0.6828 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.7076 Loss_G: 0.6995 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.7018 Loss_G: 0.7054 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6943 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6729 Loss_G: 0.7351 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6651 Loss_G: 0.7267 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.6829 Loss_G: 0.7123 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6951 Loss_G: 0.7203 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7171 Loss_G: 0.7226 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6937 Loss_G: 0.7412 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6924 Loss_G: 0.7112 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7283 Loss_G: 0.7346 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6781 Loss_G: 0.6973 acc: 93.8%\n",
      "[BATCH 70/149] Loss_D: 0.7005 Loss_G: 0.6897 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.7375 Loss_G: 0.7169 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7337 Loss_G: 0.7336 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6990 Loss_G: 0.7372 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.6959 Loss_G: 0.7609 acc: 90.6%\n",
      "[EPOCH 3650] TEST ACC is : 76.2%\n",
      "[BATCH 75/149] Loss_D: 0.6695 Loss_G: 0.7111 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6928 Loss_G: 0.7155 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6970 Loss_G: 0.7444 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6892 Loss_G: 0.7388 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6704 Loss_G: 0.7073 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.7050 Loss_G: 0.7005 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6670 Loss_G: 0.6992 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6545 Loss_G: 0.6819 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6866 Loss_G: 0.7114 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.7201 Loss_G: 0.7303 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6924 Loss_G: 0.7345 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6913 Loss_G: 0.7293 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6792 Loss_G: 0.7279 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.7330 Loss_G: 0.7207 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.6915 Loss_G: 0.7143 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6872 Loss_G: 0.7299 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6652 Loss_G: 0.7080 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6811 Loss_G: 0.7168 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6952 Loss_G: 0.6978 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6921 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6856 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6796 Loss_G: 0.7026 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7283 Loss_G: 0.7197 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6884 Loss_G: 0.7212 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7497 Loss_G: 0.7328 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6975 Loss_G: 0.7220 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.7111 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6903 Loss_G: 0.6960 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6495 Loss_G: 0.6958 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6694 Loss_G: 0.7033 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6542 Loss_G: 0.6884 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6968 Loss_G: 0.7108 acc: 81.2%\n",
      "[BATCH 107/149] Loss_D: 0.6551 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6556 Loss_G: 0.6949 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.7788 Loss_G: 0.7375 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.6793 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6700 Loss_G: 0.7001 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6764 Loss_G: 0.7049 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6853 Loss_G: 0.6958 acc: 78.1%\n",
      "[BATCH 114/149] Loss_D: 0.6709 Loss_G: 0.7011 acc: 95.3%\n",
      "[BATCH 115/149] Loss_D: 0.6675 Loss_G: 0.6989 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6512 Loss_G: 0.6797 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6854 Loss_G: 0.7051 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.6974 Loss_G: 0.7241 acc: 95.3%\n",
      "[BATCH 119/149] Loss_D: 0.6955 Loss_G: 0.7221 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.7168 Loss_G: 0.7314 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7196 Loss_G: 0.7099 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6966 Loss_G: 0.7207 acc: 96.9%\n",
      "[BATCH 123/149] Loss_D: 0.7020 Loss_G: 0.7187 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.7435 Loss_G: 0.7328 acc: 92.2%\n",
      "[EPOCH 3700] TEST ACC is : 77.0%\n",
      "[BATCH 125/149] Loss_D: 0.7194 Loss_G: 0.7366 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6838 Loss_G: 0.7192 acc: 95.3%\n",
      "[BATCH 127/149] Loss_D: 0.6960 Loss_G: 0.7088 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.6601 Loss_G: 0.7053 acc: 93.8%\n",
      "[BATCH 129/149] Loss_D: 0.7321 Loss_G: 0.7019 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6841 Loss_G: 0.7012 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7097 Loss_G: 0.7051 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.7163 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7167 Loss_G: 0.7237 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7069 Loss_G: 0.7168 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7011 Loss_G: 0.7035 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6544 Loss_G: 0.6916 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6891 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6868 Loss_G: 0.7187 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6578 Loss_G: 0.7047 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.6800 Loss_G: 0.6989 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.7162 Loss_G: 0.7100 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.6600 Loss_G: 0.7237 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6862 Loss_G: 0.7111 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6978 Loss_G: 0.7040 acc: 79.7%\n",
      "[BATCH 145/149] Loss_D: 0.6860 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6853 Loss_G: 0.7047 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.6723 Loss_G: 0.7281 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.6583 Loss_G: 0.7165 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7030 Loss_G: 0.7188 acc: 92.2%\n",
      "-----THE [25/50] epoch end-----\n",
      "-----THE [26/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7110 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6626 Loss_G: 0.7321 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6690 Loss_G: 0.7126 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.6715 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6794 Loss_G: 0.7148 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7209 Loss_G: 0.7279 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6723 Loss_G: 0.7331 acc: 95.3%\n",
      "[BATCH 8/149] Loss_D: 0.6990 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6761 Loss_G: 0.7205 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6673 Loss_G: 0.6960 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6870 Loss_G: 0.7110 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7454 Loss_G: 0.7162 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7002 Loss_G: 0.7064 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6956 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7182 Loss_G: 0.7281 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7688 Loss_G: 0.7319 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6922 Loss_G: 0.7077 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6952 Loss_G: 0.7307 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6443 Loss_G: 0.6871 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7410 Loss_G: 0.7147 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6640 Loss_G: 0.7184 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6421 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6755 Loss_G: 0.6903 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.6867 Loss_G: 0.6928 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6908 Loss_G: 0.7010 acc: 87.5%\n",
      "[EPOCH 3750] TEST ACC is : 76.4%\n",
      "[BATCH 26/149] Loss_D: 0.6956 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7349 Loss_G: 0.7578 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6570 Loss_G: 0.7275 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.6666 Loss_G: 0.7128 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6939 Loss_G: 0.7145 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6757 Loss_G: 0.7055 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6550 Loss_G: 0.7326 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6709 Loss_G: 0.7022 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6814 Loss_G: 0.6981 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7013 Loss_G: 0.7044 acc: 78.1%\n",
      "[BATCH 36/149] Loss_D: 0.6703 Loss_G: 0.7054 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6727 Loss_G: 0.7102 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.7307 Loss_G: 0.7526 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.7360 Loss_G: 0.7282 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.7019 Loss_G: 0.7289 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6937 Loss_G: 0.7147 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6497 Loss_G: 0.7036 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6998 Loss_G: 0.7132 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6854 Loss_G: 0.7088 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6741 Loss_G: 0.6998 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7515 Loss_G: 0.7164 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7285 Loss_G: 0.7408 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6638 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6666 Loss_G: 0.6883 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7050 Loss_G: 0.6925 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6989 Loss_G: 0.7086 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7047 Loss_G: 0.7073 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6467 Loss_G: 0.7146 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6700 Loss_G: 0.7093 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6707 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6800 Loss_G: 0.7142 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.6838 Loss_G: 0.7205 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7232 Loss_G: 0.7204 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6759 Loss_G: 0.7271 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6868 Loss_G: 0.7186 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.7025 Loss_G: 0.7237 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7265 Loss_G: 0.7158 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7018 Loss_G: 0.7411 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.7552 Loss_G: 0.7382 acc: 81.2%\n",
      "[BATCH 65/149] Loss_D: 0.7070 Loss_G: 0.7173 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6623 Loss_G: 0.7064 acc: 93.8%\n",
      "[BATCH 67/149] Loss_D: 0.6626 Loss_G: 0.7183 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6697 Loss_G: 0.7174 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6873 Loss_G: 0.7133 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6674 Loss_G: 0.7053 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6881 Loss_G: 0.7139 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7063 Loss_G: 0.7314 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6970 Loss_G: 0.7194 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7078 Loss_G: 0.7284 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6715 Loss_G: 0.7046 acc: 81.2%\n",
      "[EPOCH 3800] TEST ACC is : 76.4%\n",
      "[BATCH 76/149] Loss_D: 0.6796 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6954 Loss_G: 0.7378 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6735 Loss_G: 0.7012 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6639 Loss_G: 0.7014 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6972 Loss_G: 0.7188 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6789 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6948 Loss_G: 0.7317 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6910 Loss_G: 0.7677 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7781 Loss_G: 0.8030 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.6982 Loss_G: 0.7492 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6867 Loss_G: 0.7121 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6868 Loss_G: 0.6965 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.7049 Loss_G: 0.7067 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6849 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6763 Loss_G: 0.7185 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7189 Loss_G: 0.7172 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.6853 Loss_G: 0.7089 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.6449 Loss_G: 0.7036 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6820 Loss_G: 0.6834 acc: 82.8%\n",
      "[BATCH 95/149] Loss_D: 0.7034 Loss_G: 0.6945 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.6710 Loss_G: 0.7018 acc: 93.8%\n",
      "[BATCH 97/149] Loss_D: 0.6801 Loss_G: 0.7032 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6925 Loss_G: 0.6995 acc: 81.2%\n",
      "[BATCH 99/149] Loss_D: 0.7127 Loss_G: 0.7166 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6757 Loss_G: 0.7024 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7066 Loss_G: 0.7043 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6598 Loss_G: 0.6938 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.7098 Loss_G: 0.7164 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6981 Loss_G: 0.7388 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6712 Loss_G: 0.7041 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.7182 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7059 Loss_G: 0.7211 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7011 Loss_G: 0.7232 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.7181 Loss_G: 0.7333 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.7051 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6978 Loss_G: 0.7423 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6713 Loss_G: 0.7056 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6999 Loss_G: 0.7244 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7108 Loss_G: 0.7269 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6820 Loss_G: 0.7312 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.7370 Loss_G: 0.7349 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6840 Loss_G: 0.7060 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6865 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.6754 Loss_G: 0.7019 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7091 Loss_G: 0.7182 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7039 Loss_G: 0.7199 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6762 Loss_G: 0.7119 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6450 Loss_G: 0.6924 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6682 Loss_G: 0.6835 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6749 Loss_G: 0.7144 acc: 92.2%\n",
      "[EPOCH 3850] TEST ACC is : 76.2%\n",
      "[BATCH 126/149] Loss_D: 0.7278 Loss_G: 0.7271 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6912 Loss_G: 0.7337 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6552 Loss_G: 0.7185 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7049 Loss_G: 0.7122 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6848 Loss_G: 0.7210 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.7216 Loss_G: 0.7639 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.7405 Loss_G: 0.7610 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7120 Loss_G: 0.7238 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.6643 Loss_G: 0.7121 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6597 Loss_G: 0.6860 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6914 Loss_G: 0.6888 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6784 Loss_G: 0.6918 acc: 82.8%\n",
      "[BATCH 138/149] Loss_D: 0.6813 Loss_G: 0.6918 acc: 95.3%\n",
      "[BATCH 139/149] Loss_D: 0.7079 Loss_G: 0.7046 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6950 Loss_G: 0.7088 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6947 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6986 Loss_G: 0.7211 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6708 Loss_G: 0.7119 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6911 Loss_G: 0.7146 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6669 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6832 Loss_G: 0.7014 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6565 Loss_G: 0.6990 acc: 95.3%\n",
      "[BATCH 148/149] Loss_D: 0.7477 Loss_G: 0.7181 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7539 Loss_G: 0.7402 acc: 82.8%\n",
      "-----THE [26/50] epoch end-----\n",
      "-----THE [27/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6762 Loss_G: 0.7579 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6730 Loss_G: 0.7339 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.6856 Loss_G: 0.7069 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.7270 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6928 Loss_G: 0.7155 acc: 95.3%\n",
      "[BATCH 6/149] Loss_D: 0.6547 Loss_G: 0.6990 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.6875 Loss_G: 0.7066 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6841 Loss_G: 0.7155 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.7283 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6757 Loss_G: 0.7069 acc: 95.3%\n",
      "[BATCH 11/149] Loss_D: 0.6924 Loss_G: 0.7098 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6821 Loss_G: 0.7057 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6864 Loss_G: 0.6897 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.7022 Loss_G: 0.7301 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6805 Loss_G: 0.7103 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.6512 Loss_G: 0.6831 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6913 Loss_G: 0.6935 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7059 Loss_G: 0.7015 acc: 81.2%\n",
      "[BATCH 19/149] Loss_D: 0.6933 Loss_G: 0.7460 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6681 Loss_G: 0.7500 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6970 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.7084 Loss_G: 0.7244 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6724 Loss_G: 0.7190 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.7023 Loss_G: 0.7147 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6782 Loss_G: 0.7113 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6789 Loss_G: 0.7033 acc: 89.1%\n",
      "[EPOCH 3900] TEST ACC is : 77.5%\n",
      "[BATCH 27/149] Loss_D: 0.6851 Loss_G: 0.6992 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6691 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.6906 Loss_G: 0.7133 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.6442 Loss_G: 0.7043 acc: 93.8%\n",
      "[BATCH 31/149] Loss_D: 0.7453 Loss_G: 0.7068 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6864 Loss_G: 0.6911 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.6862 Loss_G: 0.6937 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7033 Loss_G: 0.6994 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.6815 Loss_G: 0.6928 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6870 Loss_G: 0.7029 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7069 Loss_G: 0.7189 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6667 Loss_G: 0.7135 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.7025 Loss_G: 0.6945 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.6562 Loss_G: 0.6955 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6580 Loss_G: 0.6961 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.7337 Loss_G: 0.7069 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.6896 Loss_G: 0.7265 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.6789 Loss_G: 0.6842 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.6835 Loss_G: 0.6997 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6481 Loss_G: 0.6939 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.7067 Loss_G: 0.6974 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6657 Loss_G: 0.7041 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.6930 Loss_G: 0.7354 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7230 Loss_G: 0.7539 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6732 Loss_G: 0.7488 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6923 Loss_G: 0.7427 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6751 Loss_G: 0.7117 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.7209 Loss_G: 0.7060 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7408 Loss_G: 0.7092 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6987 Loss_G: 0.7097 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.6958 Loss_G: 0.7078 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.7439 Loss_G: 0.7291 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6813 Loss_G: 0.7091 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6882 Loss_G: 0.7111 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6672 Loss_G: 0.7047 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6869 Loss_G: 0.7160 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.7129 Loss_G: 0.7260 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.6963 Loss_G: 0.7027 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7001 Loss_G: 0.6941 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7223 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7252 Loss_G: 0.7264 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.7078 Loss_G: 0.7343 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6915 Loss_G: 0.7219 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6806 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7348 Loss_G: 0.7434 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7157 Loss_G: 0.7463 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6604 Loss_G: 0.7110 acc: 95.3%\n",
      "[BATCH 74/149] Loss_D: 0.6773 Loss_G: 0.6991 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6838 Loss_G: 0.7079 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6825 Loss_G: 0.7009 acc: 92.2%\n",
      "[EPOCH 3950] TEST ACC is : 77.0%\n",
      "[BATCH 77/149] Loss_D: 0.6673 Loss_G: 0.6915 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.7088 Loss_G: 0.7162 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.7047 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6824 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.7007 Loss_G: 0.7000 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6720 Loss_G: 0.7030 acc: 81.2%\n",
      "[BATCH 83/149] Loss_D: 0.7334 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6614 Loss_G: 0.7133 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.6772 Loss_G: 0.7205 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.7090 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7159 Loss_G: 0.7269 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6687 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.7329 Loss_G: 0.7394 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6753 Loss_G: 0.7221 acc: 79.7%\n",
      "[BATCH 91/149] Loss_D: 0.6734 Loss_G: 0.7111 acc: 93.8%\n",
      "[BATCH 92/149] Loss_D: 0.7047 Loss_G: 0.7228 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7452 Loss_G: 0.7359 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6999 Loss_G: 0.7340 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6825 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7176 Loss_G: 0.7406 acc: 95.3%\n",
      "[BATCH 97/149] Loss_D: 0.6720 Loss_G: 0.7174 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.6779 Loss_G: 0.7160 acc: 93.8%\n",
      "[BATCH 99/149] Loss_D: 0.6760 Loss_G: 0.7060 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6815 Loss_G: 0.7076 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.6693 Loss_G: 0.7143 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7069 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6684 Loss_G: 0.6923 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6607 Loss_G: 0.7122 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6865 Loss_G: 0.6892 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.7095 Loss_G: 0.7033 acc: 81.2%\n",
      "[BATCH 107/149] Loss_D: 0.6801 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.7000 Loss_G: 0.7079 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7036 Loss_G: 0.7149 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.6913 Loss_G: 0.7166 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6744 Loss_G: 0.6947 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.6909 Loss_G: 0.6894 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6686 Loss_G: 0.7026 acc: 95.3%\n",
      "[BATCH 114/149] Loss_D: 0.6749 Loss_G: 0.6941 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6684 Loss_G: 0.7024 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6918 Loss_G: 0.6921 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7013 Loss_G: 0.7006 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6571 Loss_G: 0.7080 acc: 96.9%\n",
      "[BATCH 119/149] Loss_D: 0.6975 Loss_G: 0.7133 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6729 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6971 Loss_G: 0.7135 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6770 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.6975 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6811 Loss_G: 0.7166 acc: 96.9%\n",
      "[BATCH 125/149] Loss_D: 0.7070 Loss_G: 0.7148 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6888 Loss_G: 0.7385 acc: 90.6%\n",
      "[EPOCH 4000] TEST ACC is : 77.3%\n",
      "[BATCH 127/149] Loss_D: 0.7006 Loss_G: 0.7106 acc: 78.1%\n",
      "[BATCH 128/149] Loss_D: 0.6994 Loss_G: 0.7444 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7293 Loss_G: 0.7198 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.6888 Loss_G: 0.7122 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6703 Loss_G: 0.7187 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7146 Loss_G: 0.7399 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6621 Loss_G: 0.7167 acc: 98.4%\n",
      "[BATCH 134/149] Loss_D: 0.6867 Loss_G: 0.7093 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6946 Loss_G: 0.7291 acc: 95.3%\n",
      "[BATCH 136/149] Loss_D: 0.7008 Loss_G: 0.6990 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7471 Loss_G: 0.7121 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7044 Loss_G: 0.7296 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.6749 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6715 Loss_G: 0.7247 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6951 Loss_G: 0.7189 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.7148 Loss_G: 0.7259 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6669 Loss_G: 0.7067 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6956 Loss_G: 0.7112 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6888 Loss_G: 0.7194 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6843 Loss_G: 0.7196 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.6867 Loss_G: 0.7089 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6892 Loss_G: 0.7194 acc: 95.3%\n",
      "[BATCH 149/149] Loss_D: 0.7533 Loss_G: 0.7340 acc: 84.4%\n",
      "-----THE [27/50] epoch end-----\n",
      "-----THE [28/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7086 Loss_G: 0.7089 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7105 Loss_G: 0.7075 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7254 Loss_G: 0.7237 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6958 Loss_G: 0.7200 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6546 Loss_G: 0.7326 acc: 93.8%\n",
      "[BATCH 6/149] Loss_D: 0.6797 Loss_G: 0.7193 acc: 96.9%\n",
      "[BATCH 7/149] Loss_D: 0.6973 Loss_G: 0.7163 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.7319 Loss_G: 0.7202 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.6842 Loss_G: 0.7464 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6855 Loss_G: 0.7365 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6732 Loss_G: 0.7573 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6768 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.7058 Loss_G: 0.7111 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6796 Loss_G: 0.7180 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6806 Loss_G: 0.7240 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7143 Loss_G: 0.7210 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6893 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6715 Loss_G: 0.6988 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6731 Loss_G: 0.7010 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7374 Loss_G: 0.7147 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7223 Loss_G: 0.7352 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.7245 Loss_G: 0.7270 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6954 Loss_G: 0.7223 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7058 Loss_G: 0.7189 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7344 Loss_G: 0.7450 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6740 Loss_G: 0.7212 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6847 Loss_G: 0.7015 acc: 84.4%\n",
      "[EPOCH 4050] TEST ACC is : 75.4%\n",
      "[BATCH 28/149] Loss_D: 0.6276 Loss_G: 0.6832 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6702 Loss_G: 0.6850 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6966 Loss_G: 0.6865 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.7286 Loss_G: 0.7097 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6654 Loss_G: 0.7218 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6990 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6703 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7114 Loss_G: 0.7184 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7029 Loss_G: 0.7195 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.6876 Loss_G: 0.7284 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6726 Loss_G: 0.7271 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6688 Loss_G: 0.7072 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7003 Loss_G: 0.7293 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.7094 Loss_G: 0.7151 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6912 Loss_G: 0.7136 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6868 Loss_G: 0.7250 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6793 Loss_G: 0.7079 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6793 Loss_G: 0.7098 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6640 Loss_G: 0.7135 acc: 93.8%\n",
      "[BATCH 47/149] Loss_D: 0.7246 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6752 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6903 Loss_G: 0.7190 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.7086 Loss_G: 0.7439 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.7185 Loss_G: 0.7291 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.6741 Loss_G: 0.7160 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6846 Loss_G: 0.7386 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6882 Loss_G: 0.7142 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.7031 Loss_G: 0.7161 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.7021 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6945 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6786 Loss_G: 0.7211 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6710 Loss_G: 0.7040 acc: 81.2%\n",
      "[BATCH 60/149] Loss_D: 0.6838 Loss_G: 0.7108 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6625 Loss_G: 0.6986 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7249 Loss_G: 0.7349 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.6767 Loss_G: 0.7217 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6834 Loss_G: 0.7303 acc: 95.3%\n",
      "[BATCH 65/149] Loss_D: 0.6827 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7063 Loss_G: 0.7110 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6990 Loss_G: 0.7090 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7061 Loss_G: 0.7198 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.6951 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6892 Loss_G: 0.7079 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6841 Loss_G: 0.7102 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6910 Loss_G: 0.7023 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7030 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7201 Loss_G: 0.7448 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7059 Loss_G: 0.7447 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7009 Loss_G: 0.7349 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6930 Loss_G: 0.7191 acc: 81.2%\n",
      "[EPOCH 4100] TEST ACC is : 77.3%\n",
      "[BATCH 78/149] Loss_D: 0.6817 Loss_G: 0.7232 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.7023 Loss_G: 0.7331 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6748 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6915 Loss_G: 0.6995 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.7158 Loss_G: 0.7092 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6743 Loss_G: 0.7040 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.7011 Loss_G: 0.7126 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6706 Loss_G: 0.7264 acc: 95.3%\n",
      "[BATCH 86/149] Loss_D: 0.6598 Loss_G: 0.6958 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6580 Loss_G: 0.6689 acc: 81.2%\n",
      "[BATCH 88/149] Loss_D: 0.6865 Loss_G: 0.6860 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.6402 Loss_G: 0.7038 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6839 Loss_G: 0.7134 acc: 95.3%\n",
      "[BATCH 91/149] Loss_D: 0.6988 Loss_G: 0.7218 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6887 Loss_G: 0.6970 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.6816 Loss_G: 0.7164 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6680 Loss_G: 0.7009 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.7228 Loss_G: 0.7031 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.6736 Loss_G: 0.7029 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6813 Loss_G: 0.7118 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6644 Loss_G: 0.7264 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7238 Loss_G: 0.7283 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.7077 Loss_G: 0.7506 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6754 Loss_G: 0.7165 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.7216 Loss_G: 0.7312 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7103 Loss_G: 0.6975 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.6857 Loss_G: 0.6890 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.6499 Loss_G: 0.6963 acc: 93.8%\n",
      "[BATCH 106/149] Loss_D: 0.7033 Loss_G: 0.6970 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6998 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7022 Loss_G: 0.7310 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6542 Loss_G: 0.7116 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.7095 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6806 Loss_G: 0.6972 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.6505 Loss_G: 0.7060 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6923 Loss_G: 0.6957 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6921 Loss_G: 0.7036 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6731 Loss_G: 0.7084 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6633 Loss_G: 0.7065 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6721 Loss_G: 0.7098 acc: 95.3%\n",
      "[BATCH 118/149] Loss_D: 0.6678 Loss_G: 0.7114 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.7167 Loss_G: 0.7091 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.7067 Loss_G: 0.7067 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7071 Loss_G: 0.7188 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6949 Loss_G: 0.7215 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.6741 Loss_G: 0.7279 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.7671 Loss_G: 0.7496 acc: 81.2%\n",
      "[BATCH 125/149] Loss_D: 0.6893 Loss_G: 0.6998 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.7079 Loss_G: 0.6975 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6987 Loss_G: 0.7212 acc: 92.2%\n",
      "[EPOCH 4150] TEST ACC is : 76.8%\n",
      "[BATCH 128/149] Loss_D: 0.6899 Loss_G: 0.7127 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6816 Loss_G: 0.7023 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6721 Loss_G: 0.7014 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6727 Loss_G: 0.7044 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.7014 Loss_G: 0.7056 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.6866 Loss_G: 0.7283 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6943 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6755 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 136/149] Loss_D: 0.7274 Loss_G: 0.7194 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6905 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6836 Loss_G: 0.6927 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6812 Loss_G: 0.6869 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.6779 Loss_G: 0.6882 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6802 Loss_G: 0.6907 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6844 Loss_G: 0.7079 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.7225 Loss_G: 0.7192 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.7172 Loss_G: 0.7385 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6823 Loss_G: 0.7147 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6928 Loss_G: 0.7102 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6824 Loss_G: 0.7375 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6834 Loss_G: 0.7094 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.7293 Loss_G: 0.7259 acc: 87.5%\n",
      "-----THE [28/50] epoch end-----\n",
      "-----THE [29/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6423 Loss_G: 0.7119 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6886 Loss_G: 0.7120 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6683 Loss_G: 0.7022 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6659 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6612 Loss_G: 0.6927 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6650 Loss_G: 0.6982 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.7081 Loss_G: 0.7137 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6907 Loss_G: 0.7447 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6742 Loss_G: 0.7141 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7502 Loss_G: 0.7440 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7057 Loss_G: 0.7566 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6535 Loss_G: 0.6980 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7093 Loss_G: 0.6974 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6397 Loss_G: 0.7093 acc: 95.3%\n",
      "[BATCH 15/149] Loss_D: 0.7072 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6989 Loss_G: 0.7406 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.6811 Loss_G: 0.7285 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6819 Loss_G: 0.6888 acc: 79.7%\n",
      "[BATCH 19/149] Loss_D: 0.6872 Loss_G: 0.7064 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6901 Loss_G: 0.7549 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6687 Loss_G: 0.7251 acc: 95.3%\n",
      "[BATCH 22/149] Loss_D: 0.6998 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6695 Loss_G: 0.7062 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7108 Loss_G: 0.7247 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7077 Loss_G: 0.7228 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6719 Loss_G: 0.7358 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7045 Loss_G: 0.7315 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.6686 Loss_G: 0.7046 acc: 85.9%\n",
      "[EPOCH 4200] TEST ACC is : 77.0%\n",
      "[BATCH 29/149] Loss_D: 0.6744 Loss_G: 0.6736 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7319 Loss_G: 0.6961 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6966 Loss_G: 0.7047 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6960 Loss_G: 0.7066 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.7194 Loss_G: 0.7302 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.6862 Loss_G: 0.6942 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.6652 Loss_G: 0.7085 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6915 Loss_G: 0.7211 acc: 93.8%\n",
      "[BATCH 37/149] Loss_D: 0.6829 Loss_G: 0.7091 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6761 Loss_G: 0.7079 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6855 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7019 Loss_G: 0.7124 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.6994 Loss_G: 0.7274 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6803 Loss_G: 0.7298 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7201 Loss_G: 0.7464 acc: 81.2%\n",
      "[BATCH 44/149] Loss_D: 0.6789 Loss_G: 0.7153 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6811 Loss_G: 0.7210 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6918 Loss_G: 0.7093 acc: 82.8%\n",
      "[BATCH 47/149] Loss_D: 0.7195 Loss_G: 0.7320 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6609 Loss_G: 0.7151 acc: 95.3%\n",
      "[BATCH 49/149] Loss_D: 0.6956 Loss_G: 0.6950 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6669 Loss_G: 0.7135 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.7194 Loss_G: 0.7126 acc: 79.7%\n",
      "[BATCH 52/149] Loss_D: 0.6849 Loss_G: 0.7063 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6866 Loss_G: 0.7286 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6564 Loss_G: 0.6966 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6958 Loss_G: 0.7127 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7131 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7058 Loss_G: 0.7307 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7336 Loss_G: 0.7323 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7023 Loss_G: 0.7204 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.7023 Loss_G: 0.7194 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7108 Loss_G: 0.7392 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6992 Loss_G: 0.7273 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7369 Loss_G: 0.7321 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7239 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6787 Loss_G: 0.6968 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6826 Loss_G: 0.7225 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7283 Loss_G: 0.7100 acc: 81.2%\n",
      "[BATCH 68/149] Loss_D: 0.6822 Loss_G: 0.7133 acc: 95.3%\n",
      "[BATCH 69/149] Loss_D: 0.7062 Loss_G: 0.7183 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.6841 Loss_G: 0.7129 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7151 Loss_G: 0.7246 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6762 Loss_G: 0.7111 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7210 Loss_G: 0.7346 acc: 79.7%\n",
      "[BATCH 74/149] Loss_D: 0.7540 Loss_G: 0.7507 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7040 Loss_G: 0.7087 acc: 81.2%\n",
      "[BATCH 76/149] Loss_D: 0.6928 Loss_G: 0.6950 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.6739 Loss_G: 0.7058 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6986 Loss_G: 0.7008 acc: 82.8%\n",
      "[EPOCH 4250] TEST ACC is : 76.6%\n",
      "[BATCH 79/149] Loss_D: 0.7018 Loss_G: 0.7201 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7181 Loss_G: 0.7455 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.7060 Loss_G: 0.7371 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7329 Loss_G: 0.7237 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.7003 Loss_G: 0.7172 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6446 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6664 Loss_G: 0.7112 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.6749 Loss_G: 0.7118 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6844 Loss_G: 0.7026 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7102 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7171 Loss_G: 0.7120 acc: 81.2%\n",
      "[BATCH 90/149] Loss_D: 0.6513 Loss_G: 0.7082 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6699 Loss_G: 0.6986 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7070 Loss_G: 0.7156 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.6996 Loss_G: 0.7292 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6960 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6859 Loss_G: 0.7002 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.6837 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6999 Loss_G: 0.7137 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.7033 Loss_G: 0.7152 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6941 Loss_G: 0.7008 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.7267 Loss_G: 0.7116 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6402 Loss_G: 0.6903 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7047 Loss_G: 0.7039 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.7175 Loss_G: 0.7123 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6597 Loss_G: 0.6976 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.6887 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6927 Loss_G: 0.6962 acc: 81.2%\n",
      "[BATCH 107/149] Loss_D: 0.7560 Loss_G: 0.7301 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.6906 Loss_G: 0.7644 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6633 Loss_G: 0.7064 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.7150 Loss_G: 0.7063 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6740 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6732 Loss_G: 0.7190 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.6801 Loss_G: 0.7026 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7011 Loss_G: 0.7260 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6811 Loss_G: 0.6968 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.7024 Loss_G: 0.7026 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.7148 Loss_G: 0.7176 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7074 Loss_G: 0.7260 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6617 Loss_G: 0.7251 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.7202 Loss_G: 0.7282 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6878 Loss_G: 0.7418 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6951 Loss_G: 0.7117 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6655 Loss_G: 0.7322 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.7421 Loss_G: 0.7210 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6817 Loss_G: 0.7191 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6549 Loss_G: 0.7037 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6824 Loss_G: 0.6912 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6694 Loss_G: 0.6927 acc: 92.2%\n",
      "[EPOCH 4300] TEST ACC is : 77.0%\n",
      "[BATCH 129/149] Loss_D: 0.6582 Loss_G: 0.6999 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.7172 Loss_G: 0.6898 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6683 Loss_G: 0.6909 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6658 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6799 Loss_G: 0.6926 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7073 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6995 Loss_G: 0.7343 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.6712 Loss_G: 0.7091 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6697 Loss_G: 0.7308 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6938 Loss_G: 0.7091 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6768 Loss_G: 0.7388 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6546 Loss_G: 0.6894 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6886 Loss_G: 0.6906 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6989 Loss_G: 0.6924 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6972 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7001 Loss_G: 0.7118 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6875 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6631 Loss_G: 0.7020 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7193 Loss_G: 0.7380 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6458 Loss_G: 0.7185 acc: 96.9%\n",
      "[BATCH 149/149] Loss_D: 0.7168 Loss_G: 0.7282 acc: 89.1%\n",
      "-----THE [29/50] epoch end-----\n",
      "-----THE [30/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6599 Loss_G: 0.7249 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6929 Loss_G: 0.7022 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7277 Loss_G: 0.7138 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6877 Loss_G: 0.7193 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.7002 Loss_G: 0.6992 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.6861 Loss_G: 0.7012 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.6692 Loss_G: 0.7082 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6506 Loss_G: 0.7017 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6522 Loss_G: 0.6851 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6658 Loss_G: 0.7016 acc: 93.8%\n",
      "[BATCH 11/149] Loss_D: 0.7008 Loss_G: 0.7024 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6504 Loss_G: 0.6824 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6792 Loss_G: 0.6802 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6994 Loss_G: 0.6934 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6854 Loss_G: 0.7005 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6700 Loss_G: 0.6856 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.6639 Loss_G: 0.6870 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6872 Loss_G: 0.6856 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7082 Loss_G: 0.7078 acc: 82.8%\n",
      "[BATCH 20/149] Loss_D: 0.6654 Loss_G: 0.6945 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6976 Loss_G: 0.7210 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6946 Loss_G: 0.7413 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7542 Loss_G: 0.7460 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6947 Loss_G: 0.7329 acc: 95.3%\n",
      "[BATCH 25/149] Loss_D: 0.6897 Loss_G: 0.7162 acc: 79.7%\n",
      "[BATCH 26/149] Loss_D: 0.6802 Loss_G: 0.7056 acc: 79.7%\n",
      "[BATCH 27/149] Loss_D: 0.6839 Loss_G: 0.7265 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6706 Loss_G: 0.7159 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6791 Loss_G: 0.7332 acc: 87.5%\n",
      "[EPOCH 4350] TEST ACC is : 77.0%\n",
      "[BATCH 30/149] Loss_D: 0.7001 Loss_G: 0.7412 acc: 93.8%\n",
      "[BATCH 31/149] Loss_D: 0.7084 Loss_G: 0.7269 acc: 96.9%\n",
      "[BATCH 32/149] Loss_D: 0.6615 Loss_G: 0.7158 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.7077 Loss_G: 0.7283 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6693 Loss_G: 0.7019 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.7203 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6735 Loss_G: 0.7264 acc: 93.8%\n",
      "[BATCH 37/149] Loss_D: 0.6984 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6891 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.7351 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6891 Loss_G: 0.7144 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.7020 Loss_G: 0.7137 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6702 Loss_G: 0.7121 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.7149 Loss_G: 0.7087 acc: 79.7%\n",
      "[BATCH 44/149] Loss_D: 0.6627 Loss_G: 0.7008 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.7014 Loss_G: 0.7313 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6760 Loss_G: 0.7304 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6737 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6959 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6821 Loss_G: 0.7181 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6644 Loss_G: 0.7017 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6907 Loss_G: 0.7014 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.6895 Loss_G: 0.7115 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7481 Loss_G: 0.7502 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6738 Loss_G: 0.7272 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6499 Loss_G: 0.6952 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.7476 Loss_G: 0.7259 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.7187 Loss_G: 0.7205 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6922 Loss_G: 0.7053 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.6716 Loss_G: 0.7242 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7066 Loss_G: 0.7204 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6800 Loss_G: 0.7126 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.7118 Loss_G: 0.7107 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.6954 Loss_G: 0.7213 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6791 Loss_G: 0.7263 acc: 95.3%\n",
      "[BATCH 65/149] Loss_D: 0.6891 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6576 Loss_G: 0.7105 acc: 93.8%\n",
      "[BATCH 67/149] Loss_D: 0.7303 Loss_G: 0.7320 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7057 Loss_G: 0.7171 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.6926 Loss_G: 0.7126 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6535 Loss_G: 0.7160 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6774 Loss_G: 0.6821 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6894 Loss_G: 0.7011 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7350 Loss_G: 0.6975 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.7319 Loss_G: 0.7391 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7013 Loss_G: 0.7361 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6592 Loss_G: 0.6978 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6884 Loss_G: 0.7276 acc: 95.3%\n",
      "[BATCH 78/149] Loss_D: 0.7205 Loss_G: 0.7359 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.7255 Loss_G: 0.7318 acc: 84.4%\n",
      "[EPOCH 4400] TEST ACC is : 76.0%\n",
      "[BATCH 80/149] Loss_D: 0.7444 Loss_G: 0.7873 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7057 Loss_G: 0.7372 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.7335 Loss_G: 0.7289 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.6847 Loss_G: 0.7281 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6879 Loss_G: 0.7100 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.6933 Loss_G: 0.7171 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.7135 Loss_G: 0.7113 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6982 Loss_G: 0.7133 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.7207 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6569 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6776 Loss_G: 0.6953 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7090 Loss_G: 0.7139 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6854 Loss_G: 0.7070 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.6928 Loss_G: 0.7103 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6796 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.7073 Loss_G: 0.7038 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6931 Loss_G: 0.7225 acc: 93.8%\n",
      "[BATCH 97/149] Loss_D: 0.6566 Loss_G: 0.7017 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.7091 Loss_G: 0.7166 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6591 Loss_G: 0.6913 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6819 Loss_G: 0.6922 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6672 Loss_G: 0.6883 acc: 93.8%\n",
      "[BATCH 102/149] Loss_D: 0.6549 Loss_G: 0.6851 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.7241 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6815 Loss_G: 0.7038 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6587 Loss_G: 0.7057 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7139 Loss_G: 0.7056 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7249 Loss_G: 0.7180 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6805 Loss_G: 0.7287 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6632 Loss_G: 0.7037 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7139 Loss_G: 0.7038 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7290 Loss_G: 0.7217 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7218 Loss_G: 0.7280 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6971 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6757 Loss_G: 0.7309 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7087 Loss_G: 0.7524 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6635 Loss_G: 0.7402 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6989 Loss_G: 0.7227 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.7070 Loss_G: 0.7530 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6998 Loss_G: 0.7124 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6858 Loss_G: 0.7040 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6885 Loss_G: 0.6954 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.7119 Loss_G: 0.7248 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.6737 Loss_G: 0.6991 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6848 Loss_G: 0.7295 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6820 Loss_G: 0.7253 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6567 Loss_G: 0.7045 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.6608 Loss_G: 0.7024 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7092 Loss_G: 0.7206 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7245 Loss_G: 0.7344 acc: 85.9%\n",
      "[EPOCH 4450] TEST ACC is : 76.4%\n",
      "[BATCH 130/149] Loss_D: 0.6642 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7194 Loss_G: 0.7225 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6845 Loss_G: 0.7206 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6912 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6807 Loss_G: 0.7348 acc: 96.9%\n",
      "[BATCH 135/149] Loss_D: 0.6775 Loss_G: 0.7051 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6925 Loss_G: 0.6988 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6752 Loss_G: 0.7143 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6794 Loss_G: 0.7162 acc: 81.2%\n",
      "[BATCH 139/149] Loss_D: 0.6782 Loss_G: 0.7103 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7249 Loss_G: 0.7186 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6704 Loss_G: 0.7216 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6511 Loss_G: 0.7085 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.6787 Loss_G: 0.7103 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6647 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7012 Loss_G: 0.7156 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6805 Loss_G: 0.7321 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6798 Loss_G: 0.6950 acc: 81.2%\n",
      "[BATCH 148/149] Loss_D: 0.7070 Loss_G: 0.7128 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.6838 Loss_G: 0.7118 acc: 93.8%\n",
      "-----THE [30/50] epoch end-----\n",
      "-----THE [31/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6785 Loss_G: 0.6876 acc: 81.2%\n",
      "[BATCH 2/149] Loss_D: 0.6878 Loss_G: 0.6915 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6734 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7196 Loss_G: 0.7230 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6830 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.7128 Loss_G: 0.7392 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6889 Loss_G: 0.7253 acc: 96.9%\n",
      "[BATCH 8/149] Loss_D: 0.6349 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.7258 Loss_G: 0.7136 acc: 84.4%\n",
      "[BATCH 10/149] Loss_D: 0.7051 Loss_G: 0.7091 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.6941 Loss_G: 0.6998 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7617 Loss_G: 0.7245 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6958 Loss_G: 0.7242 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6995 Loss_G: 0.7036 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.7132 Loss_G: 0.7024 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6773 Loss_G: 0.7020 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6962 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.7199 Loss_G: 0.7061 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6816 Loss_G: 0.6887 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6779 Loss_G: 0.6960 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.7143 Loss_G: 0.7256 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6985 Loss_G: 0.7339 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.6760 Loss_G: 0.7167 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.7090 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.7027 Loss_G: 0.7164 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6925 Loss_G: 0.7247 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6987 Loss_G: 0.7312 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6883 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6847 Loss_G: 0.7322 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6903 Loss_G: 0.7118 acc: 90.6%\n",
      "[EPOCH 4500] TEST ACC is : 76.6%\n",
      "[BATCH 31/149] Loss_D: 0.6996 Loss_G: 0.7574 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7091 Loss_G: 0.7409 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6640 Loss_G: 0.7177 acc: 95.3%\n",
      "[BATCH 34/149] Loss_D: 0.7025 Loss_G: 0.7335 acc: 95.3%\n",
      "[BATCH 35/149] Loss_D: 0.6779 Loss_G: 0.7101 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6777 Loss_G: 0.7402 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6613 Loss_G: 0.6859 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6968 Loss_G: 0.6870 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7163 Loss_G: 0.7100 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6814 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.6769 Loss_G: 0.7324 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6912 Loss_G: 0.6937 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6822 Loss_G: 0.7013 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6974 Loss_G: 0.7038 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7234 Loss_G: 0.7470 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6875 Loss_G: 0.7294 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6864 Loss_G: 0.7035 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6720 Loss_G: 0.6942 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7025 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.7018 Loss_G: 0.6987 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6866 Loss_G: 0.7021 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.6531 Loss_G: 0.6980 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7178 Loss_G: 0.7391 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6752 Loss_G: 0.7560 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6840 Loss_G: 0.7403 acc: 95.3%\n",
      "[BATCH 56/149] Loss_D: 0.6585 Loss_G: 0.7331 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.6857 Loss_G: 0.7019 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7179 Loss_G: 0.6967 acc: 76.6%\n",
      "[BATCH 59/149] Loss_D: 0.6717 Loss_G: 0.6948 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6598 Loss_G: 0.7191 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7105 Loss_G: 0.7151 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6734 Loss_G: 0.7168 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7109 Loss_G: 0.7118 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6744 Loss_G: 0.7136 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6594 Loss_G: 0.6879 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6731 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6942 Loss_G: 0.7019 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6652 Loss_G: 0.6892 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6621 Loss_G: 0.6944 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7140 Loss_G: 0.7043 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6811 Loss_G: 0.7111 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7346 Loss_G: 0.7039 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.6893 Loss_G: 0.7048 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6870 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7145 Loss_G: 0.7396 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.7354 Loss_G: 0.7201 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7059 Loss_G: 0.7110 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6380 Loss_G: 0.7130 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6813 Loss_G: 0.7128 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6704 Loss_G: 0.6935 acc: 89.1%\n",
      "[EPOCH 4550] TEST ACC is : 77.0%\n",
      "[BATCH 81/149] Loss_D: 0.6739 Loss_G: 0.6900 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6982 Loss_G: 0.6953 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6658 Loss_G: 0.7032 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.7091 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.7069 Loss_G: 0.7271 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7156 Loss_G: 0.7212 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6702 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.7006 Loss_G: 0.7109 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.6709 Loss_G: 0.7053 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.7417 Loss_G: 0.7295 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6897 Loss_G: 0.7201 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6756 Loss_G: 0.6844 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7010 Loss_G: 0.7000 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6798 Loss_G: 0.7209 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6640 Loss_G: 0.7067 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6719 Loss_G: 0.6911 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.6440 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6600 Loss_G: 0.7004 acc: 95.3%\n",
      "[BATCH 99/149] Loss_D: 0.6471 Loss_G: 0.6832 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6931 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6996 Loss_G: 0.7045 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7312 Loss_G: 0.7086 acc: 76.6%\n",
      "[BATCH 103/149] Loss_D: 0.6825 Loss_G: 0.6955 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.6496 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.7047 Loss_G: 0.7168 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7171 Loss_G: 0.7136 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6954 Loss_G: 0.7013 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7272 Loss_G: 0.7147 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6923 Loss_G: 0.7172 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7110 Loss_G: 0.7159 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6912 Loss_G: 0.7149 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.6883 Loss_G: 0.7299 acc: 95.3%\n",
      "[BATCH 113/149] Loss_D: 0.6903 Loss_G: 0.7149 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6837 Loss_G: 0.7269 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6836 Loss_G: 0.7221 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7141 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6834 Loss_G: 0.7150 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.6812 Loss_G: 0.7116 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.6687 Loss_G: 0.7040 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.7048 Loss_G: 0.6949 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.7041 Loss_G: 0.7240 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6731 Loss_G: 0.7171 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6252 Loss_G: 0.6957 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6910 Loss_G: 0.6901 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6704 Loss_G: 0.7012 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6974 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6737 Loss_G: 0.7023 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7131 Loss_G: 0.7075 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6905 Loss_G: 0.7300 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6755 Loss_G: 0.7057 acc: 85.9%\n",
      "[EPOCH 4600] TEST ACC is : 75.2%\n",
      "[BATCH 131/149] Loss_D: 0.6786 Loss_G: 0.7286 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.6798 Loss_G: 0.7229 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6889 Loss_G: 0.7326 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.6832 Loss_G: 0.7042 acc: 96.9%\n",
      "[BATCH 135/149] Loss_D: 0.6873 Loss_G: 0.6950 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6851 Loss_G: 0.6932 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6570 Loss_G: 0.6915 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7285 Loss_G: 0.7125 acc: 78.1%\n",
      "[BATCH 139/149] Loss_D: 0.7415 Loss_G: 0.7527 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7304 Loss_G: 0.7697 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.7072 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7686 Loss_G: 0.7201 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7035 Loss_G: 0.7154 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6676 Loss_G: 0.7326 acc: 95.3%\n",
      "[BATCH 145/149] Loss_D: 0.6882 Loss_G: 0.7036 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.6925 Loss_G: 0.6939 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6803 Loss_G: 0.6976 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6974 Loss_G: 0.7027 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.7301 Loss_G: 0.7406 acc: 87.5%\n",
      "-----THE [31/50] epoch end-----\n",
      "-----THE [32/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6836 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7093 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6881 Loss_G: 0.7027 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6949 Loss_G: 0.6903 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6693 Loss_G: 0.6965 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6559 Loss_G: 0.6969 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7114 Loss_G: 0.7330 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6630 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.7033 Loss_G: 0.7360 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.6729 Loss_G: 0.7255 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.6962 Loss_G: 0.7054 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6750 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6748 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6539 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6486 Loss_G: 0.7231 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6706 Loss_G: 0.6989 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6970 Loss_G: 0.7094 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7289 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7170 Loss_G: 0.7283 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.6970 Loss_G: 0.7259 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7064 Loss_G: 0.7119 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7069 Loss_G: 0.7071 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6623 Loss_G: 0.7195 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6975 Loss_G: 0.7088 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.6832 Loss_G: 0.6901 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.7223 Loss_G: 0.7180 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.6949 Loss_G: 0.7027 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.7256 Loss_G: 0.7261 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6826 Loss_G: 0.7138 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6682 Loss_G: 0.6949 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6628 Loss_G: 0.7104 acc: 92.2%\n",
      "[EPOCH 4650] TEST ACC is : 76.8%\n",
      "[BATCH 32/149] Loss_D: 0.6778 Loss_G: 0.7034 acc: 93.8%\n",
      "[BATCH 33/149] Loss_D: 0.6518 Loss_G: 0.7047 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6749 Loss_G: 0.7096 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6889 Loss_G: 0.7155 acc: 81.2%\n",
      "[BATCH 36/149] Loss_D: 0.7279 Loss_G: 0.7133 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.6738 Loss_G: 0.7028 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.6933 Loss_G: 0.7043 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6458 Loss_G: 0.6908 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7055 Loss_G: 0.7193 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.7339 Loss_G: 0.7443 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.6706 Loss_G: 0.7141 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.7007 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7251 Loss_G: 0.7490 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.7020 Loss_G: 0.7250 acc: 82.8%\n",
      "[BATCH 46/149] Loss_D: 0.7143 Loss_G: 0.7250 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6870 Loss_G: 0.7129 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.7136 Loss_G: 0.7208 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.6649 Loss_G: 0.7078 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.7110 Loss_G: 0.7106 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.7064 Loss_G: 0.7010 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7015 Loss_G: 0.6901 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.6695 Loss_G: 0.6820 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.7148 Loss_G: 0.7103 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.7084 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6818 Loss_G: 0.7596 acc: 78.1%\n",
      "[BATCH 57/149] Loss_D: 0.6932 Loss_G: 0.7290 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7180 Loss_G: 0.7396 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6754 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6962 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6662 Loss_G: 0.7260 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7081 Loss_G: 0.7297 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.7056 Loss_G: 0.7198 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6485 Loss_G: 0.7099 acc: 93.8%\n",
      "[BATCH 65/149] Loss_D: 0.6887 Loss_G: 0.7163 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6830 Loss_G: 0.6914 acc: 81.2%\n",
      "[BATCH 67/149] Loss_D: 0.6741 Loss_G: 0.6901 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7037 Loss_G: 0.6998 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6946 Loss_G: 0.7241 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6697 Loss_G: 0.7368 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.7196 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6767 Loss_G: 0.7139 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.6659 Loss_G: 0.7664 acc: 96.9%\n",
      "[BATCH 74/149] Loss_D: 0.7026 Loss_G: 0.7294 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6677 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6910 Loss_G: 0.6939 acc: 93.8%\n",
      "[BATCH 77/149] Loss_D: 0.6924 Loss_G: 0.6945 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6761 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.6727 Loss_G: 0.7589 acc: 95.3%\n",
      "[BATCH 80/149] Loss_D: 0.7454 Loss_G: 0.7465 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6818 Loss_G: 0.7075 acc: 89.1%\n",
      "[EPOCH 4700] TEST ACC is : 76.4%\n",
      "[BATCH 82/149] Loss_D: 0.7105 Loss_G: 0.7039 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.7298 Loss_G: 0.7314 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6837 Loss_G: 0.7141 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6793 Loss_G: 0.6976 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6793 Loss_G: 0.6887 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.6858 Loss_G: 0.7016 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7010 Loss_G: 0.7001 acc: 79.7%\n",
      "[BATCH 89/149] Loss_D: 0.6791 Loss_G: 0.6908 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7085 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6991 Loss_G: 0.7437 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.7103 Loss_G: 0.7375 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6708 Loss_G: 0.7245 acc: 95.3%\n",
      "[BATCH 94/149] Loss_D: 0.7156 Loss_G: 0.7330 acc: 82.8%\n",
      "[BATCH 95/149] Loss_D: 0.6633 Loss_G: 0.7032 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.7183 Loss_G: 0.7039 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7289 Loss_G: 0.7127 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.6988 Loss_G: 0.7212 acc: 82.8%\n",
      "[BATCH 99/149] Loss_D: 0.6646 Loss_G: 0.7103 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6784 Loss_G: 0.7257 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6762 Loss_G: 0.7156 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7169 Loss_G: 0.7048 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6838 Loss_G: 0.7093 acc: 93.8%\n",
      "[BATCH 104/149] Loss_D: 0.6767 Loss_G: 0.7065 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6453 Loss_G: 0.7252 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6677 Loss_G: 0.6852 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6570 Loss_G: 0.6882 acc: 95.3%\n",
      "[BATCH 108/149] Loss_D: 0.6840 Loss_G: 0.7267 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.7203 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.7236 Loss_G: 0.7468 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7124 Loss_G: 0.7181 acc: 78.1%\n",
      "[BATCH 112/149] Loss_D: 0.6934 Loss_G: 0.7273 acc: 93.8%\n",
      "[BATCH 113/149] Loss_D: 0.6893 Loss_G: 0.7198 acc: 93.8%\n",
      "[BATCH 114/149] Loss_D: 0.6992 Loss_G: 0.7287 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.7158 Loss_G: 0.7280 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7400 Loss_G: 0.7399 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7039 Loss_G: 0.7097 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7023 Loss_G: 0.7165 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7039 Loss_G: 0.7300 acc: 95.3%\n",
      "[BATCH 120/149] Loss_D: 0.6768 Loss_G: 0.7189 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.6930 Loss_G: 0.7074 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.7424 Loss_G: 0.7259 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.6797 Loss_G: 0.7025 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6919 Loss_G: 0.7074 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6944 Loss_G: 0.7104 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7054 Loss_G: 0.7213 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.7078 Loss_G: 0.7290 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6548 Loss_G: 0.7149 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.7061 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.7011 Loss_G: 0.7215 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6817 Loss_G: 0.7296 acc: 87.5%\n",
      "[EPOCH 4750] TEST ACC is : 74.6%\n",
      "[BATCH 132/149] Loss_D: 0.6758 Loss_G: 0.7069 acc: 93.8%\n",
      "[BATCH 133/149] Loss_D: 0.6819 Loss_G: 0.7147 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.6410 Loss_G: 0.6987 acc: 93.8%\n",
      "[BATCH 135/149] Loss_D: 0.6693 Loss_G: 0.7086 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6897 Loss_G: 0.7115 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.7071 Loss_G: 0.7275 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6951 Loss_G: 0.7104 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6763 Loss_G: 0.7164 acc: 81.2%\n",
      "[BATCH 140/149] Loss_D: 0.6466 Loss_G: 0.6870 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6860 Loss_G: 0.6887 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6937 Loss_G: 0.6985 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6627 Loss_G: 0.6877 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6798 Loss_G: 0.7010 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.6695 Loss_G: 0.7171 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7424 Loss_G: 0.7321 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6506 Loss_G: 0.6932 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6691 Loss_G: 0.6844 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7231 Loss_G: 0.6984 acc: 87.5%\n",
      "-----THE [32/50] epoch end-----\n",
      "-----THE [33/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7090 Loss_G: 0.7059 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6915 Loss_G: 0.7429 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.7199 Loss_G: 0.7173 acc: 79.7%\n",
      "[BATCH 4/149] Loss_D: 0.6719 Loss_G: 0.7085 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7061 Loss_G: 0.7036 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6922 Loss_G: 0.7073 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.6501 Loss_G: 0.7148 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6833 Loss_G: 0.7136 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6627 Loss_G: 0.7109 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7124 Loss_G: 0.7362 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6826 Loss_G: 0.7262 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6935 Loss_G: 0.7147 acc: 92.2%\n",
      "[BATCH 13/149] Loss_D: 0.7063 Loss_G: 0.7067 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.7149 Loss_G: 0.7014 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.6435 Loss_G: 0.6969 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6702 Loss_G: 0.6945 acc: 95.3%\n",
      "[BATCH 17/149] Loss_D: 0.6707 Loss_G: 0.6950 acc: 93.8%\n",
      "[BATCH 18/149] Loss_D: 0.6937 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6844 Loss_G: 0.7195 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6885 Loss_G: 0.7384 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6767 Loss_G: 0.7147 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6894 Loss_G: 0.7230 acc: 96.9%\n",
      "[BATCH 23/149] Loss_D: 0.7165 Loss_G: 0.7110 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6929 Loss_G: 0.7000 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.7339 Loss_G: 0.7278 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6686 Loss_G: 0.7022 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6767 Loss_G: 0.7041 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6755 Loss_G: 0.6965 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6897 Loss_G: 0.6962 acc: 78.1%\n",
      "[BATCH 30/149] Loss_D: 0.6896 Loss_G: 0.6919 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6557 Loss_G: 0.6924 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6624 Loss_G: 0.7006 acc: 89.1%\n",
      "[EPOCH 4800] TEST ACC is : 77.1%\n",
      "[BATCH 33/149] Loss_D: 0.6618 Loss_G: 0.6899 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6981 Loss_G: 0.7258 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6876 Loss_G: 0.7180 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.6722 Loss_G: 0.7206 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6709 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6949 Loss_G: 0.6883 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6759 Loss_G: 0.6917 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.7131 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6823 Loss_G: 0.7059 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7377 Loss_G: 0.7322 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6533 Loss_G: 0.7109 acc: 95.3%\n",
      "[BATCH 44/149] Loss_D: 0.7273 Loss_G: 0.7286 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.7052 Loss_G: 0.7355 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.7243 Loss_G: 0.7254 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6538 Loss_G: 0.7199 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7261 Loss_G: 0.7271 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.6638 Loss_G: 0.7216 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.6969 Loss_G: 0.6942 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6631 Loss_G: 0.6906 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.7081 Loss_G: 0.7016 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6685 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6920 Loss_G: 0.6938 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.6729 Loss_G: 0.7086 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.6505 Loss_G: 0.6901 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.6506 Loss_G: 0.7059 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6794 Loss_G: 0.7083 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.6879 Loss_G: 0.7004 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6869 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.6805 Loss_G: 0.7024 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6794 Loss_G: 0.7029 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6755 Loss_G: 0.7055 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7082 Loss_G: 0.6975 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6918 Loss_G: 0.6916 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6593 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7285 Loss_G: 0.7079 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7309 Loss_G: 0.7211 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.6719 Loss_G: 0.7085 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6867 Loss_G: 0.7176 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6643 Loss_G: 0.7077 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6923 Loss_G: 0.7183 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.7356 Loss_G: 0.7229 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6964 Loss_G: 0.7324 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6822 Loss_G: 0.7346 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.7363 Loss_G: 0.7781 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.7184 Loss_G: 0.7578 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.7019 Loss_G: 0.7221 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.7049 Loss_G: 0.7195 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7234 Loss_G: 0.7208 acc: 96.9%\n",
      "[BATCH 81/149] Loss_D: 0.6676 Loss_G: 0.7184 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6981 Loss_G: 0.7197 acc: 87.5%\n",
      "[EPOCH 4850] TEST ACC is : 76.2%\n",
      "[BATCH 83/149] Loss_D: 0.7155 Loss_G: 0.7293 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7064 Loss_G: 0.7218 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7206 Loss_G: 0.7415 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6733 Loss_G: 0.7185 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7128 Loss_G: 0.7046 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.6852 Loss_G: 0.6939 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7073 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6805 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 91/149] Loss_D: 0.6649 Loss_G: 0.7069 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7397 Loss_G: 0.7308 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.6698 Loss_G: 0.7216 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6853 Loss_G: 0.7163 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6544 Loss_G: 0.7104 acc: 96.9%\n",
      "[BATCH 96/149] Loss_D: 0.6749 Loss_G: 0.6969 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6854 Loss_G: 0.7117 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6705 Loss_G: 0.7172 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6941 Loss_G: 0.7435 acc: 93.8%\n",
      "[BATCH 100/149] Loss_D: 0.6731 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6765 Loss_G: 0.7109 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.7029 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7233 Loss_G: 0.7197 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.7246 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6767 Loss_G: 0.7030 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6751 Loss_G: 0.7272 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6646 Loss_G: 0.6922 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6925 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6881 Loss_G: 0.7026 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7140 Loss_G: 0.7148 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.7166 Loss_G: 0.7221 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.6742 Loss_G: 0.7194 acc: 93.8%\n",
      "[BATCH 113/149] Loss_D: 0.6913 Loss_G: 0.7041 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6773 Loss_G: 0.6956 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.7095 Loss_G: 0.7015 acc: 79.7%\n",
      "[BATCH 116/149] Loss_D: 0.6591 Loss_G: 0.6836 acc: 95.3%\n",
      "[BATCH 117/149] Loss_D: 0.6601 Loss_G: 0.6785 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.7264 Loss_G: 0.7014 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.7033 Loss_G: 0.7319 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6802 Loss_G: 0.7472 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6942 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6920 Loss_G: 0.7205 acc: 95.3%\n",
      "[BATCH 123/149] Loss_D: 0.6641 Loss_G: 0.6873 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7269 Loss_G: 0.7141 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6981 Loss_G: 0.7249 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7032 Loss_G: 0.7154 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.6685 Loss_G: 0.7072 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6622 Loss_G: 0.7237 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.7099 Loss_G: 0.7198 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6772 Loss_G: 0.6927 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6632 Loss_G: 0.7145 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6886 Loss_G: 0.7131 acc: 93.8%\n",
      "[EPOCH 4900] TEST ACC is : 77.1%\n",
      "[BATCH 133/149] Loss_D: 0.7255 Loss_G: 0.7179 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7048 Loss_G: 0.7149 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6864 Loss_G: 0.7109 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.7754 Loss_G: 0.7184 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6776 Loss_G: 0.7310 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6805 Loss_G: 0.7191 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6862 Loss_G: 0.7105 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.7025 Loss_G: 0.7108 acc: 82.8%\n",
      "[BATCH 141/149] Loss_D: 0.7142 Loss_G: 0.6954 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7041 Loss_G: 0.7030 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7257 Loss_G: 0.7420 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.6813 Loss_G: 0.7013 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.7154 Loss_G: 0.7182 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.6958 Loss_G: 0.7217 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6765 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6577 Loss_G: 0.7071 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.7143 Loss_G: 0.7086 acc: 87.5%\n",
      "-----THE [33/50] epoch end-----\n",
      "-----THE [34/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6971 Loss_G: 0.7092 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.6694 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6803 Loss_G: 0.7239 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6838 Loss_G: 0.7079 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.7049 Loss_G: 0.7049 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6980 Loss_G: 0.7022 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6674 Loss_G: 0.7056 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6724 Loss_G: 0.6884 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6376 Loss_G: 0.7287 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.6495 Loss_G: 0.7009 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.7129 Loss_G: 0.7147 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6901 Loss_G: 0.7094 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6671 Loss_G: 0.7410 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6841 Loss_G: 0.7517 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6802 Loss_G: 0.7527 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6857 Loss_G: 0.7055 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7028 Loss_G: 0.7144 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.7095 Loss_G: 0.7173 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7100 Loss_G: 0.7413 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.6941 Loss_G: 0.7240 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7011 Loss_G: 0.7220 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.7225 Loss_G: 0.7143 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7001 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6581 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6927 Loss_G: 0.7169 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6909 Loss_G: 0.7188 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7067 Loss_G: 0.7218 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6686 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6760 Loss_G: 0.6934 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6580 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6408 Loss_G: 0.6999 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6753 Loss_G: 0.7061 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6520 Loss_G: 0.7112 acc: 87.5%\n",
      "[EPOCH 4950] TEST ACC is : 77.1%\n",
      "[BATCH 34/149] Loss_D: 0.7283 Loss_G: 0.7308 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7288 Loss_G: 0.7342 acc: 93.8%\n",
      "[BATCH 36/149] Loss_D: 0.6864 Loss_G: 0.7298 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7200 Loss_G: 0.7150 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6956 Loss_G: 0.6979 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7135 Loss_G: 0.7208 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7160 Loss_G: 0.7068 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7249 Loss_G: 0.7130 acc: 79.7%\n",
      "[BATCH 42/149] Loss_D: 0.6438 Loss_G: 0.7108 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6691 Loss_G: 0.7001 acc: 79.7%\n",
      "[BATCH 44/149] Loss_D: 0.6832 Loss_G: 0.7039 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.6796 Loss_G: 0.7135 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6563 Loss_G: 0.6936 acc: 82.8%\n",
      "[BATCH 47/149] Loss_D: 0.7028 Loss_G: 0.7082 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6740 Loss_G: 0.7010 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7056 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6815 Loss_G: 0.7315 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6804 Loss_G: 0.7298 acc: 96.9%\n",
      "[BATCH 52/149] Loss_D: 0.6988 Loss_G: 0.7234 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6667 Loss_G: 0.7281 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6836 Loss_G: 0.6863 acc: 78.1%\n",
      "[BATCH 55/149] Loss_D: 0.7625 Loss_G: 0.7204 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6882 Loss_G: 0.7042 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.6841 Loss_G: 0.6919 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.6467 Loss_G: 0.6904 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6986 Loss_G: 0.7051 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.7235 Loss_G: 0.7343 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.7055 Loss_G: 0.7287 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6867 Loss_G: 0.7229 acc: 93.8%\n",
      "[BATCH 63/149] Loss_D: 0.6979 Loss_G: 0.7073 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6979 Loss_G: 0.6988 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7138 Loss_G: 0.6998 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7132 Loss_G: 0.7231 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.6951 Loss_G: 0.7262 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.6741 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6579 Loss_G: 0.7050 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6746 Loss_G: 0.6989 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.7309 Loss_G: 0.7352 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.7172 Loss_G: 0.7376 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7006 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6679 Loss_G: 0.7276 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7045 Loss_G: 0.7237 acc: 82.8%\n",
      "[BATCH 76/149] Loss_D: 0.6760 Loss_G: 0.7023 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.6917 Loss_G: 0.6975 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7278 Loss_G: 0.7161 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6570 Loss_G: 0.7050 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.6734 Loss_G: 0.7025 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.6892 Loss_G: 0.7139 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6897 Loss_G: 0.7181 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.7088 Loss_G: 0.7094 acc: 93.8%\n",
      "[EPOCH 5000] TEST ACC is : 76.6%\n",
      "[BATCH 84/149] Loss_D: 0.7080 Loss_G: 0.7253 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6804 Loss_G: 0.7082 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6749 Loss_G: 0.7030 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6849 Loss_G: 0.7043 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7115 Loss_G: 0.7249 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.6791 Loss_G: 0.7151 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.6718 Loss_G: 0.7280 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7094 Loss_G: 0.7375 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6759 Loss_G: 0.7242 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.6566 Loss_G: 0.7013 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7062 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.7206 Loss_G: 0.7284 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6947 Loss_G: 0.7052 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6756 Loss_G: 0.7019 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.6702 Loss_G: 0.7147 acc: 93.8%\n",
      "[BATCH 99/149] Loss_D: 0.6816 Loss_G: 0.7068 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.7096 Loss_G: 0.7068 acc: 96.9%\n",
      "[BATCH 101/149] Loss_D: 0.6724 Loss_G: 0.7309 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6729 Loss_G: 0.6912 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6455 Loss_G: 0.6734 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6979 Loss_G: 0.7111 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6744 Loss_G: 0.7088 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7082 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6905 Loss_G: 0.7014 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6849 Loss_G: 0.6896 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6733 Loss_G: 0.6927 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6765 Loss_G: 0.6810 acc: 81.2%\n",
      "[BATCH 111/149] Loss_D: 0.6838 Loss_G: 0.7142 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.6902 Loss_G: 0.7218 acc: 92.2%\n",
      "[BATCH 113/149] Loss_D: 0.6446 Loss_G: 0.6980 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7629 Loss_G: 0.7125 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.7079 Loss_G: 0.7153 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6867 Loss_G: 0.7255 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6880 Loss_G: 0.7121 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6843 Loss_G: 0.7145 acc: 95.3%\n",
      "[BATCH 119/149] Loss_D: 0.6922 Loss_G: 0.6968 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6433 Loss_G: 0.6831 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7177 Loss_G: 0.7101 acc: 95.3%\n",
      "[BATCH 122/149] Loss_D: 0.6922 Loss_G: 0.7281 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6965 Loss_G: 0.7102 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.6733 Loss_G: 0.6967 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6873 Loss_G: 0.7180 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6847 Loss_G: 0.7027 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6672 Loss_G: 0.6927 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7047 Loss_G: 0.7007 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.6627 Loss_G: 0.7066 acc: 95.3%\n",
      "[BATCH 130/149] Loss_D: 0.7050 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6767 Loss_G: 0.7084 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6845 Loss_G: 0.7078 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6859 Loss_G: 0.7040 acc: 81.2%\n",
      "[EPOCH 5050] TEST ACC is : 76.8%\n",
      "[BATCH 134/149] Loss_D: 0.7143 Loss_G: 0.7220 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.7265 Loss_G: 0.7294 acc: 92.2%\n",
      "[BATCH 136/149] Loss_D: 0.6871 Loss_G: 0.7386 acc: 81.2%\n",
      "[BATCH 137/149] Loss_D: 0.6716 Loss_G: 0.7196 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6902 Loss_G: 0.7267 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7041 Loss_G: 0.7046 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7090 Loss_G: 0.7005 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.6954 Loss_G: 0.7079 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6761 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.7762 Loss_G: 0.7357 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6979 Loss_G: 0.7378 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6808 Loss_G: 0.7100 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6753 Loss_G: 0.6892 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.7083 Loss_G: 0.7035 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6821 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.7137 Loss_G: 0.7231 acc: 87.5%\n",
      "-----THE [34/50] epoch end-----\n",
      "-----THE [35/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6799 Loss_G: 0.6999 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6691 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7043 Loss_G: 0.7118 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6983 Loss_G: 0.7321 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.7024 Loss_G: 0.7197 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.7087 Loss_G: 0.7243 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.7244 Loss_G: 0.7305 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6795 Loss_G: 0.7008 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.6586 Loss_G: 0.6978 acc: 96.9%\n",
      "[BATCH 10/149] Loss_D: 0.6756 Loss_G: 0.6943 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7072 Loss_G: 0.7115 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.7222 Loss_G: 0.7177 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6601 Loss_G: 0.7342 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.6527 Loss_G: 0.7019 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6750 Loss_G: 0.6999 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7041 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6963 Loss_G: 0.7405 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.6769 Loss_G: 0.7345 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.6732 Loss_G: 0.7245 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.6753 Loss_G: 0.6909 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6683 Loss_G: 0.7079 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6698 Loss_G: 0.7043 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7217 Loss_G: 0.6949 acc: 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.6933 Loss_G: 0.6999 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6944 Loss_G: 0.7212 acc: 95.3%\n",
      "[BATCH 26/149] Loss_D: 0.7314 Loss_G: 0.7294 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7050 Loss_G: 0.7143 acc: 81.2%\n",
      "[BATCH 28/149] Loss_D: 0.7041 Loss_G: 0.7164 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6914 Loss_G: 0.7182 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7078 Loss_G: 0.7091 acc: 81.2%\n",
      "[BATCH 31/149] Loss_D: 0.6681 Loss_G: 0.7024 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.6873 Loss_G: 0.7031 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6764 Loss_G: 0.7161 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6929 Loss_G: 0.7087 acc: 87.5%\n",
      "[EPOCH 5100] TEST ACC is : 76.4%\n",
      "[BATCH 35/149] Loss_D: 0.6906 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6833 Loss_G: 0.7114 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.7057 Loss_G: 0.7099 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6689 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6928 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6878 Loss_G: 0.6948 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.6832 Loss_G: 0.7072 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7319 Loss_G: 0.7264 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7071 Loss_G: 0.6924 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6552 Loss_G: 0.6933 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.7364 Loss_G: 0.7136 acc: 79.7%\n",
      "[BATCH 46/149] Loss_D: 0.6597 Loss_G: 0.7024 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6937 Loss_G: 0.7280 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6734 Loss_G: 0.7126 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.7036 Loss_G: 0.7026 acc: 81.2%\n",
      "[BATCH 50/149] Loss_D: 0.7002 Loss_G: 0.7049 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6819 Loss_G: 0.7137 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6975 Loss_G: 0.6899 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7098 Loss_G: 0.6968 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.7059 Loss_G: 0.7081 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.6911 Loss_G: 0.7059 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6822 Loss_G: 0.7048 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6979 Loss_G: 0.7026 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6888 Loss_G: 0.7348 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.6843 Loss_G: 0.7121 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6800 Loss_G: 0.7079 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6837 Loss_G: 0.7135 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6596 Loss_G: 0.7035 acc: 96.9%\n",
      "[BATCH 63/149] Loss_D: 0.7054 Loss_G: 0.6984 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.6755 Loss_G: 0.7265 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6954 Loss_G: 0.7248 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7000 Loss_G: 0.7219 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6842 Loss_G: 0.7016 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6595 Loss_G: 0.7052 acc: 95.3%\n",
      "[BATCH 69/149] Loss_D: 0.6832 Loss_G: 0.7155 acc: 93.8%\n",
      "[BATCH 70/149] Loss_D: 0.7129 Loss_G: 0.7277 acc: 95.3%\n",
      "[BATCH 71/149] Loss_D: 0.7014 Loss_G: 0.7139 acc: 81.2%\n",
      "[BATCH 72/149] Loss_D: 0.7204 Loss_G: 0.7357 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.7281 Loss_G: 0.7143 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6948 Loss_G: 0.7215 acc: 96.9%\n",
      "[BATCH 75/149] Loss_D: 0.7091 Loss_G: 0.7215 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.6982 Loss_G: 0.7417 acc: 95.3%\n",
      "[BATCH 77/149] Loss_D: 0.7197 Loss_G: 0.7617 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.7196 Loss_G: 0.7491 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.6721 Loss_G: 0.7439 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6873 Loss_G: 0.7284 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6814 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7127 Loss_G: 0.7308 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6839 Loss_G: 0.7164 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6905 Loss_G: 0.7324 acc: 92.2%\n",
      "[EPOCH 5150] TEST ACC is : 77.3%\n",
      "[BATCH 85/149] Loss_D: 0.7019 Loss_G: 0.7070 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6885 Loss_G: 0.6895 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6921 Loss_G: 0.7056 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6751 Loss_G: 0.6999 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.7451 Loss_G: 0.7349 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7525 Loss_G: 0.7411 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6559 Loss_G: 0.7028 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7115 Loss_G: 0.7047 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.7061 Loss_G: 0.6998 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6492 Loss_G: 0.6944 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6840 Loss_G: 0.6974 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6974 Loss_G: 0.7000 acc: 82.8%\n",
      "[BATCH 97/149] Loss_D: 0.6828 Loss_G: 0.6873 acc: 81.2%\n",
      "[BATCH 98/149] Loss_D: 0.6751 Loss_G: 0.6986 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6909 Loss_G: 0.7033 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7055 Loss_G: 0.7062 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6925 Loss_G: 0.7439 acc: 93.8%\n",
      "[BATCH 102/149] Loss_D: 0.6758 Loss_G: 0.7201 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6834 Loss_G: 0.7307 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6863 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6842 Loss_G: 0.7175 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6710 Loss_G: 0.7172 acc: 95.3%\n",
      "[BATCH 107/149] Loss_D: 0.7043 Loss_G: 0.7133 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6868 Loss_G: 0.6945 acc: 84.4%\n",
      "[BATCH 109/149] Loss_D: 0.7093 Loss_G: 0.7097 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6714 Loss_G: 0.7034 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.6513 Loss_G: 0.7003 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7284 Loss_G: 0.7041 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.7105 Loss_G: 0.7159 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.6971 Loss_G: 0.7170 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6996 Loss_G: 0.7340 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7080 Loss_G: 0.7344 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.6787 Loss_G: 0.7254 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.7053 Loss_G: 0.7194 acc: 81.2%\n",
      "[BATCH 119/149] Loss_D: 0.6914 Loss_G: 0.7173 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7080 Loss_G: 0.7215 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.6761 Loss_G: 0.7178 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.6791 Loss_G: 0.7063 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6791 Loss_G: 0.6939 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.6760 Loss_G: 0.6764 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6677 Loss_G: 0.6860 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6617 Loss_G: 0.6831 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6483 Loss_G: 0.6817 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6848 Loss_G: 0.6907 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.7070 Loss_G: 0.7111 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7258 Loss_G: 0.7195 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6773 Loss_G: 0.7312 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.7231 Loss_G: 0.7374 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6637 Loss_G: 0.7220 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.6681 Loss_G: 0.7046 acc: 87.5%\n",
      "[EPOCH 5200] TEST ACC is : 77.3%\n",
      "[BATCH 135/149] Loss_D: 0.6514 Loss_G: 0.6928 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6979 Loss_G: 0.6935 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6859 Loss_G: 0.7202 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6651 Loss_G: 0.7197 acc: 98.4%\n",
      "[BATCH 139/149] Loss_D: 0.6993 Loss_G: 0.7290 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.7002 Loss_G: 0.7032 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.6850 Loss_G: 0.6943 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6780 Loss_G: 0.7114 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6700 Loss_G: 0.7137 acc: 95.3%\n",
      "[BATCH 144/149] Loss_D: 0.6871 Loss_G: 0.7248 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6864 Loss_G: 0.6933 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.6739 Loss_G: 0.6915 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6848 Loss_G: 0.7378 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.7026 Loss_G: 0.7166 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6775 Loss_G: 0.6990 acc: 89.1%\n",
      "-----THE [35/50] epoch end-----\n",
      "-----THE [36/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6855 Loss_G: 0.7033 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6762 Loss_G: 0.6987 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6620 Loss_G: 0.6930 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6707 Loss_G: 0.7006 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.6693 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6733 Loss_G: 0.7003 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6889 Loss_G: 0.7023 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6879 Loss_G: 0.7100 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.6610 Loss_G: 0.7165 acc: 96.9%\n",
      "[BATCH 10/149] Loss_D: 0.7213 Loss_G: 0.7285 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6853 Loss_G: 0.7150 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6860 Loss_G: 0.6959 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6727 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6908 Loss_G: 0.6861 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.6448 Loss_G: 0.6866 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7163 Loss_G: 0.7019 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6792 Loss_G: 0.6994 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6703 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7314 Loss_G: 0.7036 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6807 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6798 Loss_G: 0.7427 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6893 Loss_G: 0.7319 acc: 82.8%\n",
      "[BATCH 23/149] Loss_D: 0.6550 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6877 Loss_G: 0.7234 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.7108 Loss_G: 0.7336 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6663 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6981 Loss_G: 0.6991 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7069 Loss_G: 0.7031 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6942 Loss_G: 0.7413 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7386 Loss_G: 0.7472 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6795 Loss_G: 0.7220 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.6996 Loss_G: 0.6990 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6704 Loss_G: 0.6914 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6889 Loss_G: 0.7116 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7312 Loss_G: 0.7514 acc: 92.2%\n",
      "[EPOCH 5250] TEST ACC is : 77.9%\n",
      "[BATCH 36/149] Loss_D: 0.6832 Loss_G: 0.7212 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6882 Loss_G: 0.7054 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6654 Loss_G: 0.7082 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6860 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6808 Loss_G: 0.7045 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6737 Loss_G: 0.7005 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6974 Loss_G: 0.7243 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7168 Loss_G: 0.7362 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6917 Loss_G: 0.7305 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6579 Loss_G: 0.7236 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6898 Loss_G: 0.7209 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6877 Loss_G: 0.7064 acc: 93.8%\n",
      "[BATCH 48/149] Loss_D: 0.6879 Loss_G: 0.6859 acc: 93.8%\n",
      "[BATCH 49/149] Loss_D: 0.7223 Loss_G: 0.7335 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6796 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6921 Loss_G: 0.7090 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6499 Loss_G: 0.6962 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7034 Loss_G: 0.7090 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6634 Loss_G: 0.7112 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6668 Loss_G: 0.7003 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.6769 Loss_G: 0.7321 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7070 Loss_G: 0.7262 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6842 Loss_G: 0.7121 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.7268 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7103 Loss_G: 0.7133 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7261 Loss_G: 0.7303 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6811 Loss_G: 0.7262 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.6941 Loss_G: 0.7207 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6819 Loss_G: 0.7038 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7335 Loss_G: 0.7047 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7071 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6842 Loss_G: 0.6926 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.6556 Loss_G: 0.6911 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.6698 Loss_G: 0.6944 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7009 Loss_G: 0.6930 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.7079 Loss_G: 0.7193 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6884 Loss_G: 0.7195 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.6435 Loss_G: 0.6969 acc: 93.8%\n",
      "[BATCH 74/149] Loss_D: 0.6669 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.6506 Loss_G: 0.7027 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7015 Loss_G: 0.7366 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6858 Loss_G: 0.7352 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6947 Loss_G: 0.7357 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7038 Loss_G: 0.7579 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7099 Loss_G: 0.7251 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6691 Loss_G: 0.6975 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.6543 Loss_G: 0.7315 acc: 96.9%\n",
      "[BATCH 83/149] Loss_D: 0.6910 Loss_G: 0.7207 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.7072 Loss_G: 0.7045 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7012 Loss_G: 0.7090 acc: 93.8%\n",
      "[EPOCH 5300] TEST ACC is : 77.1%\n",
      "[BATCH 86/149] Loss_D: 0.7531 Loss_G: 0.7302 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6956 Loss_G: 0.7120 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.6765 Loss_G: 0.6895 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7296 Loss_G: 0.7039 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6916 Loss_G: 0.6911 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6787 Loss_G: 0.6938 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7060 Loss_G: 0.7048 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7198 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6573 Loss_G: 0.7076 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.7041 Loss_G: 0.7077 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.6646 Loss_G: 0.7204 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.7565 Loss_G: 0.7478 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6756 Loss_G: 0.7168 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.7159 Loss_G: 0.7333 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6993 Loss_G: 0.7143 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.7288 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.7104 Loss_G: 0.7262 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6774 Loss_G: 0.7152 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6796 Loss_G: 0.7134 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7062 Loss_G: 0.7364 acc: 95.3%\n",
      "[BATCH 106/149] Loss_D: 0.7198 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.7238 Loss_G: 0.7044 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6682 Loss_G: 0.7030 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6745 Loss_G: 0.6814 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7003 Loss_G: 0.7176 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6772 Loss_G: 0.7122 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7192 Loss_G: 0.7211 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6684 Loss_G: 0.7141 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6734 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6678 Loss_G: 0.7075 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6532 Loss_G: 0.7101 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.6951 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.6939 Loss_G: 0.7024 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6677 Loss_G: 0.6860 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.6656 Loss_G: 0.7009 acc: 93.8%\n",
      "[BATCH 121/149] Loss_D: 0.6555 Loss_G: 0.6945 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6391 Loss_G: 0.6968 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.6803 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6543 Loss_G: 0.6938 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7084 Loss_G: 0.6876 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.7106 Loss_G: 0.7104 acc: 96.9%\n",
      "[BATCH 127/149] Loss_D: 0.6853 Loss_G: 0.7080 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6930 Loss_G: 0.7185 acc: 95.3%\n",
      "[BATCH 129/149] Loss_D: 0.6772 Loss_G: 0.7122 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6725 Loss_G: 0.7046 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6898 Loss_G: 0.7038 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6860 Loss_G: 0.7262 acc: 95.3%\n",
      "[BATCH 133/149] Loss_D: 0.7344 Loss_G: 0.7660 acc: 93.8%\n",
      "[BATCH 134/149] Loss_D: 0.6861 Loss_G: 0.7343 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7174 Loss_G: 0.7216 acc: 84.4%\n",
      "[EPOCH 5350] TEST ACC is : 75.8%\n",
      "[BATCH 136/149] Loss_D: 0.6615 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6721 Loss_G: 0.6849 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.7275 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6772 Loss_G: 0.7113 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6951 Loss_G: 0.7199 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7444 Loss_G: 0.7399 acc: 81.2%\n",
      "[BATCH 142/149] Loss_D: 0.7092 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6768 Loss_G: 0.7228 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.6655 Loss_G: 0.7349 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6822 Loss_G: 0.7137 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6683 Loss_G: 0.7015 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6653 Loss_G: 0.7012 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6855 Loss_G: 0.7040 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.7136 Loss_G: 0.7148 acc: 84.4%\n",
      "-----THE [36/50] epoch end-----\n",
      "-----THE [37/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7086 Loss_G: 0.7305 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.6912 Loss_G: 0.7410 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.6624 Loss_G: 0.7024 acc: 95.3%\n",
      "[BATCH 4/149] Loss_D: 0.7728 Loss_G: 0.7285 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6810 Loss_G: 0.7086 acc: 76.6%\n",
      "[BATCH 6/149] Loss_D: 0.7010 Loss_G: 0.7002 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6868 Loss_G: 0.7095 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6719 Loss_G: 0.7174 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6645 Loss_G: 0.6974 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.7133 Loss_G: 0.7210 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.6814 Loss_G: 0.7122 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6786 Loss_G: 0.6902 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6767 Loss_G: 0.6890 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6963 Loss_G: 0.7025 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6851 Loss_G: 0.7105 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.6708 Loss_G: 0.7099 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6959 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6940 Loss_G: 0.7081 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6753 Loss_G: 0.7125 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.7029 Loss_G: 0.7366 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6905 Loss_G: 0.7140 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.6834 Loss_G: 0.7125 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.6851 Loss_G: 0.6885 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6988 Loss_G: 0.6913 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6807 Loss_G: 0.7137 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6690 Loss_G: 0.7085 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7137 Loss_G: 0.7414 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6896 Loss_G: 0.7327 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6975 Loss_G: 0.7371 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6875 Loss_G: 0.7346 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.7090 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7383 Loss_G: 0.7241 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.7083 Loss_G: 0.7372 acc: 95.3%\n",
      "[BATCH 34/149] Loss_D: 0.6704 Loss_G: 0.7070 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6501 Loss_G: 0.6960 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6857 Loss_G: 0.7012 acc: 87.5%\n",
      "[EPOCH 5400] TEST ACC is : 76.6%\n",
      "[BATCH 37/149] Loss_D: 0.7299 Loss_G: 0.7439 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.7193 Loss_G: 0.7536 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7044 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7079 Loss_G: 0.7404 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.6902 Loss_G: 0.7455 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6836 Loss_G: 0.7731 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.7123 Loss_G: 0.7329 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6864 Loss_G: 0.7360 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6815 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.6905 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6866 Loss_G: 0.7007 acc: 81.2%\n",
      "[BATCH 48/149] Loss_D: 0.6810 Loss_G: 0.6943 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6959 Loss_G: 0.7150 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6774 Loss_G: 0.6979 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.7231 Loss_G: 0.7128 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6721 Loss_G: 0.7089 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6498 Loss_G: 0.6932 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6931 Loss_G: 0.7035 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6986 Loss_G: 0.7136 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6720 Loss_G: 0.7018 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7452 Loss_G: 0.7312 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6816 Loss_G: 0.7054 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.6543 Loss_G: 0.6981 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6893 Loss_G: 0.6883 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7246 Loss_G: 0.7400 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6831 Loss_G: 0.7082 acc: 81.2%\n",
      "[BATCH 63/149] Loss_D: 0.6760 Loss_G: 0.7040 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.6567 Loss_G: 0.6958 acc: 93.8%\n",
      "[BATCH 65/149] Loss_D: 0.7144 Loss_G: 0.7222 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6997 Loss_G: 0.7064 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.7003 Loss_G: 0.7273 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7158 Loss_G: 0.7241 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7129 Loss_G: 0.7029 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.6611 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6821 Loss_G: 0.7136 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6789 Loss_G: 0.7169 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6883 Loss_G: 0.7304 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6475 Loss_G: 0.7217 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6647 Loss_G: 0.7057 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.7172 Loss_G: 0.7081 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7231 Loss_G: 0.6999 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6601 Loss_G: 0.6957 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6431 Loss_G: 0.7018 acc: 96.9%\n",
      "[BATCH 80/149] Loss_D: 0.6864 Loss_G: 0.7113 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7281 Loss_G: 0.7361 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.6966 Loss_G: 0.7342 acc: 93.8%\n",
      "[BATCH 83/149] Loss_D: 0.6720 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6909 Loss_G: 0.6881 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.7427 Loss_G: 0.7051 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7021 Loss_G: 0.7266 acc: 89.1%\n",
      "[EPOCH 5450] TEST ACC is : 77.0%\n",
      "[BATCH 87/149] Loss_D: 0.6883 Loss_G: 0.7345 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.6696 Loss_G: 0.6926 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6682 Loss_G: 0.6939 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.7137 Loss_G: 0.7244 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6494 Loss_G: 0.6933 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7005 Loss_G: 0.6902 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6796 Loss_G: 0.7042 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6843 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.7235 Loss_G: 0.7373 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.7059 Loss_G: 0.7398 acc: 93.8%\n",
      "[BATCH 97/149] Loss_D: 0.6722 Loss_G: 0.7359 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.7056 Loss_G: 0.7160 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6750 Loss_G: 0.7055 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6698 Loss_G: 0.7070 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.6994 Loss_G: 0.7079 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6900 Loss_G: 0.7099 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6960 Loss_G: 0.7235 acc: 93.8%\n",
      "[BATCH 104/149] Loss_D: 0.6667 Loss_G: 0.7005 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.6769 Loss_G: 0.7128 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6761 Loss_G: 0.7184 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6743 Loss_G: 0.7008 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.7057 Loss_G: 0.7012 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6638 Loss_G: 0.6832 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6770 Loss_G: 0.7065 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7358 Loss_G: 0.7348 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6674 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6798 Loss_G: 0.6880 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7214 Loss_G: 0.6864 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.6998 Loss_G: 0.6929 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7189 Loss_G: 0.7375 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.6683 Loss_G: 0.7130 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6519 Loss_G: 0.7121 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6731 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.6855 Loss_G: 0.7036 acc: 84.4%\n",
      "[BATCH 121/149] Loss_D: 0.6644 Loss_G: 0.7018 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6374 Loss_G: 0.6931 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6849 Loss_G: 0.6870 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6710 Loss_G: 0.6954 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6665 Loss_G: 0.6997 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7164 Loss_G: 0.7120 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6547 Loss_G: 0.7122 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7190 Loss_G: 0.7266 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7011 Loss_G: 0.7213 acc: 95.3%\n",
      "[BATCH 130/149] Loss_D: 0.6713 Loss_G: 0.7320 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7062 Loss_G: 0.7607 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6629 Loss_G: 0.7220 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7078 Loss_G: 0.7128 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.7179 Loss_G: 0.6957 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7116 Loss_G: 0.7024 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6572 Loss_G: 0.7115 acc: 96.9%\n",
      "[EPOCH 5500] TEST ACC is : 76.8%\n",
      "[BATCH 137/149] Loss_D: 0.6957 Loss_G: 0.7242 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7196 Loss_G: 0.7342 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.6615 Loss_G: 0.7194 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6499 Loss_G: 0.6799 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6790 Loss_G: 0.6789 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7251 Loss_G: 0.7210 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6799 Loss_G: 0.7152 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.6872 Loss_G: 0.7050 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6975 Loss_G: 0.7227 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.6974 Loss_G: 0.7127 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6744 Loss_G: 0.7146 acc: 95.3%\n",
      "[BATCH 148/149] Loss_D: 0.6804 Loss_G: 0.6992 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6778 Loss_G: 0.6932 acc: 87.5%\n",
      "-----THE [37/50] epoch end-----\n",
      "-----THE [38/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6841 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7240 Loss_G: 0.7294 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.6827 Loss_G: 0.7205 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6829 Loss_G: 0.6988 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.6741 Loss_G: 0.6984 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6801 Loss_G: 0.6908 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6758 Loss_G: 0.7077 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6934 Loss_G: 0.7208 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6937 Loss_G: 0.7178 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.6565 Loss_G: 0.6872 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7254 Loss_G: 0.7128 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6655 Loss_G: 0.7092 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6914 Loss_G: 0.7081 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.6958 Loss_G: 0.7081 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6855 Loss_G: 0.6942 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7088 Loss_G: 0.7095 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.7107 Loss_G: 0.6841 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6836 Loss_G: 0.6820 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.6735 Loss_G: 0.7022 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6606 Loss_G: 0.6948 acc: 95.3%\n",
      "[BATCH 21/149] Loss_D: 0.6820 Loss_G: 0.7019 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6899 Loss_G: 0.6953 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6833 Loss_G: 0.6964 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6879 Loss_G: 0.7062 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6663 Loss_G: 0.7027 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6655 Loss_G: 0.6839 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.6869 Loss_G: 0.6991 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6737 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6754 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6858 Loss_G: 0.7167 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7120 Loss_G: 0.7373 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7198 Loss_G: 0.7405 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6804 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7141 Loss_G: 0.7185 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7002 Loss_G: 0.7364 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6769 Loss_G: 0.7264 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6646 Loss_G: 0.7047 acc: 90.6%\n",
      "[EPOCH 5550] TEST ACC is : 76.2%\n",
      "[BATCH 38/149] Loss_D: 0.7056 Loss_G: 0.7172 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6687 Loss_G: 0.6990 acc: 81.2%\n",
      "[BATCH 40/149] Loss_D: 0.6595 Loss_G: 0.6817 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6573 Loss_G: 0.6952 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6763 Loss_G: 0.6891 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.7046 Loss_G: 0.7167 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.6539 Loss_G: 0.6983 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6496 Loss_G: 0.6878 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6767 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.7055 Loss_G: 0.7044 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7352 Loss_G: 0.7108 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.6372 Loss_G: 0.6792 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.7161 Loss_G: 0.7083 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6901 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6864 Loss_G: 0.7094 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6923 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6550 Loss_G: 0.6909 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.6861 Loss_G: 0.6967 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7048 Loss_G: 0.7129 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6837 Loss_G: 0.7249 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7102 Loss_G: 0.7498 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6890 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7156 Loss_G: 0.7457 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6752 Loss_G: 0.7263 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7223 Loss_G: 0.7201 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.6992 Loss_G: 0.7092 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6786 Loss_G: 0.7115 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6692 Loss_G: 0.6908 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7316 Loss_G: 0.6920 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6581 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6590 Loss_G: 0.6984 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7013 Loss_G: 0.7347 acc: 93.8%\n",
      "[BATCH 70/149] Loss_D: 0.6595 Loss_G: 0.7061 acc: 95.3%\n",
      "[BATCH 71/149] Loss_D: 0.7019 Loss_G: 0.7214 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6677 Loss_G: 0.7124 acc: 96.9%\n",
      "[BATCH 73/149] Loss_D: 0.6617 Loss_G: 0.6991 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.7033 Loss_G: 0.7014 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6909 Loss_G: 0.7329 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.7327 Loss_G: 0.7199 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.6997 Loss_G: 0.7077 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7023 Loss_G: 0.7024 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.6966 Loss_G: 0.7059 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6881 Loss_G: 0.7394 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.7193 Loss_G: 0.7254 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.6838 Loss_G: 0.7208 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7114 Loss_G: 0.7064 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6965 Loss_G: 0.6918 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.7113 Loss_G: 0.7165 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.6735 Loss_G: 0.7153 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7010 Loss_G: 0.7164 acc: 85.9%\n",
      "[EPOCH 5600] TEST ACC is : 77.0%\n",
      "[BATCH 88/149] Loss_D: 0.6963 Loss_G: 0.7339 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6799 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6916 Loss_G: 0.7460 acc: 96.9%\n",
      "[BATCH 91/149] Loss_D: 0.7284 Loss_G: 0.7149 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7017 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6786 Loss_G: 0.7074 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6859 Loss_G: 0.7142 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6505 Loss_G: 0.6978 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.7357 Loss_G: 0.7243 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6938 Loss_G: 0.7027 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6963 Loss_G: 0.7095 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6889 Loss_G: 0.7165 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6556 Loss_G: 0.7169 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6942 Loss_G: 0.7144 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7333 Loss_G: 0.7592 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.6977 Loss_G: 0.7368 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7153 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7265 Loss_G: 0.7265 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.6630 Loss_G: 0.7288 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.7103 Loss_G: 0.7406 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6910 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6835 Loss_G: 0.6933 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6788 Loss_G: 0.6903 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6973 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6499 Loss_G: 0.6910 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6543 Loss_G: 0.6831 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6838 Loss_G: 0.6980 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.7078 Loss_G: 0.7515 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6878 Loss_G: 0.7204 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6920 Loss_G: 0.7037 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6913 Loss_G: 0.7088 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7172 Loss_G: 0.7134 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6579 Loss_G: 0.7034 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6881 Loss_G: 0.7122 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6893 Loss_G: 0.7049 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6900 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6864 Loss_G: 0.6930 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6678 Loss_G: 0.6907 acc: 96.9%\n",
      "[BATCH 126/149] Loss_D: 0.6867 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6651 Loss_G: 0.6965 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7055 Loss_G: 0.7126 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6712 Loss_G: 0.7084 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6745 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6775 Loss_G: 0.7155 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.7612 Loss_G: 0.7397 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7176 Loss_G: 0.7271 acc: 93.8%\n",
      "[BATCH 134/149] Loss_D: 0.6847 Loss_G: 0.6989 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6763 Loss_G: 0.7038 acc: 93.8%\n",
      "[BATCH 136/149] Loss_D: 0.7144 Loss_G: 0.7185 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6855 Loss_G: 0.7401 acc: 93.8%\n",
      "[EPOCH 5650] TEST ACC is : 77.3%\n",
      "[BATCH 138/149] Loss_D: 0.6928 Loss_G: 0.7201 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6768 Loss_G: 0.7219 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6845 Loss_G: 0.6921 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6907 Loss_G: 0.7088 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6696 Loss_G: 0.7024 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6945 Loss_G: 0.7044 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.7055 Loss_G: 0.6932 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7248 Loss_G: 0.7201 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6733 Loss_G: 0.7121 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6880 Loss_G: 0.7173 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.7032 Loss_G: 0.7063 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.6861 Loss_G: 0.7200 acc: 84.4%\n",
      "-----THE [38/50] epoch end-----\n",
      "-----THE [39/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6935 Loss_G: 0.6979 acc: 79.7%\n",
      "[BATCH 2/149] Loss_D: 0.6660 Loss_G: 0.7031 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6675 Loss_G: 0.7054 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6857 Loss_G: 0.6973 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.6641 Loss_G: 0.6899 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6725 Loss_G: 0.7096 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6775 Loss_G: 0.7119 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6962 Loss_G: 0.6991 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7046 Loss_G: 0.7122 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.7103 Loss_G: 0.7203 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.6904 Loss_G: 0.6889 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6953 Loss_G: 0.7136 acc: 92.2%\n",
      "[BATCH 13/149] Loss_D: 0.6616 Loss_G: 0.7158 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6923 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6872 Loss_G: 0.7287 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6937 Loss_G: 0.7291 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.7257 Loss_G: 0.7134 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6939 Loss_G: 0.7329 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6748 Loss_G: 0.7377 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.7140 Loss_G: 0.7517 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6979 Loss_G: 0.7324 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7226 Loss_G: 0.7539 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6754 Loss_G: 0.7292 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6942 Loss_G: 0.7188 acc: 95.3%\n",
      "[BATCH 25/149] Loss_D: 0.6910 Loss_G: 0.7006 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6778 Loss_G: 0.6889 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.6801 Loss_G: 0.6845 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.7035 Loss_G: 0.7092 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6823 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6853 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6420 Loss_G: 0.7088 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.6815 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7078 Loss_G: 0.6964 acc: 81.2%\n",
      "[BATCH 34/149] Loss_D: 0.6753 Loss_G: 0.7042 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6660 Loss_G: 0.6965 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6805 Loss_G: 0.7159 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.7075 Loss_G: 0.7448 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7036 Loss_G: 0.7050 acc: 87.5%\n",
      "[EPOCH 5700] TEST ACC is : 76.4%\n",
      "[BATCH 39/149] Loss_D: 0.6476 Loss_G: 0.6810 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6659 Loss_G: 0.6817 acc: 95.3%\n",
      "[BATCH 41/149] Loss_D: 0.6844 Loss_G: 0.6939 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7162 Loss_G: 0.6970 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6677 Loss_G: 0.7004 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6838 Loss_G: 0.6908 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.7147 Loss_G: 0.7157 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.7202 Loss_G: 0.7223 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6903 Loss_G: 0.7127 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.7030 Loss_G: 0.7398 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.6833 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6874 Loss_G: 0.7254 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7103 Loss_G: 0.7511 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6774 Loss_G: 0.7119 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7229 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6945 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.7009 Loss_G: 0.7060 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.6624 Loss_G: 0.6958 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6652 Loss_G: 0.6774 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6829 Loss_G: 0.7091 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.6980 Loss_G: 0.7217 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7207 Loss_G: 0.7433 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6936 Loss_G: 0.7089 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7031 Loss_G: 0.7116 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6772 Loss_G: 0.7040 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6722 Loss_G: 0.6990 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.7185 Loss_G: 0.7075 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.6697 Loss_G: 0.6797 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.7004 Loss_G: 0.6956 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6933 Loss_G: 0.7017 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.6960 Loss_G: 0.7233 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6742 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7440 Loss_G: 0.7375 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6549 Loss_G: 0.6935 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.6306 Loss_G: 0.6809 acc: 93.8%\n",
      "[BATCH 74/149] Loss_D: 0.6652 Loss_G: 0.6958 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6752 Loss_G: 0.7015 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.6478 Loss_G: 0.7021 acc: 95.3%\n",
      "[BATCH 77/149] Loss_D: 0.6684 Loss_G: 0.6901 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.7221 Loss_G: 0.7048 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6914 Loss_G: 0.6991 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6730 Loss_G: 0.7058 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7416 Loss_G: 0.7243 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.7095 Loss_G: 0.7219 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6871 Loss_G: 0.7050 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6961 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.6854 Loss_G: 0.7176 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.7306 Loss_G: 0.7045 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6460 Loss_G: 0.6849 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6898 Loss_G: 0.6973 acc: 89.1%\n",
      "[EPOCH 5750] TEST ACC is : 76.8%\n",
      "[BATCH 89/149] Loss_D: 0.7049 Loss_G: 0.7044 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7138 Loss_G: 0.7383 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.6965 Loss_G: 0.7176 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.7157 Loss_G: 0.7444 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7119 Loss_G: 0.7234 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6554 Loss_G: 0.6996 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.6860 Loss_G: 0.6875 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6717 Loss_G: 0.7004 acc: 93.8%\n",
      "[BATCH 97/149] Loss_D: 0.6960 Loss_G: 0.7038 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6659 Loss_G: 0.6948 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7032 Loss_G: 0.7279 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6740 Loss_G: 0.7084 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6971 Loss_G: 0.7169 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6766 Loss_G: 0.7123 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6531 Loss_G: 0.6813 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6978 Loss_G: 0.6867 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7565 Loss_G: 0.7286 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.7192 Loss_G: 0.7373 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.6827 Loss_G: 0.7379 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.6716 Loss_G: 0.7175 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6911 Loss_G: 0.7166 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7008 Loss_G: 0.7105 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.7040 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.7019 Loss_G: 0.7263 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7209 Loss_G: 0.7319 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6690 Loss_G: 0.7053 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6791 Loss_G: 0.7104 acc: 81.2%\n",
      "[BATCH 116/149] Loss_D: 0.7139 Loss_G: 0.7201 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.7015 Loss_G: 0.7103 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7022 Loss_G: 0.7176 acc: 81.2%\n",
      "[BATCH 119/149] Loss_D: 0.7206 Loss_G: 0.7378 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6685 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6813 Loss_G: 0.7097 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.6638 Loss_G: 0.7079 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6526 Loss_G: 0.6879 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.7196 Loss_G: 0.7064 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6838 Loss_G: 0.7116 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6961 Loss_G: 0.7107 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6784 Loss_G: 0.7025 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6523 Loss_G: 0.6965 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6775 Loss_G: 0.7170 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7108 Loss_G: 0.7339 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.7039 Loss_G: 0.7181 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6785 Loss_G: 0.7111 acc: 95.3%\n",
      "[BATCH 133/149] Loss_D: 0.6810 Loss_G: 0.6906 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.6882 Loss_G: 0.6997 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6685 Loss_G: 0.7052 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.7326 Loss_G: 0.7063 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6702 Loss_G: 0.7052 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6760 Loss_G: 0.7086 acc: 93.8%\n",
      "[EPOCH 5800] TEST ACC is : 76.6%\n",
      "[BATCH 139/149] Loss_D: 0.6773 Loss_G: 0.6998 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6370 Loss_G: 0.6827 acc: 95.3%\n",
      "[BATCH 141/149] Loss_D: 0.6732 Loss_G: 0.6918 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7363 Loss_G: 0.7284 acc: 96.9%\n",
      "[BATCH 143/149] Loss_D: 0.6630 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6948 Loss_G: 0.7149 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6957 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7295 Loss_G: 0.7245 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6848 Loss_G: 0.7287 acc: 82.8%\n",
      "[BATCH 148/149] Loss_D: 0.6957 Loss_G: 0.7291 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6813 Loss_G: 0.7376 acc: 93.8%\n",
      "-----THE [39/50] epoch end-----\n",
      "-----THE [40/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6519 Loss_G: 0.7165 acc: 96.9%\n",
      "[BATCH 2/149] Loss_D: 0.6981 Loss_G: 0.7120 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6591 Loss_G: 0.6995 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6825 Loss_G: 0.7023 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6868 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6834 Loss_G: 0.6955 acc: 78.1%\n",
      "[BATCH 7/149] Loss_D: 0.6640 Loss_G: 0.7105 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6801 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6634 Loss_G: 0.7109 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6898 Loss_G: 0.7321 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6619 Loss_G: 0.7191 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6943 Loss_G: 0.7064 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7324 Loss_G: 0.7015 acc: 81.2%\n",
      "[BATCH 14/149] Loss_D: 0.6657 Loss_G: 0.6882 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6628 Loss_G: 0.6803 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7115 Loss_G: 0.7108 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6598 Loss_G: 0.7295 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6682 Loss_G: 0.6959 acc: 96.9%\n",
      "[BATCH 19/149] Loss_D: 0.6761 Loss_G: 0.7283 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.6913 Loss_G: 0.7333 acc: 93.8%\n",
      "[BATCH 21/149] Loss_D: 0.7098 Loss_G: 0.7668 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.7233 Loss_G: 0.7150 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7127 Loss_G: 0.7150 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7169 Loss_G: 0.7094 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6840 Loss_G: 0.6953 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6597 Loss_G: 0.6909 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6942 Loss_G: 0.6990 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6992 Loss_G: 0.6957 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.7036 Loss_G: 0.6889 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6719 Loss_G: 0.7190 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7024 Loss_G: 0.7243 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.6714 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6753 Loss_G: 0.7400 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.7071 Loss_G: 0.7342 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.6812 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.7323 Loss_G: 0.7062 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6734 Loss_G: 0.7184 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6414 Loss_G: 0.7101 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.6779 Loss_G: 0.7256 acc: 90.6%\n",
      "[EPOCH 5850] TEST ACC is : 75.6%\n",
      "[BATCH 40/149] Loss_D: 0.6641 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7325 Loss_G: 0.7062 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7297 Loss_G: 0.7136 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7044 Loss_G: 0.6968 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6779 Loss_G: 0.6969 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6848 Loss_G: 0.6992 acc: 93.8%\n",
      "[BATCH 46/149] Loss_D: 0.7037 Loss_G: 0.7185 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6696 Loss_G: 0.6989 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6771 Loss_G: 0.6920 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6879 Loss_G: 0.6989 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.6922 Loss_G: 0.7029 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.6679 Loss_G: 0.7068 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6649 Loss_G: 0.7057 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6759 Loss_G: 0.6960 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6722 Loss_G: 0.6998 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6627 Loss_G: 0.6948 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.6720 Loss_G: 0.7177 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6803 Loss_G: 0.7031 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6662 Loss_G: 0.6735 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.6990 Loss_G: 0.6986 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.7027 Loss_G: 0.7238 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6579 Loss_G: 0.7243 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6979 Loss_G: 0.7171 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6936 Loss_G: 0.7404 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6926 Loss_G: 0.7258 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.7057 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6689 Loss_G: 0.7041 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.6948 Loss_G: 0.6983 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6924 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6856 Loss_G: 0.6883 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6959 Loss_G: 0.6896 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6954 Loss_G: 0.7101 acc: 96.9%\n",
      "[BATCH 72/149] Loss_D: 0.6713 Loss_G: 0.7103 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.7329 Loss_G: 0.7138 acc: 79.7%\n",
      "[BATCH 74/149] Loss_D: 0.6848 Loss_G: 0.7209 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6777 Loss_G: 0.7032 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7359 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6842 Loss_G: 0.7120 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6825 Loss_G: 0.7069 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6664 Loss_G: 0.7152 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.7212 Loss_G: 0.7056 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6950 Loss_G: 0.7169 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.7302 Loss_G: 0.7226 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6718 Loss_G: 0.7230 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6794 Loss_G: 0.7268 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6861 Loss_G: 0.7082 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6785 Loss_G: 0.6974 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.7186 Loss_G: 0.7116 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6687 Loss_G: 0.7034 acc: 81.2%\n",
      "[BATCH 89/149] Loss_D: 0.6697 Loss_G: 0.6844 acc: 84.4%\n",
      "[EPOCH 5900] TEST ACC is : 76.4%\n",
      "[BATCH 90/149] Loss_D: 0.6713 Loss_G: 0.6890 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6527 Loss_G: 0.6998 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.7082 Loss_G: 0.7096 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6776 Loss_G: 0.7058 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6563 Loss_G: 0.7205 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6738 Loss_G: 0.7091 acc: 81.2%\n",
      "[BATCH 96/149] Loss_D: 0.6849 Loss_G: 0.6971 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6911 Loss_G: 0.7499 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.7062 Loss_G: 0.7198 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.7149 Loss_G: 0.7325 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6854 Loss_G: 0.7264 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7152 Loss_G: 0.7252 acc: 84.4%\n",
      "[BATCH 102/149] Loss_D: 0.6705 Loss_G: 0.7193 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6796 Loss_G: 0.7417 acc: 95.3%\n",
      "[BATCH 104/149] Loss_D: 0.7414 Loss_G: 0.7610 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.6953 Loss_G: 0.7177 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.6678 Loss_G: 0.7434 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.6517 Loss_G: 0.6990 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.7120 Loss_G: 0.7108 acc: 96.9%\n",
      "[BATCH 109/149] Loss_D: 0.7166 Loss_G: 0.7153 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.7369 Loss_G: 0.7374 acc: 81.2%\n",
      "[BATCH 111/149] Loss_D: 0.6253 Loss_G: 0.7307 acc: 98.4%\n",
      "[BATCH 112/149] Loss_D: 0.7029 Loss_G: 0.7027 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7033 Loss_G: 0.7105 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.6807 Loss_G: 0.7089 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6717 Loss_G: 0.7003 acc: 78.1%\n",
      "[BATCH 116/149] Loss_D: 0.7169 Loss_G: 0.7143 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.6807 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7148 Loss_G: 0.7042 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.7099 Loss_G: 0.7015 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.7150 Loss_G: 0.7125 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.7093 Loss_G: 0.7113 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.6959 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6870 Loss_G: 0.7191 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.6851 Loss_G: 0.7236 acc: 93.8%\n",
      "[BATCH 125/149] Loss_D: 0.6697 Loss_G: 0.6995 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6795 Loss_G: 0.6985 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6798 Loss_G: 0.7046 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6726 Loss_G: 0.6976 acc: 93.8%\n",
      "[BATCH 129/149] Loss_D: 0.6821 Loss_G: 0.6861 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6966 Loss_G: 0.6962 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7041 Loss_G: 0.7011 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6714 Loss_G: 0.6975 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6752 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6885 Loss_G: 0.7177 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6852 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7051 Loss_G: 0.6828 acc: 79.7%\n",
      "[BATCH 137/149] Loss_D: 0.7212 Loss_G: 0.6945 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.6773 Loss_G: 0.6981 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6898 Loss_G: 0.7103 acc: 89.1%\n",
      "[EPOCH 5950] TEST ACC is : 77.1%\n",
      "[BATCH 140/149] Loss_D: 0.6673 Loss_G: 0.7136 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.7075 Loss_G: 0.7240 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7079 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.7129 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7439 Loss_G: 0.7337 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6787 Loss_G: 0.7006 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6803 Loss_G: 0.7036 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.7127 Loss_G: 0.7232 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7005 Loss_G: 0.7007 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.7168 Loss_G: 0.7001 acc: 89.1%\n",
      "-----THE [40/50] epoch end-----\n",
      "-----THE [41/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6727 Loss_G: 0.6906 acc: 82.8%\n",
      "[BATCH 2/149] Loss_D: 0.6526 Loss_G: 0.6763 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.7033 Loss_G: 0.6959 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6873 Loss_G: 0.7026 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.7118 Loss_G: 0.7144 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.7070 Loss_G: 0.7270 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6776 Loss_G: 0.7122 acc: 95.3%\n",
      "[BATCH 8/149] Loss_D: 0.6639 Loss_G: 0.7053 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6616 Loss_G: 0.6857 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6506 Loss_G: 0.6885 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6490 Loss_G: 0.6780 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6668 Loss_G: 0.6831 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.7301 Loss_G: 0.7193 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.7081 Loss_G: 0.7616 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.7264 Loss_G: 0.7599 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6990 Loss_G: 0.7366 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6951 Loss_G: 0.7242 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6825 Loss_G: 0.7217 acc: 95.3%\n",
      "[BATCH 19/149] Loss_D: 0.6703 Loss_G: 0.7426 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6867 Loss_G: 0.7308 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6607 Loss_G: 0.7106 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6852 Loss_G: 0.7091 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6975 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6802 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6614 Loss_G: 0.7138 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.7048 Loss_G: 0.7859 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6642 Loss_G: 0.7080 acc: 84.4%\n",
      "[BATCH 28/149] Loss_D: 0.7292 Loss_G: 0.7032 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7163 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7022 Loss_G: 0.7164 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.7196 Loss_G: 0.7150 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6837 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6590 Loss_G: 0.7151 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.7034 Loss_G: 0.7298 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.6950 Loss_G: 0.7204 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7160 Loss_G: 0.7029 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.7306 Loss_G: 0.7458 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6801 Loss_G: 0.7384 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.6639 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6593 Loss_G: 0.6975 acc: 90.6%\n",
      "[EPOCH 6000] TEST ACC is : 77.1%\n",
      "[BATCH 41/149] Loss_D: 0.6733 Loss_G: 0.6771 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6588 Loss_G: 0.6649 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.7242 Loss_G: 0.6989 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6885 Loss_G: 0.6979 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.7307 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6810 Loss_G: 0.7124 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6819 Loss_G: 0.6865 acc: 81.2%\n",
      "[BATCH 48/149] Loss_D: 0.6724 Loss_G: 0.6941 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6808 Loss_G: 0.6931 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6599 Loss_G: 0.6841 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6706 Loss_G: 0.7065 acc: 95.3%\n",
      "[BATCH 52/149] Loss_D: 0.6910 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6742 Loss_G: 0.7218 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6920 Loss_G: 0.6925 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6927 Loss_G: 0.6968 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.6771 Loss_G: 0.6956 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6942 Loss_G: 0.7063 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6679 Loss_G: 0.6938 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6533 Loss_G: 0.6969 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.7061 Loss_G: 0.7108 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6745 Loss_G: 0.7246 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7074 Loss_G: 0.7566 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6921 Loss_G: 0.7088 acc: 79.7%\n",
      "[BATCH 64/149] Loss_D: 0.7065 Loss_G: 0.7263 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6946 Loss_G: 0.7207 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6678 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.6888 Loss_G: 0.7182 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6691 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7138 Loss_G: 0.7287 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6806 Loss_G: 0.7237 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6715 Loss_G: 0.7238 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6843 Loss_G: 0.7030 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6540 Loss_G: 0.6917 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6645 Loss_G: 0.6878 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.6648 Loss_G: 0.6836 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6828 Loss_G: 0.7161 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7294 Loss_G: 0.7251 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6466 Loss_G: 0.7066 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6791 Loss_G: 0.7040 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6878 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6966 Loss_G: 0.7097 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6954 Loss_G: 0.7046 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7043 Loss_G: 0.7065 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.7062 Loss_G: 0.7177 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6599 Loss_G: 0.7035 acc: 95.3%\n",
      "[BATCH 86/149] Loss_D: 0.7169 Loss_G: 0.7200 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6833 Loss_G: 0.7334 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7050 Loss_G: 0.7407 acc: 95.3%\n",
      "[BATCH 89/149] Loss_D: 0.6658 Loss_G: 0.7076 acc: 96.9%\n",
      "[BATCH 90/149] Loss_D: 0.6661 Loss_G: 0.7096 acc: 82.8%\n",
      "[EPOCH 6050] TEST ACC is : 77.3%\n",
      "[BATCH 91/149] Loss_D: 0.6504 Loss_G: 0.6850 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.7013 Loss_G: 0.6937 acc: 81.2%\n",
      "[BATCH 93/149] Loss_D: 0.6733 Loss_G: 0.6963 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6735 Loss_G: 0.6968 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6631 Loss_G: 0.6911 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7064 Loss_G: 0.7138 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7309 Loss_G: 0.7120 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7270 Loss_G: 0.7121 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7212 Loss_G: 0.7202 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6569 Loss_G: 0.7082 acc: 96.9%\n",
      "[BATCH 101/149] Loss_D: 0.6516 Loss_G: 0.7047 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7114 Loss_G: 0.7490 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.7114 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.7031 Loss_G: 0.7309 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.7087 Loss_G: 0.7341 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7113 Loss_G: 0.7112 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7297 Loss_G: 0.7294 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7124 Loss_G: 0.7345 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6960 Loss_G: 0.7384 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6880 Loss_G: 0.7406 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7253 Loss_G: 0.7265 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.7172 Loss_G: 0.7398 acc: 93.8%\n",
      "[BATCH 113/149] Loss_D: 0.7265 Loss_G: 0.7166 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.6725 Loss_G: 0.7115 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.7290 Loss_G: 0.7262 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.7352 Loss_G: 0.7282 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7252 Loss_G: 0.7148 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6951 Loss_G: 0.7287 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6775 Loss_G: 0.6997 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.6350 Loss_G: 0.6858 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6937 Loss_G: 0.6691 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6944 Loss_G: 0.6947 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.6946 Loss_G: 0.7130 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6977 Loss_G: 0.7095 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6392 Loss_G: 0.7041 acc: 95.3%\n",
      "[BATCH 126/149] Loss_D: 0.6861 Loss_G: 0.7081 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.6801 Loss_G: 0.6934 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7058 Loss_G: 0.7257 acc: 95.3%\n",
      "[BATCH 129/149] Loss_D: 0.6820 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6764 Loss_G: 0.7065 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6322 Loss_G: 0.6901 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.6527 Loss_G: 0.6954 acc: 93.8%\n",
      "[BATCH 133/149] Loss_D: 0.6986 Loss_G: 0.7026 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.6770 Loss_G: 0.6886 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.7038 Loss_G: 0.7326 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6904 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6783 Loss_G: 0.7300 acc: 96.9%\n",
      "[BATCH 138/149] Loss_D: 0.6937 Loss_G: 0.6968 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6859 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7194 Loss_G: 0.6990 acc: 82.8%\n",
      "[EPOCH 6100] TEST ACC is : 76.0%\n",
      "[BATCH 141/149] Loss_D: 0.6889 Loss_G: 0.6980 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6948 Loss_G: 0.6965 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.7250 Loss_G: 0.7262 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.7094 Loss_G: 0.7357 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7111 Loss_G: 0.7313 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6522 Loss_G: 0.6926 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6671 Loss_G: 0.6904 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6969 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7181 Loss_G: 0.7101 acc: 90.6%\n",
      "-----THE [41/50] epoch end-----\n",
      "-----THE [42/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7143 Loss_G: 0.7151 acc: 81.2%\n",
      "[BATCH 2/149] Loss_D: 0.6619 Loss_G: 0.7100 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6700 Loss_G: 0.6894 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6963 Loss_G: 0.7056 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6663 Loss_G: 0.7204 acc: 95.3%\n",
      "[BATCH 6/149] Loss_D: 0.6983 Loss_G: 0.7263 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6956 Loss_G: 0.7158 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6689 Loss_G: 0.7109 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6769 Loss_G: 0.7204 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.6929 Loss_G: 0.7073 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6891 Loss_G: 0.6945 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6973 Loss_G: 0.6974 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.7458 Loss_G: 0.7443 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6692 Loss_G: 0.7190 acc: 95.3%\n",
      "[BATCH 15/149] Loss_D: 0.6866 Loss_G: 0.7015 acc: 81.2%\n",
      "[BATCH 16/149] Loss_D: 0.6935 Loss_G: 0.7176 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6797 Loss_G: 0.7092 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6700 Loss_G: 0.7045 acc: 82.8%\n",
      "[BATCH 19/149] Loss_D: 0.7034 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6673 Loss_G: 0.7238 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6725 Loss_G: 0.7169 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.7340 Loss_G: 0.7148 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6897 Loss_G: 0.7253 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.7304 Loss_G: 0.7310 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6601 Loss_G: 0.7471 acc: 96.9%\n",
      "[BATCH 26/149] Loss_D: 0.6884 Loss_G: 0.7168 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6691 Loss_G: 0.6857 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6674 Loss_G: 0.7079 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.7134 Loss_G: 0.7208 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6838 Loss_G: 0.7026 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6874 Loss_G: 0.6985 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6707 Loss_G: 0.6940 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6683 Loss_G: 0.6769 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6722 Loss_G: 0.6678 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7223 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6956 Loss_G: 0.7005 acc: 79.7%\n",
      "[BATCH 37/149] Loss_D: 0.6469 Loss_G: 0.6936 acc: 95.3%\n",
      "[BATCH 38/149] Loss_D: 0.7055 Loss_G: 0.7028 acc: 93.8%\n",
      "[BATCH 39/149] Loss_D: 0.6636 Loss_G: 0.7008 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6831 Loss_G: 0.7158 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6882 Loss_G: 0.7079 acc: 89.1%\n",
      "[EPOCH 6150] TEST ACC is : 76.6%\n",
      "[BATCH 42/149] Loss_D: 0.6541 Loss_G: 0.7170 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6823 Loss_G: 0.7030 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6457 Loss_G: 0.6823 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.7013 Loss_G: 0.7031 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7112 Loss_G: 0.7326 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.7315 Loss_G: 0.7604 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6990 Loss_G: 0.7185 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.6700 Loss_G: 0.7193 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.7056 Loss_G: 0.7122 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.7325 Loss_G: 0.7322 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6628 Loss_G: 0.7181 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6892 Loss_G: 0.7041 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6852 Loss_G: 0.6810 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6828 Loss_G: 0.7093 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6575 Loss_G: 0.6858 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6993 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6893 Loss_G: 0.6924 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.7146 Loss_G: 0.6978 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.6985 Loss_G: 0.7140 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6641 Loss_G: 0.6920 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6787 Loss_G: 0.7048 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6888 Loss_G: 0.7077 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7063 Loss_G: 0.7225 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6964 Loss_G: 0.7134 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7208 Loss_G: 0.7324 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6773 Loss_G: 0.7108 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.6970 Loss_G: 0.7174 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7452 Loss_G: 0.7379 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6770 Loss_G: 0.7263 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.7111 Loss_G: 0.7292 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.7008 Loss_G: 0.7059 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.6825 Loss_G: 0.6982 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6811 Loss_G: 0.7064 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6717 Loss_G: 0.7053 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6378 Loss_G: 0.6926 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6791 Loss_G: 0.6794 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6633 Loss_G: 0.6975 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.7084 Loss_G: 0.6993 acc: 93.8%\n",
      "[BATCH 80/149] Loss_D: 0.6998 Loss_G: 0.6895 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6886 Loss_G: 0.6880 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6633 Loss_G: 0.7232 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6997 Loss_G: 0.7111 acc: 81.2%\n",
      "[BATCH 84/149] Loss_D: 0.7542 Loss_G: 0.7499 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6806 Loss_G: 0.7327 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.7115 Loss_G: 0.7202 acc: 84.4%\n",
      "[BATCH 87/149] Loss_D: 0.6902 Loss_G: 0.7320 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6691 Loss_G: 0.7039 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6762 Loss_G: 0.7015 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6586 Loss_G: 0.7371 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7128 Loss_G: 0.7254 acc: 87.5%\n",
      "[EPOCH 6200] TEST ACC is : 76.6%\n",
      "[BATCH 92/149] Loss_D: 0.6947 Loss_G: 0.7406 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6738 Loss_G: 0.7064 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.7258 Loss_G: 0.7329 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.6737 Loss_G: 0.7114 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6507 Loss_G: 0.6871 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6619 Loss_G: 0.6977 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.6859 Loss_G: 0.7169 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6565 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.7088 Loss_G: 0.7277 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7258 Loss_G: 0.7296 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6540 Loss_G: 0.6920 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7115 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6540 Loss_G: 0.6917 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6743 Loss_G: 0.6887 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.7289 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6617 Loss_G: 0.7070 acc: 96.9%\n",
      "[BATCH 108/149] Loss_D: 0.6522 Loss_G: 0.6971 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6819 Loss_G: 0.6966 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6824 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6787 Loss_G: 0.7067 acc: 81.2%\n",
      "[BATCH 112/149] Loss_D: 0.6934 Loss_G: 0.7095 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.7427 Loss_G: 0.7195 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.6885 Loss_G: 0.7164 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7303 Loss_G: 0.7264 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.6856 Loss_G: 0.7163 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6981 Loss_G: 0.7282 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.6480 Loss_G: 0.7153 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.6916 Loss_G: 0.7106 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.6578 Loss_G: 0.7024 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6889 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6785 Loss_G: 0.7197 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.6960 Loss_G: 0.7151 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.6846 Loss_G: 0.7008 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6718 Loss_G: 0.7000 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6592 Loss_G: 0.6990 acc: 98.4%\n",
      "[BATCH 127/149] Loss_D: 0.6682 Loss_G: 0.7135 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6935 Loss_G: 0.6994 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6835 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7009 Loss_G: 0.7218 acc: 93.8%\n",
      "[BATCH 131/149] Loss_D: 0.7049 Loss_G: 0.7070 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.6963 Loss_G: 0.7137 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6687 Loss_G: 0.6918 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.7081 Loss_G: 0.6980 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7329 Loss_G: 0.7391 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6917 Loss_G: 0.7344 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.6975 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6867 Loss_G: 0.7004 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6784 Loss_G: 0.6961 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.7280 Loss_G: 0.7055 acc: 79.7%\n",
      "[BATCH 141/149] Loss_D: 0.6742 Loss_G: 0.7163 acc: 89.1%\n",
      "[EPOCH 6250] TEST ACC is : 77.5%\n",
      "[BATCH 142/149] Loss_D: 0.7276 Loss_G: 0.7065 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6486 Loss_G: 0.7228 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.6815 Loss_G: 0.7283 acc: 96.9%\n",
      "[BATCH 145/149] Loss_D: 0.7040 Loss_G: 0.7202 acc: 81.2%\n",
      "[BATCH 146/149] Loss_D: 0.7081 Loss_G: 0.7236 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6619 Loss_G: 0.7045 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6761 Loss_G: 0.6918 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.7028 Loss_G: 0.7016 acc: 87.5%\n",
      "-----THE [42/50] epoch end-----\n",
      "-----THE [43/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6724 Loss_G: 0.7156 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.7046 Loss_G: 0.7151 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.6735 Loss_G: 0.7106 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6765 Loss_G: 0.6909 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6744 Loss_G: 0.7023 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6945 Loss_G: 0.6907 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.7020 Loss_G: 0.7004 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6763 Loss_G: 0.7049 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.6912 Loss_G: 0.7184 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6831 Loss_G: 0.7071 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.6815 Loss_G: 0.6945 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6642 Loss_G: 0.6795 acc: 79.7%\n",
      "[BATCH 13/149] Loss_D: 0.6620 Loss_G: 0.6913 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6660 Loss_G: 0.6779 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.6600 Loss_G: 0.6905 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6729 Loss_G: 0.6804 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.7296 Loss_G: 0.7235 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7181 Loss_G: 0.7150 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.6881 Loss_G: 0.7149 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6882 Loss_G: 0.6951 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.7082 Loss_G: 0.7140 acc: 95.3%\n",
      "[BATCH 22/149] Loss_D: 0.6979 Loss_G: 0.7308 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6804 Loss_G: 0.7092 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6869 Loss_G: 0.6956 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6659 Loss_G: 0.6998 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6781 Loss_G: 0.6936 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7069 Loss_G: 0.6976 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7390 Loss_G: 0.7078 acc: 78.1%\n",
      "[BATCH 29/149] Loss_D: 0.6958 Loss_G: 0.7075 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7040 Loss_G: 0.7186 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6576 Loss_G: 0.6940 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.6623 Loss_G: 0.6945 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6762 Loss_G: 0.7126 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.6771 Loss_G: 0.6998 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6929 Loss_G: 0.7290 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6500 Loss_G: 0.7045 acc: 82.8%\n",
      "[BATCH 37/149] Loss_D: 0.6889 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.7097 Loss_G: 0.7252 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.7529 Loss_G: 0.7675 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6850 Loss_G: 0.7256 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6474 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6893 Loss_G: 0.6945 acc: 89.1%\n",
      "[EPOCH 6300] TEST ACC is : 77.1%\n",
      "[BATCH 43/149] Loss_D: 0.6996 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6874 Loss_G: 0.7132 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6571 Loss_G: 0.7263 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6829 Loss_G: 0.7048 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6613 Loss_G: 0.6933 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7254 Loss_G: 0.7150 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6727 Loss_G: 0.6882 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6911 Loss_G: 0.6935 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6716 Loss_G: 0.6932 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7048 Loss_G: 0.7012 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6606 Loss_G: 0.6912 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6827 Loss_G: 0.6874 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.6551 Loss_G: 0.6918 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7246 Loss_G: 0.6984 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6616 Loss_G: 0.6982 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6908 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6578 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6692 Loss_G: 0.7081 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6993 Loss_G: 0.7266 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6857 Loss_G: 0.7101 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.6710 Loss_G: 0.6918 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7210 Loss_G: 0.7192 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7161 Loss_G: 0.6979 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6788 Loss_G: 0.7007 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7009 Loss_G: 0.7195 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6965 Loss_G: 0.7207 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6668 Loss_G: 0.6940 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.6843 Loss_G: 0.7415 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6961 Loss_G: 0.7076 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7061 Loss_G: 0.7302 acc: 95.3%\n",
      "[BATCH 73/149] Loss_D: 0.7171 Loss_G: 0.7125 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6941 Loss_G: 0.7224 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6554 Loss_G: 0.7160 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7096 Loss_G: 0.7082 acc: 95.3%\n",
      "[BATCH 77/149] Loss_D: 0.6586 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.7151 Loss_G: 0.7055 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.6961 Loss_G: 0.7043 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7380 Loss_G: 0.7551 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.6875 Loss_G: 0.7179 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.7029 Loss_G: 0.7011 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.6825 Loss_G: 0.7168 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6592 Loss_G: 0.6972 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7081 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6772 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6871 Loss_G: 0.6983 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7092 Loss_G: 0.7404 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6899 Loss_G: 0.7620 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6891 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.7235 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7007 Loss_G: 0.7059 acc: 85.9%\n",
      "[EPOCH 6350] TEST ACC is : 77.0%\n",
      "[BATCH 93/149] Loss_D: 0.6452 Loss_G: 0.7116 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6665 Loss_G: 0.6954 acc: 82.8%\n",
      "[BATCH 95/149] Loss_D: 0.7112 Loss_G: 0.7071 acc: 79.7%\n",
      "[BATCH 96/149] Loss_D: 0.6603 Loss_G: 0.6984 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6935 Loss_G: 0.7107 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6760 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6871 Loss_G: 0.7002 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.6957 Loss_G: 0.7055 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6691 Loss_G: 0.6950 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6708 Loss_G: 0.6961 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.6754 Loss_G: 0.7106 acc: 95.3%\n",
      "[BATCH 104/149] Loss_D: 0.6947 Loss_G: 0.7028 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6995 Loss_G: 0.7136 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6634 Loss_G: 0.6990 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.7155 Loss_G: 0.7133 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.6742 Loss_G: 0.7018 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.6918 Loss_G: 0.7170 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6992 Loss_G: 0.7134 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.7220 Loss_G: 0.7217 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7385 Loss_G: 0.7220 acc: 79.7%\n",
      "[BATCH 113/149] Loss_D: 0.6896 Loss_G: 0.7254 acc: 95.3%\n",
      "[BATCH 114/149] Loss_D: 0.7049 Loss_G: 0.7411 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7066 Loss_G: 0.7213 acc: 79.7%\n",
      "[BATCH 116/149] Loss_D: 0.6694 Loss_G: 0.7080 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6715 Loss_G: 0.7065 acc: 98.4%\n",
      "[BATCH 118/149] Loss_D: 0.6958 Loss_G: 0.7070 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6864 Loss_G: 0.7096 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6956 Loss_G: 0.7148 acc: 93.8%\n",
      "[BATCH 121/149] Loss_D: 0.6925 Loss_G: 0.7201 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6935 Loss_G: 0.7288 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6556 Loss_G: 0.6942 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6693 Loss_G: 0.7055 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6706 Loss_G: 0.6952 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.7190 Loss_G: 0.7033 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6857 Loss_G: 0.7183 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7334 Loss_G: 0.7316 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7012 Loss_G: 0.7298 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7398 Loss_G: 0.7365 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7256 Loss_G: 0.7218 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6934 Loss_G: 0.7175 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6811 Loss_G: 0.6984 acc: 93.8%\n",
      "[BATCH 134/149] Loss_D: 0.6761 Loss_G: 0.6919 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6751 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6569 Loss_G: 0.7210 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6716 Loss_G: 0.7092 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6803 Loss_G: 0.7352 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.6761 Loss_G: 0.6944 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7246 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7475 Loss_G: 0.7376 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.7042 Loss_G: 0.7198 acc: 87.5%\n",
      "[EPOCH 6400] TEST ACC is : 76.2%\n",
      "[BATCH 143/149] Loss_D: 0.7203 Loss_G: 0.7397 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6687 Loss_G: 0.7108 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6551 Loss_G: 0.7180 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6718 Loss_G: 0.7029 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7086 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6484 Loss_G: 0.6903 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.7142 Loss_G: 0.7075 acc: 84.4%\n",
      "-----THE [43/50] epoch end-----\n",
      "-----THE [44/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6563 Loss_G: 0.6976 acc: 95.3%\n",
      "[BATCH 2/149] Loss_D: 0.6939 Loss_G: 0.7084 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.6698 Loss_G: 0.7153 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.6788 Loss_G: 0.7248 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6727 Loss_G: 0.7021 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6468 Loss_G: 0.6748 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.7006 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6923 Loss_G: 0.7079 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6586 Loss_G: 0.7125 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6870 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7329 Loss_G: 0.7176 acc: 78.1%\n",
      "[BATCH 12/149] Loss_D: 0.7143 Loss_G: 0.7168 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6788 Loss_G: 0.6937 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.6825 Loss_G: 0.6842 acc: 95.3%\n",
      "[BATCH 15/149] Loss_D: 0.7034 Loss_G: 0.7196 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.7062 Loss_G: 0.7449 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6901 Loss_G: 0.7412 acc: 93.8%\n",
      "[BATCH 18/149] Loss_D: 0.6654 Loss_G: 0.7068 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7259 Loss_G: 0.7038 acc: 79.7%\n",
      "[BATCH 20/149] Loss_D: 0.6681 Loss_G: 0.6992 acc: 93.8%\n",
      "[BATCH 21/149] Loss_D: 0.6924 Loss_G: 0.7182 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6748 Loss_G: 0.7186 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7030 Loss_G: 0.7068 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7038 Loss_G: 0.7051 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6721 Loss_G: 0.7081 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6669 Loss_G: 0.7022 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.7419 Loss_G: 0.7090 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6961 Loss_G: 0.7132 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6980 Loss_G: 0.6969 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7104 Loss_G: 0.7035 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.6700 Loss_G: 0.7097 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7245 Loss_G: 0.7254 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7042 Loss_G: 0.7334 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6709 Loss_G: 0.7266 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.7022 Loss_G: 0.7297 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6993 Loss_G: 0.7139 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.6753 Loss_G: 0.7280 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.7159 Loss_G: 0.7069 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.6864 Loss_G: 0.7165 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6806 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6942 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6645 Loss_G: 0.7068 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6572 Loss_G: 0.6978 acc: 87.5%\n",
      "[EPOCH 6450] TEST ACC is : 76.4%\n",
      "[BATCH 44/149] Loss_D: 0.7097 Loss_G: 0.6946 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7023 Loss_G: 0.7267 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.7229 Loss_G: 0.7339 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.6661 Loss_G: 0.7259 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6862 Loss_G: 0.7046 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6940 Loss_G: 0.7075 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.6500 Loss_G: 0.6868 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6838 Loss_G: 0.7011 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6585 Loss_G: 0.6951 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.6699 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6902 Loss_G: 0.7109 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6550 Loss_G: 0.6889 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6898 Loss_G: 0.6972 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6869 Loss_G: 0.6972 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6750 Loss_G: 0.6858 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.6762 Loss_G: 0.6971 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6982 Loss_G: 0.7142 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.7065 Loss_G: 0.7392 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7070 Loss_G: 0.7119 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6621 Loss_G: 0.6931 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6756 Loss_G: 0.6993 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6662 Loss_G: 0.7240 acc: 93.8%\n",
      "[BATCH 66/149] Loss_D: 0.6819 Loss_G: 0.7111 acc: 81.2%\n",
      "[BATCH 67/149] Loss_D: 0.6788 Loss_G: 0.7101 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7180 Loss_G: 0.7293 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7168 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6796 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6756 Loss_G: 0.7056 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6516 Loss_G: 0.7137 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7138 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.6884 Loss_G: 0.7039 acc: 82.8%\n",
      "[BATCH 75/149] Loss_D: 0.6549 Loss_G: 0.7121 acc: 96.9%\n",
      "[BATCH 76/149] Loss_D: 0.6833 Loss_G: 0.7046 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6715 Loss_G: 0.6829 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6735 Loss_G: 0.6917 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6886 Loss_G: 0.7049 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6981 Loss_G: 0.7104 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6982 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6597 Loss_G: 0.6807 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6887 Loss_G: 0.6851 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6666 Loss_G: 0.7022 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6662 Loss_G: 0.6927 acc: 78.1%\n",
      "[BATCH 86/149] Loss_D: 0.7053 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6844 Loss_G: 0.7335 acc: 96.9%\n",
      "[BATCH 88/149] Loss_D: 0.6749 Loss_G: 0.7060 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.7552 Loss_G: 0.7082 acc: 79.7%\n",
      "[BATCH 90/149] Loss_D: 0.7005 Loss_G: 0.7167 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6887 Loss_G: 0.7108 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.6654 Loss_G: 0.7028 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7036 Loss_G: 0.7021 acc: 90.6%\n",
      "[EPOCH 6500] TEST ACC is : 76.6%\n",
      "[BATCH 94/149] Loss_D: 0.6893 Loss_G: 0.7081 acc: 82.8%\n",
      "[BATCH 95/149] Loss_D: 0.6869 Loss_G: 0.7044 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.6763 Loss_G: 0.7123 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7181 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6689 Loss_G: 0.7027 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6671 Loss_G: 0.6942 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6369 Loss_G: 0.6883 acc: 95.3%\n",
      "[BATCH 101/149] Loss_D: 0.7059 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6798 Loss_G: 0.7054 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7142 Loss_G: 0.7016 acc: 81.2%\n",
      "[BATCH 104/149] Loss_D: 0.6632 Loss_G: 0.6927 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.7457 Loss_G: 0.7289 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6891 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6672 Loss_G: 0.7200 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.6718 Loss_G: 0.6958 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.7288 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7170 Loss_G: 0.7353 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.6731 Loss_G: 0.7134 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.6825 Loss_G: 0.6981 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.7139 Loss_G: 0.7113 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6680 Loss_G: 0.7002 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.7011 Loss_G: 0.7154 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6957 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6797 Loss_G: 0.7369 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6852 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.7375 Loss_G: 0.7324 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6962 Loss_G: 0.7380 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6732 Loss_G: 0.7262 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6766 Loss_G: 0.7015 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.6933 Loss_G: 0.7021 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6579 Loss_G: 0.6927 acc: 93.8%\n",
      "[BATCH 125/149] Loss_D: 0.7037 Loss_G: 0.7160 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6776 Loss_G: 0.7053 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7140 Loss_G: 0.7235 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6575 Loss_G: 0.7209 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.6657 Loss_G: 0.7048 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7188 Loss_G: 0.7190 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6655 Loss_G: 0.7105 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6892 Loss_G: 0.6886 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.7062 Loss_G: 0.6925 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.6564 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.7028 Loss_G: 0.7040 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6921 Loss_G: 0.6993 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6906 Loss_G: 0.6984 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6869 Loss_G: 0.7052 acc: 93.8%\n",
      "[BATCH 139/149] Loss_D: 0.7103 Loss_G: 0.7229 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7109 Loss_G: 0.7267 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6871 Loss_G: 0.7096 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.6636 Loss_G: 0.7014 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6995 Loss_G: 0.7279 acc: 95.3%\n",
      "[EPOCH 6550] TEST ACC is : 76.8%\n",
      "[BATCH 144/149] Loss_D: 0.6961 Loss_G: 0.7040 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6578 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7126 Loss_G: 0.7148 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.7119 Loss_G: 0.7025 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6855 Loss_G: 0.7084 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.7005 Loss_G: 0.7294 acc: 89.1%\n",
      "-----THE [44/50] epoch end-----\n",
      "-----THE [45/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7227 Loss_G: 0.7278 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7378 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6948 Loss_G: 0.7078 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6601 Loss_G: 0.7000 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6993 Loss_G: 0.7091 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.7294 Loss_G: 0.7291 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.6764 Loss_G: 0.7097 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6790 Loss_G: 0.7127 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.7180 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7598 Loss_G: 0.7418 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6772 Loss_G: 0.7026 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.6923 Loss_G: 0.7115 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6822 Loss_G: 0.7291 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6879 Loss_G: 0.7410 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.7046 Loss_G: 0.7223 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.7253 Loss_G: 0.7272 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6811 Loss_G: 0.7219 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6953 Loss_G: 0.7212 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6769 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.7005 Loss_G: 0.7435 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.7049 Loss_G: 0.7350 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6808 Loss_G: 0.7365 acc: 82.8%\n",
      "[BATCH 23/149] Loss_D: 0.6681 Loss_G: 0.7023 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6946 Loss_G: 0.7246 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.7141 Loss_G: 0.7294 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.7540 Loss_G: 0.7318 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7187 Loss_G: 0.7300 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6634 Loss_G: 0.7157 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6939 Loss_G: 0.7146 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6868 Loss_G: 0.7530 acc: 95.3%\n",
      "[BATCH 31/149] Loss_D: 0.7003 Loss_G: 0.7205 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6839 Loss_G: 0.6948 acc: 79.7%\n",
      "[BATCH 33/149] Loss_D: 0.6823 Loss_G: 0.6978 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6726 Loss_G: 0.6976 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6828 Loss_G: 0.6863 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6764 Loss_G: 0.6934 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7004 Loss_G: 0.7458 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6661 Loss_G: 0.6970 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6806 Loss_G: 0.6993 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6730 Loss_G: 0.6979 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7359 Loss_G: 0.7200 acc: 81.2%\n",
      "[BATCH 42/149] Loss_D: 0.7148 Loss_G: 0.7192 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.6963 Loss_G: 0.7247 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7026 Loss_G: 0.7091 acc: 89.1%\n",
      "[EPOCH 6600] TEST ACC is : 77.3%\n",
      "[BATCH 45/149] Loss_D: 0.7146 Loss_G: 0.7037 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.7180 Loss_G: 0.7260 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.7058 Loss_G: 0.7270 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.6837 Loss_G: 0.7587 acc: 95.3%\n",
      "[BATCH 49/149] Loss_D: 0.7073 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6647 Loss_G: 0.7076 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.6546 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6582 Loss_G: 0.6986 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7044 Loss_G: 0.7347 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6945 Loss_G: 0.7417 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6943 Loss_G: 0.7269 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.6723 Loss_G: 0.7131 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6482 Loss_G: 0.6744 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7517 Loss_G: 0.7186 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6846 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6673 Loss_G: 0.7033 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6559 Loss_G: 0.7043 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.7245 Loss_G: 0.7087 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.6740 Loss_G: 0.7161 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6921 Loss_G: 0.7282 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6708 Loss_G: 0.7260 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6758 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6966 Loss_G: 0.7082 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.7307 Loss_G: 0.7216 acc: 79.7%\n",
      "[BATCH 69/149] Loss_D: 0.6610 Loss_G: 0.7044 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6854 Loss_G: 0.7191 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6523 Loss_G: 0.6870 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.6880 Loss_G: 0.6942 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6953 Loss_G: 0.7181 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6806 Loss_G: 0.7074 acc: 95.3%\n",
      "[BATCH 75/149] Loss_D: 0.6975 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6915 Loss_G: 0.7236 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.6599 Loss_G: 0.7072 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6246 Loss_G: 0.6829 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.6444 Loss_G: 0.6924 acc: 93.8%\n",
      "[BATCH 80/149] Loss_D: 0.6612 Loss_G: 0.6867 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6744 Loss_G: 0.7046 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6934 Loss_G: 0.6775 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6941 Loss_G: 0.6647 acc: 82.8%\n",
      "[BATCH 84/149] Loss_D: 0.7170 Loss_G: 0.6931 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.6843 Loss_G: 0.7221 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6865 Loss_G: 0.7255 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6697 Loss_G: 0.7029 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6928 Loss_G: 0.7004 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7046 Loss_G: 0.7129 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6924 Loss_G: 0.7106 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.7028 Loss_G: 0.7271 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6919 Loss_G: 0.7272 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7132 Loss_G: 0.7052 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6977 Loss_G: 0.6855 acc: 81.2%\n",
      "[EPOCH 6650] TEST ACC is : 76.2%\n",
      "[BATCH 95/149] Loss_D: 0.6374 Loss_G: 0.6692 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6737 Loss_G: 0.6898 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6812 Loss_G: 0.6995 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6759 Loss_G: 0.7344 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6752 Loss_G: 0.7238 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.7484 Loss_G: 0.7114 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6936 Loss_G: 0.7003 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6603 Loss_G: 0.6866 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6820 Loss_G: 0.6875 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6457 Loss_G: 0.6877 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6392 Loss_G: 0.6858 acc: 95.3%\n",
      "[BATCH 106/149] Loss_D: 0.6977 Loss_G: 0.6914 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6406 Loss_G: 0.6822 acc: 95.3%\n",
      "[BATCH 108/149] Loss_D: 0.6386 Loss_G: 0.7084 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6433 Loss_G: 0.6765 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7104 Loss_G: 0.7161 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.7122 Loss_G: 0.7059 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6639 Loss_G: 0.6937 acc: 82.8%\n",
      "[BATCH 113/149] Loss_D: 0.7125 Loss_G: 0.7234 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6900 Loss_G: 0.7352 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6738 Loss_G: 0.7007 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7263 Loss_G: 0.7072 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7005 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6828 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.6661 Loss_G: 0.7043 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7113 Loss_G: 0.7012 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.6483 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6924 Loss_G: 0.7178 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6912 Loss_G: 0.7010 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6765 Loss_G: 0.7240 acc: 93.8%\n",
      "[BATCH 125/149] Loss_D: 0.7004 Loss_G: 0.7077 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6934 Loss_G: 0.7133 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.6930 Loss_G: 0.7227 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.6825 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.6602 Loss_G: 0.7069 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6647 Loss_G: 0.6903 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6947 Loss_G: 0.6899 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.6675 Loss_G: 0.7052 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6780 Loss_G: 0.6965 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7051 Loss_G: 0.7400 acc: 93.8%\n",
      "[BATCH 135/149] Loss_D: 0.6952 Loss_G: 0.7061 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7084 Loss_G: 0.7148 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.6748 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6935 Loss_G: 0.7006 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6486 Loss_G: 0.6886 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6639 Loss_G: 0.6925 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6709 Loss_G: 0.6990 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6882 Loss_G: 0.7058 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6850 Loss_G: 0.7034 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7016 Loss_G: 0.7143 acc: 84.4%\n",
      "[EPOCH 6700] TEST ACC is : 76.8%\n",
      "[BATCH 145/149] Loss_D: 0.7380 Loss_G: 0.7286 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.7064 Loss_G: 0.7226 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6885 Loss_G: 0.7153 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.7115 Loss_G: 0.7262 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.6738 Loss_G: 0.7227 acc: 93.8%\n",
      "-----THE [45/50] epoch end-----\n",
      "-----THE [46/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6853 Loss_G: 0.7088 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7179 Loss_G: 0.7338 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.6865 Loss_G: 0.7223 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6856 Loss_G: 0.7094 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.7048 Loss_G: 0.7320 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.6751 Loss_G: 0.7067 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.6959 Loss_G: 0.7143 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6840 Loss_G: 0.7012 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6700 Loss_G: 0.7183 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.7030 Loss_G: 0.7016 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.6740 Loss_G: 0.7052 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6533 Loss_G: 0.6928 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6714 Loss_G: 0.6892 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6654 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6713 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6504 Loss_G: 0.7063 acc: 93.8%\n",
      "[BATCH 17/149] Loss_D: 0.6692 Loss_G: 0.6972 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.7098 Loss_G: 0.7363 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6751 Loss_G: 0.6988 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6983 Loss_G: 0.6854 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6687 Loss_G: 0.7227 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6844 Loss_G: 0.7159 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.6952 Loss_G: 0.7026 acc: 81.2%\n",
      "[BATCH 24/149] Loss_D: 0.6795 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7070 Loss_G: 0.6923 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6930 Loss_G: 0.6876 acc: 79.7%\n",
      "[BATCH 27/149] Loss_D: 0.6958 Loss_G: 0.6897 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6898 Loss_G: 0.7044 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6843 Loss_G: 0.7163 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6781 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6905 Loss_G: 0.7059 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6557 Loss_G: 0.7014 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6766 Loss_G: 0.6921 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.6683 Loss_G: 0.6971 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.6881 Loss_G: 0.7323 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6859 Loss_G: 0.7062 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.6668 Loss_G: 0.7147 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.6848 Loss_G: 0.7236 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.6945 Loss_G: 0.7510 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6887 Loss_G: 0.7109 acc: 95.3%\n",
      "[BATCH 41/149] Loss_D: 0.6419 Loss_G: 0.6861 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6750 Loss_G: 0.6940 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.6714 Loss_G: 0.6849 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6590 Loss_G: 0.6973 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6831 Loss_G: 0.6918 acc: 90.6%\n",
      "[EPOCH 6750] TEST ACC is : 76.4%\n",
      "[BATCH 46/149] Loss_D: 0.7069 Loss_G: 0.6830 acc: 79.7%\n",
      "[BATCH 47/149] Loss_D: 0.7278 Loss_G: 0.7196 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.7038 Loss_G: 0.7123 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.7156 Loss_G: 0.7332 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6837 Loss_G: 0.7174 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7038 Loss_G: 0.7048 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7061 Loss_G: 0.7176 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6903 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7098 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6579 Loss_G: 0.7058 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6627 Loss_G: 0.6802 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.7376 Loss_G: 0.6952 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6797 Loss_G: 0.6915 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.7042 Loss_G: 0.6922 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7163 Loss_G: 0.7452 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.6996 Loss_G: 0.7116 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7111 Loss_G: 0.6955 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.7014 Loss_G: 0.7020 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7202 Loss_G: 0.7206 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7023 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6581 Loss_G: 0.6985 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6688 Loss_G: 0.6797 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6587 Loss_G: 0.7016 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6864 Loss_G: 0.7038 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.6783 Loss_G: 0.6930 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7225 Loss_G: 0.7141 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.6792 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6613 Loss_G: 0.7251 acc: 95.3%\n",
      "[BATCH 74/149] Loss_D: 0.7132 Loss_G: 0.7539 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.7252 Loss_G: 0.7351 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7021 Loss_G: 0.7197 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6965 Loss_G: 0.6955 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.7237 Loss_G: 0.7160 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.7139 Loss_G: 0.7226 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6703 Loss_G: 0.7106 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.6497 Loss_G: 0.7057 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.6925 Loss_G: 0.6942 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.7484 Loss_G: 0.7096 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6598 Loss_G: 0.7106 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6562 Loss_G: 0.7050 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6880 Loss_G: 0.7048 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6810 Loss_G: 0.6811 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6812 Loss_G: 0.6897 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.6711 Loss_G: 0.6932 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7042 Loss_G: 0.7191 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6636 Loss_G: 0.6930 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7361 Loss_G: 0.6967 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.7064 Loss_G: 0.7306 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7131 Loss_G: 0.7403 acc: 95.3%\n",
      "[BATCH 95/149] Loss_D: 0.7156 Loss_G: 0.7148 acc: 81.2%\n",
      "[EPOCH 6800] TEST ACC is : 76.8%\n",
      "[BATCH 96/149] Loss_D: 0.6516 Loss_G: 0.7120 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6687 Loss_G: 0.7105 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.7336 Loss_G: 0.7179 acc: 82.8%\n",
      "[BATCH 99/149] Loss_D: 0.6688 Loss_G: 0.7115 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.7139 Loss_G: 0.7249 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.6879 Loss_G: 0.7336 acc: 93.8%\n",
      "[BATCH 102/149] Loss_D: 0.6885 Loss_G: 0.7138 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.6966 Loss_G: 0.7191 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.6691 Loss_G: 0.7007 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7060 Loss_G: 0.7287 acc: 95.3%\n",
      "[BATCH 106/149] Loss_D: 0.7054 Loss_G: 0.7385 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6690 Loss_G: 0.7073 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.7227 Loss_G: 0.7397 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7021 Loss_G: 0.7197 acc: 79.7%\n",
      "[BATCH 110/149] Loss_D: 0.7003 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6885 Loss_G: 0.7157 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7224 Loss_G: 0.7446 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7108 Loss_G: 0.7136 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7215 Loss_G: 0.7456 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6806 Loss_G: 0.7105 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6787 Loss_G: 0.7067 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6965 Loss_G: 0.7175 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6670 Loss_G: 0.7142 acc: 96.9%\n",
      "[BATCH 119/149] Loss_D: 0.6706 Loss_G: 0.7006 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6723 Loss_G: 0.6882 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.7008 Loss_G: 0.7001 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.6924 Loss_G: 0.7001 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6929 Loss_G: 0.7187 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6702 Loss_G: 0.6997 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6579 Loss_G: 0.6911 acc: 93.8%\n",
      "[BATCH 126/149] Loss_D: 0.6742 Loss_G: 0.6849 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.6606 Loss_G: 0.6855 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7293 Loss_G: 0.7011 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6936 Loss_G: 0.7358 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.6958 Loss_G: 0.7170 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6899 Loss_G: 0.6985 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6590 Loss_G: 0.7177 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6863 Loss_G: 0.7023 acc: 96.9%\n",
      "[BATCH 134/149] Loss_D: 0.7538 Loss_G: 0.7375 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6953 Loss_G: 0.7093 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6867 Loss_G: 0.6992 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.6752 Loss_G: 0.6951 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6858 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6804 Loss_G: 0.6928 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.7017 Loss_G: 0.7244 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6772 Loss_G: 0.7324 acc: 93.8%\n",
      "[BATCH 142/149] Loss_D: 0.6506 Loss_G: 0.6947 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.6516 Loss_G: 0.6843 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6550 Loss_G: 0.6771 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.6587 Loss_G: 0.6930 acc: 93.8%\n",
      "[EPOCH 6850] TEST ACC is : 76.6%\n",
      "[BATCH 146/149] Loss_D: 0.6727 Loss_G: 0.7016 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6951 Loss_G: 0.7063 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.7013 Loss_G: 0.7170 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6862 Loss_G: 0.7114 acc: 90.6%\n",
      "-----THE [46/50] epoch end-----\n",
      "-----THE [47/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6853 Loss_G: 0.7187 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6509 Loss_G: 0.6991 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.7262 Loss_G: 0.7232 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.6869 Loss_G: 0.7297 acc: 96.9%\n",
      "[BATCH 5/149] Loss_D: 0.6930 Loss_G: 0.7103 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6954 Loss_G: 0.7158 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.7213 Loss_G: 0.7127 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7143 Loss_G: 0.7213 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.7273 Loss_G: 0.7241 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6834 Loss_G: 0.6973 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7165 Loss_G: 0.7267 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7317 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6923 Loss_G: 0.7269 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6819 Loss_G: 0.7179 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6652 Loss_G: 0.6994 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6914 Loss_G: 0.7090 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6889 Loss_G: 0.7223 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.7207 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6596 Loss_G: 0.7019 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6809 Loss_G: 0.7038 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6975 Loss_G: 0.7161 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7145 Loss_G: 0.7381 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.6896 Loss_G: 0.7197 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6537 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6962 Loss_G: 0.7077 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6928 Loss_G: 0.7022 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7013 Loss_G: 0.6951 acc: 84.4%\n",
      "[BATCH 28/149] Loss_D: 0.7065 Loss_G: 0.6973 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6732 Loss_G: 0.7088 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6514 Loss_G: 0.7191 acc: 95.3%\n",
      "[BATCH 31/149] Loss_D: 0.6846 Loss_G: 0.7040 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7003 Loss_G: 0.7095 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.7056 Loss_G: 0.7131 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6537 Loss_G: 0.7068 acc: 96.9%\n",
      "[BATCH 35/149] Loss_D: 0.6769 Loss_G: 0.7040 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6680 Loss_G: 0.6855 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7034 Loss_G: 0.7120 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6974 Loss_G: 0.7208 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7298 Loss_G: 0.7272 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7104 Loss_G: 0.7197 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.7084 Loss_G: 0.7040 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6781 Loss_G: 0.7353 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.6664 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6757 Loss_G: 0.6980 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6645 Loss_G: 0.7011 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.7271 Loss_G: 0.7390 acc: 84.4%\n",
      "[EPOCH 6900] TEST ACC is : 76.8%\n",
      "[BATCH 47/149] Loss_D: 0.6645 Loss_G: 0.7244 acc: 95.3%\n",
      "[BATCH 48/149] Loss_D: 0.6876 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7115 Loss_G: 0.7158 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.6655 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6906 Loss_G: 0.7042 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6779 Loss_G: 0.7029 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6736 Loss_G: 0.6950 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6807 Loss_G: 0.6865 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6696 Loss_G: 0.6918 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6846 Loss_G: 0.7273 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.7004 Loss_G: 0.7550 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7372 Loss_G: 0.7404 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.6843 Loss_G: 0.7213 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7111 Loss_G: 0.7212 acc: 93.8%\n",
      "[BATCH 61/149] Loss_D: 0.6887 Loss_G: 0.7112 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6717 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7531 Loss_G: 0.7568 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7382 Loss_G: 0.7336 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7173 Loss_G: 0.7244 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6821 Loss_G: 0.7070 acc: 78.1%\n",
      "[BATCH 67/149] Loss_D: 0.6816 Loss_G: 0.6979 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6660 Loss_G: 0.7119 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7133 Loss_G: 0.7295 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.6880 Loss_G: 0.7128 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6704 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6968 Loss_G: 0.7074 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6900 Loss_G: 0.7002 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6630 Loss_G: 0.6925 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6626 Loss_G: 0.6867 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6766 Loss_G: 0.6890 acc: 95.3%\n",
      "[BATCH 77/149] Loss_D: 0.6839 Loss_G: 0.7019 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6936 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6798 Loss_G: 0.7056 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6971 Loss_G: 0.6941 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6766 Loss_G: 0.6879 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6681 Loss_G: 0.6977 acc: 81.2%\n",
      "[BATCH 83/149] Loss_D: 0.6896 Loss_G: 0.7031 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6810 Loss_G: 0.7003 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.6505 Loss_G: 0.6707 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7057 Loss_G: 0.6999 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7020 Loss_G: 0.7113 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6785 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6493 Loss_G: 0.6897 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6725 Loss_G: 0.6983 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6941 Loss_G: 0.6998 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6884 Loss_G: 0.7203 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6887 Loss_G: 0.7238 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7137 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6968 Loss_G: 0.7317 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6951 Loss_G: 0.7143 acc: 96.9%\n",
      "[EPOCH 6950] TEST ACC is : 76.4%\n",
      "[BATCH 97/149] Loss_D: 0.7204 Loss_G: 0.7287 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.6830 Loss_G: 0.7325 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7017 Loss_G: 0.7204 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6939 Loss_G: 0.7154 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.7168 Loss_G: 0.7312 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6675 Loss_G: 0.7078 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.6607 Loss_G: 0.6985 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6654 Loss_G: 0.6755 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.6898 Loss_G: 0.7058 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6814 Loss_G: 0.6938 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6752 Loss_G: 0.6930 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7206 Loss_G: 0.6913 acc: 81.2%\n",
      "[BATCH 109/149] Loss_D: 0.6545 Loss_G: 0.6893 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7147 Loss_G: 0.6945 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6695 Loss_G: 0.6847 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.6767 Loss_G: 0.6965 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6788 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6610 Loss_G: 0.7056 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.6416 Loss_G: 0.6777 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7090 Loss_G: 0.6957 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6941 Loss_G: 0.7016 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6860 Loss_G: 0.7037 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6713 Loss_G: 0.6997 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.7101 Loss_G: 0.7278 acc: 96.9%\n",
      "[BATCH 121/149] Loss_D: 0.6884 Loss_G: 0.7088 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6503 Loss_G: 0.6893 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6383 Loss_G: 0.6820 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6511 Loss_G: 0.7049 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.6629 Loss_G: 0.7205 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6963 Loss_G: 0.7405 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6487 Loss_G: 0.7150 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6963 Loss_G: 0.6997 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6676 Loss_G: 0.6955 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6892 Loss_G: 0.6839 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6810 Loss_G: 0.7008 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6788 Loss_G: 0.7192 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7291 Loss_G: 0.7207 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.6697 Loss_G: 0.7090 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6922 Loss_G: 0.7202 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.7226 Loss_G: 0.7219 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7096 Loss_G: 0.7145 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7133 Loss_G: 0.7347 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7098 Loss_G: 0.7209 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6904 Loss_G: 0.7083 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6810 Loss_G: 0.6894 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6850 Loss_G: 0.7036 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6864 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6850 Loss_G: 0.7081 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.6851 Loss_G: 0.6882 acc: 93.8%\n",
      "[BATCH 146/149] Loss_D: 0.6563 Loss_G: 0.7038 acc: 95.3%\n",
      "[EPOCH 7000] TEST ACC is : 76.2%\n",
      "[BATCH 147/149] Loss_D: 0.6807 Loss_G: 0.7131 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.6683 Loss_G: 0.7232 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6680 Loss_G: 0.6803 acc: 82.8%\n",
      "-----THE [47/50] epoch end-----\n",
      "-----THE [48/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6928 Loss_G: 0.6895 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6722 Loss_G: 0.6887 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6841 Loss_G: 0.6863 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6799 Loss_G: 0.7239 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6885 Loss_G: 0.7208 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.7291 Loss_G: 0.7400 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6880 Loss_G: 0.7244 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.6902 Loss_G: 0.7195 acc: 96.9%\n",
      "[BATCH 9/149] Loss_D: 0.7160 Loss_G: 0.7142 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6566 Loss_G: 0.7075 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6536 Loss_G: 0.7012 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6677 Loss_G: 0.6999 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6605 Loss_G: 0.6964 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6546 Loss_G: 0.7199 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.7019 Loss_G: 0.6910 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6920 Loss_G: 0.6974 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.7147 Loss_G: 0.7009 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6706 Loss_G: 0.6914 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7226 Loss_G: 0.7553 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6751 Loss_G: 0.7165 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.7002 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6674 Loss_G: 0.6978 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7155 Loss_G: 0.7133 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6893 Loss_G: 0.7015 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6939 Loss_G: 0.6992 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6594 Loss_G: 0.7060 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.6875 Loss_G: 0.6934 acc: 78.1%\n",
      "[BATCH 28/149] Loss_D: 0.6985 Loss_G: 0.6963 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6500 Loss_G: 0.7075 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.7027 Loss_G: 0.7152 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6440 Loss_G: 0.7161 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.7124 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6839 Loss_G: 0.6991 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6730 Loss_G: 0.6931 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7321 Loss_G: 0.7067 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6902 Loss_G: 0.7181 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6812 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6932 Loss_G: 0.7172 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6723 Loss_G: 0.7164 acc: 96.9%\n",
      "[BATCH 40/149] Loss_D: 0.7072 Loss_G: 0.7153 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6508 Loss_G: 0.7359 acc: 95.3%\n",
      "[BATCH 42/149] Loss_D: 0.6959 Loss_G: 0.7157 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6452 Loss_G: 0.6873 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6899 Loss_G: 0.6846 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.6881 Loss_G: 0.7074 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6910 Loss_G: 0.7147 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7000 Loss_G: 0.7137 acc: 90.6%\n",
      "[EPOCH 7050] TEST ACC is : 77.5%\n",
      "[BATCH 48/149] Loss_D: 0.7094 Loss_G: 0.6981 acc: 78.1%\n",
      "[BATCH 49/149] Loss_D: 0.6679 Loss_G: 0.7087 acc: 93.8%\n",
      "[BATCH 50/149] Loss_D: 0.7201 Loss_G: 0.7246 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6978 Loss_G: 0.7026 acc: 76.6%\n",
      "[BATCH 52/149] Loss_D: 0.6802 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6767 Loss_G: 0.6859 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6671 Loss_G: 0.6871 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.7193 Loss_G: 0.7169 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.6496 Loss_G: 0.6993 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.7075 Loss_G: 0.6998 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6843 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.7096 Loss_G: 0.7162 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6580 Loss_G: 0.6904 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6690 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6446 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7140 Loss_G: 0.7193 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6781 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7006 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7008 Loss_G: 0.7280 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.6811 Loss_G: 0.7242 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.7135 Loss_G: 0.7074 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6859 Loss_G: 0.7245 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6860 Loss_G: 0.6953 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.7060 Loss_G: 0.6943 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.7123 Loss_G: 0.7046 acc: 82.8%\n",
      "[BATCH 73/149] Loss_D: 0.6819 Loss_G: 0.7010 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7152 Loss_G: 0.7318 acc: 95.3%\n",
      "[BATCH 75/149] Loss_D: 0.6745 Loss_G: 0.7049 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7153 Loss_G: 0.7034 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.6402 Loss_G: 0.6930 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6782 Loss_G: 0.6983 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6825 Loss_G: 0.6986 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7044 Loss_G: 0.7286 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.7014 Loss_G: 0.7034 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6845 Loss_G: 0.7029 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7434 Loss_G: 0.7133 acc: 81.2%\n",
      "[BATCH 84/149] Loss_D: 0.6922 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7008 Loss_G: 0.6818 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6628 Loss_G: 0.6812 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6325 Loss_G: 0.6734 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6751 Loss_G: 0.6897 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6508 Loss_G: 0.6824 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6605 Loss_G: 0.7063 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6476 Loss_G: 0.6917 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6973 Loss_G: 0.6819 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7096 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6772 Loss_G: 0.7123 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6947 Loss_G: 0.7198 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.6835 Loss_G: 0.7339 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.7056 Loss_G: 0.7415 acc: 93.8%\n",
      "[EPOCH 7100] TEST ACC is : 76.6%\n",
      "[BATCH 98/149] Loss_D: 0.6727 Loss_G: 0.7125 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6864 Loss_G: 0.7075 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.6677 Loss_G: 0.6908 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6945 Loss_G: 0.7042 acc: 81.2%\n",
      "[BATCH 102/149] Loss_D: 0.6598 Loss_G: 0.6970 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7292 Loss_G: 0.7197 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.7174 Loss_G: 0.7372 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7111 Loss_G: 0.7395 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.6512 Loss_G: 0.7043 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6910 Loss_G: 0.7082 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7014 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.7170 Loss_G: 0.7413 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6952 Loss_G: 0.7437 acc: 96.9%\n",
      "[BATCH 111/149] Loss_D: 0.6819 Loss_G: 0.7231 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7062 Loss_G: 0.7277 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7203 Loss_G: 0.7113 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6869 Loss_G: 0.6975 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6813 Loss_G: 0.6888 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6799 Loss_G: 0.6857 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6838 Loss_G: 0.7036 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7012 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6955 Loss_G: 0.7179 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.7065 Loss_G: 0.7160 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7111 Loss_G: 0.7305 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6752 Loss_G: 0.7161 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.7208 Loss_G: 0.7188 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.6607 Loss_G: 0.7280 acc: 95.3%\n",
      "[BATCH 125/149] Loss_D: 0.6710 Loss_G: 0.7154 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.7315 Loss_G: 0.7168 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6718 Loss_G: 0.7089 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6723 Loss_G: 0.6899 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6837 Loss_G: 0.6984 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.7161 Loss_G: 0.7147 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6843 Loss_G: 0.7180 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.6844 Loss_G: 0.7267 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.7115 Loss_G: 0.7629 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.6608 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7063 Loss_G: 0.7154 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6828 Loss_G: 0.7251 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.6685 Loss_G: 0.7270 acc: 95.3%\n",
      "[BATCH 138/149] Loss_D: 0.6711 Loss_G: 0.6979 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6707 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7070 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7100 Loss_G: 0.7548 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6921 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6686 Loss_G: 0.7087 acc: 95.3%\n",
      "[BATCH 144/149] Loss_D: 0.6928 Loss_G: 0.7056 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.6909 Loss_G: 0.6959 acc: 79.7%\n",
      "[BATCH 146/149] Loss_D: 0.6997 Loss_G: 0.7041 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.6632 Loss_G: 0.7038 acc: 87.5%\n",
      "[EPOCH 7150] TEST ACC is : 76.8%\n",
      "[BATCH 148/149] Loss_D: 0.7326 Loss_G: 0.7148 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6735 Loss_G: 0.7020 acc: 87.5%\n",
      "-----THE [48/50] epoch end-----\n",
      "-----THE [49/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6647 Loss_G: 0.7053 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6688 Loss_G: 0.6874 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7047 Loss_G: 0.7092 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6617 Loss_G: 0.7017 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6564 Loss_G: 0.7007 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6611 Loss_G: 0.7041 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7711 Loss_G: 0.7358 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7084 Loss_G: 0.7324 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.7076 Loss_G: 0.7211 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.6555 Loss_G: 0.6825 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6853 Loss_G: 0.7199 acc: 96.9%\n",
      "[BATCH 12/149] Loss_D: 0.6791 Loss_G: 0.7113 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7014 Loss_G: 0.7070 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.7179 Loss_G: 0.7164 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.6937 Loss_G: 0.7095 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.6648 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6624 Loss_G: 0.7057 acc: 82.8%\n",
      "[BATCH 18/149] Loss_D: 0.6828 Loss_G: 0.7199 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7068 Loss_G: 0.7327 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.6943 Loss_G: 0.7152 acc: 81.2%\n",
      "[BATCH 21/149] Loss_D: 0.7172 Loss_G: 0.7307 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6940 Loss_G: 0.7229 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6769 Loss_G: 0.7101 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6944 Loss_G: 0.7058 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6868 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6432 Loss_G: 0.6850 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.7077 Loss_G: 0.6920 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6863 Loss_G: 0.6977 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.7216 Loss_G: 0.7246 acc: 81.2%\n",
      "[BATCH 30/149] Loss_D: 0.6566 Loss_G: 0.6853 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6584 Loss_G: 0.6939 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6690 Loss_G: 0.6813 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.7086 Loss_G: 0.6952 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6556 Loss_G: 0.7022 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.6924 Loss_G: 0.7148 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6816 Loss_G: 0.7135 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6676 Loss_G: 0.7203 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.6841 Loss_G: 0.7103 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.6597 Loss_G: 0.6953 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6778 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6887 Loss_G: 0.7181 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.7023 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6776 Loss_G: 0.7220 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6701 Loss_G: 0.7164 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.7094 Loss_G: 0.7285 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7017 Loss_G: 0.7281 acc: 96.9%\n",
      "[BATCH 47/149] Loss_D: 0.6232 Loss_G: 0.7362 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7167 Loss_G: 0.7164 acc: 84.4%\n",
      "[EPOCH 7200] TEST ACC is : 75.8%\n",
      "[BATCH 49/149] Loss_D: 0.6839 Loss_G: 0.7163 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7093 Loss_G: 0.7199 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6584 Loss_G: 0.7066 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6895 Loss_G: 0.6981 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7019 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6814 Loss_G: 0.6977 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6863 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6960 Loss_G: 0.7070 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6796 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6863 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6808 Loss_G: 0.7049 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6866 Loss_G: 0.7007 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7048 Loss_G: 0.7315 acc: 95.3%\n",
      "[BATCH 62/149] Loss_D: 0.6884 Loss_G: 0.7051 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7357 Loss_G: 0.6971 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.6597 Loss_G: 0.6980 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6840 Loss_G: 0.7063 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6996 Loss_G: 0.6898 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.6701 Loss_G: 0.6871 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7177 Loss_G: 0.6999 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.6959 Loss_G: 0.7337 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.7365 Loss_G: 0.7249 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6752 Loss_G: 0.7233 acc: 95.3%\n",
      "[BATCH 72/149] Loss_D: 0.6968 Loss_G: 0.7124 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6581 Loss_G: 0.7000 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7088 Loss_G: 0.6989 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6868 Loss_G: 0.6936 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6671 Loss_G: 0.6723 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.6994 Loss_G: 0.6941 acc: 98.4%\n",
      "[BATCH 78/149] Loss_D: 0.6570 Loss_G: 0.7029 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6568 Loss_G: 0.7053 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.7054 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7216 Loss_G: 0.7270 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.6885 Loss_G: 0.7205 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6752 Loss_G: 0.7259 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6702 Loss_G: 0.7067 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6931 Loss_G: 0.7053 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.6863 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6715 Loss_G: 0.7139 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6998 Loss_G: 0.7039 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6647 Loss_G: 0.7020 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6418 Loss_G: 0.6948 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6598 Loss_G: 0.7049 acc: 93.8%\n",
      "[BATCH 92/149] Loss_D: 0.6950 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6716 Loss_G: 0.7165 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6909 Loss_G: 0.7190 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6770 Loss_G: 0.7101 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6966 Loss_G: 0.6976 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6944 Loss_G: 0.6931 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6699 Loss_G: 0.6938 acc: 89.1%\n",
      "[EPOCH 7250] TEST ACC is : 76.8%\n",
      "[BATCH 99/149] Loss_D: 0.6563 Loss_G: 0.6859 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.6854 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6816 Loss_G: 0.6912 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6746 Loss_G: 0.6879 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6618 Loss_G: 0.7060 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6555 Loss_G: 0.7120 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.6811 Loss_G: 0.7086 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.6678 Loss_G: 0.7128 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6934 Loss_G: 0.7018 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.6951 Loss_G: 0.7429 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.7038 Loss_G: 0.7253 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.7046 Loss_G: 0.7200 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.7268 Loss_G: 0.7137 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.6659 Loss_G: 0.7065 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6784 Loss_G: 0.7069 acc: 96.9%\n",
      "[BATCH 114/149] Loss_D: 0.6939 Loss_G: 0.7158 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6935 Loss_G: 0.7312 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7121 Loss_G: 0.7362 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7049 Loss_G: 0.7097 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6823 Loss_G: 0.7082 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6933 Loss_G: 0.7185 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.6862 Loss_G: 0.7010 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6638 Loss_G: 0.7035 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.7081 Loss_G: 0.7178 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7200 Loss_G: 0.7187 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6509 Loss_G: 0.6825 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6991 Loss_G: 0.7069 acc: 81.2%\n",
      "[BATCH 126/149] Loss_D: 0.6923 Loss_G: 0.7269 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.7162 Loss_G: 0.7269 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6693 Loss_G: 0.7260 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6823 Loss_G: 0.7140 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.7058 Loss_G: 0.7250 acc: 95.3%\n",
      "[BATCH 131/149] Loss_D: 0.6634 Loss_G: 0.7043 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6649 Loss_G: 0.6735 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.6771 Loss_G: 0.7000 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7120 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6797 Loss_G: 0.7050 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7036 Loss_G: 0.7105 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6655 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.7239 Loss_G: 0.7262 acc: 81.2%\n",
      "[BATCH 139/149] Loss_D: 0.7012 Loss_G: 0.7190 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.6784 Loss_G: 0.7090 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.7005 Loss_G: 0.7272 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6653 Loss_G: 0.7122 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.7139 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6447 Loss_G: 0.6947 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.7144 Loss_G: 0.7069 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7373 Loss_G: 0.7143 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.6693 Loss_G: 0.6998 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.7336 Loss_G: 0.7033 acc: 92.2%\n",
      "[EPOCH 7300] TEST ACC is : 76.8%\n",
      "[BATCH 149/149] Loss_D: 0.6811 Loss_G: 0.7146 acc: 90.6%\n",
      "-----THE [49/50] epoch end-----\n",
      "-----THE [50/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6547 Loss_G: 0.7015 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6667 Loss_G: 0.6920 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7171 Loss_G: 0.7050 acc: 78.1%\n",
      "[BATCH 4/149] Loss_D: 0.6585 Loss_G: 0.7027 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6748 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6591 Loss_G: 0.6956 acc: 95.3%\n",
      "[BATCH 7/149] Loss_D: 0.6670 Loss_G: 0.6848 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6942 Loss_G: 0.6929 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6836 Loss_G: 0.6957 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.6896 Loss_G: 0.7154 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7438 Loss_G: 0.7343 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6802 Loss_G: 0.7062 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6722 Loss_G: 0.6942 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.7211 Loss_G: 0.7172 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.6793 Loss_G: 0.7010 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6993 Loss_G: 0.7070 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6884 Loss_G: 0.7202 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6712 Loss_G: 0.6960 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6881 Loss_G: 0.6799 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6914 Loss_G: 0.6847 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6497 Loss_G: 0.6896 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6952 Loss_G: 0.6961 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6897 Loss_G: 0.6918 acc: 93.8%\n",
      "[BATCH 24/149] Loss_D: 0.6741 Loss_G: 0.6808 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6989 Loss_G: 0.6934 acc: 95.3%\n",
      "[BATCH 26/149] Loss_D: 0.6816 Loss_G: 0.7149 acc: 95.3%\n",
      "[BATCH 27/149] Loss_D: 0.6834 Loss_G: 0.7146 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7148 Loss_G: 0.7097 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6963 Loss_G: 0.7039 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7105 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6813 Loss_G: 0.7149 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.7197 Loss_G: 0.7395 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6855 Loss_G: 0.7149 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6870 Loss_G: 0.7028 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7037 Loss_G: 0.7171 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6899 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6951 Loss_G: 0.7092 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6681 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7014 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6699 Loss_G: 0.7103 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.6834 Loss_G: 0.7126 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6629 Loss_G: 0.6840 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7124 Loss_G: 0.7112 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.7027 Loss_G: 0.7250 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7019 Loss_G: 0.7237 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.6795 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6749 Loss_G: 0.7133 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6599 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7070 Loss_G: 0.7078 acc: 85.9%\n",
      "[EPOCH 7350] TEST ACC is : 77.1%\n",
      "[BATCH 50/149] Loss_D: 0.6757 Loss_G: 0.7196 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6829 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.7065 Loss_G: 0.7283 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.7195 Loss_G: 0.7317 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6919 Loss_G: 0.7190 acc: 81.2%\n",
      "[BATCH 55/149] Loss_D: 0.6648 Loss_G: 0.6906 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6836 Loss_G: 0.7068 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6761 Loss_G: 0.7117 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6952 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6945 Loss_G: 0.7040 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6692 Loss_G: 0.6977 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.7104 Loss_G: 0.7086 acc: 93.8%\n",
      "[BATCH 62/149] Loss_D: 0.6633 Loss_G: 0.7031 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6779 Loss_G: 0.7125 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.7102 Loss_G: 0.6983 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.6703 Loss_G: 0.7139 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.6928 Loss_G: 0.7122 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.7019 Loss_G: 0.7000 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.7012 Loss_G: 0.7236 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.7167 Loss_G: 0.7042 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6876 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6629 Loss_G: 0.7221 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6778 Loss_G: 0.7143 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7066 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.7113 Loss_G: 0.7294 acc: 95.3%\n",
      "[BATCH 75/149] Loss_D: 0.6909 Loss_G: 0.7402 acc: 96.9%\n",
      "[BATCH 76/149] Loss_D: 0.7217 Loss_G: 0.7076 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.6590 Loss_G: 0.7317 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.6860 Loss_G: 0.6931 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.7338 Loss_G: 0.7155 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6946 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.7016 Loss_G: 0.7158 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6928 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6841 Loss_G: 0.7091 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.7030 Loss_G: 0.7060 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.6484 Loss_G: 0.7006 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7086 Loss_G: 0.7260 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.6865 Loss_G: 0.6963 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.7247 Loss_G: 0.6965 acc: 79.7%\n",
      "[BATCH 89/149] Loss_D: 0.7651 Loss_G: 0.7200 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.6983 Loss_G: 0.7293 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6817 Loss_G: 0.7055 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.6912 Loss_G: 0.7118 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7331 Loss_G: 0.7442 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7159 Loss_G: 0.7441 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.7052 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6729 Loss_G: 0.6965 acc: 93.8%\n",
      "[BATCH 97/149] Loss_D: 0.6774 Loss_G: 0.7210 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6817 Loss_G: 0.7210 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6957 Loss_G: 0.7221 acc: 85.9%\n",
      "[EPOCH 7400] TEST ACC is : 76.2%\n",
      "[BATCH 100/149] Loss_D: 0.6643 Loss_G: 0.7138 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7059 Loss_G: 0.7003 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6535 Loss_G: 0.6847 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6821 Loss_G: 0.6875 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7034 Loss_G: 0.7156 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.7240 Loss_G: 0.7378 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6765 Loss_G: 0.7129 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6570 Loss_G: 0.6989 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.6831 Loss_G: 0.6843 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7017 Loss_G: 0.7025 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6638 Loss_G: 0.7060 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6613 Loss_G: 0.7295 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.6593 Loss_G: 0.6862 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6817 Loss_G: 0.6884 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7230 Loss_G: 0.7120 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.6914 Loss_G: 0.7104 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6674 Loss_G: 0.6992 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.6825 Loss_G: 0.6961 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6816 Loss_G: 0.7044 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7142 Loss_G: 0.6939 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.7336 Loss_G: 0.7407 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6646 Loss_G: 0.7051 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6894 Loss_G: 0.7064 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6861 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6647 Loss_G: 0.7170 acc: 96.9%\n",
      "[BATCH 125/149] Loss_D: 0.6941 Loss_G: 0.6780 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6809 Loss_G: 0.6841 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.6900 Loss_G: 0.6902 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6645 Loss_G: 0.6940 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6764 Loss_G: 0.7080 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6907 Loss_G: 0.6994 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6615 Loss_G: 0.6991 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6800 Loss_G: 0.7039 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6697 Loss_G: 0.7108 acc: 93.8%\n",
      "[BATCH 134/149] Loss_D: 0.7241 Loss_G: 0.7631 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7013 Loss_G: 0.7278 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.6530 Loss_G: 0.7031 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7004 Loss_G: 0.7106 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6851 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6517 Loss_G: 0.6942 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.6236 Loss_G: 0.6855 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6852 Loss_G: 0.6919 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6762 Loss_G: 0.7014 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.6936 Loss_G: 0.7269 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6796 Loss_G: 0.7141 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7149 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6641 Loss_G: 0.7255 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6822 Loss_G: 0.7025 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.7073 Loss_G: 0.7227 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7019 Loss_G: 0.7223 acc: 84.4%\n",
      "[EPOCH 7450] TEST ACC is : 77.5%\n",
      "-----THE [50/50] epoch end-----\n",
      "The 6 * 50 epochs train starts:\n",
      "-----THE [1/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6543 Loss_G: 0.6984 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6809 Loss_G: 0.6901 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.6708 Loss_G: 0.7006 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.6675 Loss_G: 0.7000 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.7154 Loss_G: 0.6978 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.7247 Loss_G: 0.7178 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.7170 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6575 Loss_G: 0.6988 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6877 Loss_G: 0.7054 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6652 Loss_G: 0.6979 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6815 Loss_G: 0.6984 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.6520 Loss_G: 0.6945 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.6821 Loss_G: 0.7260 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6752 Loss_G: 0.7263 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.7221 Loss_G: 0.7192 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6637 Loss_G: 0.6989 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.7033 Loss_G: 0.7284 acc: 93.8%\n",
      "[BATCH 18/149] Loss_D: 0.7048 Loss_G: 0.7200 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.7187 Loss_G: 0.7354 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6803 Loss_G: 0.7139 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.7020 Loss_G: 0.7072 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6930 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7433 Loss_G: 0.7151 acc: 79.7%\n",
      "[BATCH 24/149] Loss_D: 0.6364 Loss_G: 0.6876 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6689 Loss_G: 0.7045 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6507 Loss_G: 0.6886 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6771 Loss_G: 0.6943 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6934 Loss_G: 0.6938 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.6829 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6889 Loss_G: 0.7237 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7018 Loss_G: 0.7407 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6689 Loss_G: 0.7146 acc: 93.8%\n",
      "[BATCH 33/149] Loss_D: 0.6907 Loss_G: 0.6989 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7096 Loss_G: 0.7178 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.6443 Loss_G: 0.6979 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6970 Loss_G: 0.7151 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6801 Loss_G: 0.7164 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.7478 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7102 Loss_G: 0.7190 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.7021 Loss_G: 0.7140 acc: 81.2%\n",
      "[BATCH 41/149] Loss_D: 0.7188 Loss_G: 0.7217 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.6929 Loss_G: 0.7116 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6805 Loss_G: 0.7235 acc: 98.4%\n",
      "[BATCH 44/149] Loss_D: 0.6566 Loss_G: 0.7158 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.6445 Loss_G: 0.6871 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.7142 Loss_G: 0.7004 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6601 Loss_G: 0.6766 acc: 78.1%\n",
      "[BATCH 48/149] Loss_D: 0.6979 Loss_G: 0.6837 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6561 Loss_G: 0.6824 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7383 Loss_G: 0.7034 acc: 84.4%\n",
      "[EPOCH 50] TEST ACC is : 76.6%\n",
      "[BATCH 51/149] Loss_D: 0.6596 Loss_G: 0.6833 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6798 Loss_G: 0.6958 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6983 Loss_G: 0.7207 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6866 Loss_G: 0.7192 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6888 Loss_G: 0.7121 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.6737 Loss_G: 0.6876 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6959 Loss_G: 0.6895 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.7297 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6754 Loss_G: 0.7439 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6755 Loss_G: 0.7405 acc: 96.9%\n",
      "[BATCH 61/149] Loss_D: 0.6831 Loss_G: 0.7076 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6827 Loss_G: 0.6998 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6911 Loss_G: 0.6945 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.7047 Loss_G: 0.7147 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6622 Loss_G: 0.7167 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6977 Loss_G: 0.7406 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7117 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6911 Loss_G: 0.7099 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7578 Loss_G: 0.7334 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6675 Loss_G: 0.7108 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6791 Loss_G: 0.7118 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6781 Loss_G: 0.7012 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7043 Loss_G: 0.7401 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6919 Loss_G: 0.7376 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6756 Loss_G: 0.7327 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.7076 Loss_G: 0.7409 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6908 Loss_G: 0.7523 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6664 Loss_G: 0.7044 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6602 Loss_G: 0.7020 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6462 Loss_G: 0.6978 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.6603 Loss_G: 0.6802 acc: 79.7%\n",
      "[BATCH 82/149] Loss_D: 0.6629 Loss_G: 0.6896 acc: 93.8%\n",
      "[BATCH 83/149] Loss_D: 0.6817 Loss_G: 0.6965 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6703 Loss_G: 0.6868 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6699 Loss_G: 0.6840 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.7270 Loss_G: 0.7241 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6774 Loss_G: 0.7121 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7130 Loss_G: 0.7224 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6729 Loss_G: 0.7043 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.7024 Loss_G: 0.6930 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6943 Loss_G: 0.7340 acc: 95.3%\n",
      "[BATCH 92/149] Loss_D: 0.6672 Loss_G: 0.6922 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.6727 Loss_G: 0.6751 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6780 Loss_G: 0.7024 acc: 93.8%\n",
      "[BATCH 95/149] Loss_D: 0.7017 Loss_G: 0.6991 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.6898 Loss_G: 0.6924 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6831 Loss_G: 0.7245 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.7034 Loss_G: 0.7297 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6863 Loss_G: 0.7307 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.7142 Loss_G: 0.7264 acc: 95.3%\n",
      "[EPOCH 100] TEST ACC is : 77.0%\n",
      "[BATCH 101/149] Loss_D: 0.6733 Loss_G: 0.7015 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.6948 Loss_G: 0.6942 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6803 Loss_G: 0.7097 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6735 Loss_G: 0.7018 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.6664 Loss_G: 0.6907 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.6623 Loss_G: 0.7016 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6765 Loss_G: 0.6864 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6984 Loss_G: 0.7057 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.6932 Loss_G: 0.7059 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.7191 Loss_G: 0.7133 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6525 Loss_G: 0.7038 acc: 96.9%\n",
      "[BATCH 112/149] Loss_D: 0.6758 Loss_G: 0.6813 acc: 82.8%\n",
      "[BATCH 113/149] Loss_D: 0.7060 Loss_G: 0.7002 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7328 Loss_G: 0.7613 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6923 Loss_G: 0.7376 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6699 Loss_G: 0.7207 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.7228 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6623 Loss_G: 0.6827 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6753 Loss_G: 0.7003 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6760 Loss_G: 0.6906 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6711 Loss_G: 0.7006 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6605 Loss_G: 0.7373 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7229 Loss_G: 0.7446 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.7023 Loss_G: 0.7432 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6880 Loss_G: 0.7182 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6991 Loss_G: 0.7165 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7117 Loss_G: 0.7212 acc: 81.2%\n",
      "[BATCH 128/149] Loss_D: 0.7074 Loss_G: 0.7431 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6855 Loss_G: 0.7260 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6951 Loss_G: 0.7146 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6662 Loss_G: 0.7098 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.7123 Loss_G: 0.7273 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6592 Loss_G: 0.7033 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7215 Loss_G: 0.7072 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7084 Loss_G: 0.7262 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6692 Loss_G: 0.7341 acc: 96.9%\n",
      "[BATCH 137/149] Loss_D: 0.6884 Loss_G: 0.7423 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6836 Loss_G: 0.7017 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6552 Loss_G: 0.6917 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6686 Loss_G: 0.6998 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6855 Loss_G: 0.7179 acc: 93.8%\n",
      "[BATCH 142/149] Loss_D: 0.6789 Loss_G: 0.6988 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6890 Loss_G: 0.7066 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.6657 Loss_G: 0.7020 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7008 Loss_G: 0.7158 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6736 Loss_G: 0.6944 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7052 Loss_G: 0.7110 acc: 82.8%\n",
      "[BATCH 148/149] Loss_D: 0.6898 Loss_G: 0.7281 acc: 96.9%\n",
      "[BATCH 149/149] Loss_D: 0.6765 Loss_G: 0.7210 acc: 90.6%\n",
      "-----THE [1/50] epoch end-----\n",
      "-----THE [2/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6892 Loss_G: 0.7040 acc: 87.5%\n",
      "[EPOCH 150] TEST ACC is : 77.0%\n",
      "[BATCH 2/149] Loss_D: 0.6963 Loss_G: 0.7120 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6755 Loss_G: 0.7149 acc: 81.2%\n",
      "[BATCH 4/149] Loss_D: 0.6676 Loss_G: 0.6944 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.6983 Loss_G: 0.7095 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6756 Loss_G: 0.6977 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7010 Loss_G: 0.7083 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6487 Loss_G: 0.6899 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6762 Loss_G: 0.7039 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6627 Loss_G: 0.6983 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6753 Loss_G: 0.6986 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6541 Loss_G: 0.6769 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6764 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6735 Loss_G: 0.7336 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6964 Loss_G: 0.7382 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6934 Loss_G: 0.7107 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.6980 Loss_G: 0.7241 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7215 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6778 Loss_G: 0.7012 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6778 Loss_G: 0.7176 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6794 Loss_G: 0.7072 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6595 Loss_G: 0.6887 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6882 Loss_G: 0.6771 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6573 Loss_G: 0.6760 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.7202 Loss_G: 0.6926 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6901 Loss_G: 0.7019 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6600 Loss_G: 0.6924 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7082 Loss_G: 0.7022 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.7033 Loss_G: 0.7132 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6838 Loss_G: 0.7234 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6772 Loss_G: 0.6891 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6821 Loss_G: 0.6977 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7266 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6751 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6782 Loss_G: 0.6966 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6659 Loss_G: 0.7126 acc: 95.3%\n",
      "[BATCH 37/149] Loss_D: 0.7578 Loss_G: 0.7424 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6890 Loss_G: 0.7286 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.7169 Loss_G: 0.6973 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6967 Loss_G: 0.7277 acc: 93.8%\n",
      "[BATCH 41/149] Loss_D: 0.7202 Loss_G: 0.7601 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.6803 Loss_G: 0.7200 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6894 Loss_G: 0.7016 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7130 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6904 Loss_G: 0.7370 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6929 Loss_G: 0.7044 acc: 78.1%\n",
      "[BATCH 47/149] Loss_D: 0.7089 Loss_G: 0.7154 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6855 Loss_G: 0.6977 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6539 Loss_G: 0.7127 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6922 Loss_G: 0.7075 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.7299 Loss_G: 0.7341 acc: 87.5%\n",
      "[EPOCH 200] TEST ACC is : 77.0%\n",
      "[BATCH 52/149] Loss_D: 0.7283 Loss_G: 0.7340 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.7021 Loss_G: 0.7193 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6866 Loss_G: 0.7098 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6854 Loss_G: 0.7077 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.6798 Loss_G: 0.7178 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6425 Loss_G: 0.7149 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7071 Loss_G: 0.7129 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7049 Loss_G: 0.7114 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6902 Loss_G: 0.7024 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6620 Loss_G: 0.7047 acc: 95.3%\n",
      "[BATCH 62/149] Loss_D: 0.6779 Loss_G: 0.7211 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6994 Loss_G: 0.7200 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7114 Loss_G: 0.7286 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7350 Loss_G: 0.7313 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6868 Loss_G: 0.7126 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6380 Loss_G: 0.6852 acc: 93.8%\n",
      "[BATCH 68/149] Loss_D: 0.6588 Loss_G: 0.6795 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.6810 Loss_G: 0.6908 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6768 Loss_G: 0.6986 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7075 Loss_G: 0.7126 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.6770 Loss_G: 0.7236 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.6821 Loss_G: 0.7113 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6706 Loss_G: 0.7253 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6695 Loss_G: 0.7042 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.6805 Loss_G: 0.7321 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.6691 Loss_G: 0.7456 acc: 96.9%\n",
      "[BATCH 78/149] Loss_D: 0.7036 Loss_G: 0.7311 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7092 Loss_G: 0.7310 acc: 96.9%\n",
      "[BATCH 80/149] Loss_D: 0.6979 Loss_G: 0.6953 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.7001 Loss_G: 0.6988 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7210 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6846 Loss_G: 0.7266 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6437 Loss_G: 0.6799 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.6936 Loss_G: 0.6941 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6960 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6917 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7032 Loss_G: 0.7249 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6874 Loss_G: 0.7004 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6443 Loss_G: 0.6784 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6590 Loss_G: 0.6897 acc: 93.8%\n",
      "[BATCH 92/149] Loss_D: 0.7095 Loss_G: 0.6857 acc: 76.6%\n",
      "[BATCH 93/149] Loss_D: 0.6687 Loss_G: 0.6818 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.7066 Loss_G: 0.6903 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7031 Loss_G: 0.6971 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6483 Loss_G: 0.6901 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6577 Loss_G: 0.7034 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6704 Loss_G: 0.7046 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6887 Loss_G: 0.7289 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.6763 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6817 Loss_G: 0.6983 acc: 90.6%\n",
      "[EPOCH 250] TEST ACC is : 76.4%\n",
      "[BATCH 102/149] Loss_D: 0.6902 Loss_G: 0.6873 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6953 Loss_G: 0.6907 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6669 Loss_G: 0.6914 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.6795 Loss_G: 0.6851 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7148 Loss_G: 0.7246 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6626 Loss_G: 0.7044 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6725 Loss_G: 0.6735 acc: 84.4%\n",
      "[BATCH 109/149] Loss_D: 0.6814 Loss_G: 0.6869 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6829 Loss_G: 0.7098 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.6734 Loss_G: 0.7036 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6913 Loss_G: 0.6966 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7059 Loss_G: 0.7089 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7125 Loss_G: 0.7046 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6545 Loss_G: 0.6864 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7108 Loss_G: 0.7158 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6892 Loss_G: 0.7101 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6930 Loss_G: 0.7018 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.6691 Loss_G: 0.6886 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6391 Loss_G: 0.6810 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.7145 Loss_G: 0.7199 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.6868 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7254 Loss_G: 0.7124 acc: 95.3%\n",
      "[BATCH 124/149] Loss_D: 0.7196 Loss_G: 0.7131 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6909 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6991 Loss_G: 0.7197 acc: 79.7%\n",
      "[BATCH 127/149] Loss_D: 0.6765 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7030 Loss_G: 0.7223 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6714 Loss_G: 0.7148 acc: 93.8%\n",
      "[BATCH 130/149] Loss_D: 0.6761 Loss_G: 0.7188 acc: 95.3%\n",
      "[BATCH 131/149] Loss_D: 0.6862 Loss_G: 0.6931 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6769 Loss_G: 0.7046 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6988 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6679 Loss_G: 0.7137 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.7051 Loss_G: 0.7208 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6897 Loss_G: 0.7142 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6950 Loss_G: 0.6960 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6845 Loss_G: 0.6917 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7005 Loss_G: 0.6986 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6745 Loss_G: 0.6986 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.7097 Loss_G: 0.6984 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.6943 Loss_G: 0.7302 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6768 Loss_G: 0.7204 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.6648 Loss_G: 0.7029 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6984 Loss_G: 0.7146 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6962 Loss_G: 0.7013 acc: 79.7%\n",
      "[BATCH 147/149] Loss_D: 0.6678 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.7008 Loss_G: 0.7216 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6770 Loss_G: 0.7178 acc: 90.6%\n",
      "-----THE [2/50] epoch end-----\n",
      "-----THE [3/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6772 Loss_G: 0.7161 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6978 Loss_G: 0.7095 acc: 84.4%\n",
      "[EPOCH 300] TEST ACC is : 76.4%\n",
      "[BATCH 3/149] Loss_D: 0.6725 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6921 Loss_G: 0.7115 acc: 81.2%\n",
      "[BATCH 5/149] Loss_D: 0.7125 Loss_G: 0.7229 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6628 Loss_G: 0.6958 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6840 Loss_G: 0.6850 acc: 100.0%\n",
      "[BATCH 8/149] Loss_D: 0.6699 Loss_G: 0.6908 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6555 Loss_G: 0.7054 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.6603 Loss_G: 0.6924 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6878 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6712 Loss_G: 0.6989 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.6696 Loss_G: 0.6887 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6777 Loss_G: 0.6941 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6533 Loss_G: 0.7051 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.7446 Loss_G: 0.7208 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7065 Loss_G: 0.7563 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6682 Loss_G: 0.7130 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6698 Loss_G: 0.7034 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6574 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6674 Loss_G: 0.6888 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6521 Loss_G: 0.6942 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.7030 Loss_G: 0.7003 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6901 Loss_G: 0.7161 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7073 Loss_G: 0.7095 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6618 Loss_G: 0.6969 acc: 82.8%\n",
      "[BATCH 27/149] Loss_D: 0.7020 Loss_G: 0.7396 acc: 93.8%\n",
      "[BATCH 28/149] Loss_D: 0.6910 Loss_G: 0.7646 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.7019 Loss_G: 0.7573 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.6917 Loss_G: 0.7062 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6966 Loss_G: 0.7255 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.6706 Loss_G: 0.7259 acc: 93.8%\n",
      "[BATCH 33/149] Loss_D: 0.7262 Loss_G: 0.7191 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6735 Loss_G: 0.7091 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6816 Loss_G: 0.7045 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6719 Loss_G: 0.6917 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6704 Loss_G: 0.7030 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6429 Loss_G: 0.7033 acc: 96.9%\n",
      "[BATCH 39/149] Loss_D: 0.6914 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6919 Loss_G: 0.7239 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6956 Loss_G: 0.7120 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6902 Loss_G: 0.7040 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6707 Loss_G: 0.6899 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6741 Loss_G: 0.6902 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6818 Loss_G: 0.7031 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6862 Loss_G: 0.7055 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6856 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6870 Loss_G: 0.7127 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7084 Loss_G: 0.7255 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6785 Loss_G: 0.7069 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7160 Loss_G: 0.7433 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6808 Loss_G: 0.7395 acc: 95.3%\n",
      "[EPOCH 350] TEST ACC is : 76.0%\n",
      "[BATCH 53/149] Loss_D: 0.6935 Loss_G: 0.7365 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.7151 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6820 Loss_G: 0.7183 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7349 Loss_G: 0.7068 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6908 Loss_G: 0.7079 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6983 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6669 Loss_G: 0.7088 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6836 Loss_G: 0.7139 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6841 Loss_G: 0.7016 acc: 78.1%\n",
      "[BATCH 62/149] Loss_D: 0.7009 Loss_G: 0.6974 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.6555 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7075 Loss_G: 0.7172 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7054 Loss_G: 0.7318 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6769 Loss_G: 0.7093 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6735 Loss_G: 0.7008 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6756 Loss_G: 0.6958 acc: 82.8%\n",
      "[BATCH 69/149] Loss_D: 0.7332 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6952 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6818 Loss_G: 0.7361 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6857 Loss_G: 0.7005 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7128 Loss_G: 0.7135 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6863 Loss_G: 0.7248 acc: 95.3%\n",
      "[BATCH 75/149] Loss_D: 0.6746 Loss_G: 0.7076 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6661 Loss_G: 0.6950 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6654 Loss_G: 0.7111 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.6598 Loss_G: 0.7146 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7146 Loss_G: 0.7090 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6943 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7001 Loss_G: 0.7289 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6757 Loss_G: 0.7420 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6636 Loss_G: 0.6969 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6765 Loss_G: 0.6954 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6789 Loss_G: 0.6908 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6899 Loss_G: 0.7306 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6505 Loss_G: 0.7077 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7407 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7063 Loss_G: 0.7252 acc: 95.3%\n",
      "[BATCH 90/149] Loss_D: 0.7254 Loss_G: 0.7542 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6688 Loss_G: 0.6972 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6838 Loss_G: 0.6911 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6687 Loss_G: 0.6947 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6676 Loss_G: 0.6868 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6846 Loss_G: 0.6962 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6957 Loss_G: 0.7308 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.6870 Loss_G: 0.7241 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6627 Loss_G: 0.7239 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6739 Loss_G: 0.6991 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.7029 Loss_G: 0.6857 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6689 Loss_G: 0.6756 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.7388 Loss_G: 0.7055 acc: 84.4%\n",
      "[EPOCH 400] TEST ACC is : 76.4%\n",
      "[BATCH 103/149] Loss_D: 0.6675 Loss_G: 0.7054 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.7118 Loss_G: 0.7144 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6767 Loss_G: 0.7177 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.6939 Loss_G: 0.7116 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.7009 Loss_G: 0.7000 acc: 81.2%\n",
      "[BATCH 108/149] Loss_D: 0.6716 Loss_G: 0.7043 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6809 Loss_G: 0.6939 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.6930 Loss_G: 0.7002 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6694 Loss_G: 0.6912 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.7105 Loss_G: 0.7099 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6739 Loss_G: 0.7053 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6611 Loss_G: 0.7042 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6572 Loss_G: 0.6817 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6709 Loss_G: 0.7052 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6754 Loss_G: 0.6948 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.7277 Loss_G: 0.7357 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.7179 Loss_G: 0.7203 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.7023 Loss_G: 0.7356 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6914 Loss_G: 0.7281 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.7077 Loss_G: 0.7030 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.7066 Loss_G: 0.6931 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6613 Loss_G: 0.6948 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6532 Loss_G: 0.6863 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6998 Loss_G: 0.7201 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7137 Loss_G: 0.7242 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.6854 Loss_G: 0.7014 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7066 Loss_G: 0.6927 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6755 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7110 Loss_G: 0.6994 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.7102 Loss_G: 0.7261 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.7055 Loss_G: 0.7139 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7234 Loss_G: 0.7276 acc: 79.7%\n",
      "[BATCH 135/149] Loss_D: 0.6856 Loss_G: 0.7030 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6722 Loss_G: 0.7029 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.7002 Loss_G: 0.6934 acc: 78.1%\n",
      "[BATCH 138/149] Loss_D: 0.6716 Loss_G: 0.6933 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6861 Loss_G: 0.7325 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6911 Loss_G: 0.7096 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6493 Loss_G: 0.6985 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6992 Loss_G: 0.6714 acc: 76.6%\n",
      "[BATCH 143/149] Loss_D: 0.6409 Loss_G: 0.6802 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6916 Loss_G: 0.6904 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6920 Loss_G: 0.6860 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.6907 Loss_G: 0.7297 acc: 95.3%\n",
      "[BATCH 147/149] Loss_D: 0.7006 Loss_G: 0.6992 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.7169 Loss_G: 0.7200 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.6865 Loss_G: 0.7032 acc: 92.2%\n",
      "-----THE [3/50] epoch end-----\n",
      "-----THE [4/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6745 Loss_G: 0.7104 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6535 Loss_G: 0.7172 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.6916 Loss_G: 0.7274 acc: 92.2%\n",
      "[EPOCH 450] TEST ACC is : 77.0%\n",
      "[BATCH 4/149] Loss_D: 0.6613 Loss_G: 0.7070 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.7033 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6892 Loss_G: 0.7031 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.6662 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6810 Loss_G: 0.7171 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6848 Loss_G: 0.7198 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6754 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6845 Loss_G: 0.7283 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6731 Loss_G: 0.6996 acc: 92.2%\n",
      "[BATCH 13/149] Loss_D: 0.6631 Loss_G: 0.6869 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.7247 Loss_G: 0.7354 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.6682 Loss_G: 0.7059 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7144 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6773 Loss_G: 0.7031 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.7238 Loss_G: 0.6966 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6889 Loss_G: 0.6888 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6675 Loss_G: 0.7079 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6689 Loss_G: 0.6869 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6678 Loss_G: 0.6785 acc: 81.2%\n",
      "[BATCH 23/149] Loss_D: 0.7277 Loss_G: 0.6932 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6498 Loss_G: 0.7147 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.6797 Loss_G: 0.6871 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6959 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6825 Loss_G: 0.7041 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6782 Loss_G: 0.6903 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.6565 Loss_G: 0.7023 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7053 Loss_G: 0.6989 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6636 Loss_G: 0.7039 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6787 Loss_G: 0.7011 acc: 79.7%\n",
      "[BATCH 33/149] Loss_D: 0.7025 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6935 Loss_G: 0.7207 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.7054 Loss_G: 0.7702 acc: 93.8%\n",
      "[BATCH 36/149] Loss_D: 0.7008 Loss_G: 0.7441 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6999 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6939 Loss_G: 0.7162 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6835 Loss_G: 0.7056 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6749 Loss_G: 0.6824 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.7170 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6874 Loss_G: 0.7221 acc: 95.3%\n",
      "[BATCH 43/149] Loss_D: 0.7064 Loss_G: 0.7071 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6534 Loss_G: 0.7173 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.6745 Loss_G: 0.7060 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6654 Loss_G: 0.7049 acc: 96.9%\n",
      "[BATCH 47/149] Loss_D: 0.6728 Loss_G: 0.6872 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7334 Loss_G: 0.7248 acc: 93.8%\n",
      "[BATCH 49/149] Loss_D: 0.6725 Loss_G: 0.7050 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.7329 Loss_G: 0.6997 acc: 82.8%\n",
      "[BATCH 51/149] Loss_D: 0.6623 Loss_G: 0.6853 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6999 Loss_G: 0.7139 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6876 Loss_G: 0.7075 acc: 85.9%\n",
      "[EPOCH 500] TEST ACC is : 76.8%\n",
      "[BATCH 54/149] Loss_D: 0.6545 Loss_G: 0.6832 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6482 Loss_G: 0.6895 acc: 93.8%\n",
      "[BATCH 56/149] Loss_D: 0.6916 Loss_G: 0.6968 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6830 Loss_G: 0.6954 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.6727 Loss_G: 0.6945 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6679 Loss_G: 0.6901 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6874 Loss_G: 0.6801 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6362 Loss_G: 0.6737 acc: 93.8%\n",
      "[BATCH 62/149] Loss_D: 0.7076 Loss_G: 0.6934 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.6579 Loss_G: 0.7020 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.6901 Loss_G: 0.7046 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.7384 Loss_G: 0.7311 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.6834 Loss_G: 0.7212 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.7083 Loss_G: 0.7145 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7040 Loss_G: 0.7013 acc: 79.7%\n",
      "[BATCH 69/149] Loss_D: 0.6971 Loss_G: 0.7073 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6891 Loss_G: 0.7205 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6755 Loss_G: 0.7051 acc: 82.8%\n",
      "[BATCH 72/149] Loss_D: 0.7266 Loss_G: 0.7334 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6921 Loss_G: 0.7063 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6947 Loss_G: 0.6958 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6938 Loss_G: 0.6912 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6626 Loss_G: 0.7006 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.7067 Loss_G: 0.6934 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6607 Loss_G: 0.6852 acc: 82.8%\n",
      "[BATCH 79/149] Loss_D: 0.6911 Loss_G: 0.6980 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6774 Loss_G: 0.6934 acc: 95.3%\n",
      "[BATCH 81/149] Loss_D: 0.7871 Loss_G: 0.7256 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7298 Loss_G: 0.7352 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.7400 Loss_G: 0.7401 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6874 Loss_G: 0.6981 acc: 79.7%\n",
      "[BATCH 85/149] Loss_D: 0.6862 Loss_G: 0.7007 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6704 Loss_G: 0.7027 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6811 Loss_G: 0.7018 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.6695 Loss_G: 0.7077 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6907 Loss_G: 0.7203 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6520 Loss_G: 0.7051 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7236 Loss_G: 0.7177 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6713 Loss_G: 0.7031 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7187 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6808 Loss_G: 0.6993 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6903 Loss_G: 0.7104 acc: 81.2%\n",
      "[BATCH 96/149] Loss_D: 0.6710 Loss_G: 0.7049 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6383 Loss_G: 0.7070 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6864 Loss_G: 0.6767 acc: 82.8%\n",
      "[BATCH 99/149] Loss_D: 0.6744 Loss_G: 0.6997 acc: 95.3%\n",
      "[BATCH 100/149] Loss_D: 0.7242 Loss_G: 0.7209 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.6907 Loss_G: 0.7149 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6618 Loss_G: 0.6897 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6712 Loss_G: 0.6757 acc: 87.5%\n",
      "[EPOCH 550] TEST ACC is : 77.1%\n",
      "[BATCH 104/149] Loss_D: 0.6942 Loss_G: 0.7213 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6589 Loss_G: 0.7065 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6664 Loss_G: 0.6891 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6714 Loss_G: 0.6928 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7066 Loss_G: 0.7245 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6791 Loss_G: 0.7015 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.6550 Loss_G: 0.6879 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6662 Loss_G: 0.7025 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.6595 Loss_G: 0.6965 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6918 Loss_G: 0.6842 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.7145 Loss_G: 0.7004 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6919 Loss_G: 0.6997 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6949 Loss_G: 0.6967 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7067 Loss_G: 0.6958 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6574 Loss_G: 0.7048 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6569 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6818 Loss_G: 0.7136 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6759 Loss_G: 0.7243 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.6531 Loss_G: 0.7217 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.7063 Loss_G: 0.7295 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6543 Loss_G: 0.6981 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.7164 Loss_G: 0.7350 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6924 Loss_G: 0.7187 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.7221 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6787 Loss_G: 0.6840 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6950 Loss_G: 0.6941 acc: 79.7%\n",
      "[BATCH 130/149] Loss_D: 0.6644 Loss_G: 0.6856 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7087 Loss_G: 0.7315 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7035 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7144 Loss_G: 0.7199 acc: 76.6%\n",
      "[BATCH 134/149] Loss_D: 0.7149 Loss_G: 0.7174 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6814 Loss_G: 0.7116 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6711 Loss_G: 0.6966 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6851 Loss_G: 0.7031 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.7408 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6861 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7026 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7075 Loss_G: 0.7144 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6777 Loss_G: 0.7465 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.7057 Loss_G: 0.7260 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.6792 Loss_G: 0.7178 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.6985 Loss_G: 0.7115 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7248 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6776 Loss_G: 0.7195 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6558 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6527 Loss_G: 0.7086 acc: 89.1%\n",
      "-----THE [4/50] epoch end-----\n",
      "-----THE [5/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6622 Loss_G: 0.6933 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6756 Loss_G: 0.6975 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.6705 Loss_G: 0.6968 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7080 Loss_G: 0.7068 acc: 90.6%\n",
      "[EPOCH 600] TEST ACC is : 76.8%\n",
      "[BATCH 5/149] Loss_D: 0.6854 Loss_G: 0.7017 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.6756 Loss_G: 0.7191 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6877 Loss_G: 0.7273 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.7611 Loss_G: 0.7583 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6642 Loss_G: 0.7087 acc: 93.8%\n",
      "[BATCH 10/149] Loss_D: 0.7114 Loss_G: 0.7067 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7115 Loss_G: 0.7181 acc: 79.7%\n",
      "[BATCH 12/149] Loss_D: 0.6847 Loss_G: 0.7034 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6610 Loss_G: 0.6982 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6688 Loss_G: 0.6885 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6579 Loss_G: 0.6908 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6773 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6611 Loss_G: 0.6890 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6573 Loss_G: 0.7050 acc: 93.8%\n",
      "[BATCH 19/149] Loss_D: 0.6799 Loss_G: 0.7171 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6727 Loss_G: 0.7213 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6653 Loss_G: 0.7051 acc: 96.9%\n",
      "[BATCH 22/149] Loss_D: 0.6911 Loss_G: 0.7114 acc: 79.7%\n",
      "[BATCH 23/149] Loss_D: 0.7183 Loss_G: 0.7057 acc: 79.7%\n",
      "[BATCH 24/149] Loss_D: 0.7135 Loss_G: 0.7077 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.6842 Loss_G: 0.7224 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.7113 Loss_G: 0.7194 acc: 95.3%\n",
      "[BATCH 27/149] Loss_D: 0.7063 Loss_G: 0.7307 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6538 Loss_G: 0.7541 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7080 Loss_G: 0.7313 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6780 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7017 Loss_G: 0.7397 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6972 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6942 Loss_G: 0.7029 acc: 81.2%\n",
      "[BATCH 34/149] Loss_D: 0.6848 Loss_G: 0.6994 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.6752 Loss_G: 0.6893 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6906 Loss_G: 0.7000 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6851 Loss_G: 0.6982 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7034 Loss_G: 0.7075 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7183 Loss_G: 0.7242 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.7346 Loss_G: 0.7419 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.6885 Loss_G: 0.7412 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.6517 Loss_G: 0.6942 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6866 Loss_G: 0.6980 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.6913 Loss_G: 0.7157 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7720 Loss_G: 0.7450 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.7083 Loss_G: 0.7190 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6814 Loss_G: 0.7065 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7347 Loss_G: 0.7017 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6693 Loss_G: 0.6966 acc: 81.2%\n",
      "[BATCH 50/149] Loss_D: 0.6914 Loss_G: 0.7111 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6588 Loss_G: 0.7078 acc: 95.3%\n",
      "[BATCH 52/149] Loss_D: 0.6892 Loss_G: 0.7236 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.7093 Loss_G: 0.7201 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6889 Loss_G: 0.7151 acc: 89.1%\n",
      "[EPOCH 650] TEST ACC is : 76.6%\n",
      "[BATCH 55/149] Loss_D: 0.6733 Loss_G: 0.7177 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6727 Loss_G: 0.7010 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6710 Loss_G: 0.6999 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.6940 Loss_G: 0.6943 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7106 Loss_G: 0.6940 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.7150 Loss_G: 0.7107 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6913 Loss_G: 0.7205 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.7042 Loss_G: 0.7049 acc: 81.2%\n",
      "[BATCH 63/149] Loss_D: 0.6340 Loss_G: 0.6928 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.7271 Loss_G: 0.7174 acc: 93.8%\n",
      "[BATCH 65/149] Loss_D: 0.7256 Loss_G: 0.7046 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6796 Loss_G: 0.7265 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.7008 Loss_G: 0.7123 acc: 78.1%\n",
      "[BATCH 68/149] Loss_D: 0.6673 Loss_G: 0.6997 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6565 Loss_G: 0.6806 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7065 Loss_G: 0.6899 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6977 Loss_G: 0.6988 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6832 Loss_G: 0.7170 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6667 Loss_G: 0.6976 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6825 Loss_G: 0.6778 acc: 82.8%\n",
      "[BATCH 75/149] Loss_D: 0.6983 Loss_G: 0.7086 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6838 Loss_G: 0.7018 acc: 93.8%\n",
      "[BATCH 77/149] Loss_D: 0.6693 Loss_G: 0.6857 acc: 85.9%\n",
      "[BATCH 78/149] Loss_D: 0.6916 Loss_G: 0.7048 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6566 Loss_G: 0.7082 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6557 Loss_G: 0.6925 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.6802 Loss_G: 0.7041 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.6997 Loss_G: 0.6959 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.7093 Loss_G: 0.6916 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6885 Loss_G: 0.6893 acc: 81.2%\n",
      "[BATCH 85/149] Loss_D: 0.6641 Loss_G: 0.6830 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6602 Loss_G: 0.7189 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.7036 Loss_G: 0.7264 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6856 Loss_G: 0.7327 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.6742 Loss_G: 0.7108 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6685 Loss_G: 0.7051 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6832 Loss_G: 0.6921 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6699 Loss_G: 0.7051 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6636 Loss_G: 0.7083 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7211 Loss_G: 0.7392 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6737 Loss_G: 0.7213 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6970 Loss_G: 0.7337 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6741 Loss_G: 0.7284 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.6482 Loss_G: 0.7082 acc: 93.8%\n",
      "[BATCH 99/149] Loss_D: 0.6882 Loss_G: 0.7017 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6782 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6399 Loss_G: 0.6868 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6848 Loss_G: 0.6994 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.6603 Loss_G: 0.7177 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.7345 Loss_G: 0.7635 acc: 89.1%\n",
      "[EPOCH 700] TEST ACC is : 77.1%\n",
      "[BATCH 105/149] Loss_D: 0.6873 Loss_G: 0.7383 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6759 Loss_G: 0.7222 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.7458 Loss_G: 0.7389 acc: 81.2%\n",
      "[BATCH 108/149] Loss_D: 0.6705 Loss_G: 0.7241 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6513 Loss_G: 0.7003 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7043 Loss_G: 0.7093 acc: 78.1%\n",
      "[BATCH 111/149] Loss_D: 0.6900 Loss_G: 0.7008 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6880 Loss_G: 0.6962 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6641 Loss_G: 0.6937 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6715 Loss_G: 0.6953 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6374 Loss_G: 0.6963 acc: 93.8%\n",
      "[BATCH 116/149] Loss_D: 0.7003 Loss_G: 0.6928 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6358 Loss_G: 0.6967 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.6889 Loss_G: 0.6885 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6968 Loss_G: 0.6986 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6501 Loss_G: 0.7075 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6967 Loss_G: 0.6971 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6776 Loss_G: 0.6784 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.6740 Loss_G: 0.6824 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.7432 Loss_G: 0.7156 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7334 Loss_G: 0.7180 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6777 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6723 Loss_G: 0.6935 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6741 Loss_G: 0.6878 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7287 Loss_G: 0.7310 acc: 95.3%\n",
      "[BATCH 130/149] Loss_D: 0.6954 Loss_G: 0.7404 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6805 Loss_G: 0.7202 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6728 Loss_G: 0.6883 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6707 Loss_G: 0.7104 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7098 Loss_G: 0.7136 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6561 Loss_G: 0.7042 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6819 Loss_G: 0.7172 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6946 Loss_G: 0.7107 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6733 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6606 Loss_G: 0.7085 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7123 Loss_G: 0.7295 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6910 Loss_G: 0.7271 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6776 Loss_G: 0.7135 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6769 Loss_G: 0.6988 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6918 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6945 Loss_G: 0.7329 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7062 Loss_G: 0.7169 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6672 Loss_G: 0.7037 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6690 Loss_G: 0.7078 acc: 95.3%\n",
      "[BATCH 149/149] Loss_D: 0.6648 Loss_G: 0.7284 acc: 87.5%\n",
      "-----THE [5/50] epoch end-----\n",
      "-----THE [6/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6983 Loss_G: 0.7156 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7083 Loss_G: 0.7500 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.6681 Loss_G: 0.7217 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6856 Loss_G: 0.6964 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7066 Loss_G: 0.7247 acc: 89.1%\n",
      "[EPOCH 750] TEST ACC is : 77.1%\n",
      "[BATCH 6/149] Loss_D: 0.7318 Loss_G: 0.7156 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6729 Loss_G: 0.7251 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6679 Loss_G: 0.7103 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6492 Loss_G: 0.6832 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6830 Loss_G: 0.7006 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6661 Loss_G: 0.6995 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6778 Loss_G: 0.7197 acc: 92.2%\n",
      "[BATCH 13/149] Loss_D: 0.6917 Loss_G: 0.7130 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6569 Loss_G: 0.6874 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6765 Loss_G: 0.6745 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6305 Loss_G: 0.6844 acc: 95.3%\n",
      "[BATCH 17/149] Loss_D: 0.7020 Loss_G: 0.7044 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6598 Loss_G: 0.6903 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6811 Loss_G: 0.6892 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6731 Loss_G: 0.6892 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.6699 Loss_G: 0.6987 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6750 Loss_G: 0.6945 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6923 Loss_G: 0.6952 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7084 Loss_G: 0.6988 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6788 Loss_G: 0.7043 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6388 Loss_G: 0.6945 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6752 Loss_G: 0.6760 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.7240 Loss_G: 0.7000 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.7032 Loss_G: 0.7311 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.7305 Loss_G: 0.7255 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.7788 Loss_G: 0.7399 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7301 Loss_G: 0.7387 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6565 Loss_G: 0.7170 acc: 95.3%\n",
      "[BATCH 34/149] Loss_D: 0.6801 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.7196 Loss_G: 0.7113 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6584 Loss_G: 0.6857 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.7333 Loss_G: 0.7085 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6424 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6572 Loss_G: 0.6918 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6653 Loss_G: 0.6960 acc: 95.3%\n",
      "[BATCH 41/149] Loss_D: 0.7226 Loss_G: 0.7079 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7036 Loss_G: 0.7353 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6905 Loss_G: 0.7161 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6760 Loss_G: 0.7373 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6604 Loss_G: 0.7328 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6937 Loss_G: 0.7112 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6850 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.7293 Loss_G: 0.7394 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6779 Loss_G: 0.7164 acc: 95.3%\n",
      "[BATCH 50/149] Loss_D: 0.6831 Loss_G: 0.7066 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6710 Loss_G: 0.6967 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.7138 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.7242 Loss_G: 0.7391 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6572 Loss_G: 0.7136 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.7021 Loss_G: 0.7071 acc: 89.1%\n",
      "[EPOCH 800] TEST ACC is : 77.3%\n",
      "[BATCH 56/149] Loss_D: 0.7094 Loss_G: 0.6975 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.7104 Loss_G: 0.6975 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6838 Loss_G: 0.7009 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6814 Loss_G: 0.6968 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.7265 Loss_G: 0.7049 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6890 Loss_G: 0.7186 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6581 Loss_G: 0.7051 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6722 Loss_G: 0.6908 acc: 76.6%\n",
      "[BATCH 64/149] Loss_D: 0.6731 Loss_G: 0.7053 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6728 Loss_G: 0.7024 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6603 Loss_G: 0.6982 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6523 Loss_G: 0.6924 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6817 Loss_G: 0.6929 acc: 82.8%\n",
      "[BATCH 69/149] Loss_D: 0.6558 Loss_G: 0.6887 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6643 Loss_G: 0.6898 acc: 93.8%\n",
      "[BATCH 71/149] Loss_D: 0.7174 Loss_G: 0.7360 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6806 Loss_G: 0.7206 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6885 Loss_G: 0.7076 acc: 95.3%\n",
      "[BATCH 74/149] Loss_D: 0.6888 Loss_G: 0.7191 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7037 Loss_G: 0.7471 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6657 Loss_G: 0.7332 acc: 95.3%\n",
      "[BATCH 77/149] Loss_D: 0.7141 Loss_G: 0.7085 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.7023 Loss_G: 0.7244 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6534 Loss_G: 0.7091 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.7065 Loss_G: 0.7112 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7087 Loss_G: 0.7067 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.6724 Loss_G: 0.6917 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6582 Loss_G: 0.6987 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.6787 Loss_G: 0.6862 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6921 Loss_G: 0.7216 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7209 Loss_G: 0.6999 acc: 78.1%\n",
      "[BATCH 87/149] Loss_D: 0.6671 Loss_G: 0.6972 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.6730 Loss_G: 0.6949 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.7392 Loss_G: 0.6969 acc: 81.2%\n",
      "[BATCH 90/149] Loss_D: 0.6581 Loss_G: 0.7039 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6695 Loss_G: 0.7396 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6800 Loss_G: 0.7364 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7022 Loss_G: 0.7584 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.6864 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6748 Loss_G: 0.6813 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.7069 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.7038 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6650 Loss_G: 0.7130 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6725 Loss_G: 0.6950 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.6770 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6589 Loss_G: 0.7030 acc: 93.8%\n",
      "[BATCH 102/149] Loss_D: 0.6845 Loss_G: 0.7133 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6784 Loss_G: 0.7186 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6669 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 105/149] Loss_D: 0.6809 Loss_G: 0.7058 acc: 90.6%\n",
      "[EPOCH 850] TEST ACC is : 77.1%\n",
      "[BATCH 106/149] Loss_D: 0.6403 Loss_G: 0.6995 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7051 Loss_G: 0.7206 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7408 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.7036 Loss_G: 0.7760 acc: 95.3%\n",
      "[BATCH 110/149] Loss_D: 0.7024 Loss_G: 0.7483 acc: 96.9%\n",
      "[BATCH 111/149] Loss_D: 0.6937 Loss_G: 0.7040 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.6742 Loss_G: 0.7233 acc: 93.8%\n",
      "[BATCH 113/149] Loss_D: 0.7022 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7333 Loss_G: 0.7082 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6568 Loss_G: 0.6991 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6611 Loss_G: 0.6863 acc: 90.6%\n",
      "[BATCH 117/149] Loss_D: 0.6480 Loss_G: 0.6699 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6544 Loss_G: 0.6643 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.6804 Loss_G: 0.6841 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.6631 Loss_G: 0.7143 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6238 Loss_G: 0.6932 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.7025 Loss_G: 0.7029 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.6795 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6712 Loss_G: 0.6969 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.7063 Loss_G: 0.7250 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6673 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.7060 Loss_G: 0.7096 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6609 Loss_G: 0.7035 acc: 93.8%\n",
      "[BATCH 129/149] Loss_D: 0.6896 Loss_G: 0.6978 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7232 Loss_G: 0.7048 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6763 Loss_G: 0.7285 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6873 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6924 Loss_G: 0.7097 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.6914 Loss_G: 0.7367 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.6748 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6740 Loss_G: 0.7210 acc: 81.2%\n",
      "[BATCH 137/149] Loss_D: 0.6873 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.7269 Loss_G: 0.6932 acc: 81.2%\n",
      "[BATCH 139/149] Loss_D: 0.6932 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7167 Loss_G: 0.7114 acc: 95.3%\n",
      "[BATCH 141/149] Loss_D: 0.7390 Loss_G: 0.7261 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7017 Loss_G: 0.7081 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7116 Loss_G: 0.6980 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.7061 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7131 Loss_G: 0.7106 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.6666 Loss_G: 0.7103 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6845 Loss_G: 0.7155 acc: 79.7%\n",
      "[BATCH 148/149] Loss_D: 0.6588 Loss_G: 0.7076 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6670 Loss_G: 0.6857 acc: 85.9%\n",
      "-----THE [6/50] epoch end-----\n",
      "-----THE [7/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7047 Loss_G: 0.7000 acc: 81.2%\n",
      "[BATCH 2/149] Loss_D: 0.6826 Loss_G: 0.7035 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6978 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6948 Loss_G: 0.6916 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.6592 Loss_G: 0.7085 acc: 96.9%\n",
      "[BATCH 6/149] Loss_D: 0.6714 Loss_G: 0.7146 acc: 87.5%\n",
      "[EPOCH 900] TEST ACC is : 77.1%\n",
      "[BATCH 7/149] Loss_D: 0.7075 Loss_G: 0.7029 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6550 Loss_G: 0.6973 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6560 Loss_G: 0.7093 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6718 Loss_G: 0.7010 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6888 Loss_G: 0.7177 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7010 Loss_G: 0.7037 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6838 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.7005 Loss_G: 0.7270 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7098 Loss_G: 0.7399 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6707 Loss_G: 0.7027 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6596 Loss_G: 0.7040 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7106 Loss_G: 0.7167 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6649 Loss_G: 0.6958 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.6595 Loss_G: 0.7217 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6540 Loss_G: 0.6845 acc: 95.3%\n",
      "[BATCH 22/149] Loss_D: 0.6412 Loss_G: 0.6709 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6768 Loss_G: 0.7031 acc: 95.3%\n",
      "[BATCH 24/149] Loss_D: 0.6564 Loss_G: 0.6968 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.7148 Loss_G: 0.7106 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7264 Loss_G: 0.7322 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6766 Loss_G: 0.7170 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.6969 Loss_G: 0.7109 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.7158 Loss_G: 0.7519 acc: 93.8%\n",
      "[BATCH 30/149] Loss_D: 0.7210 Loss_G: 0.7478 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6871 Loss_G: 0.7114 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6683 Loss_G: 0.6916 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6528 Loss_G: 0.6691 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6624 Loss_G: 0.6912 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6865 Loss_G: 0.6972 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7010 Loss_G: 0.6965 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6962 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6984 Loss_G: 0.7159 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7388 Loss_G: 0.7261 acc: 95.3%\n",
      "[BATCH 40/149] Loss_D: 0.6900 Loss_G: 0.7168 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6625 Loss_G: 0.7144 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.7089 Loss_G: 0.7344 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6722 Loss_G: 0.7258 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.7171 Loss_G: 0.7063 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6766 Loss_G: 0.6912 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6761 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.7179 Loss_G: 0.7242 acc: 82.8%\n",
      "[BATCH 48/149] Loss_D: 0.6727 Loss_G: 0.6989 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6922 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7089 Loss_G: 0.7113 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6493 Loss_G: 0.6905 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6531 Loss_G: 0.6980 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6839 Loss_G: 0.7020 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6757 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6848 Loss_G: 0.6998 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.6607 Loss_G: 0.6756 acc: 89.1%\n",
      "[EPOCH 950] TEST ACC is : 75.2%\n",
      "[BATCH 57/149] Loss_D: 0.6821 Loss_G: 0.6858 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6871 Loss_G: 0.6895 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7106 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.7120 Loss_G: 0.7564 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7198 Loss_G: 0.7082 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7022 Loss_G: 0.6989 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6919 Loss_G: 0.6973 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.6989 Loss_G: 0.7014 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6663 Loss_G: 0.7010 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.6808 Loss_G: 0.7120 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6570 Loss_G: 0.6912 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6508 Loss_G: 0.6930 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6591 Loss_G: 0.6889 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.6704 Loss_G: 0.6867 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6804 Loss_G: 0.6864 acc: 82.8%\n",
      "[BATCH 72/149] Loss_D: 0.6924 Loss_G: 0.7151 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6743 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6970 Loss_G: 0.7344 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6795 Loss_G: 0.6917 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6618 Loss_G: 0.6928 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6665 Loss_G: 0.6953 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6750 Loss_G: 0.6953 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6620 Loss_G: 0.7202 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6889 Loss_G: 0.6943 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.7241 Loss_G: 0.7263 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6810 Loss_G: 0.7294 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6634 Loss_G: 0.6977 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.7247 Loss_G: 0.7065 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7047 Loss_G: 0.7048 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6877 Loss_G: 0.6990 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.7174 Loss_G: 0.7122 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.6728 Loss_G: 0.7163 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6888 Loss_G: 0.7193 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6907 Loss_G: 0.7045 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6781 Loss_G: 0.6968 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6701 Loss_G: 0.7021 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7628 Loss_G: 0.7539 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6747 Loss_G: 0.7187 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6812 Loss_G: 0.7152 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6868 Loss_G: 0.6956 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.6941 Loss_G: 0.7082 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6771 Loss_G: 0.7011 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6941 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6762 Loss_G: 0.7019 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6968 Loss_G: 0.7127 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6968 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6646 Loss_G: 0.6952 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.6519 Loss_G: 0.7024 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.7171 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7302 Loss_G: 0.7368 acc: 85.9%\n",
      "[EPOCH 1000] TEST ACC is : 76.2%\n",
      "[BATCH 107/149] Loss_D: 0.6708 Loss_G: 0.6888 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6807 Loss_G: 0.7185 acc: 95.3%\n",
      "[BATCH 109/149] Loss_D: 0.7044 Loss_G: 0.7134 acc: 95.3%\n",
      "[BATCH 110/149] Loss_D: 0.6867 Loss_G: 0.7161 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6898 Loss_G: 0.7215 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.7086 Loss_G: 0.7139 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6817 Loss_G: 0.7007 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7669 Loss_G: 0.7218 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6659 Loss_G: 0.6931 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6865 Loss_G: 0.6929 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6992 Loss_G: 0.7086 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6820 Loss_G: 0.7099 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6980 Loss_G: 0.7308 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6431 Loss_G: 0.6992 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6898 Loss_G: 0.7085 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6511 Loss_G: 0.6883 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7109 Loss_G: 0.7026 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.7185 Loss_G: 0.6988 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6621 Loss_G: 0.6950 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7085 Loss_G: 0.7178 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.6908 Loss_G: 0.7100 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7190 Loss_G: 0.7182 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7053 Loss_G: 0.7051 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6628 Loss_G: 0.6917 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.7084 Loss_G: 0.7042 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7221 Loss_G: 0.7096 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6973 Loss_G: 0.7352 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6989 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6466 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6911 Loss_G: 0.6933 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.6799 Loss_G: 0.6925 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6800 Loss_G: 0.6936 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.7111 Loss_G: 0.6983 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.6645 Loss_G: 0.6814 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.6993 Loss_G: 0.6909 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.6911 Loss_G: 0.7060 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6820 Loss_G: 0.7053 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.7029 Loss_G: 0.7353 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6784 Loss_G: 0.7123 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.7127 Loss_G: 0.7085 acc: 81.2%\n",
      "[BATCH 147/149] Loss_D: 0.6530 Loss_G: 0.7081 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6777 Loss_G: 0.6927 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6422 Loss_G: 0.7132 acc: 95.3%\n",
      "-----THE [7/50] epoch end-----\n",
      "-----THE [8/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7069 Loss_G: 0.7073 acc: 79.7%\n",
      "[BATCH 2/149] Loss_D: 0.7030 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.6648 Loss_G: 0.6895 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.7269 Loss_G: 0.7210 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6726 Loss_G: 0.7114 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6692 Loss_G: 0.6805 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6749 Loss_G: 0.7089 acc: 90.6%\n",
      "[EPOCH 1050] TEST ACC is : 77.0%\n",
      "[BATCH 8/149] Loss_D: 0.6785 Loss_G: 0.7194 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6529 Loss_G: 0.7054 acc: 92.2%\n",
      "[BATCH 10/149] Loss_D: 0.6801 Loss_G: 0.7156 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7087 Loss_G: 0.7141 acc: 95.3%\n",
      "[BATCH 12/149] Loss_D: 0.7004 Loss_G: 0.6978 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6324 Loss_G: 0.6872 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6844 Loss_G: 0.7126 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.6960 Loss_G: 0.7398 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6618 Loss_G: 0.7078 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.7210 Loss_G: 0.7384 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.7328 Loss_G: 0.7214 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6697 Loss_G: 0.7036 acc: 96.9%\n",
      "[BATCH 20/149] Loss_D: 0.6632 Loss_G: 0.7232 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.7267 Loss_G: 0.7273 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6722 Loss_G: 0.7141 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6986 Loss_G: 0.6920 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.6555 Loss_G: 0.6967 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7000 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6856 Loss_G: 0.7043 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.6759 Loss_G: 0.7050 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6877 Loss_G: 0.7072 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.7138 Loss_G: 0.7292 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6756 Loss_G: 0.7125 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6857 Loss_G: 0.7124 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.6787 Loss_G: 0.6832 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6930 Loss_G: 0.6909 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7149 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6752 Loss_G: 0.7021 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6732 Loss_G: 0.7043 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7188 Loss_G: 0.7237 acc: 95.3%\n",
      "[BATCH 38/149] Loss_D: 0.6876 Loss_G: 0.7065 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.6947 Loss_G: 0.6992 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6705 Loss_G: 0.6993 acc: 93.8%\n",
      "[BATCH 41/149] Loss_D: 0.6533 Loss_G: 0.7094 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6557 Loss_G: 0.7117 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6770 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6531 Loss_G: 0.6918 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6733 Loss_G: 0.6965 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7299 Loss_G: 0.7178 acc: 93.8%\n",
      "[BATCH 47/149] Loss_D: 0.6618 Loss_G: 0.6969 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6630 Loss_G: 0.7021 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7064 Loss_G: 0.6947 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6801 Loss_G: 0.7044 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6624 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6805 Loss_G: 0.7093 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6680 Loss_G: 0.6756 acc: 79.7%\n",
      "[BATCH 54/149] Loss_D: 0.6824 Loss_G: 0.6910 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.7111 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6734 Loss_G: 0.6998 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6971 Loss_G: 0.7054 acc: 90.6%\n",
      "[EPOCH 1100] TEST ACC is : 76.2%\n",
      "[BATCH 58/149] Loss_D: 0.7085 Loss_G: 0.6981 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.7036 Loss_G: 0.7202 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6736 Loss_G: 0.7083 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.6884 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6895 Loss_G: 0.6979 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7243 Loss_G: 0.6967 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.6678 Loss_G: 0.6984 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7053 Loss_G: 0.7048 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6697 Loss_G: 0.6944 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7019 Loss_G: 0.7007 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6921 Loss_G: 0.7396 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6772 Loss_G: 0.7543 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6894 Loss_G: 0.7328 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6953 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6645 Loss_G: 0.7239 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.6937 Loss_G: 0.7113 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.6546 Loss_G: 0.6940 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.6946 Loss_G: 0.6852 acc: 79.7%\n",
      "[BATCH 76/149] Loss_D: 0.7217 Loss_G: 0.7291 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.7282 Loss_G: 0.7461 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.7046 Loss_G: 0.7409 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.6927 Loss_G: 0.7540 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.6957 Loss_G: 0.7048 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6569 Loss_G: 0.7065 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7080 Loss_G: 0.7179 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6871 Loss_G: 0.7212 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6553 Loss_G: 0.7100 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.6722 Loss_G: 0.7135 acc: 95.3%\n",
      "[BATCH 86/149] Loss_D: 0.6571 Loss_G: 0.6843 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6896 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6997 Loss_G: 0.7053 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6674 Loss_G: 0.7081 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7080 Loss_G: 0.7312 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6817 Loss_G: 0.7090 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6607 Loss_G: 0.6919 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.6921 Loss_G: 0.6876 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6882 Loss_G: 0.6908 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6729 Loss_G: 0.7105 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.6738 Loss_G: 0.7222 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6660 Loss_G: 0.7019 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6911 Loss_G: 0.7009 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6813 Loss_G: 0.6894 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6662 Loss_G: 0.7012 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6838 Loss_G: 0.6958 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6820 Loss_G: 0.7085 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6716 Loss_G: 0.6898 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.7128 Loss_G: 0.6947 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.6766 Loss_G: 0.7010 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.7150 Loss_G: 0.7007 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6700 Loss_G: 0.7089 acc: 90.6%\n",
      "[EPOCH 1150] TEST ACC is : 77.1%\n",
      "[BATCH 108/149] Loss_D: 0.7638 Loss_G: 0.7496 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.7017 Loss_G: 0.7230 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7058 Loss_G: 0.7191 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.6829 Loss_G: 0.6972 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.6788 Loss_G: 0.7003 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6962 Loss_G: 0.7053 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.6988 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6907 Loss_G: 0.7090 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6788 Loss_G: 0.6930 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6817 Loss_G: 0.6982 acc: 81.2%\n",
      "[BATCH 118/149] Loss_D: 0.6770 Loss_G: 0.6955 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6994 Loss_G: 0.7136 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.6646 Loss_G: 0.7341 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6416 Loss_G: 0.7101 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6553 Loss_G: 0.6995 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6975 Loss_G: 0.7199 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6979 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6844 Loss_G: 0.7065 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7006 Loss_G: 0.7323 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6587 Loss_G: 0.6973 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6767 Loss_G: 0.7003 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.7061 Loss_G: 0.7122 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6772 Loss_G: 0.6983 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6860 Loss_G: 0.6933 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.7092 Loss_G: 0.7355 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6912 Loss_G: 0.7327 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.7134 Loss_G: 0.7277 acc: 81.2%\n",
      "[BATCH 135/149] Loss_D: 0.6891 Loss_G: 0.7023 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6859 Loss_G: 0.6954 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.6716 Loss_G: 0.6934 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6897 Loss_G: 0.7056 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.6929 Loss_G: 0.6934 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.6972 Loss_G: 0.7104 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6862 Loss_G: 0.7029 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6970 Loss_G: 0.6945 acc: 79.7%\n",
      "[BATCH 143/149] Loss_D: 0.6465 Loss_G: 0.6935 acc: 95.3%\n",
      "[BATCH 144/149] Loss_D: 0.7184 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6672 Loss_G: 0.7083 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6896 Loss_G: 0.6919 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6959 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6848 Loss_G: 0.7028 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.6727 Loss_G: 0.6980 acc: 90.6%\n",
      "-----THE [8/50] epoch end-----\n",
      "-----THE [9/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6711 Loss_G: 0.6927 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.6714 Loss_G: 0.7180 acc: 96.9%\n",
      "[BATCH 3/149] Loss_D: 0.7249 Loss_G: 0.7178 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.7316 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6461 Loss_G: 0.7122 acc: 95.3%\n",
      "[BATCH 6/149] Loss_D: 0.6581 Loss_G: 0.6897 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.7082 Loss_G: 0.6919 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6726 Loss_G: 0.6923 acc: 81.2%\n",
      "[EPOCH 1200] TEST ACC is : 76.8%\n",
      "[BATCH 9/149] Loss_D: 0.7321 Loss_G: 0.7199 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6696 Loss_G: 0.7188 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6546 Loss_G: 0.7141 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6868 Loss_G: 0.7185 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6733 Loss_G: 0.7031 acc: 98.4%\n",
      "[BATCH 14/149] Loss_D: 0.7147 Loss_G: 0.7073 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6637 Loss_G: 0.6907 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6908 Loss_G: 0.6966 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6461 Loss_G: 0.6946 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7025 Loss_G: 0.7097 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7063 Loss_G: 0.7476 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.7124 Loss_G: 0.7202 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6538 Loss_G: 0.7069 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6960 Loss_G: 0.7006 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.7086 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6629 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6466 Loss_G: 0.7003 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7029 Loss_G: 0.7093 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.6430 Loss_G: 0.6997 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.7093 Loss_G: 0.6967 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.7040 Loss_G: 0.7241 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7089 Loss_G: 0.7058 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6817 Loss_G: 0.7060 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6744 Loss_G: 0.7244 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6884 Loss_G: 0.7040 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6728 Loss_G: 0.7004 acc: 95.3%\n",
      "[BATCH 35/149] Loss_D: 0.7596 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6608 Loss_G: 0.6778 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.7402 Loss_G: 0.7124 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6475 Loss_G: 0.7020 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6586 Loss_G: 0.6930 acc: 96.9%\n",
      "[BATCH 40/149] Loss_D: 0.7073 Loss_G: 0.7046 acc: 81.2%\n",
      "[BATCH 41/149] Loss_D: 0.6464 Loss_G: 0.6898 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6785 Loss_G: 0.6958 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6848 Loss_G: 0.6994 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.7033 Loss_G: 0.7237 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6609 Loss_G: 0.7099 acc: 95.3%\n",
      "[BATCH 46/149] Loss_D: 0.6598 Loss_G: 0.7072 acc: 93.8%\n",
      "[BATCH 47/149] Loss_D: 0.6627 Loss_G: 0.6966 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6870 Loss_G: 0.6935 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6603 Loss_G: 0.6923 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6737 Loss_G: 0.6915 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6744 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7054 Loss_G: 0.7259 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6764 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6998 Loss_G: 0.7076 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6974 Loss_G: 0.7037 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7062 Loss_G: 0.7207 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7144 Loss_G: 0.7268 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.7202 Loss_G: 0.7095 acc: 81.2%\n",
      "[EPOCH 1250] TEST ACC is : 77.1%\n",
      "[BATCH 59/149] Loss_D: 0.6980 Loss_G: 0.7160 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6759 Loss_G: 0.7118 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6990 Loss_G: 0.7190 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6501 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6796 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6540 Loss_G: 0.6911 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6885 Loss_G: 0.7474 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7021 Loss_G: 0.7139 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6803 Loss_G: 0.7304 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6867 Loss_G: 0.7081 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.7269 Loss_G: 0.7232 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6990 Loss_G: 0.6913 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.7212 Loss_G: 0.6924 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6897 Loss_G: 0.7076 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.7017 Loss_G: 0.6981 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6737 Loss_G: 0.6887 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.7180 Loss_G: 0.6958 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.7034 Loss_G: 0.7202 acc: 95.3%\n",
      "[BATCH 77/149] Loss_D: 0.6878 Loss_G: 0.7015 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.6651 Loss_G: 0.7227 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.6961 Loss_G: 0.7086 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6418 Loss_G: 0.6994 acc: 95.3%\n",
      "[BATCH 81/149] Loss_D: 0.6829 Loss_G: 0.6957 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6869 Loss_G: 0.7029 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6945 Loss_G: 0.7290 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6722 Loss_G: 0.7156 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.6455 Loss_G: 0.6977 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.6700 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6669 Loss_G: 0.6841 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.7026 Loss_G: 0.6908 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6798 Loss_G: 0.7014 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7027 Loss_G: 0.7079 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6690 Loss_G: 0.7276 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6403 Loss_G: 0.6818 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7067 Loss_G: 0.6954 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7116 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6815 Loss_G: 0.7184 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6660 Loss_G: 0.7019 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.7343 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.7066 Loss_G: 0.7086 acc: 79.7%\n",
      "[BATCH 99/149] Loss_D: 0.6924 Loss_G: 0.6871 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.6854 Loss_G: 0.6847 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6842 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6606 Loss_G: 0.7026 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.6770 Loss_G: 0.6998 acc: 93.8%\n",
      "[BATCH 104/149] Loss_D: 0.6830 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6724 Loss_G: 0.6894 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.7377 Loss_G: 0.7080 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.6757 Loss_G: 0.7006 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.6689 Loss_G: 0.6782 acc: 85.9%\n",
      "[EPOCH 1300] TEST ACC is : 76.6%\n",
      "[BATCH 109/149] Loss_D: 0.6394 Loss_G: 0.6997 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6748 Loss_G: 0.6923 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6805 Loss_G: 0.7011 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7150 Loss_G: 0.7141 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6677 Loss_G: 0.7267 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7047 Loss_G: 0.7144 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7133 Loss_G: 0.7061 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6852 Loss_G: 0.7113 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6903 Loss_G: 0.6992 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6629 Loss_G: 0.6984 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6689 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6673 Loss_G: 0.6862 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6862 Loss_G: 0.6975 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6971 Loss_G: 0.7154 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6778 Loss_G: 0.7296 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6849 Loss_G: 0.7024 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6655 Loss_G: 0.7028 acc: 93.8%\n",
      "[BATCH 126/149] Loss_D: 0.7057 Loss_G: 0.7118 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6490 Loss_G: 0.7185 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7091 Loss_G: 0.7186 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7329 Loss_G: 0.7449 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6933 Loss_G: 0.7246 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.7062 Loss_G: 0.7648 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6765 Loss_G: 0.7237 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6878 Loss_G: 0.7259 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6609 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6771 Loss_G: 0.6891 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.6830 Loss_G: 0.6915 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.7187 Loss_G: 0.7049 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.7085 Loss_G: 0.7150 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6645 Loss_G: 0.6990 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.6633 Loss_G: 0.6850 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6553 Loss_G: 0.6710 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6807 Loss_G: 0.6768 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6739 Loss_G: 0.7037 acc: 95.3%\n",
      "[BATCH 144/149] Loss_D: 0.6883 Loss_G: 0.7260 acc: 93.8%\n",
      "[BATCH 145/149] Loss_D: 0.6664 Loss_G: 0.7098 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6986 Loss_G: 0.7115 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6854 Loss_G: 0.7111 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6862 Loss_G: 0.7243 acc: 81.2%\n",
      "[BATCH 149/149] Loss_D: 0.6895 Loss_G: 0.7194 acc: 90.6%\n",
      "-----THE [9/50] epoch end-----\n",
      "-----THE [10/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6936 Loss_G: 0.6992 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6908 Loss_G: 0.7176 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6389 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6804 Loss_G: 0.7029 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7613 Loss_G: 0.7463 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6986 Loss_G: 0.7301 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.6659 Loss_G: 0.6982 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.7184 Loss_G: 0.7050 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6669 Loss_G: 0.6958 acc: 82.8%\n",
      "[EPOCH 1350] TEST ACC is : 77.0%\n",
      "[BATCH 10/149] Loss_D: 0.6852 Loss_G: 0.6889 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6823 Loss_G: 0.6920 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.6821 Loss_G: 0.6852 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6983 Loss_G: 0.6991 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6776 Loss_G: 0.6985 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6927 Loss_G: 0.6943 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6810 Loss_G: 0.6897 acc: 93.8%\n",
      "[BATCH 17/149] Loss_D: 0.6636 Loss_G: 0.6974 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6822 Loss_G: 0.7186 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6647 Loss_G: 0.7086 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6662 Loss_G: 0.6965 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.7150 Loss_G: 0.6970 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.6780 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7071 Loss_G: 0.7131 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6678 Loss_G: 0.6944 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6548 Loss_G: 0.7121 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6775 Loss_G: 0.6938 acc: 82.8%\n",
      "[BATCH 27/149] Loss_D: 0.6884 Loss_G: 0.7076 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7081 Loss_G: 0.7279 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6810 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6973 Loss_G: 0.7370 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6814 Loss_G: 0.7010 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.7518 Loss_G: 0.7077 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6815 Loss_G: 0.7103 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.7080 Loss_G: 0.7107 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7087 Loss_G: 0.7230 acc: 93.8%\n",
      "[BATCH 36/149] Loss_D: 0.6793 Loss_G: 0.7420 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6998 Loss_G: 0.7191 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6921 Loss_G: 0.6964 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.6775 Loss_G: 0.7036 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6812 Loss_G: 0.6924 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6986 Loss_G: 0.7060 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.7374 Loss_G: 0.7344 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6964 Loss_G: 0.7088 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7102 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.7098 Loss_G: 0.7293 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6573 Loss_G: 0.7190 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6821 Loss_G: 0.7305 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6672 Loss_G: 0.6985 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.6554 Loss_G: 0.6917 acc: 96.9%\n",
      "[BATCH 50/149] Loss_D: 0.6745 Loss_G: 0.7142 acc: 81.2%\n",
      "[BATCH 51/149] Loss_D: 0.6565 Loss_G: 0.7200 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.6757 Loss_G: 0.6964 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6762 Loss_G: 0.7032 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6722 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7018 Loss_G: 0.7145 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7246 Loss_G: 0.7038 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6976 Loss_G: 0.7071 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.6532 Loss_G: 0.6764 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6727 Loss_G: 0.6879 acc: 89.1%\n",
      "[EPOCH 1400] TEST ACC is : 76.2%\n",
      "[BATCH 60/149] Loss_D: 0.7021 Loss_G: 0.7108 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6787 Loss_G: 0.7083 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6928 Loss_G: 0.7020 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6808 Loss_G: 0.6924 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.7037 Loss_G: 0.7099 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6445 Loss_G: 0.6966 acc: 93.8%\n",
      "[BATCH 66/149] Loss_D: 0.6621 Loss_G: 0.7014 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.7148 Loss_G: 0.7188 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6982 Loss_G: 0.7202 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6525 Loss_G: 0.6983 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7109 Loss_G: 0.6933 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.7087 Loss_G: 0.6893 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6849 Loss_G: 0.6972 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6479 Loss_G: 0.6810 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6947 Loss_G: 0.6934 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.6648 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6985 Loss_G: 0.7199 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6941 Loss_G: 0.7241 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.6733 Loss_G: 0.7232 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6884 Loss_G: 0.6956 acc: 82.8%\n",
      "[BATCH 80/149] Loss_D: 0.7169 Loss_G: 0.7081 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.6900 Loss_G: 0.7082 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7050 Loss_G: 0.7184 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6636 Loss_G: 0.7235 acc: 98.4%\n",
      "[BATCH 84/149] Loss_D: 0.6936 Loss_G: 0.6935 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.6664 Loss_G: 0.6841 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.7041 Loss_G: 0.6991 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7408 Loss_G: 0.7239 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.6820 Loss_G: 0.7213 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.6949 Loss_G: 0.7115 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6477 Loss_G: 0.6901 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7455 Loss_G: 0.7182 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.6759 Loss_G: 0.7250 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6512 Loss_G: 0.6938 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6605 Loss_G: 0.6850 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6780 Loss_G: 0.6946 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6711 Loss_G: 0.6916 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6529 Loss_G: 0.6991 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.6866 Loss_G: 0.7131 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6637 Loss_G: 0.6968 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.6834 Loss_G: 0.7113 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7047 Loss_G: 0.7127 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.6868 Loss_G: 0.7062 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6671 Loss_G: 0.7029 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6840 Loss_G: 0.7121 acc: 95.3%\n",
      "[BATCH 105/149] Loss_D: 0.7011 Loss_G: 0.6906 acc: 79.7%\n",
      "[BATCH 106/149] Loss_D: 0.6653 Loss_G: 0.6792 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.7242 Loss_G: 0.7492 acc: 95.3%\n",
      "[BATCH 108/149] Loss_D: 0.6648 Loss_G: 0.7179 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6555 Loss_G: 0.7368 acc: 93.8%\n",
      "[EPOCH 1450] TEST ACC is : 76.0%\n",
      "[BATCH 110/149] Loss_D: 0.6528 Loss_G: 0.6977 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.7146 Loss_G: 0.7044 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7055 Loss_G: 0.7020 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6688 Loss_G: 0.7013 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6682 Loss_G: 0.6874 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.6755 Loss_G: 0.7001 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6887 Loss_G: 0.7019 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6981 Loss_G: 0.7194 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.6736 Loss_G: 0.7002 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6739 Loss_G: 0.6931 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6804 Loss_G: 0.7012 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6881 Loss_G: 0.6971 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.6795 Loss_G: 0.7149 acc: 100.0%\n",
      "[BATCH 123/149] Loss_D: 0.6941 Loss_G: 0.7099 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6696 Loss_G: 0.7165 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.7075 Loss_G: 0.6970 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6916 Loss_G: 0.6962 acc: 79.7%\n",
      "[BATCH 127/149] Loss_D: 0.6920 Loss_G: 0.6912 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6794 Loss_G: 0.6901 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6756 Loss_G: 0.7191 acc: 93.8%\n",
      "[BATCH 130/149] Loss_D: 0.6763 Loss_G: 0.7009 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6738 Loss_G: 0.7010 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6955 Loss_G: 0.6811 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.7325 Loss_G: 0.6978 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.6689 Loss_G: 0.7091 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.6622 Loss_G: 0.7057 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7044 Loss_G: 0.6968 acc: 81.2%\n",
      "[BATCH 137/149] Loss_D: 0.6803 Loss_G: 0.7237 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6711 Loss_G: 0.7050 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7198 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6630 Loss_G: 0.7015 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7035 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7145 Loss_G: 0.7397 acc: 93.8%\n",
      "[BATCH 143/149] Loss_D: 0.7008 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6973 Loss_G: 0.7292 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6671 Loss_G: 0.7140 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6654 Loss_G: 0.7026 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6595 Loss_G: 0.6826 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.7311 Loss_G: 0.6950 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6744 Loss_G: 0.7167 acc: 93.8%\n",
      "-----THE [10/50] epoch end-----\n",
      "-----THE [11/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7077 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6917 Loss_G: 0.7066 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6830 Loss_G: 0.6877 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6846 Loss_G: 0.7295 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6902 Loss_G: 0.7199 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.6998 Loss_G: 0.7070 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6668 Loss_G: 0.7077 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6633 Loss_G: 0.6848 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6975 Loss_G: 0.6921 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6960 Loss_G: 0.7091 acc: 90.6%\n",
      "[EPOCH 1500] TEST ACC is : 76.6%\n",
      "[BATCH 11/149] Loss_D: 0.7026 Loss_G: 0.7133 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7018 Loss_G: 0.7161 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6646 Loss_G: 0.7181 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6878 Loss_G: 0.7109 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6896 Loss_G: 0.7101 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6863 Loss_G: 0.7433 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.7112 Loss_G: 0.7392 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6977 Loss_G: 0.7315 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7016 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.7019 Loss_G: 0.7144 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6677 Loss_G: 0.7043 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7049 Loss_G: 0.7133 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.7005 Loss_G: 0.7205 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6975 Loss_G: 0.7066 acc: 82.8%\n",
      "[BATCH 25/149] Loss_D: 0.6634 Loss_G: 0.6988 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6729 Loss_G: 0.6870 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6591 Loss_G: 0.6749 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6563 Loss_G: 0.6805 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6569 Loss_G: 0.6767 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6633 Loss_G: 0.6963 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7036 Loss_G: 0.6918 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6791 Loss_G: 0.6860 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.7129 Loss_G: 0.6908 acc: 81.2%\n",
      "[BATCH 34/149] Loss_D: 0.7236 Loss_G: 0.6967 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7008 Loss_G: 0.7090 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6826 Loss_G: 0.7126 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.6972 Loss_G: 0.7100 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6759 Loss_G: 0.6937 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.6878 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6607 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6576 Loss_G: 0.6857 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.7065 Loss_G: 0.6950 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6533 Loss_G: 0.7298 acc: 95.3%\n",
      "[BATCH 44/149] Loss_D: 0.6635 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6527 Loss_G: 0.6963 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6722 Loss_G: 0.7144 acc: 84.4%\n",
      "[BATCH 47/149] Loss_D: 0.6746 Loss_G: 0.6804 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.6405 Loss_G: 0.6969 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7241 Loss_G: 0.7208 acc: 93.8%\n",
      "[BATCH 50/149] Loss_D: 0.6752 Loss_G: 0.7110 acc: 95.3%\n",
      "[BATCH 51/149] Loss_D: 0.6583 Loss_G: 0.6821 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.7005 Loss_G: 0.6995 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6807 Loss_G: 0.7113 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6739 Loss_G: 0.7020 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6732 Loss_G: 0.6943 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6752 Loss_G: 0.7028 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6857 Loss_G: 0.6987 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6791 Loss_G: 0.7370 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.7025 Loss_G: 0.7209 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6396 Loss_G: 0.7056 acc: 90.6%\n",
      "[EPOCH 1550] TEST ACC is : 75.8%\n",
      "[BATCH 61/149] Loss_D: 0.6430 Loss_G: 0.6887 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6618 Loss_G: 0.7140 acc: 93.8%\n",
      "[BATCH 63/149] Loss_D: 0.6947 Loss_G: 0.6975 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.6820 Loss_G: 0.6918 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7074 Loss_G: 0.6968 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.6825 Loss_G: 0.7004 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6825 Loss_G: 0.7068 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.7249 Loss_G: 0.7333 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7031 Loss_G: 0.7112 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.6819 Loss_G: 0.7160 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.6687 Loss_G: 0.6956 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.6853 Loss_G: 0.6965 acc: 82.8%\n",
      "[BATCH 73/149] Loss_D: 0.6498 Loss_G: 0.7098 acc: 93.8%\n",
      "[BATCH 74/149] Loss_D: 0.7272 Loss_G: 0.7371 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6430 Loss_G: 0.7127 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6672 Loss_G: 0.7157 acc: 93.8%\n",
      "[BATCH 77/149] Loss_D: 0.7135 Loss_G: 0.7172 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7061 Loss_G: 0.7230 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.7067 Loss_G: 0.7115 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.6595 Loss_G: 0.7072 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6786 Loss_G: 0.7188 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6876 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6955 Loss_G: 0.7150 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.6877 Loss_G: 0.7119 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.7295 Loss_G: 0.7234 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6538 Loss_G: 0.7264 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.7244 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6776 Loss_G: 0.7130 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.6896 Loss_G: 0.7217 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6773 Loss_G: 0.7074 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6784 Loss_G: 0.7029 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6695 Loss_G: 0.7172 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.6633 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6820 Loss_G: 0.7225 acc: 95.3%\n",
      "[BATCH 95/149] Loss_D: 0.7122 Loss_G: 0.7433 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.6796 Loss_G: 0.7100 acc: 95.3%\n",
      "[BATCH 97/149] Loss_D: 0.7024 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7350 Loss_G: 0.7314 acc: 82.8%\n",
      "[BATCH 99/149] Loss_D: 0.6778 Loss_G: 0.7048 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.6882 Loss_G: 0.7064 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6618 Loss_G: 0.6881 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6980 Loss_G: 0.7192 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.6810 Loss_G: 0.7009 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6806 Loss_G: 0.6882 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.7000 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7051 Loss_G: 0.7307 acc: 95.3%\n",
      "[BATCH 107/149] Loss_D: 0.6886 Loss_G: 0.7174 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.7023 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6867 Loss_G: 0.7018 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6357 Loss_G: 0.6895 acc: 89.1%\n",
      "[EPOCH 1600] TEST ACC is : 76.8%\n",
      "[BATCH 111/149] Loss_D: 0.6584 Loss_G: 0.6906 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6716 Loss_G: 0.6849 acc: 93.8%\n",
      "[BATCH 113/149] Loss_D: 0.6956 Loss_G: 0.6762 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.7005 Loss_G: 0.7086 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6985 Loss_G: 0.6984 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6718 Loss_G: 0.7055 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6944 Loss_G: 0.7068 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6551 Loss_G: 0.6879 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6762 Loss_G: 0.6970 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.6697 Loss_G: 0.6836 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6719 Loss_G: 0.6810 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6566 Loss_G: 0.6960 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7035 Loss_G: 0.7114 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.6452 Loss_G: 0.6860 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6597 Loss_G: 0.6796 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.7180 Loss_G: 0.7023 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.7148 Loss_G: 0.7198 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6733 Loss_G: 0.7516 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.6796 Loss_G: 0.6900 acc: 81.2%\n",
      "[BATCH 130/149] Loss_D: 0.7010 Loss_G: 0.6997 acc: 93.8%\n",
      "[BATCH 131/149] Loss_D: 0.6537 Loss_G: 0.7072 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6966 Loss_G: 0.6864 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.7105 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6852 Loss_G: 0.7124 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6954 Loss_G: 0.7068 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6925 Loss_G: 0.7006 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7220 Loss_G: 0.7495 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.7066 Loss_G: 0.6990 acc: 81.2%\n",
      "[BATCH 139/149] Loss_D: 0.6797 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7194 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6975 Loss_G: 0.7048 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7011 Loss_G: 0.7433 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6912 Loss_G: 0.7265 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6971 Loss_G: 0.7113 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7107 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6690 Loss_G: 0.7024 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.7035 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6673 Loss_G: 0.7233 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.7043 Loss_G: 0.7059 acc: 90.6%\n",
      "-----THE [11/50] epoch end-----\n",
      "-----THE [12/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7295 Loss_G: 0.6965 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6809 Loss_G: 0.6883 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6844 Loss_G: 0.7072 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6620 Loss_G: 0.7165 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.7226 Loss_G: 0.7304 acc: 81.2%\n",
      "[BATCH 6/149] Loss_D: 0.6937 Loss_G: 0.7407 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6792 Loss_G: 0.7303 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6875 Loss_G: 0.7124 acc: 84.4%\n",
      "[BATCH 9/149] Loss_D: 0.7084 Loss_G: 0.7158 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6601 Loss_G: 0.7083 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6684 Loss_G: 0.7047 acc: 90.6%\n",
      "[EPOCH 1650] TEST ACC is : 76.0%\n",
      "[BATCH 12/149] Loss_D: 0.7466 Loss_G: 0.7187 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.6702 Loss_G: 0.7413 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.7187 Loss_G: 0.7370 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7115 Loss_G: 0.7308 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6898 Loss_G: 0.7218 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6708 Loss_G: 0.7063 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6841 Loss_G: 0.7106 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6477 Loss_G: 0.6959 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6693 Loss_G: 0.7113 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6629 Loss_G: 0.6790 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6724 Loss_G: 0.6924 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.7028 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6987 Loss_G: 0.7327 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.6861 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6765 Loss_G: 0.7252 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6632 Loss_G: 0.7185 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6733 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6818 Loss_G: 0.7068 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7112 Loss_G: 0.7385 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.6862 Loss_G: 0.7123 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6700 Loss_G: 0.7056 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6789 Loss_G: 0.6804 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6768 Loss_G: 0.6921 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6171 Loss_G: 0.6825 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6607 Loss_G: 0.6600 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6661 Loss_G: 0.6766 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6610 Loss_G: 0.6678 acc: 93.8%\n",
      "[BATCH 39/149] Loss_D: 0.6448 Loss_G: 0.6927 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.6739 Loss_G: 0.6810 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7103 Loss_G: 0.6991 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.7011 Loss_G: 0.7253 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6765 Loss_G: 0.7102 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7154 Loss_G: 0.7033 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.7019 Loss_G: 0.7266 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.6847 Loss_G: 0.7043 acc: 95.3%\n",
      "[BATCH 47/149] Loss_D: 0.6824 Loss_G: 0.7003 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6543 Loss_G: 0.6865 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6891 Loss_G: 0.6945 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.6638 Loss_G: 0.6899 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6552 Loss_G: 0.6979 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.7013 Loss_G: 0.7093 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7163 Loss_G: 0.7001 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.6876 Loss_G: 0.7028 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6697 Loss_G: 0.7055 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.6927 Loss_G: 0.7284 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6980 Loss_G: 0.7085 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.6597 Loss_G: 0.6967 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.7046 Loss_G: 0.7045 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6489 Loss_G: 0.6791 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6378 Loss_G: 0.6867 acc: 92.2%\n",
      "[EPOCH 1700] TEST ACC is : 76.4%\n",
      "[BATCH 62/149] Loss_D: 0.6909 Loss_G: 0.6982 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6992 Loss_G: 0.6887 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6794 Loss_G: 0.7027 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6893 Loss_G: 0.6858 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.6804 Loss_G: 0.6914 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6860 Loss_G: 0.7250 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.7384 Loss_G: 0.7239 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.6662 Loss_G: 0.7129 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6401 Loss_G: 0.6784 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6512 Loss_G: 0.6710 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6977 Loss_G: 0.6858 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.6766 Loss_G: 0.7028 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7018 Loss_G: 0.7104 acc: 81.2%\n",
      "[BATCH 75/149] Loss_D: 0.6713 Loss_G: 0.7035 acc: 82.8%\n",
      "[BATCH 76/149] Loss_D: 0.7094 Loss_G: 0.7238 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.7044 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6762 Loss_G: 0.7377 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.6621 Loss_G: 0.6933 acc: 93.8%\n",
      "[BATCH 80/149] Loss_D: 0.6862 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6899 Loss_G: 0.7102 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.7021 Loss_G: 0.7149 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6790 Loss_G: 0.7260 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6468 Loss_G: 0.6991 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6468 Loss_G: 0.6769 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6879 Loss_G: 0.6951 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6978 Loss_G: 0.6901 acc: 81.2%\n",
      "[BATCH 88/149] Loss_D: 0.7087 Loss_G: 0.6899 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.7005 Loss_G: 0.7068 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6588 Loss_G: 0.7072 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6769 Loss_G: 0.6885 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.7092 Loss_G: 0.6937 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6627 Loss_G: 0.6976 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6713 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6603 Loss_G: 0.6890 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6524 Loss_G: 0.6686 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6794 Loss_G: 0.6819 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6956 Loss_G: 0.7076 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6955 Loss_G: 0.6948 acc: 82.8%\n",
      "[BATCH 100/149] Loss_D: 0.7939 Loss_G: 0.7428 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7057 Loss_G: 0.7575 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6686 Loss_G: 0.7216 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6887 Loss_G: 0.6903 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6989 Loss_G: 0.6986 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6597 Loss_G: 0.7019 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6524 Loss_G: 0.6870 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6731 Loss_G: 0.6740 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6816 Loss_G: 0.6896 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.7285 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6454 Loss_G: 0.6994 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.6748 Loss_G: 0.7011 acc: 93.8%\n",
      "[EPOCH 1750] TEST ACC is : 76.0%\n",
      "[BATCH 112/149] Loss_D: 0.6717 Loss_G: 0.7106 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7056 Loss_G: 0.7343 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6863 Loss_G: 0.7229 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.6571 Loss_G: 0.6950 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6511 Loss_G: 0.7009 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6790 Loss_G: 0.7134 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.7251 Loss_G: 0.7055 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6866 Loss_G: 0.7316 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.6648 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6924 Loss_G: 0.6943 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.7030 Loss_G: 0.6929 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.6493 Loss_G: 0.6868 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6661 Loss_G: 0.6984 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6883 Loss_G: 0.6930 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.7115 Loss_G: 0.7257 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.7076 Loss_G: 0.7452 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.7029 Loss_G: 0.7354 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6982 Loss_G: 0.7385 acc: 96.9%\n",
      "[BATCH 130/149] Loss_D: 0.6904 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.7066 Loss_G: 0.7159 acc: 84.4%\n",
      "[BATCH 132/149] Loss_D: 0.7304 Loss_G: 0.7220 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.6813 Loss_G: 0.7116 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7302 Loss_G: 0.7285 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.7394 Loss_G: 0.7287 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.6769 Loss_G: 0.6929 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.6516 Loss_G: 0.6815 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6950 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6789 Loss_G: 0.7116 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6639 Loss_G: 0.7101 acc: 95.3%\n",
      "[BATCH 141/149] Loss_D: 0.6841 Loss_G: 0.7107 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6926 Loss_G: 0.7225 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.7087 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6833 Loss_G: 0.7161 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6915 Loss_G: 0.7066 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7043 Loss_G: 0.7232 acc: 78.1%\n",
      "[BATCH 147/149] Loss_D: 0.6727 Loss_G: 0.7080 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6914 Loss_G: 0.6923 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.7427 Loss_G: 0.7389 acc: 92.2%\n",
      "-----THE [12/50] epoch end-----\n",
      "-----THE [13/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6673 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6656 Loss_G: 0.7110 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7114 Loss_G: 0.7083 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.6993 Loss_G: 0.6846 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.6639 Loss_G: 0.6836 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.7024 Loss_G: 0.6931 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6856 Loss_G: 0.7408 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6938 Loss_G: 0.7217 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6811 Loss_G: 0.7325 acc: 96.9%\n",
      "[BATCH 10/149] Loss_D: 0.6666 Loss_G: 0.6899 acc: 84.4%\n",
      "[BATCH 11/149] Loss_D: 0.7220 Loss_G: 0.6963 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.6821 Loss_G: 0.7123 acc: 87.5%\n",
      "[EPOCH 1800] TEST ACC is : 76.8%\n",
      "[BATCH 13/149] Loss_D: 0.7121 Loss_G: 0.7429 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.7176 Loss_G: 0.7089 acc: 78.1%\n",
      "[BATCH 15/149] Loss_D: 0.6599 Loss_G: 0.6980 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6855 Loss_G: 0.6761 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.6796 Loss_G: 0.7100 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6789 Loss_G: 0.6872 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6789 Loss_G: 0.6979 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6757 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6920 Loss_G: 0.7037 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.6836 Loss_G: 0.7391 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6893 Loss_G: 0.7321 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6653 Loss_G: 0.7059 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.7135 Loss_G: 0.7068 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7334 Loss_G: 0.7419 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6652 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6703 Loss_G: 0.7184 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6823 Loss_G: 0.7235 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6633 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6861 Loss_G: 0.6983 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7300 Loss_G: 0.7051 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6812 Loss_G: 0.6990 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.6610 Loss_G: 0.6951 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6845 Loss_G: 0.6910 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6879 Loss_G: 0.6916 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.7249 Loss_G: 0.7009 acc: 79.7%\n",
      "[BATCH 38/149] Loss_D: 0.6916 Loss_G: 0.7160 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6725 Loss_G: 0.7156 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6731 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6904 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6894 Loss_G: 0.6971 acc: 79.7%\n",
      "[BATCH 43/149] Loss_D: 0.7052 Loss_G: 0.7042 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6688 Loss_G: 0.7072 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6968 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6893 Loss_G: 0.7436 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.7266 Loss_G: 0.7330 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.6695 Loss_G: 0.7442 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6712 Loss_G: 0.6970 acc: 82.8%\n",
      "[BATCH 50/149] Loss_D: 0.7186 Loss_G: 0.6997 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.6568 Loss_G: 0.6861 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.7191 Loss_G: 0.6964 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.6525 Loss_G: 0.6941 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6865 Loss_G: 0.7030 acc: 95.3%\n",
      "[BATCH 55/149] Loss_D: 0.6911 Loss_G: 0.6937 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.7244 Loss_G: 0.7264 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6818 Loss_G: 0.7136 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6679 Loss_G: 0.6981 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6943 Loss_G: 0.7165 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6869 Loss_G: 0.7301 acc: 93.8%\n",
      "[BATCH 61/149] Loss_D: 0.6876 Loss_G: 0.7052 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6715 Loss_G: 0.7136 acc: 87.5%\n",
      "[EPOCH 1850] TEST ACC is : 76.0%\n",
      "[BATCH 63/149] Loss_D: 0.6704 Loss_G: 0.7116 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.6895 Loss_G: 0.7008 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6752 Loss_G: 0.6909 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6702 Loss_G: 0.6943 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6870 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6644 Loss_G: 0.7154 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7093 Loss_G: 0.6859 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6978 Loss_G: 0.6942 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6774 Loss_G: 0.6985 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6793 Loss_G: 0.7046 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.6955 Loss_G: 0.7095 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6926 Loss_G: 0.7095 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.6977 Loss_G: 0.7058 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6741 Loss_G: 0.6968 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6759 Loss_G: 0.6939 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6542 Loss_G: 0.6768 acc: 84.4%\n",
      "[BATCH 79/149] Loss_D: 0.6924 Loss_G: 0.6887 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6553 Loss_G: 0.7022 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6951 Loss_G: 0.6892 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.7078 Loss_G: 0.6909 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6569 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6990 Loss_G: 0.7172 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.6772 Loss_G: 0.7163 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7298 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6622 Loss_G: 0.6867 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6644 Loss_G: 0.6922 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6842 Loss_G: 0.6919 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.7234 Loss_G: 0.7043 acc: 95.3%\n",
      "[BATCH 91/149] Loss_D: 0.6848 Loss_G: 0.7140 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6459 Loss_G: 0.7141 acc: 98.4%\n",
      "[BATCH 93/149] Loss_D: 0.6846 Loss_G: 0.7089 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.7021 Loss_G: 0.7098 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6465 Loss_G: 0.6817 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.6762 Loss_G: 0.6844 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6523 Loss_G: 0.6740 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6432 Loss_G: 0.6946 acc: 98.4%\n",
      "[BATCH 99/149] Loss_D: 0.6859 Loss_G: 0.7002 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6812 Loss_G: 0.6945 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.6447 Loss_G: 0.7049 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7056 Loss_G: 0.6879 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6700 Loss_G: 0.7067 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6969 Loss_G: 0.7150 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6797 Loss_G: 0.7122 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6909 Loss_G: 0.7138 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6828 Loss_G: 0.7020 acc: 78.1%\n",
      "[BATCH 108/149] Loss_D: 0.6899 Loss_G: 0.6855 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6704 Loss_G: 0.6862 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7114 Loss_G: 0.7036 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6863 Loss_G: 0.7029 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.6839 Loss_G: 0.7068 acc: 89.1%\n",
      "[EPOCH 1900] TEST ACC is : 77.3%\n",
      "[BATCH 113/149] Loss_D: 0.7148 Loss_G: 0.7227 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.6529 Loss_G: 0.6955 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6937 Loss_G: 0.7076 acc: 93.8%\n",
      "[BATCH 116/149] Loss_D: 0.7077 Loss_G: 0.7035 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.6870 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6532 Loss_G: 0.7225 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6913 Loss_G: 0.7169 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6592 Loss_G: 0.7137 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.7254 Loss_G: 0.7183 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.6874 Loss_G: 0.7239 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7276 Loss_G: 0.7346 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6706 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6674 Loss_G: 0.6837 acc: 82.8%\n",
      "[BATCH 126/149] Loss_D: 0.6651 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6952 Loss_G: 0.7380 acc: 95.3%\n",
      "[BATCH 128/149] Loss_D: 0.6890 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6719 Loss_G: 0.7115 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6966 Loss_G: 0.6862 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.7268 Loss_G: 0.7239 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6972 Loss_G: 0.7359 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6644 Loss_G: 0.6942 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6702 Loss_G: 0.6830 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6673 Loss_G: 0.6733 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6596 Loss_G: 0.6809 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.6705 Loss_G: 0.6938 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.7118 Loss_G: 0.7092 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6986 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7157 Loss_G: 0.7200 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6642 Loss_G: 0.6938 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.6980 Loss_G: 0.6937 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6752 Loss_G: 0.7083 acc: 98.4%\n",
      "[BATCH 144/149] Loss_D: 0.7062 Loss_G: 0.7171 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6782 Loss_G: 0.7024 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6835 Loss_G: 0.7068 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6683 Loss_G: 0.6950 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6883 Loss_G: 0.7074 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6985 Loss_G: 0.7103 acc: 89.1%\n",
      "-----THE [13/50] epoch end-----\n",
      "-----THE [14/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6681 Loss_G: 0.7154 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6500 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6948 Loss_G: 0.7266 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6860 Loss_G: 0.7399 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6744 Loss_G: 0.7255 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6694 Loss_G: 0.7303 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6882 Loss_G: 0.7257 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6706 Loss_G: 0.6925 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.6631 Loss_G: 0.6849 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7012 Loss_G: 0.6903 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.6802 Loss_G: 0.6980 acc: 82.8%\n",
      "[BATCH 12/149] Loss_D: 0.6533 Loss_G: 0.6943 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6759 Loss_G: 0.7196 acc: 93.8%\n",
      "[EPOCH 1950] TEST ACC is : 77.3%\n",
      "[BATCH 14/149] Loss_D: 0.7127 Loss_G: 0.7252 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.7005 Loss_G: 0.7257 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6928 Loss_G: 0.7051 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.7376 Loss_G: 0.7038 acc: 81.2%\n",
      "[BATCH 18/149] Loss_D: 0.7017 Loss_G: 0.7226 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6687 Loss_G: 0.7065 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.6842 Loss_G: 0.7070 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6888 Loss_G: 0.7403 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.7027 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6967 Loss_G: 0.7172 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7057 Loss_G: 0.7034 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.7199 Loss_G: 0.7183 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6665 Loss_G: 0.7138 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6844 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6849 Loss_G: 0.7049 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.6474 Loss_G: 0.6958 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7000 Loss_G: 0.7057 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.7027 Loss_G: 0.7165 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7265 Loss_G: 0.7154 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6965 Loss_G: 0.7247 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6447 Loss_G: 0.6928 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7027 Loss_G: 0.6873 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7094 Loss_G: 0.6990 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6883 Loss_G: 0.7555 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.7064 Loss_G: 0.7332 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6357 Loss_G: 0.7238 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6686 Loss_G: 0.6908 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6483 Loss_G: 0.6813 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.7075 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.7146 Loss_G: 0.7140 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6817 Loss_G: 0.6960 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6825 Loss_G: 0.6809 acc: 81.2%\n",
      "[BATCH 46/149] Loss_D: 0.6607 Loss_G: 0.7007 acc: 96.9%\n",
      "[BATCH 47/149] Loss_D: 0.7195 Loss_G: 0.7227 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6894 Loss_G: 0.7391 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6982 Loss_G: 0.7059 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6857 Loss_G: 0.6915 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7130 Loss_G: 0.7161 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6796 Loss_G: 0.7202 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7246 Loss_G: 0.7421 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7018 Loss_G: 0.7265 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6915 Loss_G: 0.7246 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.6879 Loss_G: 0.6937 acc: 84.4%\n",
      "[BATCH 57/149] Loss_D: 0.7066 Loss_G: 0.7075 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6850 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6838 Loss_G: 0.6899 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7031 Loss_G: 0.7064 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6827 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6812 Loss_G: 0.7014 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7064 Loss_G: 0.6992 acc: 87.5%\n",
      "[EPOCH 2000] TEST ACC is : 77.3%\n",
      "[BATCH 64/149] Loss_D: 0.6811 Loss_G: 0.6932 acc: 93.8%\n",
      "[BATCH 65/149] Loss_D: 0.7442 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6505 Loss_G: 0.7053 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.6791 Loss_G: 0.7064 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6944 Loss_G: 0.7388 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.6905 Loss_G: 0.7115 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.7174 Loss_G: 0.7149 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6662 Loss_G: 0.7104 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.7138 Loss_G: 0.7240 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.7078 Loss_G: 0.7160 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.7020 Loss_G: 0.7358 acc: 96.9%\n",
      "[BATCH 75/149] Loss_D: 0.6777 Loss_G: 0.6907 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6601 Loss_G: 0.6911 acc: 95.3%\n",
      "[BATCH 77/149] Loss_D: 0.6595 Loss_G: 0.6983 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.7032 Loss_G: 0.7085 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.6857 Loss_G: 0.6951 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6329 Loss_G: 0.7017 acc: 98.4%\n",
      "[BATCH 81/149] Loss_D: 0.6691 Loss_G: 0.6970 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.6750 Loss_G: 0.6968 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.6996 Loss_G: 0.7320 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.6814 Loss_G: 0.7196 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6596 Loss_G: 0.7019 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6883 Loss_G: 0.7353 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6751 Loss_G: 0.6977 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6898 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6758 Loss_G: 0.7142 acc: 96.9%\n",
      "[BATCH 90/149] Loss_D: 0.6797 Loss_G: 0.7057 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7392 Loss_G: 0.7412 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6863 Loss_G: 0.7232 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.6525 Loss_G: 0.6997 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6800 Loss_G: 0.6895 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.7019 Loss_G: 0.7090 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.6478 Loss_G: 0.6872 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.7066 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6791 Loss_G: 0.6974 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6970 Loss_G: 0.7113 acc: 93.8%\n",
      "[BATCH 100/149] Loss_D: 0.6812 Loss_G: 0.6965 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7045 Loss_G: 0.6899 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6627 Loss_G: 0.6821 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6949 Loss_G: 0.7113 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6329 Loss_G: 0.6990 acc: 95.3%\n",
      "[BATCH 105/149] Loss_D: 0.6770 Loss_G: 0.7090 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.6617 Loss_G: 0.6804 acc: 82.8%\n",
      "[BATCH 107/149] Loss_D: 0.6680 Loss_G: 0.6811 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6980 Loss_G: 0.6846 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6692 Loss_G: 0.6911 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6875 Loss_G: 0.6873 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6825 Loss_G: 0.7081 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.7209 Loss_G: 0.7029 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7308 Loss_G: 0.7151 acc: 90.6%\n",
      "[EPOCH 2050] TEST ACC is : 76.8%\n",
      "[BATCH 114/149] Loss_D: 0.6377 Loss_G: 0.7070 acc: 95.3%\n",
      "[BATCH 115/149] Loss_D: 0.7044 Loss_G: 0.6989 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6939 Loss_G: 0.6864 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7270 Loss_G: 0.7034 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.6628 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6940 Loss_G: 0.7065 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.6759 Loss_G: 0.7120 acc: 95.3%\n",
      "[BATCH 121/149] Loss_D: 0.6580 Loss_G: 0.6781 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6935 Loss_G: 0.6822 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.7310 Loss_G: 0.7070 acc: 78.1%\n",
      "[BATCH 124/149] Loss_D: 0.6887 Loss_G: 0.6964 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6768 Loss_G: 0.6903 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.7197 Loss_G: 0.7005 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6672 Loss_G: 0.7007 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6597 Loss_G: 0.7094 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6997 Loss_G: 0.7133 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6797 Loss_G: 0.7072 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6643 Loss_G: 0.7075 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6531 Loss_G: 0.7242 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6924 Loss_G: 0.6956 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.6859 Loss_G: 0.7009 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.6940 Loss_G: 0.7070 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7098 Loss_G: 0.7002 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.6856 Loss_G: 0.6929 acc: 81.2%\n",
      "[BATCH 138/149] Loss_D: 0.6760 Loss_G: 0.7057 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6916 Loss_G: 0.7124 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6629 Loss_G: 0.6848 acc: 79.7%\n",
      "[BATCH 141/149] Loss_D: 0.7086 Loss_G: 0.6910 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6890 Loss_G: 0.6998 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.7064 Loss_G: 0.7013 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6454 Loss_G: 0.6977 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6537 Loss_G: 0.6835 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6389 Loss_G: 0.6799 acc: 98.4%\n",
      "[BATCH 147/149] Loss_D: 0.6539 Loss_G: 0.6851 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6449 Loss_G: 0.6796 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.7098 Loss_G: 0.6917 acc: 87.5%\n",
      "-----THE [14/50] epoch end-----\n",
      "-----THE [15/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6476 Loss_G: 0.6955 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6987 Loss_G: 0.7137 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6633 Loss_G: 0.6965 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.7156 Loss_G: 0.7146 acc: 100.0%\n",
      "[BATCH 5/149] Loss_D: 0.7057 Loss_G: 0.7150 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.6648 Loss_G: 0.7073 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6733 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.7275 Loss_G: 0.7382 acc: 82.8%\n",
      "[BATCH 9/149] Loss_D: 0.6851 Loss_G: 0.7125 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6730 Loss_G: 0.6824 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.6524 Loss_G: 0.6858 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6927 Loss_G: 0.6921 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6954 Loss_G: 0.7066 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.6835 Loss_G: 0.6915 acc: 96.9%\n",
      "[EPOCH 2100] TEST ACC is : 76.6%\n",
      "[BATCH 15/149] Loss_D: 0.6769 Loss_G: 0.6895 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6739 Loss_G: 0.6911 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6747 Loss_G: 0.6774 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6917 Loss_G: 0.6990 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6466 Loss_G: 0.7107 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6858 Loss_G: 0.7290 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.7224 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6952 Loss_G: 0.7220 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6813 Loss_G: 0.7056 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6785 Loss_G: 0.7015 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.7039 Loss_G: 0.7028 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.6916 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6811 Loss_G: 0.7034 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.7016 Loss_G: 0.7014 acc: 82.8%\n",
      "[BATCH 29/149] Loss_D: 0.6735 Loss_G: 0.6969 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7142 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6980 Loss_G: 0.7161 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6535 Loss_G: 0.7097 acc: 93.8%\n",
      "[BATCH 33/149] Loss_D: 0.6979 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6411 Loss_G: 0.6970 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7146 Loss_G: 0.6942 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7229 Loss_G: 0.7327 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7014 Loss_G: 0.7167 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6706 Loss_G: 0.7032 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6633 Loss_G: 0.6889 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6844 Loss_G: 0.7094 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6820 Loss_G: 0.7229 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6768 Loss_G: 0.7126 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.6573 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.6937 Loss_G: 0.7121 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6878 Loss_G: 0.6865 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7167 Loss_G: 0.7148 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6727 Loss_G: 0.7374 acc: 93.8%\n",
      "[BATCH 48/149] Loss_D: 0.6922 Loss_G: 0.7182 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.7140 Loss_G: 0.7082 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6916 Loss_G: 0.7286 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6644 Loss_G: 0.7115 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6573 Loss_G: 0.6922 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6925 Loss_G: 0.7022 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6666 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7062 Loss_G: 0.6986 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.6863 Loss_G: 0.7211 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6448 Loss_G: 0.6958 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6512 Loss_G: 0.6788 acc: 96.9%\n",
      "[BATCH 59/149] Loss_D: 0.6812 Loss_G: 0.6940 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6748 Loss_G: 0.6761 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6777 Loss_G: 0.6942 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6847 Loss_G: 0.6958 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.7208 Loss_G: 0.7186 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.7311 Loss_G: 0.7347 acc: 89.1%\n",
      "[EPOCH 2150] TEST ACC is : 77.1%\n",
      "[BATCH 65/149] Loss_D: 0.6934 Loss_G: 0.7212 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7083 Loss_G: 0.7029 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6466 Loss_G: 0.6858 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6998 Loss_G: 0.6975 acc: 82.8%\n",
      "[BATCH 69/149] Loss_D: 0.6956 Loss_G: 0.7067 acc: 85.9%\n",
      "[BATCH 70/149] Loss_D: 0.6746 Loss_G: 0.7224 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6804 Loss_G: 0.7244 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6622 Loss_G: 0.6971 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.7017 Loss_G: 0.7005 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6634 Loss_G: 0.7093 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6891 Loss_G: 0.6952 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6866 Loss_G: 0.7005 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6631 Loss_G: 0.6885 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6501 Loss_G: 0.6837 acc: 92.2%\n",
      "[BATCH 79/149] Loss_D: 0.7023 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6928 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6960 Loss_G: 0.7203 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7084 Loss_G: 0.7256 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.7445 Loss_G: 0.7247 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.7221 Loss_G: 0.7242 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6885 Loss_G: 0.6949 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.6853 Loss_G: 0.7256 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.7371 Loss_G: 0.7481 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6498 Loss_G: 0.7131 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.6761 Loss_G: 0.6828 acc: 81.2%\n",
      "[BATCH 90/149] Loss_D: 0.6932 Loss_G: 0.6784 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7184 Loss_G: 0.6866 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6575 Loss_G: 0.6872 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.6743 Loss_G: 0.7077 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.6898 Loss_G: 0.6875 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6752 Loss_G: 0.6768 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.7008 Loss_G: 0.6980 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6831 Loss_G: 0.7001 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.6887 Loss_G: 0.7209 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6936 Loss_G: 0.7119 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6899 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6923 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6870 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6745 Loss_G: 0.7076 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6548 Loss_G: 0.7088 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6721 Loss_G: 0.7142 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.6711 Loss_G: 0.6965 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6587 Loss_G: 0.7081 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7112 Loss_G: 0.7152 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6580 Loss_G: 0.6818 acc: 85.9%\n",
      "[BATCH 110/149] Loss_D: 0.6845 Loss_G: 0.6955 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.7004 Loss_G: 0.6877 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.7071 Loss_G: 0.7055 acc: 82.8%\n",
      "[BATCH 113/149] Loss_D: 0.6716 Loss_G: 0.6892 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6464 Loss_G: 0.6621 acc: 85.9%\n",
      "[EPOCH 2200] TEST ACC is : 77.1%\n",
      "[BATCH 115/149] Loss_D: 0.6903 Loss_G: 0.6892 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6488 Loss_G: 0.6744 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7065 Loss_G: 0.7092 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7220 Loss_G: 0.7021 acc: 75.0%\n",
      "[BATCH 119/149] Loss_D: 0.6433 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.6757 Loss_G: 0.7025 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6564 Loss_G: 0.6981 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.6927 Loss_G: 0.7092 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7337 Loss_G: 0.7745 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6832 Loss_G: 0.7169 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6982 Loss_G: 0.7275 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6870 Loss_G: 0.7223 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6842 Loss_G: 0.7070 acc: 79.7%\n",
      "[BATCH 128/149] Loss_D: 0.7081 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.7276 Loss_G: 0.7337 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6839 Loss_G: 0.7169 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.7245 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6964 Loss_G: 0.6876 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.6914 Loss_G: 0.7139 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7163 Loss_G: 0.7147 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6650 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6655 Loss_G: 0.6996 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6812 Loss_G: 0.6964 acc: 95.3%\n",
      "[BATCH 138/149] Loss_D: 0.6772 Loss_G: 0.7097 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.6521 Loss_G: 0.6908 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6600 Loss_G: 0.6958 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.7021 Loss_G: 0.6968 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6683 Loss_G: 0.6845 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6670 Loss_G: 0.6765 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.6567 Loss_G: 0.6804 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6686 Loss_G: 0.6751 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7338 Loss_G: 0.7324 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6535 Loss_G: 0.7213 acc: 96.9%\n",
      "[BATCH 148/149] Loss_D: 0.6742 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6540 Loss_G: 0.6936 acc: 92.2%\n",
      "-----THE [15/50] epoch end-----\n",
      "-----THE [16/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7312 Loss_G: 0.7103 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6484 Loss_G: 0.7000 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6713 Loss_G: 0.6907 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6708 Loss_G: 0.6966 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.7144 Loss_G: 0.7082 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.7062 Loss_G: 0.6929 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6862 Loss_G: 0.7115 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6967 Loss_G: 0.7065 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6906 Loss_G: 0.7007 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6839 Loss_G: 0.6932 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.7166 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6564 Loss_G: 0.7161 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.6699 Loss_G: 0.7164 acc: 96.9%\n",
      "[BATCH 14/149] Loss_D: 0.6623 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6773 Loss_G: 0.6879 acc: 92.2%\n",
      "[EPOCH 2250] TEST ACC is : 77.3%\n",
      "[BATCH 16/149] Loss_D: 0.7137 Loss_G: 0.7149 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6859 Loss_G: 0.7043 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.7054 Loss_G: 0.7273 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6609 Loss_G: 0.7216 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6833 Loss_G: 0.6992 acc: 81.2%\n",
      "[BATCH 21/149] Loss_D: 0.6943 Loss_G: 0.7140 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6536 Loss_G: 0.6820 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.6501 Loss_G: 0.6727 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6599 Loss_G: 0.6865 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6811 Loss_G: 0.6902 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6975 Loss_G: 0.7282 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.7004 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6879 Loss_G: 0.7125 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.6639 Loss_G: 0.7033 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6920 Loss_G: 0.6972 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6944 Loss_G: 0.7225 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6856 Loss_G: 0.6987 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6444 Loss_G: 0.6811 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.6671 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7055 Loss_G: 0.7147 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.6755 Loss_G: 0.6997 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.7146 Loss_G: 0.7063 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6627 Loss_G: 0.7130 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.6729 Loss_G: 0.7067 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6740 Loss_G: 0.7135 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6601 Loss_G: 0.6935 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6806 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6974 Loss_G: 0.6918 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6709 Loss_G: 0.7093 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6795 Loss_G: 0.7019 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.7135 Loss_G: 0.7093 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6775 Loss_G: 0.6929 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6969 Loss_G: 0.7161 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6704 Loss_G: 0.7040 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6983 Loss_G: 0.7180 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6819 Loss_G: 0.6931 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.7162 Loss_G: 0.7186 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6463 Loss_G: 0.7260 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6967 Loss_G: 0.7482 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.7199 Loss_G: 0.7335 acc: 92.2%\n",
      "[BATCH 56/149] Loss_D: 0.6656 Loss_G: 0.7036 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6786 Loss_G: 0.6916 acc: 95.3%\n",
      "[BATCH 58/149] Loss_D: 0.7037 Loss_G: 0.6969 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6913 Loss_G: 0.7230 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6840 Loss_G: 0.6916 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.7267 Loss_G: 0.7299 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.7187 Loss_G: 0.7511 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.7283 Loss_G: 0.7388 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.7057 Loss_G: 0.7229 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6670 Loss_G: 0.6945 acc: 92.2%\n",
      "[EPOCH 2300] TEST ACC is : 77.0%\n",
      "[BATCH 66/149] Loss_D: 0.6759 Loss_G: 0.6916 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6353 Loss_G: 0.6771 acc: 95.3%\n",
      "[BATCH 68/149] Loss_D: 0.6757 Loss_G: 0.6828 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7011 Loss_G: 0.6876 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6736 Loss_G: 0.7076 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6560 Loss_G: 0.6982 acc: 95.3%\n",
      "[BATCH 72/149] Loss_D: 0.6731 Loss_G: 0.6801 acc: 75.0%\n",
      "[BATCH 73/149] Loss_D: 0.7128 Loss_G: 0.7721 acc: 95.3%\n",
      "[BATCH 74/149] Loss_D: 0.7426 Loss_G: 0.7783 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7175 Loss_G: 0.7323 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6803 Loss_G: 0.7093 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7160 Loss_G: 0.7112 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6607 Loss_G: 0.7120 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6779 Loss_G: 0.7305 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6592 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6858 Loss_G: 0.7014 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.6588 Loss_G: 0.7166 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6986 Loss_G: 0.6985 acc: 96.9%\n",
      "[BATCH 84/149] Loss_D: 0.6659 Loss_G: 0.7290 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.7139 Loss_G: 0.7034 acc: 76.6%\n",
      "[BATCH 86/149] Loss_D: 0.7343 Loss_G: 0.7380 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6636 Loss_G: 0.7493 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.6884 Loss_G: 0.7093 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.6977 Loss_G: 0.6922 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7025 Loss_G: 0.7168 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6644 Loss_G: 0.6947 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6625 Loss_G: 0.6958 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6525 Loss_G: 0.6789 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.6845 Loss_G: 0.6823 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6453 Loss_G: 0.6796 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6638 Loss_G: 0.6957 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6740 Loss_G: 0.6884 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6882 Loss_G: 0.7102 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6878 Loss_G: 0.7173 acc: 93.8%\n",
      "[BATCH 100/149] Loss_D: 0.6952 Loss_G: 0.7253 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6817 Loss_G: 0.7123 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.6787 Loss_G: 0.7071 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6993 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6614 Loss_G: 0.6880 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7160 Loss_G: 0.7116 acc: 93.8%\n",
      "[BATCH 106/149] Loss_D: 0.6740 Loss_G: 0.7396 acc: 95.3%\n",
      "[BATCH 107/149] Loss_D: 0.6894 Loss_G: 0.7160 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6979 Loss_G: 0.7170 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6593 Loss_G: 0.7142 acc: 96.9%\n",
      "[BATCH 110/149] Loss_D: 0.6705 Loss_G: 0.7176 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6527 Loss_G: 0.7061 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.7259 Loss_G: 0.6920 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6796 Loss_G: 0.6928 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6914 Loss_G: 0.6929 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7171 Loss_G: 0.6942 acc: 81.2%\n",
      "[EPOCH 2350] TEST ACC is : 77.0%\n",
      "[BATCH 116/149] Loss_D: 0.7044 Loss_G: 0.7077 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6931 Loss_G: 0.6991 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.6953 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7107 Loss_G: 0.7124 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6403 Loss_G: 0.7017 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6654 Loss_G: 0.6932 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6720 Loss_G: 0.6881 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6789 Loss_G: 0.6863 acc: 81.2%\n",
      "[BATCH 124/149] Loss_D: 0.6974 Loss_G: 0.7122 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6921 Loss_G: 0.7179 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7047 Loss_G: 0.7390 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6776 Loss_G: 0.7200 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6981 Loss_G: 0.7105 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.6667 Loss_G: 0.6844 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6701 Loss_G: 0.6811 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6800 Loss_G: 0.6959 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6741 Loss_G: 0.6986 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6850 Loss_G: 0.6981 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.6506 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6807 Loss_G: 0.6956 acc: 82.8%\n",
      "[BATCH 136/149] Loss_D: 0.6745 Loss_G: 0.6884 acc: 89.1%\n",
      "[BATCH 137/149] Loss_D: 0.7276 Loss_G: 0.7006 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6635 Loss_G: 0.6881 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6664 Loss_G: 0.6867 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7221 Loss_G: 0.7048 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6960 Loss_G: 0.7236 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6990 Loss_G: 0.7109 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.7457 Loss_G: 0.7249 acc: 78.1%\n",
      "[BATCH 144/149] Loss_D: 0.6985 Loss_G: 0.7192 acc: 79.7%\n",
      "[BATCH 145/149] Loss_D: 0.6448 Loss_G: 0.7015 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6319 Loss_G: 0.6947 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.7021 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.7299 Loss_G: 0.7142 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6912 Loss_G: 0.7039 acc: 82.8%\n",
      "-----THE [16/50] epoch end-----\n",
      "-----THE [17/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6676 Loss_G: 0.7007 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6835 Loss_G: 0.6941 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6831 Loss_G: 0.6874 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.6656 Loss_G: 0.7227 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.7143 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.6900 Loss_G: 0.6902 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6741 Loss_G: 0.6939 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6957 Loss_G: 0.7020 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6569 Loss_G: 0.6938 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6697 Loss_G: 0.6979 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.6846 Loss_G: 0.6962 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6671 Loss_G: 0.6931 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6676 Loss_G: 0.6979 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6645 Loss_G: 0.6808 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6938 Loss_G: 0.6849 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6906 Loss_G: 0.7061 acc: 92.2%\n",
      "[EPOCH 2400] TEST ACC is : 77.1%\n",
      "[BATCH 17/149] Loss_D: 0.6518 Loss_G: 0.6884 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7180 Loss_G: 0.7193 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6701 Loss_G: 0.7091 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6865 Loss_G: 0.6945 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6911 Loss_G: 0.6875 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6928 Loss_G: 0.6844 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6945 Loss_G: 0.6983 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6796 Loss_G: 0.6982 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6753 Loss_G: 0.6946 acc: 95.3%\n",
      "[BATCH 26/149] Loss_D: 0.6580 Loss_G: 0.6877 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6860 Loss_G: 0.6949 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7140 Loss_G: 0.6951 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6584 Loss_G: 0.6945 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6776 Loss_G: 0.7332 acc: 95.3%\n",
      "[BATCH 31/149] Loss_D: 0.6505 Loss_G: 0.7063 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.7044 Loss_G: 0.7034 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6780 Loss_G: 0.7049 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.6827 Loss_G: 0.6891 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6679 Loss_G: 0.6948 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6890 Loss_G: 0.7223 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6894 Loss_G: 0.7148 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6509 Loss_G: 0.7199 acc: 96.9%\n",
      "[BATCH 39/149] Loss_D: 0.6450 Loss_G: 0.7152 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.6906 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.7027 Loss_G: 0.7073 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6863 Loss_G: 0.7050 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.6412 Loss_G: 0.7013 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.6483 Loss_G: 0.6936 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6987 Loss_G: 0.6947 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6582 Loss_G: 0.7128 acc: 93.8%\n",
      "[BATCH 47/149] Loss_D: 0.7212 Loss_G: 0.7130 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7164 Loss_G: 0.7600 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.6779 Loss_G: 0.7296 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7004 Loss_G: 0.7286 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7101 Loss_G: 0.7298 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6909 Loss_G: 0.7005 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6724 Loss_G: 0.7065 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6876 Loss_G: 0.6915 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6858 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6767 Loss_G: 0.6945 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6979 Loss_G: 0.7093 acc: 82.8%\n",
      "[BATCH 58/149] Loss_D: 0.6645 Loss_G: 0.7054 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6906 Loss_G: 0.7236 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.7064 Loss_G: 0.7514 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6700 Loss_G: 0.6968 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6729 Loss_G: 0.6925 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7191 Loss_G: 0.7060 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6732 Loss_G: 0.7256 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6537 Loss_G: 0.6849 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6838 Loss_G: 0.7056 acc: 89.1%\n",
      "[EPOCH 2450] TEST ACC is : 76.6%\n",
      "[BATCH 67/149] Loss_D: 0.6675 Loss_G: 0.6920 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7053 Loss_G: 0.6936 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.7322 Loss_G: 0.7044 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6918 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.7050 Loss_G: 0.6886 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6787 Loss_G: 0.6930 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6550 Loss_G: 0.6756 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6824 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.7116 Loss_G: 0.7129 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6715 Loss_G: 0.7322 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6385 Loss_G: 0.6862 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6789 Loss_G: 0.6917 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6784 Loss_G: 0.7166 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.7022 Loss_G: 0.7109 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.6586 Loss_G: 0.6998 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7264 Loss_G: 0.7000 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6757 Loss_G: 0.6951 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6661 Loss_G: 0.6930 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6845 Loss_G: 0.6950 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.6894 Loss_G: 0.7075 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.7349 Loss_G: 0.7111 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.7065 Loss_G: 0.7250 acc: 95.3%\n",
      "[BATCH 89/149] Loss_D: 0.6661 Loss_G: 0.7074 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.7628 Loss_G: 0.7128 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.6941 Loss_G: 0.7331 acc: 96.9%\n",
      "[BATCH 92/149] Loss_D: 0.6598 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6694 Loss_G: 0.6942 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6799 Loss_G: 0.6904 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6554 Loss_G: 0.6766 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6744 Loss_G: 0.6928 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6516 Loss_G: 0.6956 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6437 Loss_G: 0.6896 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6727 Loss_G: 0.7015 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6402 Loss_G: 0.6876 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.7276 Loss_G: 0.6813 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.7041 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6920 Loss_G: 0.7129 acc: 95.3%\n",
      "[BATCH 104/149] Loss_D: 0.6707 Loss_G: 0.7018 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7083 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7337 Loss_G: 0.7139 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.7135 Loss_G: 0.7181 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6630 Loss_G: 0.6957 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.7066 Loss_G: 0.7317 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6882 Loss_G: 0.7320 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.7224 Loss_G: 0.7099 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6933 Loss_G: 0.6928 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6663 Loss_G: 0.7097 acc: 93.8%\n",
      "[BATCH 114/149] Loss_D: 0.7025 Loss_G: 0.7110 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.7201 Loss_G: 0.7336 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.7018 Loss_G: 0.7200 acc: 84.4%\n",
      "[EPOCH 2500] TEST ACC is : 76.6%\n",
      "[BATCH 117/149] Loss_D: 0.6688 Loss_G: 0.7097 acc: 84.4%\n",
      "[BATCH 118/149] Loss_D: 0.6642 Loss_G: 0.7024 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6959 Loss_G: 0.7278 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.7051 Loss_G: 0.7135 acc: 81.2%\n",
      "[BATCH 121/149] Loss_D: 0.6730 Loss_G: 0.7016 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6988 Loss_G: 0.7149 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6714 Loss_G: 0.7124 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.6897 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6879 Loss_G: 0.7018 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6872 Loss_G: 0.7130 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.6647 Loss_G: 0.6749 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6999 Loss_G: 0.6899 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6649 Loss_G: 0.6985 acc: 93.8%\n",
      "[BATCH 130/149] Loss_D: 0.6955 Loss_G: 0.6903 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.7012 Loss_G: 0.7029 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6890 Loss_G: 0.6978 acc: 81.2%\n",
      "[BATCH 133/149] Loss_D: 0.6624 Loss_G: 0.6817 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6927 Loss_G: 0.6971 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.6825 Loss_G: 0.7167 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6494 Loss_G: 0.6924 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7045 Loss_G: 0.6969 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7215 Loss_G: 0.7443 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6753 Loss_G: 0.7320 acc: 98.4%\n",
      "[BATCH 140/149] Loss_D: 0.6647 Loss_G: 0.7206 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.6756 Loss_G: 0.6942 acc: 93.8%\n",
      "[BATCH 142/149] Loss_D: 0.6882 Loss_G: 0.7070 acc: 95.3%\n",
      "[BATCH 143/149] Loss_D: 0.7143 Loss_G: 0.7392 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7248 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6596 Loss_G: 0.7162 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6967 Loss_G: 0.7051 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.7069 Loss_G: 0.7072 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6960 Loss_G: 0.6978 acc: 79.7%\n",
      "[BATCH 149/149] Loss_D: 0.6544 Loss_G: 0.7189 acc: 90.6%\n",
      "-----THE [17/50] epoch end-----\n",
      "-----THE [18/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6843 Loss_G: 0.7415 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6852 Loss_G: 0.7265 acc: 82.8%\n",
      "[BATCH 3/149] Loss_D: 0.6950 Loss_G: 0.7036 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6585 Loss_G: 0.6874 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.7215 Loss_G: 0.7093 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6977 Loss_G: 0.7277 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6446 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6935 Loss_G: 0.6875 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.6724 Loss_G: 0.7040 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6594 Loss_G: 0.7065 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7055 Loss_G: 0.7202 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6884 Loss_G: 0.7195 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6599 Loss_G: 0.6845 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.7167 Loss_G: 0.6882 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.7534 Loss_G: 0.7248 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.6971 Loss_G: 0.7016 acc: 81.2%\n",
      "[BATCH 17/149] Loss_D: 0.6649 Loss_G: 0.7009 acc: 92.2%\n",
      "[EPOCH 2550] TEST ACC is : 76.4%\n",
      "[BATCH 18/149] Loss_D: 0.6776 Loss_G: 0.6801 acc: 79.7%\n",
      "[BATCH 19/149] Loss_D: 0.7032 Loss_G: 0.6976 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7035 Loss_G: 0.7100 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6993 Loss_G: 0.7162 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6807 Loss_G: 0.7202 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7114 Loss_G: 0.7391 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.7214 Loss_G: 0.7322 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.6797 Loss_G: 0.6998 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6617 Loss_G: 0.7097 acc: 95.3%\n",
      "[BATCH 27/149] Loss_D: 0.6595 Loss_G: 0.6894 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.6584 Loss_G: 0.7045 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6813 Loss_G: 0.6766 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.7257 Loss_G: 0.6857 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.6645 Loss_G: 0.6978 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.6569 Loss_G: 0.6771 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6488 Loss_G: 0.6805 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6780 Loss_G: 0.6969 acc: 82.8%\n",
      "[BATCH 35/149] Loss_D: 0.6635 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.7106 Loss_G: 0.7049 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.6718 Loss_G: 0.6971 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6639 Loss_G: 0.7071 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6648 Loss_G: 0.6931 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.7029 Loss_G: 0.7012 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6610 Loss_G: 0.6836 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6855 Loss_G: 0.6945 acc: 93.8%\n",
      "[BATCH 43/149] Loss_D: 0.7036 Loss_G: 0.7124 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.6889 Loss_G: 0.7439 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.7005 Loss_G: 0.7203 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6896 Loss_G: 0.7206 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6599 Loss_G: 0.7016 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6769 Loss_G: 0.7023 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7287 Loss_G: 0.7269 acc: 78.1%\n",
      "[BATCH 50/149] Loss_D: 0.6634 Loss_G: 0.6849 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6719 Loss_G: 0.6791 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6773 Loss_G: 0.6945 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6734 Loss_G: 0.7010 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6516 Loss_G: 0.6757 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.7040 Loss_G: 0.6982 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6925 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6957 Loss_G: 0.6970 acc: 78.1%\n",
      "[BATCH 58/149] Loss_D: 0.6798 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 59/149] Loss_D: 0.6529 Loss_G: 0.7042 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6509 Loss_G: 0.6889 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.7077 Loss_G: 0.7222 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6750 Loss_G: 0.7004 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.6760 Loss_G: 0.7121 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.6834 Loss_G: 0.6837 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.7434 Loss_G: 0.6982 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6731 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7014 Loss_G: 0.7344 acc: 92.2%\n",
      "[EPOCH 2600] TEST ACC is : 77.5%\n",
      "[BATCH 68/149] Loss_D: 0.6703 Loss_G: 0.7065 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6880 Loss_G: 0.7045 acc: 96.9%\n",
      "[BATCH 70/149] Loss_D: 0.6646 Loss_G: 0.6969 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6602 Loss_G: 0.7050 acc: 95.3%\n",
      "[BATCH 72/149] Loss_D: 0.6647 Loss_G: 0.7037 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7021 Loss_G: 0.7255 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6774 Loss_G: 0.7053 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.7121 Loss_G: 0.7381 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7087 Loss_G: 0.7148 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.7203 Loss_G: 0.7177 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.6934 Loss_G: 0.7141 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6679 Loss_G: 0.7131 acc: 96.9%\n",
      "[BATCH 80/149] Loss_D: 0.6735 Loss_G: 0.6911 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6934 Loss_G: 0.7130 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.7048 Loss_G: 0.7329 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.6512 Loss_G: 0.7022 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6977 Loss_G: 0.7231 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6608 Loss_G: 0.6948 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.7113 Loss_G: 0.7152 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6459 Loss_G: 0.6879 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6507 Loss_G: 0.6976 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6700 Loss_G: 0.6866 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7136 Loss_G: 0.7476 acc: 93.8%\n",
      "[BATCH 91/149] Loss_D: 0.6640 Loss_G: 0.7230 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6883 Loss_G: 0.7032 acc: 98.4%\n",
      "[BATCH 93/149] Loss_D: 0.7043 Loss_G: 0.7177 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7127 Loss_G: 0.7154 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7242 Loss_G: 0.7216 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.7088 Loss_G: 0.7096 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.6838 Loss_G: 0.7195 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.6668 Loss_G: 0.7091 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.7038 Loss_G: 0.7220 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6831 Loss_G: 0.7025 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6911 Loss_G: 0.7077 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6975 Loss_G: 0.7121 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6919 Loss_G: 0.7241 acc: 81.2%\n",
      "[BATCH 104/149] Loss_D: 0.7025 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.7378 Loss_G: 0.7202 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.6720 Loss_G: 0.7080 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6770 Loss_G: 0.6974 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.7033 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6667 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6631 Loss_G: 0.7031 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.6756 Loss_G: 0.6912 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6658 Loss_G: 0.6790 acc: 81.2%\n",
      "[BATCH 113/149] Loss_D: 0.6786 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6883 Loss_G: 0.6988 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6643 Loss_G: 0.6955 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.7095 Loss_G: 0.7159 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6819 Loss_G: 0.7001 acc: 92.2%\n",
      "[EPOCH 2650] TEST ACC is : 76.4%\n",
      "[BATCH 118/149] Loss_D: 0.6793 Loss_G: 0.6961 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6747 Loss_G: 0.6748 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6659 Loss_G: 0.6892 acc: 93.8%\n",
      "[BATCH 121/149] Loss_D: 0.6757 Loss_G: 0.7026 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6694 Loss_G: 0.6986 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6946 Loss_G: 0.7244 acc: 87.5%\n",
      "[BATCH 124/149] Loss_D: 0.6650 Loss_G: 0.7173 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6917 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6995 Loss_G: 0.7237 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6960 Loss_G: 0.7163 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.7244 Loss_G: 0.7078 acc: 79.7%\n",
      "[BATCH 129/149] Loss_D: 0.6996 Loss_G: 0.6966 acc: 81.2%\n",
      "[BATCH 130/149] Loss_D: 0.6920 Loss_G: 0.7196 acc: 82.8%\n",
      "[BATCH 131/149] Loss_D: 0.6912 Loss_G: 0.7229 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6425 Loss_G: 0.7024 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6717 Loss_G: 0.7037 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.6865 Loss_G: 0.7124 acc: 95.3%\n",
      "[BATCH 135/149] Loss_D: 0.6562 Loss_G: 0.6942 acc: 92.2%\n",
      "[BATCH 136/149] Loss_D: 0.6571 Loss_G: 0.7235 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6744 Loss_G: 0.7231 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.7630 Loss_G: 0.7367 acc: 81.2%\n",
      "[BATCH 139/149] Loss_D: 0.6543 Loss_G: 0.6953 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6934 Loss_G: 0.6847 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.7280 Loss_G: 0.7249 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6796 Loss_G: 0.6862 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.6684 Loss_G: 0.6746 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.6833 Loss_G: 0.6916 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7008 Loss_G: 0.7100 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6870 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6821 Loss_G: 0.7117 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6736 Loss_G: 0.6983 acc: 95.3%\n",
      "[BATCH 149/149] Loss_D: 0.6765 Loss_G: 0.6973 acc: 96.9%\n",
      "-----THE [18/50] epoch end-----\n",
      "-----THE [19/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6773 Loss_G: 0.7034 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7326 Loss_G: 0.7245 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6914 Loss_G: 0.7497 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.7056 Loss_G: 0.7254 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6668 Loss_G: 0.6974 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6872 Loss_G: 0.6829 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.7115 Loss_G: 0.6941 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.7073 Loss_G: 0.6986 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6796 Loss_G: 0.6946 acc: 78.1%\n",
      "[BATCH 10/149] Loss_D: 0.6586 Loss_G: 0.6972 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6715 Loss_G: 0.6930 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6910 Loss_G: 0.6986 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6861 Loss_G: 0.7132 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6849 Loss_G: 0.7044 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7165 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.7039 Loss_G: 0.7354 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7507 Loss_G: 0.7357 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6797 Loss_G: 0.7178 acc: 92.2%\n",
      "[EPOCH 2700] TEST ACC is : 76.2%\n",
      "[BATCH 19/149] Loss_D: 0.7122 Loss_G: 0.7347 acc: 82.8%\n",
      "[BATCH 20/149] Loss_D: 0.6991 Loss_G: 0.7415 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6942 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6458 Loss_G: 0.6923 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6805 Loss_G: 0.6901 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.7153 Loss_G: 0.6923 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.6878 Loss_G: 0.6953 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6944 Loss_G: 0.7019 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6714 Loss_G: 0.6877 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6852 Loss_G: 0.7021 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6970 Loss_G: 0.7173 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6664 Loss_G: 0.7046 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.7150 Loss_G: 0.7050 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6933 Loss_G: 0.6866 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6476 Loss_G: 0.6889 acc: 96.9%\n",
      "[BATCH 34/149] Loss_D: 0.6880 Loss_G: 0.6995 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.6693 Loss_G: 0.6982 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6799 Loss_G: 0.7083 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6598 Loss_G: 0.7165 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6889 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6899 Loss_G: 0.7202 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6887 Loss_G: 0.7028 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6529 Loss_G: 0.6914 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6895 Loss_G: 0.7152 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6754 Loss_G: 0.7190 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6777 Loss_G: 0.7011 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.6837 Loss_G: 0.7172 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6652 Loss_G: 0.7094 acc: 93.8%\n",
      "[BATCH 47/149] Loss_D: 0.6955 Loss_G: 0.7083 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6592 Loss_G: 0.6954 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6958 Loss_G: 0.6946 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6657 Loss_G: 0.6933 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6959 Loss_G: 0.6975 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6791 Loss_G: 0.7055 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.6878 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6731 Loss_G: 0.6971 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6657 Loss_G: 0.6836 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6433 Loss_G: 0.6861 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6671 Loss_G: 0.6784 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6924 Loss_G: 0.6777 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6909 Loss_G: 0.7095 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6541 Loss_G: 0.7029 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7022 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.7161 Loss_G: 0.7206 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6577 Loss_G: 0.7000 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6822 Loss_G: 0.6891 acc: 79.7%\n",
      "[BATCH 65/149] Loss_D: 0.7077 Loss_G: 0.7070 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7010 Loss_G: 0.7083 acc: 84.4%\n",
      "[BATCH 67/149] Loss_D: 0.7148 Loss_G: 0.7060 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6425 Loss_G: 0.7026 acc: 92.2%\n",
      "[EPOCH 2750] TEST ACC is : 76.6%\n",
      "[BATCH 69/149] Loss_D: 0.6717 Loss_G: 0.6942 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6556 Loss_G: 0.7128 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6445 Loss_G: 0.6895 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6561 Loss_G: 0.7000 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6727 Loss_G: 0.6913 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6738 Loss_G: 0.6959 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6673 Loss_G: 0.7038 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6880 Loss_G: 0.7008 acc: 81.2%\n",
      "[BATCH 77/149] Loss_D: 0.6645 Loss_G: 0.7004 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6725 Loss_G: 0.7172 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.7222 Loss_G: 0.7087 acc: 82.8%\n",
      "[BATCH 80/149] Loss_D: 0.6806 Loss_G: 0.6896 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6834 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6917 Loss_G: 0.7074 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6587 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6785 Loss_G: 0.6935 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6788 Loss_G: 0.7021 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6520 Loss_G: 0.7002 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6667 Loss_G: 0.6918 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6958 Loss_G: 0.6936 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6953 Loss_G: 0.7009 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7225 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6580 Loss_G: 0.6972 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6965 Loss_G: 0.6937 acc: 82.8%\n",
      "[BATCH 93/149] Loss_D: 0.7048 Loss_G: 0.7001 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6933 Loss_G: 0.6991 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6705 Loss_G: 0.7107 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6816 Loss_G: 0.7040 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6991 Loss_G: 0.6961 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.6881 Loss_G: 0.7117 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6659 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6814 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6630 Loss_G: 0.7080 acc: 93.8%\n",
      "[BATCH 102/149] Loss_D: 0.6772 Loss_G: 0.6967 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.6843 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.7124 Loss_G: 0.6850 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6818 Loss_G: 0.6871 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6768 Loss_G: 0.6922 acc: 93.8%\n",
      "[BATCH 107/149] Loss_D: 0.6784 Loss_G: 0.7067 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6596 Loss_G: 0.6933 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6890 Loss_G: 0.7021 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6730 Loss_G: 0.7174 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.7118 Loss_G: 0.7226 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6834 Loss_G: 0.7127 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7019 Loss_G: 0.7052 acc: 79.7%\n",
      "[BATCH 114/149] Loss_D: 0.6823 Loss_G: 0.7167 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6822 Loss_G: 0.6844 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.6649 Loss_G: 0.6913 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6647 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6728 Loss_G: 0.6964 acc: 92.2%\n",
      "[EPOCH 2800] TEST ACC is : 77.7%\n",
      "[BATCH 119/149] Loss_D: 0.6796 Loss_G: 0.6867 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6987 Loss_G: 0.6737 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.6598 Loss_G: 0.6958 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.7166 Loss_G: 0.7209 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.7102 Loss_G: 0.7251 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6566 Loss_G: 0.6836 acc: 81.2%\n",
      "[BATCH 125/149] Loss_D: 0.6801 Loss_G: 0.6895 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6972 Loss_G: 0.6945 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.6858 Loss_G: 0.6947 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.7209 Loss_G: 0.7067 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.6971 Loss_G: 0.6995 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6877 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6808 Loss_G: 0.7011 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6967 Loss_G: 0.7160 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6685 Loss_G: 0.6968 acc: 93.8%\n",
      "[BATCH 134/149] Loss_D: 0.6770 Loss_G: 0.6897 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.6874 Loss_G: 0.7352 acc: 93.8%\n",
      "[BATCH 136/149] Loss_D: 0.6804 Loss_G: 0.7453 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6766 Loss_G: 0.7469 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6667 Loss_G: 0.7067 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6658 Loss_G: 0.6959 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.6764 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 141/149] Loss_D: 0.6910 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6665 Loss_G: 0.7108 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.6651 Loss_G: 0.7224 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7119 Loss_G: 0.7320 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.7684 Loss_G: 0.7419 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.7306 Loss_G: 0.7213 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6791 Loss_G: 0.6850 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6966 Loss_G: 0.6914 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.7120 Loss_G: 0.7074 acc: 89.1%\n",
      "-----THE [19/50] epoch end-----\n",
      "-----THE [20/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6833 Loss_G: 0.7053 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6874 Loss_G: 0.6967 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6844 Loss_G: 0.6973 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6818 Loss_G: 0.6840 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.6537 Loss_G: 0.6769 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.6585 Loss_G: 0.6924 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6875 Loss_G: 0.7093 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6827 Loss_G: 0.7099 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7061 Loss_G: 0.6779 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6533 Loss_G: 0.6782 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6677 Loss_G: 0.6952 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6660 Loss_G: 0.6813 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.7050 Loss_G: 0.7143 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6744 Loss_G: 0.7151 acc: 93.8%\n",
      "[BATCH 15/149] Loss_D: 0.7078 Loss_G: 0.7246 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6565 Loss_G: 0.6982 acc: 95.3%\n",
      "[BATCH 17/149] Loss_D: 0.7127 Loss_G: 0.7075 acc: 81.2%\n",
      "[BATCH 18/149] Loss_D: 0.6810 Loss_G: 0.7005 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6587 Loss_G: 0.6938 acc: 93.8%\n",
      "[EPOCH 2850] TEST ACC is : 77.9%\n",
      "[BATCH 20/149] Loss_D: 0.6787 Loss_G: 0.6918 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6849 Loss_G: 0.7079 acc: 92.2%\n",
      "[BATCH 22/149] Loss_D: 0.6963 Loss_G: 0.7016 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6444 Loss_G: 0.6913 acc: 95.3%\n",
      "[BATCH 24/149] Loss_D: 0.7012 Loss_G: 0.6996 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6665 Loss_G: 0.6841 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6555 Loss_G: 0.6856 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.6628 Loss_G: 0.6862 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6649 Loss_G: 0.7050 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6759 Loss_G: 0.7002 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.7449 Loss_G: 0.7088 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6624 Loss_G: 0.7142 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.6614 Loss_G: 0.7047 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7052 Loss_G: 0.7046 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6549 Loss_G: 0.6966 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6741 Loss_G: 0.6876 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6791 Loss_G: 0.6892 acc: 84.4%\n",
      "[BATCH 37/149] Loss_D: 0.6790 Loss_G: 0.6842 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6753 Loss_G: 0.6886 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7103 Loss_G: 0.6910 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.6729 Loss_G: 0.6896 acc: 93.8%\n",
      "[BATCH 41/149] Loss_D: 0.6581 Loss_G: 0.6907 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6865 Loss_G: 0.7206 acc: 93.8%\n",
      "[BATCH 43/149] Loss_D: 0.6564 Loss_G: 0.7210 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7006 Loss_G: 0.7004 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.7054 Loss_G: 0.7247 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6862 Loss_G: 0.6924 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6786 Loss_G: 0.7096 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6446 Loss_G: 0.6854 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6742 Loss_G: 0.7160 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.7200 Loss_G: 0.7341 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6741 Loss_G: 0.7121 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7185 Loss_G: 0.7175 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6831 Loss_G: 0.7183 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6962 Loss_G: 0.7191 acc: 82.8%\n",
      "[BATCH 55/149] Loss_D: 0.6986 Loss_G: 0.6953 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6825 Loss_G: 0.6928 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.6729 Loss_G: 0.7060 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6934 Loss_G: 0.6800 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.7184 Loss_G: 0.7032 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7012 Loss_G: 0.6990 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.7120 Loss_G: 0.7018 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6715 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6836 Loss_G: 0.7153 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6660 Loss_G: 0.7045 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6740 Loss_G: 0.6888 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6984 Loss_G: 0.6924 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6914 Loss_G: 0.6783 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6774 Loss_G: 0.6979 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6972 Loss_G: 0.6839 acc: 82.8%\n",
      "[EPOCH 2900] TEST ACC is : 77.5%\n",
      "[BATCH 70/149] Loss_D: 0.7079 Loss_G: 0.6892 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.7091 Loss_G: 0.7039 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7154 Loss_G: 0.7157 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6557 Loss_G: 0.7130 acc: 84.4%\n",
      "[BATCH 74/149] Loss_D: 0.6689 Loss_G: 0.6857 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6727 Loss_G: 0.6876 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6849 Loss_G: 0.6954 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6747 Loss_G: 0.7082 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6982 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6867 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6584 Loss_G: 0.6882 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6878 Loss_G: 0.6997 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.7196 Loss_G: 0.7490 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.7153 Loss_G: 0.7551 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6801 Loss_G: 0.7214 acc: 92.2%\n",
      "[BATCH 85/149] Loss_D: 0.6835 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6878 Loss_G: 0.7130 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6820 Loss_G: 0.7062 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6623 Loss_G: 0.7000 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6590 Loss_G: 0.6874 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6867 Loss_G: 0.6832 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6914 Loss_G: 0.6947 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6825 Loss_G: 0.6894 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6984 Loss_G: 0.7052 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7146 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6619 Loss_G: 0.7029 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6869 Loss_G: 0.7366 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6849 Loss_G: 0.7163 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6797 Loss_G: 0.7117 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.7073 Loss_G: 0.7198 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.6897 Loss_G: 0.7132 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6548 Loss_G: 0.6951 acc: 95.3%\n",
      "[BATCH 102/149] Loss_D: 0.6664 Loss_G: 0.6983 acc: 95.3%\n",
      "[BATCH 103/149] Loss_D: 0.6679 Loss_G: 0.6777 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6810 Loss_G: 0.6888 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.6463 Loss_G: 0.6944 acc: 95.3%\n",
      "[BATCH 106/149] Loss_D: 0.7183 Loss_G: 0.7111 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.6810 Loss_G: 0.7234 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6808 Loss_G: 0.7043 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6934 Loss_G: 0.6882 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6531 Loss_G: 0.6794 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6401 Loss_G: 0.6706 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.7174 Loss_G: 0.7217 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6902 Loss_G: 0.7165 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6784 Loss_G: 0.7084 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6900 Loss_G: 0.7324 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6965 Loss_G: 0.7045 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.6600 Loss_G: 0.7096 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6851 Loss_G: 0.6944 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.7209 Loss_G: 0.7047 acc: 85.9%\n",
      "[EPOCH 2950] TEST ACC is : 76.0%\n",
      "[BATCH 120/149] Loss_D: 0.6711 Loss_G: 0.6978 acc: 78.1%\n",
      "[BATCH 121/149] Loss_D: 0.6754 Loss_G: 0.7133 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.7213 Loss_G: 0.7520 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6628 Loss_G: 0.7203 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6915 Loss_G: 0.7020 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.7163 Loss_G: 0.6993 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6760 Loss_G: 0.6972 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.6991 Loss_G: 0.7220 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6671 Loss_G: 0.7025 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.7217 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6984 Loss_G: 0.7157 acc: 79.7%\n",
      "[BATCH 131/149] Loss_D: 0.6635 Loss_G: 0.7072 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6837 Loss_G: 0.7376 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6969 Loss_G: 0.7057 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.6836 Loss_G: 0.6974 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.7156 Loss_G: 0.6930 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.6892 Loss_G: 0.7222 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6890 Loss_G: 0.7097 acc: 95.3%\n",
      "[BATCH 138/149] Loss_D: 0.6853 Loss_G: 0.7056 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6746 Loss_G: 0.6988 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7209 Loss_G: 0.7539 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7055 Loss_G: 0.7240 acc: 93.8%\n",
      "[BATCH 142/149] Loss_D: 0.6874 Loss_G: 0.7366 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6794 Loss_G: 0.7399 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7119 Loss_G: 0.6993 acc: 81.2%\n",
      "[BATCH 145/149] Loss_D: 0.6928 Loss_G: 0.7142 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.6902 Loss_G: 0.7230 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6592 Loss_G: 0.7003 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6609 Loss_G: 0.6952 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6726 Loss_G: 0.7113 acc: 90.6%\n",
      "-----THE [20/50] epoch end-----\n",
      "-----THE [21/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6869 Loss_G: 0.7342 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6751 Loss_G: 0.7359 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.7072 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6967 Loss_G: 0.7137 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6842 Loss_G: 0.6944 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6966 Loss_G: 0.6927 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.7082 Loss_G: 0.7074 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7086 Loss_G: 0.7260 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.7155 Loss_G: 0.7292 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6665 Loss_G: 0.7125 acc: 95.3%\n",
      "[BATCH 11/149] Loss_D: 0.6704 Loss_G: 0.7014 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6777 Loss_G: 0.6930 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6820 Loss_G: 0.7024 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6805 Loss_G: 0.6837 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6739 Loss_G: 0.6987 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6913 Loss_G: 0.7069 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6738 Loss_G: 0.7017 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6484 Loss_G: 0.6963 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.7191 Loss_G: 0.7173 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6639 Loss_G: 0.6911 acc: 89.1%\n",
      "[EPOCH 3000] TEST ACC is : 76.8%\n",
      "[BATCH 21/149] Loss_D: 0.6770 Loss_G: 0.7146 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6697 Loss_G: 0.6933 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6609 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 24/149] Loss_D: 0.6735 Loss_G: 0.7000 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6839 Loss_G: 0.7419 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7090 Loss_G: 0.7131 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6785 Loss_G: 0.7122 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6871 Loss_G: 0.7040 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6747 Loss_G: 0.7093 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6736 Loss_G: 0.7164 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6811 Loss_G: 0.7080 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6775 Loss_G: 0.7091 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.6824 Loss_G: 0.7298 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.6780 Loss_G: 0.7256 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7072 Loss_G: 0.7129 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.6728 Loss_G: 0.7097 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6913 Loss_G: 0.6964 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6823 Loss_G: 0.7093 acc: 85.9%\n",
      "[BATCH 39/149] Loss_D: 0.7052 Loss_G: 0.7085 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6807 Loss_G: 0.6884 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6709 Loss_G: 0.7095 acc: 95.3%\n",
      "[BATCH 42/149] Loss_D: 0.6708 Loss_G: 0.6933 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6792 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6548 Loss_G: 0.6789 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6877 Loss_G: 0.7083 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6971 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6674 Loss_G: 0.6865 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6992 Loss_G: 0.6966 acc: 81.2%\n",
      "[BATCH 49/149] Loss_D: 0.6813 Loss_G: 0.6934 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6696 Loss_G: 0.6805 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6697 Loss_G: 0.6962 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7168 Loss_G: 0.7193 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6719 Loss_G: 0.7036 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.7204 Loss_G: 0.6998 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6872 Loss_G: 0.7027 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7124 Loss_G: 0.7162 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6736 Loss_G: 0.7151 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6704 Loss_G: 0.7159 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6933 Loss_G: 0.7011 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6888 Loss_G: 0.7002 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6511 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6583 Loss_G: 0.6697 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6582 Loss_G: 0.6701 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.7073 Loss_G: 0.6912 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6703 Loss_G: 0.6884 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6847 Loss_G: 0.7116 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7082 Loss_G: 0.7175 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6890 Loss_G: 0.7119 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.6629 Loss_G: 0.6858 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.6865 Loss_G: 0.7209 acc: 92.2%\n",
      "[EPOCH 3050] TEST ACC is : 76.6%\n",
      "[BATCH 71/149] Loss_D: 0.6490 Loss_G: 0.6881 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6885 Loss_G: 0.6720 acc: 82.8%\n",
      "[BATCH 73/149] Loss_D: 0.7089 Loss_G: 0.7158 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7020 Loss_G: 0.7271 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6831 Loss_G: 0.7107 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.7380 Loss_G: 0.7352 acc: 82.8%\n",
      "[BATCH 77/149] Loss_D: 0.7373 Loss_G: 0.7326 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.7307 Loss_G: 0.7320 acc: 78.1%\n",
      "[BATCH 79/149] Loss_D: 0.6829 Loss_G: 0.7304 acc: 79.7%\n",
      "[BATCH 80/149] Loss_D: 0.6789 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6694 Loss_G: 0.6938 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6698 Loss_G: 0.6992 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6534 Loss_G: 0.6939 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6845 Loss_G: 0.6947 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6690 Loss_G: 0.6921 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.6450 Loss_G: 0.6837 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6883 Loss_G: 0.7126 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.6422 Loss_G: 0.7141 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.7339 Loss_G: 0.7310 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6734 Loss_G: 0.7337 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7019 Loss_G: 0.7016 acc: 81.2%\n",
      "[BATCH 92/149] Loss_D: 0.7238 Loss_G: 0.7070 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.7020 Loss_G: 0.7256 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.6826 Loss_G: 0.7047 acc: 96.9%\n",
      "[BATCH 95/149] Loss_D: 0.6795 Loss_G: 0.7025 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7114 Loss_G: 0.7077 acc: 82.8%\n",
      "[BATCH 97/149] Loss_D: 0.6822 Loss_G: 0.7017 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6408 Loss_G: 0.6948 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6924 Loss_G: 0.6838 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.6876 Loss_G: 0.6905 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6886 Loss_G: 0.7037 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6467 Loss_G: 0.6951 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6699 Loss_G: 0.6862 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6807 Loss_G: 0.7000 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6581 Loss_G: 0.6976 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6817 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6584 Loss_G: 0.7036 acc: 92.2%\n",
      "[BATCH 108/149] Loss_D: 0.7442 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.7134 Loss_G: 0.7007 acc: 82.8%\n",
      "[BATCH 110/149] Loss_D: 0.6646 Loss_G: 0.6838 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6905 Loss_G: 0.7035 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6485 Loss_G: 0.6983 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6807 Loss_G: 0.7240 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6603 Loss_G: 0.6884 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6924 Loss_G: 0.7186 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6886 Loss_G: 0.7197 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7180 Loss_G: 0.7330 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.6812 Loss_G: 0.7221 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6892 Loss_G: 0.7043 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6941 Loss_G: 0.7108 acc: 89.1%\n",
      "[EPOCH 3100] TEST ACC is : 77.0%\n",
      "[BATCH 121/149] Loss_D: 0.7207 Loss_G: 0.7103 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6764 Loss_G: 0.6966 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7075 Loss_G: 0.7178 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.7082 Loss_G: 0.7063 acc: 93.8%\n",
      "[BATCH 125/149] Loss_D: 0.6439 Loss_G: 0.7094 acc: 96.9%\n",
      "[BATCH 126/149] Loss_D: 0.6542 Loss_G: 0.7041 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.7254 Loss_G: 0.7191 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.7202 Loss_G: 0.7163 acc: 81.2%\n",
      "[BATCH 129/149] Loss_D: 0.6799 Loss_G: 0.7095 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6525 Loss_G: 0.6873 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.7041 Loss_G: 0.7178 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6760 Loss_G: 0.7106 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.7136 Loss_G: 0.7244 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.7024 Loss_G: 0.7205 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6816 Loss_G: 0.7073 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6983 Loss_G: 0.6919 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7073 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6711 Loss_G: 0.7053 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6476 Loss_G: 0.6973 acc: 98.4%\n",
      "[BATCH 140/149] Loss_D: 0.6676 Loss_G: 0.6887 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6799 Loss_G: 0.6995 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.6780 Loss_G: 0.6989 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6768 Loss_G: 0.6978 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6604 Loss_G: 0.7090 acc: 82.8%\n",
      "[BATCH 145/149] Loss_D: 0.6488 Loss_G: 0.7033 acc: 92.2%\n",
      "[BATCH 146/149] Loss_D: 0.6554 Loss_G: 0.6836 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6950 Loss_G: 0.6809 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6606 Loss_G: 0.6881 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.6866 Loss_G: 0.7062 acc: 95.3%\n",
      "-----THE [21/50] epoch end-----\n",
      "-----THE [22/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6445 Loss_G: 0.6867 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.7098 Loss_G: 0.6859 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.6895 Loss_G: 0.7057 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.6591 Loss_G: 0.7079 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.6713 Loss_G: 0.7116 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6607 Loss_G: 0.6927 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6759 Loss_G: 0.6879 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6563 Loss_G: 0.6935 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6834 Loss_G: 0.6860 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6526 Loss_G: 0.6952 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6559 Loss_G: 0.6857 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6731 Loss_G: 0.7078 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6839 Loss_G: 0.7191 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.6586 Loss_G: 0.6954 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6719 Loss_G: 0.6847 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6814 Loss_G: 0.6970 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.7304 Loss_G: 0.6819 acc: 76.6%\n",
      "[BATCH 18/149] Loss_D: 0.6854 Loss_G: 0.6923 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.7252 Loss_G: 0.7117 acc: 82.8%\n",
      "[BATCH 20/149] Loss_D: 0.7023 Loss_G: 0.7337 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6718 Loss_G: 0.7266 acc: 90.6%\n",
      "[EPOCH 3150] TEST ACC is : 77.1%\n",
      "[BATCH 22/149] Loss_D: 0.7014 Loss_G: 0.7317 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6546 Loss_G: 0.6966 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.7080 Loss_G: 0.6848 acc: 79.7%\n",
      "[BATCH 25/149] Loss_D: 0.6760 Loss_G: 0.6850 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6930 Loss_G: 0.7023 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7322 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7025 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.7014 Loss_G: 0.7172 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6834 Loss_G: 0.7064 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6787 Loss_G: 0.6964 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.6817 Loss_G: 0.6825 acc: 87.5%\n",
      "[BATCH 33/149] Loss_D: 0.6309 Loss_G: 0.6822 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.6701 Loss_G: 0.6800 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6882 Loss_G: 0.7008 acc: 96.9%\n",
      "[BATCH 36/149] Loss_D: 0.6731 Loss_G: 0.6950 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6532 Loss_G: 0.7029 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6670 Loss_G: 0.7036 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6387 Loss_G: 0.6801 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.7248 Loss_G: 0.7015 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6809 Loss_G: 0.7131 acc: 95.3%\n",
      "[BATCH 42/149] Loss_D: 0.6546 Loss_G: 0.6972 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6746 Loss_G: 0.7065 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6979 Loss_G: 0.7015 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6890 Loss_G: 0.7113 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6762 Loss_G: 0.7203 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6585 Loss_G: 0.7035 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6724 Loss_G: 0.6920 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.6486 Loss_G: 0.6741 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6667 Loss_G: 0.6942 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6646 Loss_G: 0.7131 acc: 95.3%\n",
      "[BATCH 52/149] Loss_D: 0.6895 Loss_G: 0.7091 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6705 Loss_G: 0.6948 acc: 84.4%\n",
      "[BATCH 54/149] Loss_D: 0.7077 Loss_G: 0.7021 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6874 Loss_G: 0.6943 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.7010 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6872 Loss_G: 0.7157 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6888 Loss_G: 0.6984 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6538 Loss_G: 0.7201 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6615 Loss_G: 0.7222 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6820 Loss_G: 0.7172 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.6806 Loss_G: 0.7063 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6303 Loss_G: 0.6795 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.6720 Loss_G: 0.6952 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6792 Loss_G: 0.6943 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6961 Loss_G: 0.7165 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6924 Loss_G: 0.7565 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6864 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6975 Loss_G: 0.6977 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6749 Loss_G: 0.6842 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.7024 Loss_G: 0.7126 acc: 90.6%\n",
      "[EPOCH 3200] TEST ACC is : 77.1%\n",
      "[BATCH 72/149] Loss_D: 0.6943 Loss_G: 0.7241 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.6641 Loss_G: 0.7116 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6473 Loss_G: 0.6852 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.6591 Loss_G: 0.7000 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7343 Loss_G: 0.7110 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6617 Loss_G: 0.6747 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.7005 Loss_G: 0.6779 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.7119 Loss_G: 0.7002 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6858 Loss_G: 0.7039 acc: 82.8%\n",
      "[BATCH 81/149] Loss_D: 0.6558 Loss_G: 0.7045 acc: 95.3%\n",
      "[BATCH 82/149] Loss_D: 0.6757 Loss_G: 0.6917 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.7185 Loss_G: 0.7258 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6563 Loss_G: 0.7012 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.7005 Loss_G: 0.6908 acc: 81.2%\n",
      "[BATCH 86/149] Loss_D: 0.6572 Loss_G: 0.7079 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6479 Loss_G: 0.6754 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6788 Loss_G: 0.6955 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6906 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6855 Loss_G: 0.6987 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6947 Loss_G: 0.6859 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.6808 Loss_G: 0.6931 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6916 Loss_G: 0.6971 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6753 Loss_G: 0.6921 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6454 Loss_G: 0.7012 acc: 96.9%\n",
      "[BATCH 96/149] Loss_D: 0.6808 Loss_G: 0.7001 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6664 Loss_G: 0.6799 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.6505 Loss_G: 0.6975 acc: 95.3%\n",
      "[BATCH 99/149] Loss_D: 0.7084 Loss_G: 0.6935 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6762 Loss_G: 0.6907 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.7080 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.7189 Loss_G: 0.7010 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7052 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6985 Loss_G: 0.7268 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6631 Loss_G: 0.7097 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6965 Loss_G: 0.6999 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.7233 Loss_G: 0.6991 acc: 93.8%\n",
      "[BATCH 108/149] Loss_D: 0.6447 Loss_G: 0.6906 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.7098 Loss_G: 0.7315 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.7334 Loss_G: 0.7347 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6852 Loss_G: 0.7320 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6771 Loss_G: 0.7286 acc: 92.2%\n",
      "[BATCH 113/149] Loss_D: 0.7084 Loss_G: 0.7321 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.7505 Loss_G: 0.7421 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.7301 Loss_G: 0.7468 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.6536 Loss_G: 0.7095 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.6935 Loss_G: 0.7195 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6678 Loss_G: 0.7202 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.6823 Loss_G: 0.7116 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6850 Loss_G: 0.7157 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7265 Loss_G: 0.6938 acc: 81.2%\n",
      "[EPOCH 3250] TEST ACC is : 76.4%\n",
      "[BATCH 122/149] Loss_D: 0.6901 Loss_G: 0.7167 acc: 96.9%\n",
      "[BATCH 123/149] Loss_D: 0.6842 Loss_G: 0.7008 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6833 Loss_G: 0.7123 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6766 Loss_G: 0.7081 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.6966 Loss_G: 0.7056 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.6905 Loss_G: 0.6988 acc: 82.8%\n",
      "[BATCH 128/149] Loss_D: 0.6563 Loss_G: 0.6938 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.7539 Loss_G: 0.7364 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6635 Loss_G: 0.7022 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6998 Loss_G: 0.7018 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.7066 Loss_G: 0.7034 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6631 Loss_G: 0.6977 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6451 Loss_G: 0.6931 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.7228 Loss_G: 0.6957 acc: 85.9%\n",
      "[BATCH 136/149] Loss_D: 0.7096 Loss_G: 0.6972 acc: 81.2%\n",
      "[BATCH 137/149] Loss_D: 0.7189 Loss_G: 0.7080 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6746 Loss_G: 0.6982 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.7136 Loss_G: 0.7270 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6662 Loss_G: 0.7254 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6656 Loss_G: 0.7062 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6733 Loss_G: 0.6977 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.7075 Loss_G: 0.7030 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6852 Loss_G: 0.6948 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7311 Loss_G: 0.7066 acc: 82.8%\n",
      "[BATCH 146/149] Loss_D: 0.7060 Loss_G: 0.7217 acc: 95.3%\n",
      "[BATCH 147/149] Loss_D: 0.6594 Loss_G: 0.7001 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.7200 Loss_G: 0.7282 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.6866 Loss_G: 0.7285 acc: 92.2%\n",
      "-----THE [22/50] epoch end-----\n",
      "-----THE [23/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6668 Loss_G: 0.6944 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7140 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6731 Loss_G: 0.7083 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.6888 Loss_G: 0.7013 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6635 Loss_G: 0.6862 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6824 Loss_G: 0.6791 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.7015 Loss_G: 0.6879 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6705 Loss_G: 0.6853 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6781 Loss_G: 0.6768 acc: 81.2%\n",
      "[BATCH 10/149] Loss_D: 0.6989 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7196 Loss_G: 0.6989 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.7079 Loss_G: 0.7128 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6358 Loss_G: 0.7184 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6768 Loss_G: 0.7031 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7059 Loss_G: 0.6985 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6576 Loss_G: 0.6979 acc: 96.9%\n",
      "[BATCH 17/149] Loss_D: 0.7335 Loss_G: 0.7158 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6808 Loss_G: 0.7070 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7122 Loss_G: 0.7269 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7330 Loss_G: 0.7529 acc: 93.8%\n",
      "[BATCH 21/149] Loss_D: 0.6685 Loss_G: 0.7176 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6772 Loss_G: 0.7052 acc: 84.4%\n",
      "[EPOCH 3300] TEST ACC is : 77.5%\n",
      "[BATCH 23/149] Loss_D: 0.7007 Loss_G: 0.6872 acc: 78.1%\n",
      "[BATCH 24/149] Loss_D: 0.6937 Loss_G: 0.7057 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.6757 Loss_G: 0.7069 acc: 84.4%\n",
      "[BATCH 26/149] Loss_D: 0.6676 Loss_G: 0.7194 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.7052 Loss_G: 0.7133 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6628 Loss_G: 0.6977 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6894 Loss_G: 0.6870 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6546 Loss_G: 0.6801 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6568 Loss_G: 0.6790 acc: 93.8%\n",
      "[BATCH 32/149] Loss_D: 0.6936 Loss_G: 0.6959 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6849 Loss_G: 0.7197 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6564 Loss_G: 0.7079 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6660 Loss_G: 0.7205 acc: 95.3%\n",
      "[BATCH 36/149] Loss_D: 0.7213 Loss_G: 0.7268 acc: 82.8%\n",
      "[BATCH 37/149] Loss_D: 0.6718 Loss_G: 0.7261 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.7070 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.7117 Loss_G: 0.7169 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.6540 Loss_G: 0.7200 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6496 Loss_G: 0.6975 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.6721 Loss_G: 0.6853 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.7223 Loss_G: 0.7119 acc: 82.8%\n",
      "[BATCH 44/149] Loss_D: 0.7070 Loss_G: 0.7444 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6972 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6773 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6749 Loss_G: 0.7076 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.7025 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6528 Loss_G: 0.6865 acc: 93.8%\n",
      "[BATCH 50/149] Loss_D: 0.6733 Loss_G: 0.7053 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6934 Loss_G: 0.7104 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6551 Loss_G: 0.7174 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6943 Loss_G: 0.7003 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6570 Loss_G: 0.6971 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6907 Loss_G: 0.6987 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6715 Loss_G: 0.6823 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.7078 Loss_G: 0.6860 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6782 Loss_G: 0.6895 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.7072 Loss_G: 0.6958 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6614 Loss_G: 0.7185 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6742 Loss_G: 0.7198 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6546 Loss_G: 0.6923 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6785 Loss_G: 0.6702 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.6549 Loss_G: 0.6816 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6627 Loss_G: 0.6731 acc: 84.4%\n",
      "[BATCH 66/149] Loss_D: 0.7043 Loss_G: 0.7158 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6636 Loss_G: 0.7152 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6922 Loss_G: 0.7347 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.6935 Loss_G: 0.7196 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6640 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6692 Loss_G: 0.6830 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.7018 Loss_G: 0.7036 acc: 87.5%\n",
      "[EPOCH 3350] TEST ACC is : 77.1%\n",
      "[BATCH 73/149] Loss_D: 0.6835 Loss_G: 0.7264 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.7166 Loss_G: 0.7400 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6901 Loss_G: 0.7235 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6786 Loss_G: 0.7154 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6806 Loss_G: 0.7053 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6816 Loss_G: 0.6977 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6745 Loss_G: 0.6811 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6820 Loss_G: 0.6990 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.7179 Loss_G: 0.7322 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6987 Loss_G: 0.7080 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6872 Loss_G: 0.7020 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6520 Loss_G: 0.6859 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.6818 Loss_G: 0.7082 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6785 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6836 Loss_G: 0.7442 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.6799 Loss_G: 0.7436 acc: 95.3%\n",
      "[BATCH 89/149] Loss_D: 0.6665 Loss_G: 0.7256 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6981 Loss_G: 0.7403 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6674 Loss_G: 0.7223 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6889 Loss_G: 0.7219 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.7492 Loss_G: 0.7053 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.6844 Loss_G: 0.7117 acc: 96.9%\n",
      "[BATCH 95/149] Loss_D: 0.6877 Loss_G: 0.7210 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.6743 Loss_G: 0.7089 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6784 Loss_G: 0.7214 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.7053 Loss_G: 0.7183 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6688 Loss_G: 0.7110 acc: 82.8%\n",
      "[BATCH 100/149] Loss_D: 0.7285 Loss_G: 0.7186 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.7023 Loss_G: 0.7051 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6725 Loss_G: 0.7075 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.6905 Loss_G: 0.7111 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6817 Loss_G: 0.7078 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6579 Loss_G: 0.7138 acc: 93.8%\n",
      "[BATCH 106/149] Loss_D: 0.7005 Loss_G: 0.7127 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.6720 Loss_G: 0.7108 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6501 Loss_G: 0.6946 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6573 Loss_G: 0.6964 acc: 92.2%\n",
      "[BATCH 110/149] Loss_D: 0.6471 Loss_G: 0.6835 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6814 Loss_G: 0.6840 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.7048 Loss_G: 0.7091 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.7040 Loss_G: 0.7239 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7132 Loss_G: 0.7136 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6597 Loss_G: 0.6976 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6829 Loss_G: 0.7199 acc: 96.9%\n",
      "[BATCH 117/149] Loss_D: 0.6617 Loss_G: 0.7156 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6983 Loss_G: 0.7076 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.7177 Loss_G: 0.7303 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6809 Loss_G: 0.6995 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.7001 Loss_G: 0.6865 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.6651 Loss_G: 0.6841 acc: 92.2%\n",
      "[EPOCH 3400] TEST ACC is : 77.1%\n",
      "[BATCH 123/149] Loss_D: 0.7056 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6690 Loss_G: 0.7087 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7019 Loss_G: 0.7080 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6857 Loss_G: 0.6899 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6740 Loss_G: 0.6881 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6889 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.7221 Loss_G: 0.7184 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.7225 Loss_G: 0.7167 acc: 78.1%\n",
      "[BATCH 131/149] Loss_D: 0.6954 Loss_G: 0.7240 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6789 Loss_G: 0.7141 acc: 93.8%\n",
      "[BATCH 133/149] Loss_D: 0.6435 Loss_G: 0.7219 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.6640 Loss_G: 0.7112 acc: 95.3%\n",
      "[BATCH 135/149] Loss_D: 0.6618 Loss_G: 0.6835 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.7068 Loss_G: 0.6899 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.6813 Loss_G: 0.7005 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6770 Loss_G: 0.6870 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6622 Loss_G: 0.6828 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.7013 Loss_G: 0.6928 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6746 Loss_G: 0.7110 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.6348 Loss_G: 0.6915 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.6857 Loss_G: 0.6896 acc: 84.4%\n",
      "[BATCH 144/149] Loss_D: 0.6660 Loss_G: 0.7009 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6733 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6743 Loss_G: 0.6877 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6812 Loss_G: 0.7432 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.7140 Loss_G: 0.7077 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.6698 Loss_G: 0.6989 acc: 90.6%\n",
      "-----THE [23/50] epoch end-----\n",
      "-----THE [24/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6797 Loss_G: 0.6826 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.7269 Loss_G: 0.6826 acc: 79.7%\n",
      "[BATCH 3/149] Loss_D: 0.6746 Loss_G: 0.6817 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6461 Loss_G: 0.7009 acc: 96.9%\n",
      "[BATCH 5/149] Loss_D: 0.6888 Loss_G: 0.7130 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6687 Loss_G: 0.6973 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.7163 Loss_G: 0.7145 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6887 Loss_G: 0.7065 acc: 81.2%\n",
      "[BATCH 9/149] Loss_D: 0.7298 Loss_G: 0.7205 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7082 Loss_G: 0.7152 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6487 Loss_G: 0.6881 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6646 Loss_G: 0.6936 acc: 93.8%\n",
      "[BATCH 13/149] Loss_D: 0.6685 Loss_G: 0.6749 acc: 84.4%\n",
      "[BATCH 14/149] Loss_D: 0.6705 Loss_G: 0.6832 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6970 Loss_G: 0.7163 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.6930 Loss_G: 0.7041 acc: 84.4%\n",
      "[BATCH 17/149] Loss_D: 0.6893 Loss_G: 0.7001 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.6731 Loss_G: 0.6850 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.7079 Loss_G: 0.6937 acc: 82.8%\n",
      "[BATCH 20/149] Loss_D: 0.7083 Loss_G: 0.7092 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6801 Loss_G: 0.7001 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6578 Loss_G: 0.6925 acc: 95.3%\n",
      "[BATCH 23/149] Loss_D: 0.6733 Loss_G: 0.6951 acc: 90.6%\n",
      "[EPOCH 3450] TEST ACC is : 76.6%\n",
      "[BATCH 24/149] Loss_D: 0.6768 Loss_G: 0.7040 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.7047 Loss_G: 0.7102 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6817 Loss_G: 0.7071 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6820 Loss_G: 0.6959 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.6610 Loss_G: 0.7149 acc: 95.3%\n",
      "[BATCH 29/149] Loss_D: 0.6857 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6752 Loss_G: 0.6911 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6841 Loss_G: 0.7272 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6663 Loss_G: 0.7175 acc: 93.8%\n",
      "[BATCH 33/149] Loss_D: 0.7056 Loss_G: 0.7115 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6704 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 35/149] Loss_D: 0.6940 Loss_G: 0.6914 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6800 Loss_G: 0.6993 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6747 Loss_G: 0.6953 acc: 84.4%\n",
      "[BATCH 38/149] Loss_D: 0.6559 Loss_G: 0.7098 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.6946 Loss_G: 0.6884 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6867 Loss_G: 0.6938 acc: 84.4%\n",
      "[BATCH 41/149] Loss_D: 0.6759 Loss_G: 0.7002 acc: 96.9%\n",
      "[BATCH 42/149] Loss_D: 0.6987 Loss_G: 0.7022 acc: 84.4%\n",
      "[BATCH 43/149] Loss_D: 0.6578 Loss_G: 0.6882 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6940 Loss_G: 0.6971 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.6773 Loss_G: 0.7036 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.7152 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6975 Loss_G: 0.7724 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6845 Loss_G: 0.7176 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.7415 Loss_G: 0.7096 acc: 81.2%\n",
      "[BATCH 50/149] Loss_D: 0.7016 Loss_G: 0.7371 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.7261 Loss_G: 0.7411 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.7070 Loss_G: 0.7010 acc: 84.4%\n",
      "[BATCH 53/149] Loss_D: 0.6514 Loss_G: 0.7006 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6742 Loss_G: 0.7101 acc: 98.4%\n",
      "[BATCH 55/149] Loss_D: 0.6973 Loss_G: 0.6968 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7076 Loss_G: 0.7226 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6886 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7135 Loss_G: 0.7228 acc: 92.2%\n",
      "[BATCH 59/149] Loss_D: 0.6941 Loss_G: 0.7163 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7008 Loss_G: 0.7070 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.7024 Loss_G: 0.6850 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7002 Loss_G: 0.7018 acc: 81.2%\n",
      "[BATCH 63/149] Loss_D: 0.6948 Loss_G: 0.7176 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6602 Loss_G: 0.7117 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6391 Loss_G: 0.6864 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6826 Loss_G: 0.6841 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6705 Loss_G: 0.6837 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6643 Loss_G: 0.6823 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6917 Loss_G: 0.7113 acc: 93.8%\n",
      "[BATCH 70/149] Loss_D: 0.6650 Loss_G: 0.7052 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6688 Loss_G: 0.7012 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6605 Loss_G: 0.6944 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6713 Loss_G: 0.6956 acc: 84.4%\n",
      "[EPOCH 3500] TEST ACC is : 77.3%\n",
      "[BATCH 74/149] Loss_D: 0.6834 Loss_G: 0.6962 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.7078 Loss_G: 0.7193 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6666 Loss_G: 0.7200 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6786 Loss_G: 0.7077 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6952 Loss_G: 0.6978 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6548 Loss_G: 0.6859 acc: 95.3%\n",
      "[BATCH 80/149] Loss_D: 0.6512 Loss_G: 0.6916 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6962 Loss_G: 0.6901 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.6564 Loss_G: 0.7033 acc: 93.8%\n",
      "[BATCH 83/149] Loss_D: 0.7045 Loss_G: 0.6873 acc: 81.2%\n",
      "[BATCH 84/149] Loss_D: 0.7169 Loss_G: 0.7025 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6961 Loss_G: 0.6883 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.6519 Loss_G: 0.6897 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.7185 Loss_G: 0.7136 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.6734 Loss_G: 0.7023 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6554 Loss_G: 0.7098 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6931 Loss_G: 0.6907 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6876 Loss_G: 0.7121 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6845 Loss_G: 0.7016 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.7099 Loss_G: 0.7034 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.7350 Loss_G: 0.7471 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6918 Loss_G: 0.7219 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6702 Loss_G: 0.7257 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6830 Loss_G: 0.7177 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6790 Loss_G: 0.7319 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7036 Loss_G: 0.7472 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6530 Loss_G: 0.7215 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.6738 Loss_G: 0.7243 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6892 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.7404 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.6493 Loss_G: 0.6923 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6603 Loss_G: 0.6990 acc: 87.5%\n",
      "[BATCH 106/149] Loss_D: 0.6673 Loss_G: 0.6999 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6795 Loss_G: 0.7187 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6623 Loss_G: 0.6938 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6586 Loss_G: 0.6926 acc: 93.8%\n",
      "[BATCH 110/149] Loss_D: 0.6634 Loss_G: 0.6875 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.6822 Loss_G: 0.6874 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6831 Loss_G: 0.6882 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7345 Loss_G: 0.7240 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6739 Loss_G: 0.6863 acc: 81.2%\n",
      "[BATCH 115/149] Loss_D: 0.7247 Loss_G: 0.6999 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6939 Loss_G: 0.7056 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.6950 Loss_G: 0.7010 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.6578 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7181 Loss_G: 0.7030 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.7095 Loss_G: 0.7325 acc: 95.3%\n",
      "[BATCH 121/149] Loss_D: 0.6689 Loss_G: 0.7039 acc: 82.8%\n",
      "[BATCH 122/149] Loss_D: 0.6724 Loss_G: 0.7073 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.6923 Loss_G: 0.7171 acc: 92.2%\n",
      "[EPOCH 3550] TEST ACC is : 77.7%\n",
      "[BATCH 124/149] Loss_D: 0.7031 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6592 Loss_G: 0.7247 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6788 Loss_G: 0.6933 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6824 Loss_G: 0.6879 acc: 81.2%\n",
      "[BATCH 128/149] Loss_D: 0.6708 Loss_G: 0.6675 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6887 Loss_G: 0.6803 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.6819 Loss_G: 0.7001 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.7008 Loss_G: 0.7401 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6686 Loss_G: 0.7146 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6873 Loss_G: 0.7212 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6672 Loss_G: 0.6828 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.6958 Loss_G: 0.6888 acc: 81.2%\n",
      "[BATCH 136/149] Loss_D: 0.6783 Loss_G: 0.6977 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6735 Loss_G: 0.6969 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.6919 Loss_G: 0.7071 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.7039 Loss_G: 0.7153 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.6872 Loss_G: 0.7166 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6510 Loss_G: 0.7002 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6846 Loss_G: 0.7062 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.6681 Loss_G: 0.7070 acc: 89.1%\n",
      "[BATCH 144/149] Loss_D: 0.6779 Loss_G: 0.6893 acc: 81.2%\n",
      "[BATCH 145/149] Loss_D: 0.6739 Loss_G: 0.6938 acc: 93.8%\n",
      "[BATCH 146/149] Loss_D: 0.6718 Loss_G: 0.6945 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6874 Loss_G: 0.7024 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6697 Loss_G: 0.7009 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.6579 Loss_G: 0.6874 acc: 90.6%\n",
      "-----THE [24/50] epoch end-----\n",
      "-----THE [25/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6796 Loss_G: 0.6975 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.6478 Loss_G: 0.6761 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6907 Loss_G: 0.6826 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.6828 Loss_G: 0.6869 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6753 Loss_G: 0.7022 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6766 Loss_G: 0.7197 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6991 Loss_G: 0.7241 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6812 Loss_G: 0.7023 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7092 Loss_G: 0.7113 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.7322 Loss_G: 0.7146 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.7187 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.7472 Loss_G: 0.7722 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6896 Loss_G: 0.7362 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.7350 Loss_G: 0.7039 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.6762 Loss_G: 0.7005 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6803 Loss_G: 0.6886 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6600 Loss_G: 0.6923 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.7070 Loss_G: 0.7125 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.6629 Loss_G: 0.7090 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7132 Loss_G: 0.7048 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6872 Loss_G: 0.7010 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.6879 Loss_G: 0.7334 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.7358 Loss_G: 0.7525 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6739 Loss_G: 0.7096 acc: 84.4%\n",
      "[EPOCH 3600] TEST ACC is : 77.1%\n",
      "[BATCH 25/149] Loss_D: 0.6530 Loss_G: 0.7096 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6607 Loss_G: 0.6881 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6886 Loss_G: 0.6997 acc: 84.4%\n",
      "[BATCH 28/149] Loss_D: 0.6592 Loss_G: 0.7028 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6590 Loss_G: 0.6846 acc: 93.8%\n",
      "[BATCH 30/149] Loss_D: 0.7157 Loss_G: 0.7068 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6630 Loss_G: 0.7046 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.6544 Loss_G: 0.7146 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6686 Loss_G: 0.6894 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6893 Loss_G: 0.6906 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6571 Loss_G: 0.6868 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6538 Loss_G: 0.6685 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.6483 Loss_G: 0.6898 acc: 92.2%\n",
      "[BATCH 38/149] Loss_D: 0.6769 Loss_G: 0.6795 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6778 Loss_G: 0.6628 acc: 79.7%\n",
      "[BATCH 40/149] Loss_D: 0.7236 Loss_G: 0.6960 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6514 Loss_G: 0.6814 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7193 Loss_G: 0.7066 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6739 Loss_G: 0.7144 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.6729 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6473 Loss_G: 0.6742 acc: 82.8%\n",
      "[BATCH 46/149] Loss_D: 0.6805 Loss_G: 0.6886 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6897 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.7059 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6833 Loss_G: 0.7063 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6929 Loss_G: 0.6946 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6679 Loss_G: 0.6934 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6355 Loss_G: 0.6978 acc: 98.4%\n",
      "[BATCH 53/149] Loss_D: 0.6465 Loss_G: 0.7333 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.6813 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6928 Loss_G: 0.6994 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.6979 Loss_G: 0.7163 acc: 95.3%\n",
      "[BATCH 57/149] Loss_D: 0.6801 Loss_G: 0.6951 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6648 Loss_G: 0.6820 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6920 Loss_G: 0.6885 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.7141 Loss_G: 0.7180 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.7239 Loss_G: 0.7548 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6714 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7068 Loss_G: 0.7160 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.7093 Loss_G: 0.7070 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6889 Loss_G: 0.6993 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6952 Loss_G: 0.7042 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.7303 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7170 Loss_G: 0.7435 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6863 Loss_G: 0.7374 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6960 Loss_G: 0.7085 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.6583 Loss_G: 0.7074 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6538 Loss_G: 0.7102 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6805 Loss_G: 0.6894 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6737 Loss_G: 0.6842 acc: 92.2%\n",
      "[EPOCH 3650] TEST ACC is : 77.0%\n",
      "[BATCH 75/149] Loss_D: 0.7337 Loss_G: 0.7243 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7036 Loss_G: 0.7107 acc: 92.2%\n",
      "[BATCH 77/149] Loss_D: 0.6690 Loss_G: 0.6941 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6986 Loss_G: 0.7129 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6613 Loss_G: 0.7026 acc: 93.8%\n",
      "[BATCH 80/149] Loss_D: 0.6554 Loss_G: 0.6862 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.7019 Loss_G: 0.7066 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.7147 Loss_G: 0.7262 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6742 Loss_G: 0.6984 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6641 Loss_G: 0.7069 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.6805 Loss_G: 0.7058 acc: 93.8%\n",
      "[BATCH 86/149] Loss_D: 0.6746 Loss_G: 0.6994 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6679 Loss_G: 0.6896 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6798 Loss_G: 0.6970 acc: 79.7%\n",
      "[BATCH 89/149] Loss_D: 0.6979 Loss_G: 0.7248 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6704 Loss_G: 0.6966 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.7119 Loss_G: 0.7141 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7008 Loss_G: 0.7086 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6427 Loss_G: 0.6802 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6392 Loss_G: 0.6769 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6711 Loss_G: 0.7015 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6846 Loss_G: 0.6932 acc: 82.8%\n",
      "[BATCH 97/149] Loss_D: 0.6622 Loss_G: 0.6930 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6837 Loss_G: 0.7034 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6801 Loss_G: 0.6870 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6647 Loss_G: 0.6927 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.7493 Loss_G: 0.7268 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6820 Loss_G: 0.7114 acc: 84.4%\n",
      "[BATCH 103/149] Loss_D: 0.6578 Loss_G: 0.6990 acc: 96.9%\n",
      "[BATCH 104/149] Loss_D: 0.7033 Loss_G: 0.7255 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6585 Loss_G: 0.7025 acc: 93.8%\n",
      "[BATCH 106/149] Loss_D: 0.7058 Loss_G: 0.7175 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6805 Loss_G: 0.7173 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7005 Loss_G: 0.7083 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.7161 Loss_G: 0.7393 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.6812 Loss_G: 0.7301 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6904 Loss_G: 0.7250 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6555 Loss_G: 0.7234 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6906 Loss_G: 0.7215 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6589 Loss_G: 0.6988 acc: 82.8%\n",
      "[BATCH 115/149] Loss_D: 0.6871 Loss_G: 0.6926 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6750 Loss_G: 0.6818 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6780 Loss_G: 0.6979 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6782 Loss_G: 0.6998 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.6809 Loss_G: 0.7086 acc: 95.3%\n",
      "[BATCH 120/149] Loss_D: 0.6933 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6781 Loss_G: 0.6999 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6748 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.7107 Loss_G: 0.7538 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.7034 Loss_G: 0.7173 acc: 85.9%\n",
      "[EPOCH 3700] TEST ACC is : 75.4%\n",
      "[BATCH 125/149] Loss_D: 0.6863 Loss_G: 0.6986 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6769 Loss_G: 0.6893 acc: 81.2%\n",
      "[BATCH 127/149] Loss_D: 0.6772 Loss_G: 0.6863 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.6718 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.7068 Loss_G: 0.7232 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6796 Loss_G: 0.7072 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6675 Loss_G: 0.6962 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6950 Loss_G: 0.6955 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6802 Loss_G: 0.7064 acc: 93.8%\n",
      "[BATCH 134/149] Loss_D: 0.6616 Loss_G: 0.7189 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.6518 Loss_G: 0.6949 acc: 95.3%\n",
      "[BATCH 136/149] Loss_D: 0.6685 Loss_G: 0.6884 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6769 Loss_G: 0.6940 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6811 Loss_G: 0.6807 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6542 Loss_G: 0.6872 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7165 Loss_G: 0.6954 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6592 Loss_G: 0.6909 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6755 Loss_G: 0.6906 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6995 Loss_G: 0.6908 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6806 Loss_G: 0.6763 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7174 Loss_G: 0.7020 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.7105 Loss_G: 0.7105 acc: 84.4%\n",
      "[BATCH 147/149] Loss_D: 0.6751 Loss_G: 0.7143 acc: 95.3%\n",
      "[BATCH 148/149] Loss_D: 0.6454 Loss_G: 0.7087 acc: 95.3%\n",
      "[BATCH 149/149] Loss_D: 0.6701 Loss_G: 0.6794 acc: 79.7%\n",
      "-----THE [25/50] epoch end-----\n",
      "-----THE [26/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6899 Loss_G: 0.7028 acc: 87.5%\n",
      "[BATCH 2/149] Loss_D: 0.6944 Loss_G: 0.7389 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.6601 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6904 Loss_G: 0.7185 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6579 Loss_G: 0.7108 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.6550 Loss_G: 0.6797 acc: 87.5%\n",
      "[BATCH 7/149] Loss_D: 0.6764 Loss_G: 0.6830 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6834 Loss_G: 0.7090 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6883 Loss_G: 0.7123 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6538 Loss_G: 0.6836 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6913 Loss_G: 0.6791 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7301 Loss_G: 0.7062 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.7132 Loss_G: 0.7103 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.6935 Loss_G: 0.7010 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.7158 Loss_G: 0.6972 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6738 Loss_G: 0.6882 acc: 93.8%\n",
      "[BATCH 17/149] Loss_D: 0.6815 Loss_G: 0.6946 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6480 Loss_G: 0.6884 acc: 96.9%\n",
      "[BATCH 19/149] Loss_D: 0.6646 Loss_G: 0.6930 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6818 Loss_G: 0.6968 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6913 Loss_G: 0.6975 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6636 Loss_G: 0.6889 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.7094 Loss_G: 0.7111 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.6786 Loss_G: 0.7268 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6987 Loss_G: 0.7341 acc: 90.6%\n",
      "[EPOCH 3750] TEST ACC is : 76.2%\n",
      "[BATCH 26/149] Loss_D: 0.6805 Loss_G: 0.7077 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7233 Loss_G: 0.7188 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6651 Loss_G: 0.7438 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6761 Loss_G: 0.7243 acc: 95.3%\n",
      "[BATCH 30/149] Loss_D: 0.7018 Loss_G: 0.6947 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6855 Loss_G: 0.7280 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.6453 Loss_G: 0.6905 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6874 Loss_G: 0.6935 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6763 Loss_G: 0.6900 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6979 Loss_G: 0.7097 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.7115 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6774 Loss_G: 0.6955 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.6786 Loss_G: 0.6921 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6717 Loss_G: 0.7081 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.6590 Loss_G: 0.6868 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6682 Loss_G: 0.7019 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.6628 Loss_G: 0.6960 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.6558 Loss_G: 0.6802 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7500 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6506 Loss_G: 0.7042 acc: 95.3%\n",
      "[BATCH 46/149] Loss_D: 0.6690 Loss_G: 0.7125 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6524 Loss_G: 0.6897 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7029 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.6718 Loss_G: 0.7150 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.6801 Loss_G: 0.7205 acc: 93.8%\n",
      "[BATCH 51/149] Loss_D: 0.6742 Loss_G: 0.7054 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.6328 Loss_G: 0.6836 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6820 Loss_G: 0.6940 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6987 Loss_G: 0.7033 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7049 Loss_G: 0.7186 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7141 Loss_G: 0.7257 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6715 Loss_G: 0.7224 acc: 81.2%\n",
      "[BATCH 58/149] Loss_D: 0.6660 Loss_G: 0.6960 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6785 Loss_G: 0.7002 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6978 Loss_G: 0.7028 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6729 Loss_G: 0.6865 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6690 Loss_G: 0.6864 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6913 Loss_G: 0.6994 acc: 81.2%\n",
      "[BATCH 64/149] Loss_D: 0.6673 Loss_G: 0.6901 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6709 Loss_G: 0.6939 acc: 95.3%\n",
      "[BATCH 66/149] Loss_D: 0.7072 Loss_G: 0.7202 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6993 Loss_G: 0.7483 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6837 Loss_G: 0.7297 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6701 Loss_G: 0.7033 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6973 Loss_G: 0.7052 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.6942 Loss_G: 0.7085 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6725 Loss_G: 0.7099 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.6765 Loss_G: 0.7053 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7128 Loss_G: 0.6959 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6623 Loss_G: 0.6876 acc: 93.8%\n",
      "[EPOCH 3800] TEST ACC is : 77.3%\n",
      "[BATCH 76/149] Loss_D: 0.6628 Loss_G: 0.6844 acc: 93.8%\n",
      "[BATCH 77/149] Loss_D: 0.6348 Loss_G: 0.6745 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6547 Loss_G: 0.6779 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.7502 Loss_G: 0.7175 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6671 Loss_G: 0.7072 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6565 Loss_G: 0.6884 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6745 Loss_G: 0.6923 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6758 Loss_G: 0.6920 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6844 Loss_G: 0.6969 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.6734 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6578 Loss_G: 0.7025 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6668 Loss_G: 0.6933 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6693 Loss_G: 0.6913 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.6381 Loss_G: 0.6893 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.7013 Loss_G: 0.7037 acc: 84.4%\n",
      "[BATCH 91/149] Loss_D: 0.6999 Loss_G: 0.6980 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6826 Loss_G: 0.7054 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7196 Loss_G: 0.7235 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.7378 Loss_G: 0.7328 acc: 76.6%\n",
      "[BATCH 95/149] Loss_D: 0.6601 Loss_G: 0.7323 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6964 Loss_G: 0.7018 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6653 Loss_G: 0.6837 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.7100 Loss_G: 0.7066 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.7792 Loss_G: 0.7509 acc: 79.7%\n",
      "[BATCH 100/149] Loss_D: 0.7038 Loss_G: 0.7414 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6672 Loss_G: 0.7075 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6982 Loss_G: 0.6899 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.7092 Loss_G: 0.7045 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6872 Loss_G: 0.6952 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7180 Loss_G: 0.6941 acc: 93.8%\n",
      "[BATCH 106/149] Loss_D: 0.6550 Loss_G: 0.6801 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6775 Loss_G: 0.6875 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6872 Loss_G: 0.7036 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.6459 Loss_G: 0.7274 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.6878 Loss_G: 0.6992 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6964 Loss_G: 0.6979 acc: 82.8%\n",
      "[BATCH 112/149] Loss_D: 0.7109 Loss_G: 0.6977 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6763 Loss_G: 0.6905 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.7009 Loss_G: 0.6913 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6595 Loss_G: 0.6943 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.6653 Loss_G: 0.6718 acc: 92.2%\n",
      "[BATCH 117/149] Loss_D: 0.7124 Loss_G: 0.6910 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.6666 Loss_G: 0.6963 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6715 Loss_G: 0.6979 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.6437 Loss_G: 0.6930 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.7049 Loss_G: 0.7034 acc: 92.2%\n",
      "[BATCH 122/149] Loss_D: 0.7051 Loss_G: 0.7103 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6816 Loss_G: 0.7018 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6693 Loss_G: 0.6945 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6822 Loss_G: 0.6884 acc: 85.9%\n",
      "[EPOCH 3850] TEST ACC is : 77.1%\n",
      "[BATCH 126/149] Loss_D: 0.6682 Loss_G: 0.7087 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6867 Loss_G: 0.7169 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6450 Loss_G: 0.7089 acc: 92.2%\n",
      "[BATCH 129/149] Loss_D: 0.6428 Loss_G: 0.6817 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6708 Loss_G: 0.6984 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6612 Loss_G: 0.7202 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6816 Loss_G: 0.6824 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7069 Loss_G: 0.7170 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6938 Loss_G: 0.7265 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6732 Loss_G: 0.7238 acc: 95.3%\n",
      "[BATCH 136/149] Loss_D: 0.6903 Loss_G: 0.7025 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.6454 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6931 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7093 Loss_G: 0.7685 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.7273 Loss_G: 0.7419 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6611 Loss_G: 0.7221 acc: 95.3%\n",
      "[BATCH 142/149] Loss_D: 0.7080 Loss_G: 0.7026 acc: 92.2%\n",
      "[BATCH 143/149] Loss_D: 0.7031 Loss_G: 0.7042 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.6826 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 145/149] Loss_D: 0.6938 Loss_G: 0.7015 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.6812 Loss_G: 0.7079 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7039 Loss_G: 0.7033 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6974 Loss_G: 0.7159 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.7214 Loss_G: 0.7482 acc: 85.9%\n",
      "-----THE [26/50] epoch end-----\n",
      "-----THE [27/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6790 Loss_G: 0.7043 acc: 82.8%\n",
      "[BATCH 2/149] Loss_D: 0.6864 Loss_G: 0.6768 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.6743 Loss_G: 0.6892 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6803 Loss_G: 0.7023 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6978 Loss_G: 0.7463 acc: 93.8%\n",
      "[BATCH 6/149] Loss_D: 0.6863 Loss_G: 0.7299 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6557 Loss_G: 0.6809 acc: 81.2%\n",
      "[BATCH 8/149] Loss_D: 0.6891 Loss_G: 0.6814 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.7085 Loss_G: 0.6833 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6695 Loss_G: 0.6870 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.6955 Loss_G: 0.7029 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6720 Loss_G: 0.7045 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6451 Loss_G: 0.7014 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6936 Loss_G: 0.7030 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6776 Loss_G: 0.6946 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6868 Loss_G: 0.7006 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6948 Loss_G: 0.6965 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6409 Loss_G: 0.6887 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6466 Loss_G: 0.7044 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.6533 Loss_G: 0.7386 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6862 Loss_G: 0.7010 acc: 84.4%\n",
      "[BATCH 22/149] Loss_D: 0.6819 Loss_G: 0.6944 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.7108 Loss_G: 0.6850 acc: 84.4%\n",
      "[BATCH 24/149] Loss_D: 0.6859 Loss_G: 0.6946 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7059 Loss_G: 0.7275 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.7170 Loss_G: 0.7054 acc: 85.9%\n",
      "[EPOCH 3900] TEST ACC is : 77.1%\n",
      "[BATCH 27/149] Loss_D: 0.6966 Loss_G: 0.7244 acc: 95.3%\n",
      "[BATCH 28/149] Loss_D: 0.6688 Loss_G: 0.7250 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6538 Loss_G: 0.7021 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6743 Loss_G: 0.6708 acc: 81.2%\n",
      "[BATCH 31/149] Loss_D: 0.6568 Loss_G: 0.6802 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6917 Loss_G: 0.6997 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6563 Loss_G: 0.6993 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6999 Loss_G: 0.7150 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6796 Loss_G: 0.7004 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.6790 Loss_G: 0.6950 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6844 Loss_G: 0.6964 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.7027 Loss_G: 0.7345 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.6855 Loss_G: 0.6982 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6787 Loss_G: 0.7031 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6845 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6780 Loss_G: 0.6985 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6770 Loss_G: 0.6931 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6958 Loss_G: 0.7082 acc: 93.8%\n",
      "[BATCH 45/149] Loss_D: 0.6475 Loss_G: 0.6900 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6751 Loss_G: 0.6861 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6708 Loss_G: 0.6855 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.7086 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.7143 Loss_G: 0.7319 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.6572 Loss_G: 0.6933 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6600 Loss_G: 0.6904 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.6970 Loss_G: 0.7005 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6826 Loss_G: 0.7217 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6925 Loss_G: 0.7440 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6620 Loss_G: 0.7053 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6531 Loss_G: 0.7095 acc: 95.3%\n",
      "[BATCH 57/149] Loss_D: 0.6702 Loss_G: 0.7210 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6906 Loss_G: 0.7168 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6906 Loss_G: 0.6910 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7141 Loss_G: 0.6870 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6818 Loss_G: 0.6840 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6687 Loss_G: 0.6943 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.7011 Loss_G: 0.7166 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6897 Loss_G: 0.7300 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7088 Loss_G: 0.7403 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.7022 Loss_G: 0.7353 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6759 Loss_G: 0.7309 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.7380 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6665 Loss_G: 0.6927 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6741 Loss_G: 0.7050 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6571 Loss_G: 0.6777 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6971 Loss_G: 0.6889 acc: 89.1%\n",
      "[BATCH 73/149] Loss_D: 0.6959 Loss_G: 0.7025 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6799 Loss_G: 0.7056 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6871 Loss_G: 0.6990 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6888 Loss_G: 0.6963 acc: 84.4%\n",
      "[EPOCH 3950] TEST ACC is : 77.5%\n",
      "[BATCH 77/149] Loss_D: 0.7117 Loss_G: 0.6938 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.6811 Loss_G: 0.6983 acc: 95.3%\n",
      "[BATCH 79/149] Loss_D: 0.6749 Loss_G: 0.6886 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6715 Loss_G: 0.6906 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6560 Loss_G: 0.6855 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6676 Loss_G: 0.6840 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6930 Loss_G: 0.6867 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.7006 Loss_G: 0.7074 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6945 Loss_G: 0.7079 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6451 Loss_G: 0.6925 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.7022 Loss_G: 0.6887 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6976 Loss_G: 0.7067 acc: 93.8%\n",
      "[BATCH 89/149] Loss_D: 0.6892 Loss_G: 0.6870 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.6880 Loss_G: 0.6880 acc: 90.6%\n",
      "[BATCH 91/149] Loss_D: 0.6717 Loss_G: 0.6796 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.7176 Loss_G: 0.6886 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6620 Loss_G: 0.7076 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6617 Loss_G: 0.6981 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6655 Loss_G: 0.6756 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7067 Loss_G: 0.6787 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6950 Loss_G: 0.7004 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6950 Loss_G: 0.7176 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6783 Loss_G: 0.7049 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.6801 Loss_G: 0.7176 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6745 Loss_G: 0.7126 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6788 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6801 Loss_G: 0.6847 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.7002 Loss_G: 0.7015 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.6492 Loss_G: 0.7204 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6598 Loss_G: 0.7035 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.6760 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6518 Loss_G: 0.7216 acc: 92.2%\n",
      "[BATCH 109/149] Loss_D: 0.6914 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.7067 Loss_G: 0.7044 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6478 Loss_G: 0.6966 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.6559 Loss_G: 0.7008 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6684 Loss_G: 0.7166 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6763 Loss_G: 0.7100 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6666 Loss_G: 0.6934 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.6857 Loss_G: 0.6869 acc: 81.2%\n",
      "[BATCH 117/149] Loss_D: 0.6846 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6891 Loss_G: 0.7125 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6972 Loss_G: 0.7082 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.6823 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7125 Loss_G: 0.7207 acc: 95.3%\n",
      "[BATCH 122/149] Loss_D: 0.7219 Loss_G: 0.7353 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6950 Loss_G: 0.7201 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6865 Loss_G: 0.7312 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6609 Loss_G: 0.6954 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6816 Loss_G: 0.6815 acc: 89.1%\n",
      "[EPOCH 4000] TEST ACC is : 76.4%\n",
      "[BATCH 127/149] Loss_D: 0.7352 Loss_G: 0.7358 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6945 Loss_G: 0.7155 acc: 93.8%\n",
      "[BATCH 129/149] Loss_D: 0.6859 Loss_G: 0.6948 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6982 Loss_G: 0.7037 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6714 Loss_G: 0.6976 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.6785 Loss_G: 0.6902 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.7029 Loss_G: 0.7100 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6710 Loss_G: 0.6941 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.7056 Loss_G: 0.7283 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6948 Loss_G: 0.7232 acc: 84.4%\n",
      "[BATCH 137/149] Loss_D: 0.7024 Loss_G: 0.7177 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6751 Loss_G: 0.6961 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6835 Loss_G: 0.7096 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6631 Loss_G: 0.6995 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6694 Loss_G: 0.6954 acc: 95.3%\n",
      "[BATCH 142/149] Loss_D: 0.6949 Loss_G: 0.7062 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.6626 Loss_G: 0.6998 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6847 Loss_G: 0.7003 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7251 Loss_G: 0.7044 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7015 Loss_G: 0.7297 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6591 Loss_G: 0.6918 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6809 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7247 Loss_G: 0.6888 acc: 87.5%\n",
      "-----THE [27/50] epoch end-----\n",
      "-----THE [28/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6918 Loss_G: 0.7062 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.7029 Loss_G: 0.6990 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6883 Loss_G: 0.7018 acc: 85.9%\n",
      "[BATCH 4/149] Loss_D: 0.6882 Loss_G: 0.6881 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.6834 Loss_G: 0.6982 acc: 93.8%\n",
      "[BATCH 6/149] Loss_D: 0.6649 Loss_G: 0.6833 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6654 Loss_G: 0.6934 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.7116 Loss_G: 0.7091 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6752 Loss_G: 0.6929 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6946 Loss_G: 0.7100 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6691 Loss_G: 0.7093 acc: 84.4%\n",
      "[BATCH 12/149] Loss_D: 0.7001 Loss_G: 0.7105 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6828 Loss_G: 0.6951 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6965 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6946 Loss_G: 0.6900 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6742 Loss_G: 0.6994 acc: 93.8%\n",
      "[BATCH 17/149] Loss_D: 0.7201 Loss_G: 0.7219 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.7118 Loss_G: 0.7481 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.6578 Loss_G: 0.7047 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6993 Loss_G: 0.6975 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6618 Loss_G: 0.6925 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6833 Loss_G: 0.6842 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6938 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6937 Loss_G: 0.7104 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.6701 Loss_G: 0.7022 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.7124 Loss_G: 0.7126 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6642 Loss_G: 0.7079 acc: 85.9%\n",
      "[EPOCH 4050] TEST ACC is : 76.8%\n",
      "[BATCH 28/149] Loss_D: 0.6817 Loss_G: 0.7110 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.6333 Loss_G: 0.6842 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6706 Loss_G: 0.6939 acc: 93.8%\n",
      "[BATCH 31/149] Loss_D: 0.7055 Loss_G: 0.7062 acc: 87.5%\n",
      "[BATCH 32/149] Loss_D: 0.6952 Loss_G: 0.7412 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6802 Loss_G: 0.7268 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7245 Loss_G: 0.7171 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.7012 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6947 Loss_G: 0.7062 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.7037 Loss_G: 0.7276 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6551 Loss_G: 0.6918 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6661 Loss_G: 0.6855 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6970 Loss_G: 0.6986 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6791 Loss_G: 0.7068 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.6708 Loss_G: 0.6894 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6782 Loss_G: 0.7022 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6934 Loss_G: 0.7033 acc: 89.1%\n",
      "[BATCH 45/149] Loss_D: 0.6961 Loss_G: 0.7022 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7123 Loss_G: 0.7185 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.7016 Loss_G: 0.7259 acc: 85.9%\n",
      "[BATCH 48/149] Loss_D: 0.6701 Loss_G: 0.6995 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6929 Loss_G: 0.7110 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6829 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6638 Loss_G: 0.6848 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.6803 Loss_G: 0.7072 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6469 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6833 Loss_G: 0.6934 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6527 Loss_G: 0.7181 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6577 Loss_G: 0.7015 acc: 95.3%\n",
      "[BATCH 57/149] Loss_D: 0.6957 Loss_G: 0.7015 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6768 Loss_G: 0.6901 acc: 82.8%\n",
      "[BATCH 59/149] Loss_D: 0.7320 Loss_G: 0.7071 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6883 Loss_G: 0.6997 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6680 Loss_G: 0.6981 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6711 Loss_G: 0.7125 acc: 93.8%\n",
      "[BATCH 63/149] Loss_D: 0.6678 Loss_G: 0.6842 acc: 90.6%\n",
      "[BATCH 64/149] Loss_D: 0.6964 Loss_G: 0.7055 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6659 Loss_G: 0.6915 acc: 90.6%\n",
      "[BATCH 66/149] Loss_D: 0.7013 Loss_G: 0.6903 acc: 81.2%\n",
      "[BATCH 67/149] Loss_D: 0.7046 Loss_G: 0.6816 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.7072 Loss_G: 0.6940 acc: 81.2%\n",
      "[BATCH 69/149] Loss_D: 0.6911 Loss_G: 0.6982 acc: 82.8%\n",
      "[BATCH 70/149] Loss_D: 0.6851 Loss_G: 0.6995 acc: 98.4%\n",
      "[BATCH 71/149] Loss_D: 0.6672 Loss_G: 0.7096 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.7157 Loss_G: 0.7217 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6953 Loss_G: 0.7035 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6425 Loss_G: 0.7037 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7115 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6890 Loss_G: 0.7122 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6657 Loss_G: 0.7052 acc: 87.5%\n",
      "[EPOCH 4100] TEST ACC is : 77.1%\n",
      "[BATCH 78/149] Loss_D: 0.6656 Loss_G: 0.6819 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6752 Loss_G: 0.6894 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6971 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.7027 Loss_G: 0.7040 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6766 Loss_G: 0.7138 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6546 Loss_G: 0.6845 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6595 Loss_G: 0.6999 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6658 Loss_G: 0.7042 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6906 Loss_G: 0.7346 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6708 Loss_G: 0.7069 acc: 93.8%\n",
      "[BATCH 88/149] Loss_D: 0.6805 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6862 Loss_G: 0.7405 acc: 95.3%\n",
      "[BATCH 90/149] Loss_D: 0.7179 Loss_G: 0.7010 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.7033 Loss_G: 0.7076 acc: 84.4%\n",
      "[BATCH 92/149] Loss_D: 0.6897 Loss_G: 0.7075 acc: 81.2%\n",
      "[BATCH 93/149] Loss_D: 0.7085 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6851 Loss_G: 0.7226 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6790 Loss_G: 0.7393 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.7174 Loss_G: 0.7189 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6579 Loss_G: 0.7280 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6791 Loss_G: 0.7106 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6965 Loss_G: 0.6992 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6861 Loss_G: 0.6989 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.7229 Loss_G: 0.7051 acc: 82.8%\n",
      "[BATCH 102/149] Loss_D: 0.6766 Loss_G: 0.7234 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6718 Loss_G: 0.7016 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6903 Loss_G: 0.7280 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6849 Loss_G: 0.7128 acc: 82.8%\n",
      "[BATCH 106/149] Loss_D: 0.6871 Loss_G: 0.7072 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6878 Loss_G: 0.7071 acc: 78.1%\n",
      "[BATCH 108/149] Loss_D: 0.7028 Loss_G: 0.7141 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6819 Loss_G: 0.7194 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6660 Loss_G: 0.6995 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.7191 Loss_G: 0.7033 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6976 Loss_G: 0.7092 acc: 93.8%\n",
      "[BATCH 113/149] Loss_D: 0.6784 Loss_G: 0.6956 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6861 Loss_G: 0.7260 acc: 95.3%\n",
      "[BATCH 115/149] Loss_D: 0.6493 Loss_G: 0.6834 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.7011 Loss_G: 0.6848 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6769 Loss_G: 0.6993 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.7037 Loss_G: 0.7128 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.6810 Loss_G: 0.7021 acc: 81.2%\n",
      "[BATCH 120/149] Loss_D: 0.7288 Loss_G: 0.7257 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6536 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6634 Loss_G: 0.7055 acc: 93.8%\n",
      "[BATCH 123/149] Loss_D: 0.6745 Loss_G: 0.7161 acc: 85.9%\n",
      "[BATCH 124/149] Loss_D: 0.6873 Loss_G: 0.7283 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6530 Loss_G: 0.6883 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6833 Loss_G: 0.6973 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.6713 Loss_G: 0.6987 acc: 84.4%\n",
      "[EPOCH 4150] TEST ACC is : 76.4%\n",
      "[BATCH 128/149] Loss_D: 0.6766 Loss_G: 0.7027 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6402 Loss_G: 0.6962 acc: 93.8%\n",
      "[BATCH 130/149] Loss_D: 0.6323 Loss_G: 0.6913 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6745 Loss_G: 0.7265 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6823 Loss_G: 0.7232 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6949 Loss_G: 0.6967 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.6923 Loss_G: 0.7100 acc: 78.1%\n",
      "[BATCH 135/149] Loss_D: 0.6629 Loss_G: 0.6781 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6471 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6774 Loss_G: 0.6922 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6760 Loss_G: 0.7081 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6494 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6590 Loss_G: 0.6914 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6962 Loss_G: 0.6897 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.7099 Loss_G: 0.7049 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6798 Loss_G: 0.6988 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6907 Loss_G: 0.6984 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.7170 Loss_G: 0.7019 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.7018 Loss_G: 0.7272 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.6377 Loss_G: 0.7076 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.6738 Loss_G: 0.7064 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6739 Loss_G: 0.7017 acc: 89.1%\n",
      "-----THE [28/50] epoch end-----\n",
      "-----THE [29/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7168 Loss_G: 0.7048 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6570 Loss_G: 0.6938 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6908 Loss_G: 0.6997 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.6563 Loss_G: 0.6900 acc: 84.4%\n",
      "[BATCH 5/149] Loss_D: 0.6881 Loss_G: 0.6663 acc: 79.7%\n",
      "[BATCH 6/149] Loss_D: 0.6871 Loss_G: 0.6895 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6783 Loss_G: 0.6985 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6808 Loss_G: 0.7007 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6712 Loss_G: 0.7023 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6836 Loss_G: 0.7032 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6886 Loss_G: 0.6906 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.6511 Loss_G: 0.6887 acc: 92.2%\n",
      "[BATCH 13/149] Loss_D: 0.6656 Loss_G: 0.6951 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6822 Loss_G: 0.7289 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.7050 Loss_G: 0.7195 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6984 Loss_G: 0.7245 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.7071 Loss_G: 0.7292 acc: 93.8%\n",
      "[BATCH 18/149] Loss_D: 0.6704 Loss_G: 0.7040 acc: 87.5%\n",
      "[BATCH 19/149] Loss_D: 0.7208 Loss_G: 0.7025 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6663 Loss_G: 0.7071 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6880 Loss_G: 0.7062 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6744 Loss_G: 0.7269 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6547 Loss_G: 0.7035 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6758 Loss_G: 0.6952 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.6796 Loss_G: 0.7106 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6902 Loss_G: 0.7142 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6996 Loss_G: 0.7114 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7185 Loss_G: 0.6992 acc: 84.4%\n",
      "[EPOCH 4200] TEST ACC is : 77.0%\n",
      "[BATCH 29/149] Loss_D: 0.6598 Loss_G: 0.6975 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6671 Loss_G: 0.6946 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.6857 Loss_G: 0.6838 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6779 Loss_G: 0.6965 acc: 82.8%\n",
      "[BATCH 33/149] Loss_D: 0.6763 Loss_G: 0.6965 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.7022 Loss_G: 0.7068 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7076 Loss_G: 0.7057 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6593 Loss_G: 0.7132 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6928 Loss_G: 0.7075 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7000 Loss_G: 0.7053 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.7272 Loss_G: 0.7161 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.6604 Loss_G: 0.6843 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.7203 Loss_G: 0.6988 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6920 Loss_G: 0.6968 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.7109 Loss_G: 0.7134 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6852 Loss_G: 0.7206 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6735 Loss_G: 0.6999 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6719 Loss_G: 0.6902 acc: 96.9%\n",
      "[BATCH 47/149] Loss_D: 0.7294 Loss_G: 0.7082 acc: 82.8%\n",
      "[BATCH 48/149] Loss_D: 0.6717 Loss_G: 0.7041 acc: 93.8%\n",
      "[BATCH 49/149] Loss_D: 0.6865 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6481 Loss_G: 0.6886 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6892 Loss_G: 0.7031 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6930 Loss_G: 0.6974 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.7023 Loss_G: 0.6960 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6815 Loss_G: 0.7185 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.7009 Loss_G: 0.7108 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.6655 Loss_G: 0.6935 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6413 Loss_G: 0.6910 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6883 Loss_G: 0.6824 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6990 Loss_G: 0.6761 acc: 82.8%\n",
      "[BATCH 60/149] Loss_D: 0.6672 Loss_G: 0.6852 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.7570 Loss_G: 0.7300 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6730 Loss_G: 0.7180 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6891 Loss_G: 0.7502 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.6938 Loss_G: 0.6937 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.7115 Loss_G: 0.7092 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6924 Loss_G: 0.7285 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.6712 Loss_G: 0.7262 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6596 Loss_G: 0.7027 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7174 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.7080 Loss_G: 0.6898 acc: 87.5%\n",
      "[BATCH 71/149] Loss_D: 0.6803 Loss_G: 0.6984 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6670 Loss_G: 0.6862 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6537 Loss_G: 0.6923 acc: 95.3%\n",
      "[BATCH 74/149] Loss_D: 0.6821 Loss_G: 0.6886 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6305 Loss_G: 0.6768 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6990 Loss_G: 0.6796 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7228 Loss_G: 0.7000 acc: 82.8%\n",
      "[BATCH 78/149] Loss_D: 0.6842 Loss_G: 0.6931 acc: 84.4%\n",
      "[EPOCH 4250] TEST ACC is : 77.9%\n",
      "[BATCH 79/149] Loss_D: 0.6988 Loss_G: 0.7249 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.6710 Loss_G: 0.7179 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.6635 Loss_G: 0.7178 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6934 Loss_G: 0.7111 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6551 Loss_G: 0.6851 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.7082 Loss_G: 0.7218 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.6810 Loss_G: 0.7587 acc: 96.9%\n",
      "[BATCH 86/149] Loss_D: 0.6643 Loss_G: 0.7252 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.6713 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6673 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6648 Loss_G: 0.6837 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.6684 Loss_G: 0.6977 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6548 Loss_G: 0.6959 acc: 95.3%\n",
      "[BATCH 92/149] Loss_D: 0.6868 Loss_G: 0.6779 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6866 Loss_G: 0.7097 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6693 Loss_G: 0.6899 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7075 Loss_G: 0.7174 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6607 Loss_G: 0.7016 acc: 98.4%\n",
      "[BATCH 97/149] Loss_D: 0.6923 Loss_G: 0.6951 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6606 Loss_G: 0.6974 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6564 Loss_G: 0.6893 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6794 Loss_G: 0.6906 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.6707 Loss_G: 0.6883 acc: 84.4%\n",
      "[BATCH 102/149] Loss_D: 0.6853 Loss_G: 0.6880 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6914 Loss_G: 0.7013 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6689 Loss_G: 0.7101 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7027 Loss_G: 0.6994 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.6948 Loss_G: 0.6921 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6687 Loss_G: 0.6979 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6645 Loss_G: 0.6999 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6901 Loss_G: 0.7201 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.6589 Loss_G: 0.7126 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.6731 Loss_G: 0.7196 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.6847 Loss_G: 0.6953 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6865 Loss_G: 0.6957 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6661 Loss_G: 0.7146 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.6831 Loss_G: 0.6940 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.6790 Loss_G: 0.6921 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.6810 Loss_G: 0.6887 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6555 Loss_G: 0.6946 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6985 Loss_G: 0.7176 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6749 Loss_G: 0.6934 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6753 Loss_G: 0.6945 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6766 Loss_G: 0.6951 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6809 Loss_G: 0.7066 acc: 95.3%\n",
      "[BATCH 124/149] Loss_D: 0.6949 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6851 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7086 Loss_G: 0.7259 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6833 Loss_G: 0.7108 acc: 82.8%\n",
      "[BATCH 128/149] Loss_D: 0.6878 Loss_G: 0.7033 acc: 82.8%\n",
      "[EPOCH 4300] TEST ACC is : 77.5%\n",
      "[BATCH 129/149] Loss_D: 0.6673 Loss_G: 0.7254 acc: 95.3%\n",
      "[BATCH 130/149] Loss_D: 0.6764 Loss_G: 0.7182 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6533 Loss_G: 0.6989 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.7168 Loss_G: 0.7189 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6952 Loss_G: 0.6926 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6580 Loss_G: 0.6879 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.7183 Loss_G: 0.7050 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6871 Loss_G: 0.7158 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6982 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6892 Loss_G: 0.6976 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6793 Loss_G: 0.6994 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6791 Loss_G: 0.7002 acc: 96.9%\n",
      "[BATCH 141/149] Loss_D: 0.6714 Loss_G: 0.7000 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.7049 Loss_G: 0.7315 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6488 Loss_G: 0.6923 acc: 96.9%\n",
      "[BATCH 144/149] Loss_D: 0.6806 Loss_G: 0.6887 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.6620 Loss_G: 0.7164 acc: 96.9%\n",
      "[BATCH 146/149] Loss_D: 0.7002 Loss_G: 0.7390 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.6965 Loss_G: 0.7419 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6920 Loss_G: 0.6886 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.7018 Loss_G: 0.7081 acc: 89.1%\n",
      "-----THE [29/50] epoch end-----\n",
      "-----THE [30/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6824 Loss_G: 0.6903 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6847 Loss_G: 0.7177 acc: 81.2%\n",
      "[BATCH 3/149] Loss_D: 0.6726 Loss_G: 0.7146 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6695 Loss_G: 0.6984 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6761 Loss_G: 0.7013 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.7181 Loss_G: 0.7255 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.7202 Loss_G: 0.6937 acc: 79.7%\n",
      "[BATCH 8/149] Loss_D: 0.6917 Loss_G: 0.6810 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.7044 Loss_G: 0.7081 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.7200 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.7124 Loss_G: 0.7186 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6618 Loss_G: 0.6981 acc: 82.8%\n",
      "[BATCH 13/149] Loss_D: 0.6892 Loss_G: 0.7074 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.6900 Loss_G: 0.7257 acc: 92.2%\n",
      "[BATCH 15/149] Loss_D: 0.7315 Loss_G: 0.7350 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6537 Loss_G: 0.7042 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6978 Loss_G: 0.6884 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6738 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6752 Loss_G: 0.6934 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6695 Loss_G: 0.6979 acc: 93.8%\n",
      "[BATCH 21/149] Loss_D: 0.6622 Loss_G: 0.6947 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6897 Loss_G: 0.6957 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6818 Loss_G: 0.6959 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6535 Loss_G: 0.6876 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6551 Loss_G: 0.6938 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6759 Loss_G: 0.7003 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6872 Loss_G: 0.7193 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6577 Loss_G: 0.6982 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6816 Loss_G: 0.7145 acc: 92.2%\n",
      "[EPOCH 4350] TEST ACC is : 77.1%\n",
      "[BATCH 30/149] Loss_D: 0.6968 Loss_G: 0.7014 acc: 90.6%\n",
      "[BATCH 31/149] Loss_D: 0.6779 Loss_G: 0.6915 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.6973 Loss_G: 0.7122 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6684 Loss_G: 0.7117 acc: 95.3%\n",
      "[BATCH 34/149] Loss_D: 0.6558 Loss_G: 0.6896 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6899 Loss_G: 0.7146 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6550 Loss_G: 0.6925 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6747 Loss_G: 0.6750 acc: 82.8%\n",
      "[BATCH 38/149] Loss_D: 0.6508 Loss_G: 0.6892 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6973 Loss_G: 0.7113 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.6431 Loss_G: 0.7043 acc: 95.3%\n",
      "[BATCH 41/149] Loss_D: 0.6603 Loss_G: 0.7024 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.6763 Loss_G: 0.7058 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6751 Loss_G: 0.6855 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6781 Loss_G: 0.6971 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.7013 Loss_G: 0.6925 acc: 78.1%\n",
      "[BATCH 46/149] Loss_D: 0.6684 Loss_G: 0.7006 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.7127 Loss_G: 0.7096 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6676 Loss_G: 0.7164 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.7203 Loss_G: 0.7221 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6992 Loss_G: 0.7134 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.7301 Loss_G: 0.7375 acc: 84.4%\n",
      "[BATCH 52/149] Loss_D: 0.6780 Loss_G: 0.7220 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6713 Loss_G: 0.7199 acc: 93.8%\n",
      "[BATCH 54/149] Loss_D: 0.7378 Loss_G: 0.7301 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.7099 Loss_G: 0.7194 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6762 Loss_G: 0.7153 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.6921 Loss_G: 0.7167 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6829 Loss_G: 0.7070 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6937 Loss_G: 0.7172 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6918 Loss_G: 0.7239 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7130 Loss_G: 0.7489 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6445 Loss_G: 0.7139 acc: 92.2%\n",
      "[BATCH 63/149] Loss_D: 0.6949 Loss_G: 0.6989 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6741 Loss_G: 0.7010 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6562 Loss_G: 0.6834 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7004 Loss_G: 0.6907 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7018 Loss_G: 0.6983 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.6716 Loss_G: 0.6922 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6556 Loss_G: 0.6865 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6905 Loss_G: 0.6999 acc: 82.8%\n",
      "[BATCH 71/149] Loss_D: 0.6694 Loss_G: 0.6923 acc: 82.8%\n",
      "[BATCH 72/149] Loss_D: 0.6896 Loss_G: 0.6899 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6847 Loss_G: 0.7072 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6489 Loss_G: 0.6847 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6591 Loss_G: 0.6989 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6767 Loss_G: 0.6945 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.7038 Loss_G: 0.7121 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6657 Loss_G: 0.7144 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6779 Loss_G: 0.6885 acc: 89.1%\n",
      "[EPOCH 4400] TEST ACC is : 76.6%\n",
      "[BATCH 80/149] Loss_D: 0.7039 Loss_G: 0.7066 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6472 Loss_G: 0.6784 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6631 Loss_G: 0.6785 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6727 Loss_G: 0.7084 acc: 84.4%\n",
      "[BATCH 84/149] Loss_D: 0.6703 Loss_G: 0.7222 acc: 95.3%\n",
      "[BATCH 85/149] Loss_D: 0.7076 Loss_G: 0.7424 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7236 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6809 Loss_G: 0.7329 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6991 Loss_G: 0.7244 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7186 Loss_G: 0.7169 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.6725 Loss_G: 0.6908 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6413 Loss_G: 0.7057 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6558 Loss_G: 0.6874 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6907 Loss_G: 0.6924 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.7300 Loss_G: 0.7266 acc: 82.8%\n",
      "[BATCH 95/149] Loss_D: 0.6758 Loss_G: 0.7036 acc: 81.2%\n",
      "[BATCH 96/149] Loss_D: 0.6789 Loss_G: 0.7057 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6419 Loss_G: 0.6921 acc: 95.3%\n",
      "[BATCH 98/149] Loss_D: 0.6707 Loss_G: 0.6857 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6488 Loss_G: 0.6765 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6309 Loss_G: 0.6658 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6359 Loss_G: 0.6618 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.6985 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6960 Loss_G: 0.7027 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6809 Loss_G: 0.7193 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6635 Loss_G: 0.7157 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6703 Loss_G: 0.7035 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6659 Loss_G: 0.6833 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6672 Loss_G: 0.6845 acc: 82.8%\n",
      "[BATCH 109/149] Loss_D: 0.7028 Loss_G: 0.7086 acc: 95.3%\n",
      "[BATCH 110/149] Loss_D: 0.6835 Loss_G: 0.7133 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.6893 Loss_G: 0.7019 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.6538 Loss_G: 0.7035 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6782 Loss_G: 0.6943 acc: 84.4%\n",
      "[BATCH 114/149] Loss_D: 0.6497 Loss_G: 0.6721 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6961 Loss_G: 0.6816 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6922 Loss_G: 0.7020 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6455 Loss_G: 0.6756 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6963 Loss_G: 0.6777 acc: 79.7%\n",
      "[BATCH 119/149] Loss_D: 0.6828 Loss_G: 0.6807 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.6962 Loss_G: 0.7092 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6618 Loss_G: 0.6883 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.7095 Loss_G: 0.7096 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6547 Loss_G: 0.6752 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6930 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 125/149] Loss_D: 0.6890 Loss_G: 0.6966 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6860 Loss_G: 0.7066 acc: 92.2%\n",
      "[BATCH 127/149] Loss_D: 0.7509 Loss_G: 0.7166 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6968 Loss_G: 0.7247 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6763 Loss_G: 0.7107 acc: 89.1%\n",
      "[EPOCH 4450] TEST ACC is : 77.5%\n",
      "[BATCH 130/149] Loss_D: 0.7143 Loss_G: 0.6916 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6548 Loss_G: 0.6857 acc: 92.2%\n",
      "[BATCH 132/149] Loss_D: 0.6933 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6755 Loss_G: 0.6853 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6703 Loss_G: 0.7052 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6670 Loss_G: 0.7023 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.6469 Loss_G: 0.7158 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6989 Loss_G: 0.7158 acc: 84.4%\n",
      "[BATCH 138/149] Loss_D: 0.6817 Loss_G: 0.7098 acc: 95.3%\n",
      "[BATCH 139/149] Loss_D: 0.6813 Loss_G: 0.7120 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.6955 Loss_G: 0.7421 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.7128 Loss_G: 0.7588 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7002 Loss_G: 0.7401 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.7549 Loss_G: 0.7402 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7069 Loss_G: 0.7169 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7126 Loss_G: 0.7088 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.6657 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6583 Loss_G: 0.6848 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6846 Loss_G: 0.7082 acc: 95.3%\n",
      "[BATCH 149/149] Loss_D: 0.6594 Loss_G: 0.6991 acc: 95.3%\n",
      "-----THE [30/50] epoch end-----\n",
      "-----THE [31/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7122 Loss_G: 0.7054 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.6755 Loss_G: 0.6882 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6946 Loss_G: 0.6924 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.6736 Loss_G: 0.7067 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.6840 Loss_G: 0.7048 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6751 Loss_G: 0.6892 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.7204 Loss_G: 0.7146 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6281 Loss_G: 0.6936 acc: 95.3%\n",
      "[BATCH 9/149] Loss_D: 0.6713 Loss_G: 0.6865 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6624 Loss_G: 0.6882 acc: 96.9%\n",
      "[BATCH 11/149] Loss_D: 0.6814 Loss_G: 0.6907 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6734 Loss_G: 0.7024 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6911 Loss_G: 0.7023 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.7321 Loss_G: 0.7289 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6550 Loss_G: 0.6863 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6614 Loss_G: 0.6947 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7000 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6944 Loss_G: 0.7002 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.6656 Loss_G: 0.6753 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6679 Loss_G: 0.6807 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6685 Loss_G: 0.6769 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6878 Loss_G: 0.7000 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6837 Loss_G: 0.6879 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7194 Loss_G: 0.6951 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.6953 Loss_G: 0.6906 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6804 Loss_G: 0.6799 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7008 Loss_G: 0.6952 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.6627 Loss_G: 0.6748 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.7006 Loss_G: 0.7161 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.6612 Loss_G: 0.6986 acc: 85.9%\n",
      "[EPOCH 4500] TEST ACC is : 76.4%\n",
      "[BATCH 31/149] Loss_D: 0.6853 Loss_G: 0.6853 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6700 Loss_G: 0.6930 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6919 Loss_G: 0.6922 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.6542 Loss_G: 0.6962 acc: 95.3%\n",
      "[BATCH 35/149] Loss_D: 0.7080 Loss_G: 0.7109 acc: 81.2%\n",
      "[BATCH 36/149] Loss_D: 0.6957 Loss_G: 0.7266 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6843 Loss_G: 0.7109 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.6707 Loss_G: 0.7108 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6859 Loss_G: 0.7232 acc: 96.9%\n",
      "[BATCH 40/149] Loss_D: 0.6764 Loss_G: 0.7011 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.6891 Loss_G: 0.6914 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.7147 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6771 Loss_G: 0.6903 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.7025 Loss_G: 0.6984 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6767 Loss_G: 0.7157 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6702 Loss_G: 0.7017 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6879 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6708 Loss_G: 0.6965 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.6989 Loss_G: 0.7202 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.7004 Loss_G: 0.7088 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6921 Loss_G: 0.6877 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6970 Loss_G: 0.6943 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6785 Loss_G: 0.6852 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6679 Loss_G: 0.6729 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6812 Loss_G: 0.6874 acc: 84.4%\n",
      "[BATCH 56/149] Loss_D: 0.6376 Loss_G: 0.6889 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.7067 Loss_G: 0.6931 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6920 Loss_G: 0.7137 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6740 Loss_G: 0.7080 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6591 Loss_G: 0.7087 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6951 Loss_G: 0.7423 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6570 Loss_G: 0.7104 acc: 85.9%\n",
      "[BATCH 63/149] Loss_D: 0.6807 Loss_G: 0.7050 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6825 Loss_G: 0.6857 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6811 Loss_G: 0.7060 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7040 Loss_G: 0.6944 acc: 79.7%\n",
      "[BATCH 67/149] Loss_D: 0.6909 Loss_G: 0.7061 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6626 Loss_G: 0.6975 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6967 Loss_G: 0.6963 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6997 Loss_G: 0.7343 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.7246 Loss_G: 0.7470 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6689 Loss_G: 0.7357 acc: 95.3%\n",
      "[BATCH 73/149] Loss_D: 0.6925 Loss_G: 0.6995 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6863 Loss_G: 0.6898 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.7049 Loss_G: 0.7236 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.7194 Loss_G: 0.7207 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6582 Loss_G: 0.6950 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.7318 Loss_G: 0.7072 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6802 Loss_G: 0.7083 acc: 85.9%\n",
      "[BATCH 80/149] Loss_D: 0.6358 Loss_G: 0.7037 acc: 93.8%\n",
      "[EPOCH 4550] TEST ACC is : 77.7%\n",
      "[BATCH 81/149] Loss_D: 0.6658 Loss_G: 0.6914 acc: 82.8%\n",
      "[BATCH 82/149] Loss_D: 0.6848 Loss_G: 0.6834 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6736 Loss_G: 0.6808 acc: 82.8%\n",
      "[BATCH 84/149] Loss_D: 0.6670 Loss_G: 0.6992 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6500 Loss_G: 0.6917 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6876 Loss_G: 0.6918 acc: 92.2%\n",
      "[BATCH 87/149] Loss_D: 0.6629 Loss_G: 0.6915 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.7045 Loss_G: 0.6892 acc: 81.2%\n",
      "[BATCH 89/149] Loss_D: 0.6919 Loss_G: 0.7002 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6828 Loss_G: 0.6812 acc: 82.8%\n",
      "[BATCH 91/149] Loss_D: 0.6632 Loss_G: 0.6914 acc: 93.8%\n",
      "[BATCH 92/149] Loss_D: 0.6858 Loss_G: 0.6888 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.6685 Loss_G: 0.6798 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6564 Loss_G: 0.7220 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6668 Loss_G: 0.6958 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.6471 Loss_G: 0.6771 acc: 92.2%\n",
      "[BATCH 97/149] Loss_D: 0.6864 Loss_G: 0.6853 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6808 Loss_G: 0.7000 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6984 Loss_G: 0.7271 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6784 Loss_G: 0.7226 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6687 Loss_G: 0.7194 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.6698 Loss_G: 0.7011 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6720 Loss_G: 0.7205 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6832 Loss_G: 0.7484 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.6506 Loss_G: 0.6975 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6464 Loss_G: 0.7362 acc: 98.4%\n",
      "[BATCH 107/149] Loss_D: 0.6883 Loss_G: 0.6884 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6748 Loss_G: 0.7066 acc: 84.4%\n",
      "[BATCH 109/149] Loss_D: 0.6976 Loss_G: 0.7181 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6784 Loss_G: 0.7137 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.7187 Loss_G: 0.7473 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6833 Loss_G: 0.7242 acc: 82.8%\n",
      "[BATCH 113/149] Loss_D: 0.6877 Loss_G: 0.7016 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.7042 Loss_G: 0.7428 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6556 Loss_G: 0.7212 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6720 Loss_G: 0.6969 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6390 Loss_G: 0.7103 acc: 95.3%\n",
      "[BATCH 118/149] Loss_D: 0.7079 Loss_G: 0.6882 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.6865 Loss_G: 0.7222 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6978 Loss_G: 0.6943 acc: 85.9%\n",
      "[BATCH 121/149] Loss_D: 0.6683 Loss_G: 0.6875 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6910 Loss_G: 0.6771 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.7155 Loss_G: 0.7078 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6861 Loss_G: 0.7109 acc: 95.3%\n",
      "[BATCH 125/149] Loss_D: 0.6548 Loss_G: 0.7152 acc: 93.8%\n",
      "[BATCH 126/149] Loss_D: 0.7030 Loss_G: 0.7154 acc: 87.5%\n",
      "[BATCH 127/149] Loss_D: 0.6919 Loss_G: 0.7016 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6922 Loss_G: 0.6885 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.7112 Loss_G: 0.7105 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.7219 Loss_G: 0.7146 acc: 93.8%\n",
      "[EPOCH 4600] TEST ACC is : 77.1%\n",
      "[BATCH 131/149] Loss_D: 0.6720 Loss_G: 0.7122 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6725 Loss_G: 0.7041 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6964 Loss_G: 0.7391 acc: 84.4%\n",
      "[BATCH 134/149] Loss_D: 0.6865 Loss_G: 0.6946 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6815 Loss_G: 0.6839 acc: 84.4%\n",
      "[BATCH 136/149] Loss_D: 0.7378 Loss_G: 0.7309 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6916 Loss_G: 0.7168 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6725 Loss_G: 0.7176 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.6797 Loss_G: 0.6963 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7183 Loss_G: 0.7074 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.6364 Loss_G: 0.6818 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6777 Loss_G: 0.6888 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6923 Loss_G: 0.6775 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.6898 Loss_G: 0.7270 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6497 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7253 Loss_G: 0.7103 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6699 Loss_G: 0.7335 acc: 98.4%\n",
      "[BATCH 148/149] Loss_D: 0.6617 Loss_G: 0.7338 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.6926 Loss_G: 0.7231 acc: 90.6%\n",
      "-----THE [31/50] epoch end-----\n",
      "-----THE [32/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6808 Loss_G: 0.6939 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6776 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6513 Loss_G: 0.6833 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7044 Loss_G: 0.6911 acc: 79.7%\n",
      "[BATCH 5/149] Loss_D: 0.6857 Loss_G: 0.6813 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6897 Loss_G: 0.6886 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.7019 Loss_G: 0.6956 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6783 Loss_G: 0.6873 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6497 Loss_G: 0.6920 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6977 Loss_G: 0.7041 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.7025 Loss_G: 0.7171 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6547 Loss_G: 0.6828 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6687 Loss_G: 0.6762 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.6864 Loss_G: 0.6821 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6833 Loss_G: 0.7002 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6632 Loss_G: 0.7105 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6696 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6713 Loss_G: 0.7114 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6989 Loss_G: 0.7045 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.7059 Loss_G: 0.7066 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6662 Loss_G: 0.6956 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6669 Loss_G: 0.7031 acc: 96.9%\n",
      "[BATCH 23/149] Loss_D: 0.6808 Loss_G: 0.6912 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.7065 Loss_G: 0.7004 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.6694 Loss_G: 0.7132 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6752 Loss_G: 0.7186 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.7441 Loss_G: 0.7396 acc: 81.2%\n",
      "[BATCH 28/149] Loss_D: 0.6946 Loss_G: 0.7146 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6777 Loss_G: 0.7241 acc: 96.9%\n",
      "[BATCH 30/149] Loss_D: 0.6881 Loss_G: 0.6898 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.7084 Loss_G: 0.6960 acc: 81.2%\n",
      "[EPOCH 4650] TEST ACC is : 77.3%\n",
      "[BATCH 32/149] Loss_D: 0.6946 Loss_G: 0.6905 acc: 79.7%\n",
      "[BATCH 33/149] Loss_D: 0.6572 Loss_G: 0.6858 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6908 Loss_G: 0.6915 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6636 Loss_G: 0.7107 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6550 Loss_G: 0.6914 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6580 Loss_G: 0.7023 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.6780 Loss_G: 0.6902 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.6726 Loss_G: 0.6732 acc: 81.2%\n",
      "[BATCH 40/149] Loss_D: 0.6526 Loss_G: 0.6771 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6521 Loss_G: 0.6685 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6796 Loss_G: 0.6878 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6778 Loss_G: 0.7086 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6924 Loss_G: 0.7052 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6989 Loss_G: 0.7087 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6798 Loss_G: 0.6855 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.7022 Loss_G: 0.7033 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7057 Loss_G: 0.7003 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.6809 Loss_G: 0.7187 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6762 Loss_G: 0.6870 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6655 Loss_G: 0.6971 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.7046 Loss_G: 0.7131 acc: 82.8%\n",
      "[BATCH 53/149] Loss_D: 0.6617 Loss_G: 0.6988 acc: 79.7%\n",
      "[BATCH 54/149] Loss_D: 0.6670 Loss_G: 0.7134 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6692 Loss_G: 0.6979 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.7016 Loss_G: 0.7041 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6787 Loss_G: 0.7370 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.6890 Loss_G: 0.6908 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6786 Loss_G: 0.6929 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6962 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6887 Loss_G: 0.6959 acc: 84.4%\n",
      "[BATCH 62/149] Loss_D: 0.7094 Loss_G: 0.7327 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6622 Loss_G: 0.6992 acc: 93.8%\n",
      "[BATCH 64/149] Loss_D: 0.7025 Loss_G: 0.7253 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6738 Loss_G: 0.7047 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6567 Loss_G: 0.6854 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6732 Loss_G: 0.6976 acc: 93.8%\n",
      "[BATCH 68/149] Loss_D: 0.6742 Loss_G: 0.6924 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.6789 Loss_G: 0.7030 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6617 Loss_G: 0.6941 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6895 Loss_G: 0.7016 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.7267 Loss_G: 0.7160 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6966 Loss_G: 0.7524 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7180 Loss_G: 0.7293 acc: 87.5%\n",
      "[BATCH 75/149] Loss_D: 0.6796 Loss_G: 0.7108 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.7445 Loss_G: 0.7354 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6833 Loss_G: 0.7291 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6853 Loss_G: 0.7240 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.7237 Loss_G: 0.7281 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6681 Loss_G: 0.7230 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6729 Loss_G: 0.7146 acc: 90.6%\n",
      "[EPOCH 4700] TEST ACC is : 77.3%\n",
      "[BATCH 82/149] Loss_D: 0.6795 Loss_G: 0.7162 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6593 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6563 Loss_G: 0.7091 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.6755 Loss_G: 0.6974 acc: 92.2%\n",
      "[BATCH 86/149] Loss_D: 0.6987 Loss_G: 0.6884 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.6747 Loss_G: 0.7035 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6525 Loss_G: 0.7237 acc: 96.9%\n",
      "[BATCH 89/149] Loss_D: 0.6758 Loss_G: 0.7285 acc: 89.1%\n",
      "[BATCH 90/149] Loss_D: 0.7119 Loss_G: 0.7196 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6959 Loss_G: 0.6980 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6892 Loss_G: 0.7141 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.7468 Loss_G: 0.7223 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.7008 Loss_G: 0.7438 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6882 Loss_G: 0.7165 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6696 Loss_G: 0.7017 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6858 Loss_G: 0.7028 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6832 Loss_G: 0.7047 acc: 93.8%\n",
      "[BATCH 99/149] Loss_D: 0.6780 Loss_G: 0.7161 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.6641 Loss_G: 0.6957 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6678 Loss_G: 0.6873 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6430 Loss_G: 0.6866 acc: 93.8%\n",
      "[BATCH 103/149] Loss_D: 0.6614 Loss_G: 0.6752 acc: 81.2%\n",
      "[BATCH 104/149] Loss_D: 0.6701 Loss_G: 0.6706 acc: 85.9%\n",
      "[BATCH 105/149] Loss_D: 0.6576 Loss_G: 0.6795 acc: 93.8%\n",
      "[BATCH 106/149] Loss_D: 0.6543 Loss_G: 0.6768 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6630 Loss_G: 0.6793 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7487 Loss_G: 0.7392 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6671 Loss_G: 0.7040 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6662 Loss_G: 0.6886 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.6623 Loss_G: 0.6782 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7015 Loss_G: 0.6907 acc: 85.9%\n",
      "[BATCH 113/149] Loss_D: 0.6713 Loss_G: 0.7039 acc: 82.8%\n",
      "[BATCH 114/149] Loss_D: 0.7020 Loss_G: 0.7092 acc: 84.4%\n",
      "[BATCH 115/149] Loss_D: 0.6733 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 116/149] Loss_D: 0.7142 Loss_G: 0.6866 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6854 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.7404 Loss_G: 0.7216 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6901 Loss_G: 0.7178 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.7032 Loss_G: 0.7096 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6527 Loss_G: 0.7084 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.7159 Loss_G: 0.7110 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6798 Loss_G: 0.7159 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6736 Loss_G: 0.7035 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6574 Loss_G: 0.6960 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.7097 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6930 Loss_G: 0.7308 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6771 Loss_G: 0.7047 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6965 Loss_G: 0.7056 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7408 Loss_G: 0.7239 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.7032 Loss_G: 0.7062 acc: 87.5%\n",
      "[EPOCH 4750] TEST ACC is : 77.0%\n",
      "[BATCH 132/149] Loss_D: 0.6981 Loss_G: 0.7031 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6813 Loss_G: 0.7117 acc: 92.2%\n",
      "[BATCH 134/149] Loss_D: 0.7085 Loss_G: 0.7082 acc: 84.4%\n",
      "[BATCH 135/149] Loss_D: 0.6541 Loss_G: 0.7080 acc: 92.2%\n",
      "[BATCH 136/149] Loss_D: 0.6739 Loss_G: 0.7131 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6493 Loss_G: 0.6959 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6555 Loss_G: 0.6871 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6692 Loss_G: 0.7228 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6910 Loss_G: 0.7039 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.6844 Loss_G: 0.7217 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.6561 Loss_G: 0.7213 acc: 96.9%\n",
      "[BATCH 143/149] Loss_D: 0.6727 Loss_G: 0.6923 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.6520 Loss_G: 0.6757 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6739 Loss_G: 0.6888 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7164 Loss_G: 0.7008 acc: 82.8%\n",
      "[BATCH 147/149] Loss_D: 0.6551 Loss_G: 0.6854 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.7003 Loss_G: 0.6936 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.6703 Loss_G: 0.6988 acc: 87.5%\n",
      "-----THE [32/50] epoch end-----\n",
      "-----THE [33/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6602 Loss_G: 0.6917 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6685 Loss_G: 0.6917 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.6520 Loss_G: 0.6974 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6571 Loss_G: 0.6827 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6513 Loss_G: 0.6887 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6567 Loss_G: 0.6850 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6685 Loss_G: 0.7115 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6610 Loss_G: 0.6990 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6899 Loss_G: 0.7020 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6618 Loss_G: 0.6788 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7092 Loss_G: 0.6781 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.6525 Loss_G: 0.6846 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6812 Loss_G: 0.6954 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.7207 Loss_G: 0.7345 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6884 Loss_G: 0.7118 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.6614 Loss_G: 0.6845 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6711 Loss_G: 0.6802 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6855 Loss_G: 0.6934 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6876 Loss_G: 0.6960 acc: 85.9%\n",
      "[BATCH 20/149] Loss_D: 0.6694 Loss_G: 0.7040 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6929 Loss_G: 0.7010 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6691 Loss_G: 0.7014 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6562 Loss_G: 0.6828 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.7041 Loss_G: 0.7041 acc: 90.6%\n",
      "[BATCH 25/149] Loss_D: 0.7102 Loss_G: 0.7559 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6961 Loss_G: 0.7519 acc: 90.6%\n",
      "[BATCH 27/149] Loss_D: 0.6494 Loss_G: 0.7260 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6769 Loss_G: 0.7128 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.6709 Loss_G: 0.7402 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.7305 Loss_G: 0.7030 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.7274 Loss_G: 0.7311 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6694 Loss_G: 0.7241 acc: 84.4%\n",
      "[EPOCH 4800] TEST ACC is : 76.8%\n",
      "[BATCH 33/149] Loss_D: 0.6468 Loss_G: 0.7004 acc: 95.3%\n",
      "[BATCH 34/149] Loss_D: 0.6792 Loss_G: 0.6977 acc: 93.8%\n",
      "[BATCH 35/149] Loss_D: 0.6648 Loss_G: 0.6870 acc: 93.8%\n",
      "[BATCH 36/149] Loss_D: 0.7031 Loss_G: 0.7060 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6977 Loss_G: 0.7076 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6682 Loss_G: 0.6977 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6536 Loss_G: 0.6904 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6403 Loss_G: 0.7177 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.6572 Loss_G: 0.7139 acc: 93.8%\n",
      "[BATCH 42/149] Loss_D: 0.6667 Loss_G: 0.6948 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.7334 Loss_G: 0.7206 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6485 Loss_G: 0.6945 acc: 95.3%\n",
      "[BATCH 45/149] Loss_D: 0.6779 Loss_G: 0.6804 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.7036 Loss_G: 0.6995 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7214 Loss_G: 0.7172 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6904 Loss_G: 0.6976 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.7239 Loss_G: 0.7153 acc: 84.4%\n",
      "[BATCH 50/149] Loss_D: 0.6660 Loss_G: 0.6890 acc: 89.1%\n",
      "[BATCH 51/149] Loss_D: 0.6733 Loss_G: 0.6793 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6622 Loss_G: 0.6819 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6766 Loss_G: 0.6972 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7162 Loss_G: 0.7252 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6334 Loss_G: 0.6924 acc: 81.2%\n",
      "[BATCH 56/149] Loss_D: 0.6633 Loss_G: 0.6942 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6496 Loss_G: 0.6876 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6724 Loss_G: 0.6937 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6574 Loss_G: 0.6658 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6729 Loss_G: 0.6680 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6533 Loss_G: 0.6824 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6578 Loss_G: 0.6790 acc: 95.3%\n",
      "[BATCH 63/149] Loss_D: 0.6998 Loss_G: 0.7004 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6827 Loss_G: 0.7102 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.6526 Loss_G: 0.7057 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.7189 Loss_G: 0.7420 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.6699 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6706 Loss_G: 0.6775 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.6893 Loss_G: 0.6868 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6897 Loss_G: 0.6816 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6711 Loss_G: 0.6785 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6843 Loss_G: 0.6942 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.6805 Loss_G: 0.6917 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.7417 Loss_G: 0.7154 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.6456 Loss_G: 0.6936 acc: 92.2%\n",
      "[BATCH 76/149] Loss_D: 0.6680 Loss_G: 0.6910 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6847 Loss_G: 0.7102 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6487 Loss_G: 0.6858 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6756 Loss_G: 0.6895 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.7334 Loss_G: 0.6934 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6657 Loss_G: 0.6961 acc: 81.2%\n",
      "[BATCH 82/149] Loss_D: 0.6660 Loss_G: 0.6868 acc: 89.1%\n",
      "[EPOCH 4850] TEST ACC is : 76.8%\n",
      "[BATCH 83/149] Loss_D: 0.7037 Loss_G: 0.7176 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.6762 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6671 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6607 Loss_G: 0.7086 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.7194 Loss_G: 0.7310 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6630 Loss_G: 0.6950 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6623 Loss_G: 0.7177 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6855 Loss_G: 0.7139 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6595 Loss_G: 0.7012 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6989 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.7351 Loss_G: 0.7035 acc: 81.2%\n",
      "[BATCH 94/149] Loss_D: 0.6574 Loss_G: 0.6984 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6832 Loss_G: 0.7521 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6775 Loss_G: 0.7174 acc: 93.8%\n",
      "[BATCH 97/149] Loss_D: 0.6748 Loss_G: 0.6932 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6722 Loss_G: 0.6766 acc: 84.4%\n",
      "[BATCH 99/149] Loss_D: 0.6837 Loss_G: 0.6983 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6731 Loss_G: 0.6871 acc: 82.8%\n",
      "[BATCH 101/149] Loss_D: 0.6655 Loss_G: 0.6988 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.7434 Loss_G: 0.7178 acc: 78.1%\n",
      "[BATCH 103/149] Loss_D: 0.6931 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6556 Loss_G: 0.7001 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.6982 Loss_G: 0.7050 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6970 Loss_G: 0.6974 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.6773 Loss_G: 0.7127 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.7358 Loss_G: 0.7205 acc: 81.2%\n",
      "[BATCH 109/149] Loss_D: 0.6954 Loss_G: 0.7202 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.6662 Loss_G: 0.7150 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6751 Loss_G: 0.7169 acc: 95.3%\n",
      "[BATCH 112/149] Loss_D: 0.6863 Loss_G: 0.7417 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.7291 Loss_G: 0.7212 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6906 Loss_G: 0.7090 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6564 Loss_G: 0.6862 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6809 Loss_G: 0.6891 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.7237 Loss_G: 0.6963 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.6884 Loss_G: 0.7142 acc: 82.8%\n",
      "[BATCH 119/149] Loss_D: 0.6940 Loss_G: 0.6901 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.6897 Loss_G: 0.7011 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6535 Loss_G: 0.6888 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.7140 Loss_G: 0.6937 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6684 Loss_G: 0.6935 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6783 Loss_G: 0.6990 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.7131 Loss_G: 0.7266 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6513 Loss_G: 0.7293 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6854 Loss_G: 0.7070 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6847 Loss_G: 0.6906 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.7299 Loss_G: 0.7117 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.6966 Loss_G: 0.7429 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.7133 Loss_G: 0.7264 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.7225 Loss_G: 0.7243 acc: 87.5%\n",
      "[EPOCH 4900] TEST ACC is : 77.0%\n",
      "[BATCH 133/149] Loss_D: 0.7035 Loss_G: 0.7058 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.6725 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7266 Loss_G: 0.7218 acc: 79.7%\n",
      "[BATCH 136/149] Loss_D: 0.6981 Loss_G: 0.7300 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6878 Loss_G: 0.7068 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.7091 Loss_G: 0.7219 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6800 Loss_G: 0.7117 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.6866 Loss_G: 0.6988 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6930 Loss_G: 0.6948 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7054 Loss_G: 0.7236 acc: 81.2%\n",
      "[BATCH 143/149] Loss_D: 0.6567 Loss_G: 0.7102 acc: 95.3%\n",
      "[BATCH 144/149] Loss_D: 0.7036 Loss_G: 0.7111 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.7185 Loss_G: 0.7131 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.6674 Loss_G: 0.6947 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6759 Loss_G: 0.6901 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6786 Loss_G: 0.6937 acc: 90.6%\n",
      "[BATCH 149/149] Loss_D: 0.6651 Loss_G: 0.6986 acc: 89.1%\n",
      "-----THE [33/50] epoch end-----\n",
      "-----THE [34/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7058 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6647 Loss_G: 0.7020 acc: 93.8%\n",
      "[BATCH 3/149] Loss_D: 0.7099 Loss_G: 0.7376 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.6666 Loss_G: 0.7320 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.6755 Loss_G: 0.7172 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6690 Loss_G: 0.7120 acc: 93.8%\n",
      "[BATCH 7/149] Loss_D: 0.6502 Loss_G: 0.6909 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6390 Loss_G: 0.6885 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6940 Loss_G: 0.7139 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6761 Loss_G: 0.7121 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6725 Loss_G: 0.7057 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6807 Loss_G: 0.6919 acc: 85.9%\n",
      "[BATCH 13/149] Loss_D: 0.6714 Loss_G: 0.6846 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6680 Loss_G: 0.6860 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6698 Loss_G: 0.6759 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6991 Loss_G: 0.7030 acc: 85.9%\n",
      "[BATCH 17/149] Loss_D: 0.6571 Loss_G: 0.6935 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.7090 Loss_G: 0.7047 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6811 Loss_G: 0.6871 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.6832 Loss_G: 0.6859 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6855 Loss_G: 0.7071 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.7162 Loss_G: 0.7072 acc: 93.8%\n",
      "[BATCH 23/149] Loss_D: 0.6801 Loss_G: 0.6951 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6442 Loss_G: 0.7187 acc: 89.1%\n",
      "[BATCH 25/149] Loss_D: 0.7198 Loss_G: 0.7030 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.7563 Loss_G: 0.7376 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.6734 Loss_G: 0.7119 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.7069 Loss_G: 0.6826 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.6957 Loss_G: 0.6920 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6480 Loss_G: 0.7037 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7081 Loss_G: 0.7402 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.7106 Loss_G: 0.7497 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6580 Loss_G: 0.6943 acc: 79.7%\n",
      "[EPOCH 4950] TEST ACC is : 75.8%\n",
      "[BATCH 34/149] Loss_D: 0.6652 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6958 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.6751 Loss_G: 0.6967 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.6589 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6642 Loss_G: 0.6959 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.6710 Loss_G: 0.7058 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6843 Loss_G: 0.7291 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6770 Loss_G: 0.7315 acc: 85.9%\n",
      "[BATCH 42/149] Loss_D: 0.6847 Loss_G: 0.7357 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.7106 Loss_G: 0.7075 acc: 84.4%\n",
      "[BATCH 44/149] Loss_D: 0.6842 Loss_G: 0.6879 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6905 Loss_G: 0.6804 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.7069 Loss_G: 0.6940 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6729 Loss_G: 0.6985 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7064 Loss_G: 0.6998 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6830 Loss_G: 0.6997 acc: 79.7%\n",
      "[BATCH 50/149] Loss_D: 0.6766 Loss_G: 0.6898 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6632 Loss_G: 0.6859 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.6754 Loss_G: 0.6865 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6777 Loss_G: 0.6819 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.6619 Loss_G: 0.6876 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6898 Loss_G: 0.6935 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6786 Loss_G: 0.7088 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6860 Loss_G: 0.7221 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.6796 Loss_G: 0.7184 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.7209 Loss_G: 0.7826 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.6867 Loss_G: 0.7009 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7058 Loss_G: 0.6765 acc: 79.7%\n",
      "[BATCH 62/149] Loss_D: 0.6980 Loss_G: 0.7081 acc: 93.8%\n",
      "[BATCH 63/149] Loss_D: 0.6939 Loss_G: 0.7163 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.6865 Loss_G: 0.7177 acc: 82.8%\n",
      "[BATCH 65/149] Loss_D: 0.6931 Loss_G: 0.7353 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6895 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.6828 Loss_G: 0.6866 acc: 93.8%\n",
      "[BATCH 68/149] Loss_D: 0.6565 Loss_G: 0.6970 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6782 Loss_G: 0.6806 acc: 84.4%\n",
      "[BATCH 70/149] Loss_D: 0.6558 Loss_G: 0.6901 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6536 Loss_G: 0.6738 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6442 Loss_G: 0.6548 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6581 Loss_G: 0.6742 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6258 Loss_G: 0.6901 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6737 Loss_G: 0.7171 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7053 Loss_G: 0.7214 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6921 Loss_G: 0.7361 acc: 95.3%\n",
      "[BATCH 78/149] Loss_D: 0.6611 Loss_G: 0.6844 acc: 93.8%\n",
      "[BATCH 79/149] Loss_D: 0.6838 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6847 Loss_G: 0.7042 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6734 Loss_G: 0.7011 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7160 Loss_G: 0.7004 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6721 Loss_G: 0.7153 acc: 90.6%\n",
      "[EPOCH 5000] TEST ACC is : 76.4%\n",
      "[BATCH 84/149] Loss_D: 0.6699 Loss_G: 0.7317 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6921 Loss_G: 0.7174 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6636 Loss_G: 0.6993 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6904 Loss_G: 0.7097 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.7057 Loss_G: 0.7179 acc: 82.8%\n",
      "[BATCH 89/149] Loss_D: 0.6723 Loss_G: 0.6979 acc: 81.2%\n",
      "[BATCH 90/149] Loss_D: 0.7253 Loss_G: 0.6992 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.7074 Loss_G: 0.7098 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6863 Loss_G: 0.6895 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.7232 Loss_G: 0.7099 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6693 Loss_G: 0.7261 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6790 Loss_G: 0.7298 acc: 93.8%\n",
      "[BATCH 96/149] Loss_D: 0.6918 Loss_G: 0.7079 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6996 Loss_G: 0.7102 acc: 84.4%\n",
      "[BATCH 98/149] Loss_D: 0.6602 Loss_G: 0.6883 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.7187 Loss_G: 0.7309 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.6878 Loss_G: 0.7184 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6446 Loss_G: 0.6971 acc: 93.8%\n",
      "[BATCH 102/149] Loss_D: 0.7099 Loss_G: 0.7318 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6916 Loss_G: 0.7357 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6723 Loss_G: 0.7226 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7094 Loss_G: 0.7105 acc: 76.6%\n",
      "[BATCH 106/149] Loss_D: 0.6530 Loss_G: 0.6898 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.6681 Loss_G: 0.6934 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7210 Loss_G: 0.6935 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6855 Loss_G: 0.7078 acc: 95.3%\n",
      "[BATCH 110/149] Loss_D: 0.7185 Loss_G: 0.7110 acc: 76.6%\n",
      "[BATCH 111/149] Loss_D: 0.6760 Loss_G: 0.7025 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.6741 Loss_G: 0.7034 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6839 Loss_G: 0.6799 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6722 Loss_G: 0.6807 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6737 Loss_G: 0.6934 acc: 93.8%\n",
      "[BATCH 116/149] Loss_D: 0.6518 Loss_G: 0.7021 acc: 84.4%\n",
      "[BATCH 117/149] Loss_D: 0.6851 Loss_G: 0.7012 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.6694 Loss_G: 0.6920 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6931 Loss_G: 0.7021 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.6362 Loss_G: 0.6832 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7245 Loss_G: 0.7075 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6966 Loss_G: 0.7168 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6714 Loss_G: 0.7072 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.6923 Loss_G: 0.6991 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6745 Loss_G: 0.6963 acc: 93.8%\n",
      "[BATCH 126/149] Loss_D: 0.6648 Loss_G: 0.7064 acc: 95.3%\n",
      "[BATCH 127/149] Loss_D: 0.6515 Loss_G: 0.6811 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.6730 Loss_G: 0.6883 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6649 Loss_G: 0.6918 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7037 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6633 Loss_G: 0.7085 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.7257 Loss_G: 0.7175 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7050 Loss_G: 0.7109 acc: 87.5%\n",
      "[EPOCH 5050] TEST ACC is : 77.0%\n",
      "[BATCH 134/149] Loss_D: 0.6578 Loss_G: 0.6846 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6525 Loss_G: 0.6685 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6990 Loss_G: 0.6945 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7010 Loss_G: 0.6909 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.7261 Loss_G: 0.7418 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.7076 Loss_G: 0.7404 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.6772 Loss_G: 0.7068 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6825 Loss_G: 0.7131 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6754 Loss_G: 0.7092 acc: 96.9%\n",
      "[BATCH 143/149] Loss_D: 0.7095 Loss_G: 0.7137 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6562 Loss_G: 0.6982 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6562 Loss_G: 0.7046 acc: 95.3%\n",
      "[BATCH 146/149] Loss_D: 0.6913 Loss_G: 0.6998 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6707 Loss_G: 0.7182 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6524 Loss_G: 0.6952 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.6563 Loss_G: 0.6725 acc: 90.6%\n",
      "-----THE [34/50] epoch end-----\n",
      "-----THE [35/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7233 Loss_G: 0.7032 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.7135 Loss_G: 0.7367 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6950 Loss_G: 0.7305 acc: 81.2%\n",
      "[BATCH 4/149] Loss_D: 0.6956 Loss_G: 0.7642 acc: 90.6%\n",
      "[BATCH 5/149] Loss_D: 0.6868 Loss_G: 0.7248 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.6797 Loss_G: 0.7189 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.7066 Loss_G: 0.7120 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6785 Loss_G: 0.7053 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.6914 Loss_G: 0.6958 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6924 Loss_G: 0.7152 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6742 Loss_G: 0.6899 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6448 Loss_G: 0.6855 acc: 96.9%\n",
      "[BATCH 13/149] Loss_D: 0.6460 Loss_G: 0.6743 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.7098 Loss_G: 0.7185 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.6617 Loss_G: 0.7033 acc: 85.9%\n",
      "[BATCH 16/149] Loss_D: 0.6854 Loss_G: 0.6837 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6985 Loss_G: 0.6965 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6688 Loss_G: 0.7051 acc: 92.2%\n",
      "[BATCH 19/149] Loss_D: 0.6610 Loss_G: 0.6917 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.7008 Loss_G: 0.7430 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6850 Loss_G: 0.7328 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6625 Loss_G: 0.7128 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6811 Loss_G: 0.7052 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6623 Loss_G: 0.7098 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.6512 Loss_G: 0.6773 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6467 Loss_G: 0.6841 acc: 95.3%\n",
      "[BATCH 27/149] Loss_D: 0.6920 Loss_G: 0.7042 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6640 Loss_G: 0.6884 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6827 Loss_G: 0.7275 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.7061 Loss_G: 0.7678 acc: 95.3%\n",
      "[BATCH 31/149] Loss_D: 0.6945 Loss_G: 0.7003 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7272 Loss_G: 0.6972 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6781 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6688 Loss_G: 0.7134 acc: 92.2%\n",
      "[EPOCH 5100] TEST ACC is : 77.7%\n",
      "[BATCH 35/149] Loss_D: 0.6285 Loss_G: 0.6948 acc: 92.2%\n",
      "[BATCH 36/149] Loss_D: 0.7046 Loss_G: 0.7049 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.6725 Loss_G: 0.7011 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6873 Loss_G: 0.7100 acc: 82.8%\n",
      "[BATCH 39/149] Loss_D: 0.7056 Loss_G: 0.7029 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6756 Loss_G: 0.6863 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7068 Loss_G: 0.7318 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6929 Loss_G: 0.7091 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6939 Loss_G: 0.6973 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6474 Loss_G: 0.6803 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6672 Loss_G: 0.6780 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6777 Loss_G: 0.6712 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6438 Loss_G: 0.6764 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6584 Loss_G: 0.6947 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6617 Loss_G: 0.7044 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6731 Loss_G: 0.7031 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6829 Loss_G: 0.6914 acc: 90.6%\n",
      "[BATCH 52/149] Loss_D: 0.6750 Loss_G: 0.7034 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7060 Loss_G: 0.7056 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6713 Loss_G: 0.7270 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6466 Loss_G: 0.6965 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.6836 Loss_G: 0.6953 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6568 Loss_G: 0.6774 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6746 Loss_G: 0.6844 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6844 Loss_G: 0.6788 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6835 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6445 Loss_G: 0.6697 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.7025 Loss_G: 0.7090 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.7081 Loss_G: 0.7034 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6973 Loss_G: 0.7021 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6888 Loss_G: 0.7072 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6594 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6720 Loss_G: 0.6730 acc: 85.9%\n",
      "[BATCH 68/149] Loss_D: 0.6553 Loss_G: 0.6745 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.6722 Loss_G: 0.6829 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6607 Loss_G: 0.6779 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6532 Loss_G: 0.6722 acc: 93.8%\n",
      "[BATCH 72/149] Loss_D: 0.6855 Loss_G: 0.6771 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7042 Loss_G: 0.6867 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6603 Loss_G: 0.7001 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.7178 Loss_G: 0.7084 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6867 Loss_G: 0.7242 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6854 Loss_G: 0.6988 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.6708 Loss_G: 0.6996 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.7097 Loss_G: 0.7178 acc: 89.1%\n",
      "[BATCH 80/149] Loss_D: 0.7164 Loss_G: 0.7226 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6565 Loss_G: 0.7230 acc: 98.4%\n",
      "[BATCH 82/149] Loss_D: 0.7079 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 83/149] Loss_D: 0.6740 Loss_G: 0.6940 acc: 82.8%\n",
      "[BATCH 84/149] Loss_D: 0.6997 Loss_G: 0.6904 acc: 84.4%\n",
      "[EPOCH 5150] TEST ACC is : 77.3%\n",
      "[BATCH 85/149] Loss_D: 0.6616 Loss_G: 0.7090 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.6628 Loss_G: 0.7072 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6646 Loss_G: 0.7117 acc: 96.9%\n",
      "[BATCH 88/149] Loss_D: 0.6838 Loss_G: 0.7093 acc: 89.1%\n",
      "[BATCH 89/149] Loss_D: 0.6923 Loss_G: 0.6952 acc: 82.8%\n",
      "[BATCH 90/149] Loss_D: 0.7030 Loss_G: 0.7165 acc: 81.2%\n",
      "[BATCH 91/149] Loss_D: 0.6747 Loss_G: 0.7309 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.7056 Loss_G: 0.7354 acc: 92.2%\n",
      "[BATCH 93/149] Loss_D: 0.7016 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.6826 Loss_G: 0.6992 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6828 Loss_G: 0.7037 acc: 82.8%\n",
      "[BATCH 96/149] Loss_D: 0.7363 Loss_G: 0.7007 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.7126 Loss_G: 0.7235 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6698 Loss_G: 0.7181 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6960 Loss_G: 0.7399 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.6786 Loss_G: 0.7103 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6735 Loss_G: 0.6981 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6540 Loss_G: 0.6956 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6988 Loss_G: 0.7061 acc: 82.8%\n",
      "[BATCH 104/149] Loss_D: 0.6786 Loss_G: 0.6962 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6706 Loss_G: 0.7050 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6907 Loss_G: 0.6999 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.7095 Loss_G: 0.7033 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.6533 Loss_G: 0.6961 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6720 Loss_G: 0.7189 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6887 Loss_G: 0.7157 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.6798 Loss_G: 0.6995 acc: 92.2%\n",
      "[BATCH 112/149] Loss_D: 0.6608 Loss_G: 0.7088 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6636 Loss_G: 0.7072 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.7040 Loss_G: 0.7079 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6668 Loss_G: 0.7164 acc: 93.8%\n",
      "[BATCH 116/149] Loss_D: 0.6611 Loss_G: 0.7000 acc: 93.8%\n",
      "[BATCH 117/149] Loss_D: 0.6755 Loss_G: 0.7088 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.6869 Loss_G: 0.7299 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6966 Loss_G: 0.7085 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.7067 Loss_G: 0.7189 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6848 Loss_G: 0.7122 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.6650 Loss_G: 0.6871 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.6811 Loss_G: 0.6910 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.6618 Loss_G: 0.7027 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6789 Loss_G: 0.6908 acc: 84.4%\n",
      "[BATCH 126/149] Loss_D: 0.6979 Loss_G: 0.6993 acc: 90.6%\n",
      "[BATCH 127/149] Loss_D: 0.6804 Loss_G: 0.7336 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6497 Loss_G: 0.6865 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.7027 Loss_G: 0.7139 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7041 Loss_G: 0.7272 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6948 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6798 Loss_G: 0.6904 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6675 Loss_G: 0.7047 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6711 Loss_G: 0.6843 acc: 89.1%\n",
      "[EPOCH 5200] TEST ACC is : 77.1%\n",
      "[BATCH 135/149] Loss_D: 0.6802 Loss_G: 0.7003 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6647 Loss_G: 0.6958 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6826 Loss_G: 0.6992 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.7081 Loss_G: 0.7064 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7178 Loss_G: 0.6990 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6782 Loss_G: 0.6871 acc: 81.2%\n",
      "[BATCH 141/149] Loss_D: 0.7039 Loss_G: 0.6917 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6659 Loss_G: 0.6965 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6803 Loss_G: 0.6960 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6598 Loss_G: 0.6906 acc: 89.1%\n",
      "[BATCH 145/149] Loss_D: 0.6823 Loss_G: 0.7243 acc: 93.8%\n",
      "[BATCH 146/149] Loss_D: 0.7069 Loss_G: 0.7098 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.7047 Loss_G: 0.6956 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6880 Loss_G: 0.6887 acc: 81.2%\n",
      "[BATCH 149/149] Loss_D: 0.6635 Loss_G: 0.6855 acc: 85.9%\n",
      "-----THE [35/50] epoch end-----\n",
      "-----THE [36/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6486 Loss_G: 0.6805 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6879 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6883 Loss_G: 0.7118 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6831 Loss_G: 0.6959 acc: 95.3%\n",
      "[BATCH 5/149] Loss_D: 0.6726 Loss_G: 0.6887 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6871 Loss_G: 0.6753 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6956 Loss_G: 0.6898 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6353 Loss_G: 0.7035 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6418 Loss_G: 0.6805 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6725 Loss_G: 0.6710 acc: 82.8%\n",
      "[BATCH 11/149] Loss_D: 0.6345 Loss_G: 0.6641 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6836 Loss_G: 0.6801 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6739 Loss_G: 0.6981 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6554 Loss_G: 0.6847 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6546 Loss_G: 0.6911 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6768 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.7220 Loss_G: 0.7365 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6740 Loss_G: 0.7577 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.6916 Loss_G: 0.7315 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6810 Loss_G: 0.7044 acc: 82.8%\n",
      "[BATCH 21/149] Loss_D: 0.6544 Loss_G: 0.6875 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6910 Loss_G: 0.7128 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6716 Loss_G: 0.6950 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6725 Loss_G: 0.7104 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.7287 Loss_G: 0.6998 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6744 Loss_G: 0.7028 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6935 Loss_G: 0.6920 acc: 87.5%\n",
      "[BATCH 28/149] Loss_D: 0.7128 Loss_G: 0.6977 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.6999 Loss_G: 0.7264 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6905 Loss_G: 0.7651 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6856 Loss_G: 0.7297 acc: 92.2%\n",
      "[BATCH 32/149] Loss_D: 0.6724 Loss_G: 0.7163 acc: 95.3%\n",
      "[BATCH 33/149] Loss_D: 0.6748 Loss_G: 0.6961 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.6736 Loss_G: 0.6937 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6977 Loss_G: 0.7075 acc: 90.6%\n",
      "[EPOCH 5250] TEST ACC is : 77.3%\n",
      "[BATCH 36/149] Loss_D: 0.6748 Loss_G: 0.7139 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6807 Loss_G: 0.7029 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6972 Loss_G: 0.7002 acc: 79.7%\n",
      "[BATCH 39/149] Loss_D: 0.6793 Loss_G: 0.7022 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6514 Loss_G: 0.6910 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6658 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6684 Loss_G: 0.6910 acc: 81.2%\n",
      "[BATCH 43/149] Loss_D: 0.6961 Loss_G: 0.6883 acc: 90.6%\n",
      "[BATCH 44/149] Loss_D: 0.6772 Loss_G: 0.6832 acc: 85.9%\n",
      "[BATCH 45/149] Loss_D: 0.6709 Loss_G: 0.7052 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.7246 Loss_G: 0.7203 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.6808 Loss_G: 0.7137 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6884 Loss_G: 0.7479 acc: 93.8%\n",
      "[BATCH 49/149] Loss_D: 0.6425 Loss_G: 0.6969 acc: 96.9%\n",
      "[BATCH 50/149] Loss_D: 0.6786 Loss_G: 0.7095 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.6742 Loss_G: 0.6944 acc: 82.8%\n",
      "[BATCH 52/149] Loss_D: 0.6923 Loss_G: 0.6880 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6493 Loss_G: 0.6901 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6866 Loss_G: 0.6862 acc: 84.4%\n",
      "[BATCH 55/149] Loss_D: 0.6669 Loss_G: 0.7099 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6550 Loss_G: 0.6900 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6937 Loss_G: 0.6909 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6538 Loss_G: 0.6981 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.7062 Loss_G: 0.7139 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.7073 Loss_G: 0.7301 acc: 82.8%\n",
      "[BATCH 61/149] Loss_D: 0.6756 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6997 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6826 Loss_G: 0.6947 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6908 Loss_G: 0.6995 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6817 Loss_G: 0.6847 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.6484 Loss_G: 0.6924 acc: 93.8%\n",
      "[BATCH 67/149] Loss_D: 0.6329 Loss_G: 0.6673 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6643 Loss_G: 0.6799 acc: 95.3%\n",
      "[BATCH 69/149] Loss_D: 0.7497 Loss_G: 0.7190 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6834 Loss_G: 0.7216 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6994 Loss_G: 0.7375 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6939 Loss_G: 0.7526 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.7145 Loss_G: 0.7333 acc: 85.9%\n",
      "[BATCH 74/149] Loss_D: 0.7189 Loss_G: 0.7286 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6531 Loss_G: 0.7063 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6696 Loss_G: 0.7027 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6688 Loss_G: 0.7061 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6943 Loss_G: 0.7007 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.7090 Loss_G: 0.7169 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6726 Loss_G: 0.7172 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6758 Loss_G: 0.7041 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.7006 Loss_G: 0.7171 acc: 81.2%\n",
      "[BATCH 83/149] Loss_D: 0.6534 Loss_G: 0.7003 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6850 Loss_G: 0.6852 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7010 Loss_G: 0.7033 acc: 93.8%\n",
      "[EPOCH 5300] TEST ACC is : 76.4%\n",
      "[BATCH 86/149] Loss_D: 0.6776 Loss_G: 0.7054 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.7062 Loss_G: 0.7075 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.7221 Loss_G: 0.7194 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.6430 Loss_G: 0.6951 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6654 Loss_G: 0.6976 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6601 Loss_G: 0.7058 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6582 Loss_G: 0.7091 acc: 95.3%\n",
      "[BATCH 93/149] Loss_D: 0.6886 Loss_G: 0.6972 acc: 85.9%\n",
      "[BATCH 94/149] Loss_D: 0.7081 Loss_G: 0.6973 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.7078 Loss_G: 0.7275 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.7292 Loss_G: 0.7185 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.7137 Loss_G: 0.7104 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6794 Loss_G: 0.7084 acc: 87.5%\n",
      "[BATCH 99/149] Loss_D: 0.6677 Loss_G: 0.7107 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6920 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6644 Loss_G: 0.7058 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.7191 Loss_G: 0.7185 acc: 82.8%\n",
      "[BATCH 103/149] Loss_D: 0.6821 Loss_G: 0.7377 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6639 Loss_G: 0.7232 acc: 95.3%\n",
      "[BATCH 105/149] Loss_D: 0.6917 Loss_G: 0.7115 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6919 Loss_G: 0.6908 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6721 Loss_G: 0.7105 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6912 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6882 Loss_G: 0.7163 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6882 Loss_G: 0.7257 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7131 Loss_G: 0.7310 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6600 Loss_G: 0.7110 acc: 93.8%\n",
      "[BATCH 113/149] Loss_D: 0.7025 Loss_G: 0.7111 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6447 Loss_G: 0.6896 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6560 Loss_G: 0.7029 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6790 Loss_G: 0.6816 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6575 Loss_G: 0.6935 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.6662 Loss_G: 0.6930 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6849 Loss_G: 0.6618 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.6926 Loss_G: 0.6805 acc: 81.2%\n",
      "[BATCH 121/149] Loss_D: 0.7263 Loss_G: 0.7406 acc: 85.9%\n",
      "[BATCH 122/149] Loss_D: 0.7166 Loss_G: 0.7218 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7163 Loss_G: 0.7217 acc: 90.6%\n",
      "[BATCH 124/149] Loss_D: 0.6668 Loss_G: 0.6976 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6668 Loss_G: 0.6975 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6420 Loss_G: 0.7278 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6638 Loss_G: 0.6965 acc: 85.9%\n",
      "[BATCH 128/149] Loss_D: 0.6917 Loss_G: 0.7025 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6585 Loss_G: 0.6980 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6874 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 131/149] Loss_D: 0.6520 Loss_G: 0.6837 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.6764 Loss_G: 0.6973 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.7182 Loss_G: 0.7435 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6926 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7111 Loss_G: 0.7241 acc: 92.2%\n",
      "[EPOCH 5350] TEST ACC is : 78.5%\n",
      "[BATCH 136/149] Loss_D: 0.6825 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6520 Loss_G: 0.6935 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6769 Loss_G: 0.6943 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6906 Loss_G: 0.6997 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.7146 Loss_G: 0.7105 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.6739 Loss_G: 0.6981 acc: 87.5%\n",
      "[BATCH 142/149] Loss_D: 0.6875 Loss_G: 0.7055 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6982 Loss_G: 0.7016 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6816 Loss_G: 0.6938 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6458 Loss_G: 0.6984 acc: 95.3%\n",
      "[BATCH 146/149] Loss_D: 0.6804 Loss_G: 0.6958 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6711 Loss_G: 0.6814 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6882 Loss_G: 0.7056 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6716 Loss_G: 0.7065 acc: 87.5%\n",
      "-----THE [36/50] epoch end-----\n",
      "-----THE [37/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7050 Loss_G: 0.7277 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.6739 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6478 Loss_G: 0.7078 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6700 Loss_G: 0.7039 acc: 98.4%\n",
      "[BATCH 5/149] Loss_D: 0.7149 Loss_G: 0.7007 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.7083 Loss_G: 0.7136 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6807 Loss_G: 0.7083 acc: 85.9%\n",
      "[BATCH 8/149] Loss_D: 0.6636 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6691 Loss_G: 0.6952 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6632 Loss_G: 0.6866 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6679 Loss_G: 0.7004 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6642 Loss_G: 0.6887 acc: 92.2%\n",
      "[BATCH 13/149] Loss_D: 0.6359 Loss_G: 0.6761 acc: 92.2%\n",
      "[BATCH 14/149] Loss_D: 0.6686 Loss_G: 0.6906 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6284 Loss_G: 0.6866 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6987 Loss_G: 0.6967 acc: 87.5%\n",
      "[BATCH 17/149] Loss_D: 0.6723 Loss_G: 0.7063 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.7057 Loss_G: 0.7237 acc: 79.7%\n",
      "[BATCH 19/149] Loss_D: 0.6801 Loss_G: 0.7725 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6835 Loss_G: 0.7118 acc: 92.2%\n",
      "[BATCH 21/149] Loss_D: 0.6704 Loss_G: 0.7059 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6398 Loss_G: 0.6962 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6958 Loss_G: 0.6915 acc: 93.8%\n",
      "[BATCH 24/149] Loss_D: 0.6712 Loss_G: 0.7132 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.6490 Loss_G: 0.6975 acc: 95.3%\n",
      "[BATCH 26/149] Loss_D: 0.7095 Loss_G: 0.7187 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.6914 Loss_G: 0.7164 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6684 Loss_G: 0.6954 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6654 Loss_G: 0.6882 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.7083 Loss_G: 0.7051 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.6976 Loss_G: 0.7083 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7165 Loss_G: 0.6873 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6892 Loss_G: 0.7071 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.6734 Loss_G: 0.6965 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6863 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6960 Loss_G: 0.7039 acc: 85.9%\n",
      "[EPOCH 5400] TEST ACC is : 77.3%\n",
      "[BATCH 37/149] Loss_D: 0.6549 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6884 Loss_G: 0.6883 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6702 Loss_G: 0.6848 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6679 Loss_G: 0.6961 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6689 Loss_G: 0.6879 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6880 Loss_G: 0.6811 acc: 93.8%\n",
      "[BATCH 43/149] Loss_D: 0.7266 Loss_G: 0.7022 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6996 Loss_G: 0.7246 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6586 Loss_G: 0.6931 acc: 93.8%\n",
      "[BATCH 46/149] Loss_D: 0.6472 Loss_G: 0.6841 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6731 Loss_G: 0.6900 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.7007 Loss_G: 0.7021 acc: 87.5%\n",
      "[BATCH 49/149] Loss_D: 0.7117 Loss_G: 0.7207 acc: 89.1%\n",
      "[BATCH 50/149] Loss_D: 0.6528 Loss_G: 0.6910 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.7014 Loss_G: 0.7011 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.7208 Loss_G: 0.7043 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.7214 Loss_G: 0.7221 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.7046 Loss_G: 0.7369 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.7016 Loss_G: 0.7176 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.7202 Loss_G: 0.7149 acc: 75.0%\n",
      "[BATCH 57/149] Loss_D: 0.6696 Loss_G: 0.6856 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.7058 Loss_G: 0.6985 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6782 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 60/149] Loss_D: 0.7003 Loss_G: 0.7327 acc: 84.4%\n",
      "[BATCH 61/149] Loss_D: 0.6686 Loss_G: 0.7110 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6823 Loss_G: 0.7099 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6892 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6909 Loss_G: 0.7000 acc: 84.4%\n",
      "[BATCH 65/149] Loss_D: 0.6796 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6763 Loss_G: 0.7139 acc: 92.2%\n",
      "[BATCH 67/149] Loss_D: 0.6974 Loss_G: 0.7138 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.6604 Loss_G: 0.6970 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7098 Loss_G: 0.7196 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7259 Loss_G: 0.7251 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6644 Loss_G: 0.6924 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6536 Loss_G: 0.6956 acc: 92.2%\n",
      "[BATCH 73/149] Loss_D: 0.6566 Loss_G: 0.6928 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6790 Loss_G: 0.6886 acc: 82.8%\n",
      "[BATCH 75/149] Loss_D: 0.6840 Loss_G: 0.7258 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6571 Loss_G: 0.6864 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6485 Loss_G: 0.6698 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.7147 Loss_G: 0.6793 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.6407 Loss_G: 0.6873 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.6743 Loss_G: 0.6952 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.7136 Loss_G: 0.6997 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7036 Loss_G: 0.6932 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6639 Loss_G: 0.6866 acc: 89.1%\n",
      "[BATCH 84/149] Loss_D: 0.6608 Loss_G: 0.6730 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.7230 Loss_G: 0.7063 acc: 84.4%\n",
      "[BATCH 86/149] Loss_D: 0.6668 Loss_G: 0.7067 acc: 89.1%\n",
      "[EPOCH 5450] TEST ACC is : 77.3%\n",
      "[BATCH 87/149] Loss_D: 0.6564 Loss_G: 0.6984 acc: 78.1%\n",
      "[BATCH 88/149] Loss_D: 0.6978 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6641 Loss_G: 0.7020 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6773 Loss_G: 0.7071 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6928 Loss_G: 0.7078 acc: 90.6%\n",
      "[BATCH 92/149] Loss_D: 0.6748 Loss_G: 0.7088 acc: 79.7%\n",
      "[BATCH 93/149] Loss_D: 0.6667 Loss_G: 0.6917 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.7012 Loss_G: 0.7209 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6563 Loss_G: 0.7080 acc: 87.5%\n",
      "[BATCH 96/149] Loss_D: 0.6836 Loss_G: 0.6916 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6897 Loss_G: 0.7007 acc: 90.6%\n",
      "[BATCH 98/149] Loss_D: 0.6545 Loss_G: 0.6859 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6965 Loss_G: 0.6841 acc: 85.9%\n",
      "[BATCH 100/149] Loss_D: 0.7169 Loss_G: 0.7081 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6540 Loss_G: 0.6843 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.6558 Loss_G: 0.6865 acc: 95.3%\n",
      "[BATCH 103/149] Loss_D: 0.6971 Loss_G: 0.7019 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.6439 Loss_G: 0.6797 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.6805 Loss_G: 0.6766 acc: 90.6%\n",
      "[BATCH 106/149] Loss_D: 0.6951 Loss_G: 0.6931 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.6786 Loss_G: 0.6821 acc: 89.1%\n",
      "[BATCH 108/149] Loss_D: 0.6505 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6733 Loss_G: 0.6989 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6881 Loss_G: 0.7135 acc: 84.4%\n",
      "[BATCH 111/149] Loss_D: 0.6766 Loss_G: 0.6940 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6689 Loss_G: 0.6936 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6757 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6707 Loss_G: 0.7035 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.6696 Loss_G: 0.7001 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6769 Loss_G: 0.7042 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6811 Loss_G: 0.6860 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6704 Loss_G: 0.6873 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.6810 Loss_G: 0.7014 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6870 Loss_G: 0.6943 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6761 Loss_G: 0.7122 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6547 Loss_G: 0.6885 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6803 Loss_G: 0.6964 acc: 95.3%\n",
      "[BATCH 124/149] Loss_D: 0.6881 Loss_G: 0.7105 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.7099 Loss_G: 0.7031 acc: 78.1%\n",
      "[BATCH 126/149] Loss_D: 0.6834 Loss_G: 0.6794 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.7167 Loss_G: 0.7105 acc: 84.4%\n",
      "[BATCH 128/149] Loss_D: 0.6534 Loss_G: 0.6936 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.6835 Loss_G: 0.6982 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.7270 Loss_G: 0.7015 acc: 85.9%\n",
      "[BATCH 131/149] Loss_D: 0.6819 Loss_G: 0.7167 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6795 Loss_G: 0.7358 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6848 Loss_G: 0.7325 acc: 95.3%\n",
      "[BATCH 134/149] Loss_D: 0.6850 Loss_G: 0.7049 acc: 85.9%\n",
      "[BATCH 135/149] Loss_D: 0.7069 Loss_G: 0.7024 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6932 Loss_G: 0.7056 acc: 84.4%\n",
      "[EPOCH 5500] TEST ACC is : 76.8%\n",
      "[BATCH 137/149] Loss_D: 0.6922 Loss_G: 0.6911 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6832 Loss_G: 0.6867 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6941 Loss_G: 0.7140 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.6849 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.6680 Loss_G: 0.6944 acc: 90.6%\n",
      "[BATCH 142/149] Loss_D: 0.7078 Loss_G: 0.7136 acc: 82.8%\n",
      "[BATCH 143/149] Loss_D: 0.7076 Loss_G: 0.7265 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.6418 Loss_G: 0.7119 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6809 Loss_G: 0.7543 acc: 95.3%\n",
      "[BATCH 146/149] Loss_D: 0.7004 Loss_G: 0.7314 acc: 98.4%\n",
      "[BATCH 147/149] Loss_D: 0.6844 Loss_G: 0.6982 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6664 Loss_G: 0.6860 acc: 85.9%\n",
      "[BATCH 149/149] Loss_D: 0.6797 Loss_G: 0.6968 acc: 92.2%\n",
      "-----THE [37/50] epoch end-----\n",
      "-----THE [38/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6924 Loss_G: 0.7217 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6741 Loss_G: 0.7360 acc: 95.3%\n",
      "[BATCH 3/149] Loss_D: 0.6457 Loss_G: 0.7050 acc: 96.9%\n",
      "[BATCH 4/149] Loss_D: 0.6661 Loss_G: 0.6990 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6463 Loss_G: 0.6950 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6642 Loss_G: 0.6817 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6831 Loss_G: 0.6841 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.6799 Loss_G: 0.7077 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.6715 Loss_G: 0.7047 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6744 Loss_G: 0.6945 acc: 87.5%\n",
      "[BATCH 11/149] Loss_D: 0.6786 Loss_G: 0.7059 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6746 Loss_G: 0.7060 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6846 Loss_G: 0.6881 acc: 81.2%\n",
      "[BATCH 14/149] Loss_D: 0.6712 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6736 Loss_G: 0.6963 acc: 89.1%\n",
      "[BATCH 16/149] Loss_D: 0.6961 Loss_G: 0.7189 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6623 Loss_G: 0.6943 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6624 Loss_G: 0.7043 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.6887 Loss_G: 0.6869 acc: 84.4%\n",
      "[BATCH 20/149] Loss_D: 0.6800 Loss_G: 0.6813 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6813 Loss_G: 0.6933 acc: 82.8%\n",
      "[BATCH 22/149] Loss_D: 0.6896 Loss_G: 0.7399 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6923 Loss_G: 0.7147 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6961 Loss_G: 0.6950 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.6829 Loss_G: 0.7047 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6765 Loss_G: 0.6912 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6782 Loss_G: 0.7138 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6568 Loss_G: 0.6891 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6535 Loss_G: 0.6955 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6743 Loss_G: 0.6918 acc: 82.8%\n",
      "[BATCH 31/149] Loss_D: 0.6489 Loss_G: 0.6858 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.6618 Loss_G: 0.6819 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.7219 Loss_G: 0.6975 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7009 Loss_G: 0.7404 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7064 Loss_G: 0.7043 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6707 Loss_G: 0.6949 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7071 Loss_G: 0.6970 acc: 89.1%\n",
      "[EPOCH 5550] TEST ACC is : 77.0%\n",
      "[BATCH 38/149] Loss_D: 0.6983 Loss_G: 0.6848 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6819 Loss_G: 0.6900 acc: 89.1%\n",
      "[BATCH 40/149] Loss_D: 0.7194 Loss_G: 0.7016 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6560 Loss_G: 0.6901 acc: 90.6%\n",
      "[BATCH 42/149] Loss_D: 0.6546 Loss_G: 0.6919 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.7053 Loss_G: 0.7013 acc: 89.1%\n",
      "[BATCH 44/149] Loss_D: 0.6781 Loss_G: 0.7152 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6979 Loss_G: 0.6970 acc: 85.9%\n",
      "[BATCH 46/149] Loss_D: 0.6731 Loss_G: 0.6891 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6678 Loss_G: 0.6929 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.7037 Loss_G: 0.7163 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6852 Loss_G: 0.7062 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6989 Loss_G: 0.7064 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6975 Loss_G: 0.7220 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6836 Loss_G: 0.7013 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6774 Loss_G: 0.7169 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6526 Loss_G: 0.6848 acc: 92.2%\n",
      "[BATCH 55/149] Loss_D: 0.6592 Loss_G: 0.6706 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.7415 Loss_G: 0.7088 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.7058 Loss_G: 0.7055 acc: 85.9%\n",
      "[BATCH 58/149] Loss_D: 0.6969 Loss_G: 0.6994 acc: 81.2%\n",
      "[BATCH 59/149] Loss_D: 0.6568 Loss_G: 0.6836 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.6901 Loss_G: 0.6881 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6867 Loss_G: 0.6970 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6845 Loss_G: 0.7049 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6656 Loss_G: 0.6904 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6663 Loss_G: 0.6797 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.7237 Loss_G: 0.6917 acc: 92.2%\n",
      "[BATCH 66/149] Loss_D: 0.6905 Loss_G: 0.6920 acc: 87.5%\n",
      "[BATCH 67/149] Loss_D: 0.7115 Loss_G: 0.7005 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6899 Loss_G: 0.7182 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6723 Loss_G: 0.6997 acc: 96.9%\n",
      "[BATCH 70/149] Loss_D: 0.6828 Loss_G: 0.7109 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6880 Loss_G: 0.7220 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.7072 Loss_G: 0.7134 acc: 84.4%\n",
      "[BATCH 73/149] Loss_D: 0.6489 Loss_G: 0.6902 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6679 Loss_G: 0.7247 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.7026 Loss_G: 0.7070 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.6622 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6703 Loss_G: 0.6831 acc: 92.2%\n",
      "[BATCH 78/149] Loss_D: 0.6741 Loss_G: 0.6872 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6796 Loss_G: 0.6946 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6722 Loss_G: 0.7013 acc: 92.2%\n",
      "[BATCH 81/149] Loss_D: 0.6680 Loss_G: 0.7164 acc: 90.6%\n",
      "[BATCH 82/149] Loss_D: 0.6534 Loss_G: 0.6881 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.6533 Loss_G: 0.6899 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6963 Loss_G: 0.6974 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.6800 Loss_G: 0.7032 acc: 82.8%\n",
      "[BATCH 86/149] Loss_D: 0.6687 Loss_G: 0.7059 acc: 82.8%\n",
      "[BATCH 87/149] Loss_D: 0.6833 Loss_G: 0.6946 acc: 84.4%\n",
      "[EPOCH 5600] TEST ACC is : 77.1%\n",
      "[BATCH 88/149] Loss_D: 0.6764 Loss_G: 0.6936 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.6599 Loss_G: 0.7026 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6678 Loss_G: 0.6911 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.6610 Loss_G: 0.6911 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.6551 Loss_G: 0.7033 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6889 Loss_G: 0.6949 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.7089 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.7234 Loss_G: 0.7154 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.6569 Loss_G: 0.7190 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6738 Loss_G: 0.6946 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6670 Loss_G: 0.6886 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6764 Loss_G: 0.6989 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.6857 Loss_G: 0.6932 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.7068 Loss_G: 0.7016 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.6464 Loss_G: 0.7042 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6738 Loss_G: 0.6953 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.6756 Loss_G: 0.7221 acc: 90.6%\n",
      "[BATCH 105/149] Loss_D: 0.6929 Loss_G: 0.7219 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7081 Loss_G: 0.7168 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6578 Loss_G: 0.6844 acc: 84.4%\n",
      "[BATCH 108/149] Loss_D: 0.6493 Loss_G: 0.6861 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6674 Loss_G: 0.6996 acc: 95.3%\n",
      "[BATCH 110/149] Loss_D: 0.7098 Loss_G: 0.7447 acc: 87.5%\n",
      "[BATCH 111/149] Loss_D: 0.7540 Loss_G: 0.7511 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.6777 Loss_G: 0.7077 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6507 Loss_G: 0.6866 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.7005 Loss_G: 0.7036 acc: 92.2%\n",
      "[BATCH 115/149] Loss_D: 0.7196 Loss_G: 0.7158 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6865 Loss_G: 0.7032 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6985 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7170 Loss_G: 0.7051 acc: 81.2%\n",
      "[BATCH 119/149] Loss_D: 0.6312 Loss_G: 0.6744 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6668 Loss_G: 0.6822 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6749 Loss_G: 0.6757 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.6625 Loss_G: 0.6795 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6771 Loss_G: 0.6808 acc: 79.7%\n",
      "[BATCH 124/149] Loss_D: 0.6911 Loss_G: 0.7143 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6845 Loss_G: 0.6947 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6722 Loss_G: 0.7021 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6596 Loss_G: 0.6969 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6594 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.6980 Loss_G: 0.7388 acc: 89.1%\n",
      "[BATCH 130/149] Loss_D: 0.6865 Loss_G: 0.7269 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7045 Loss_G: 0.7009 acc: 87.5%\n",
      "[BATCH 132/149] Loss_D: 0.7165 Loss_G: 0.7100 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6874 Loss_G: 0.7131 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6483 Loss_G: 0.6883 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6914 Loss_G: 0.7008 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.7186 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.7114 Loss_G: 0.7273 acc: 84.4%\n",
      "[EPOCH 5650] TEST ACC is : 76.6%\n",
      "[BATCH 138/149] Loss_D: 0.6837 Loss_G: 0.7205 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.6577 Loss_G: 0.7097 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6591 Loss_G: 0.6911 acc: 92.2%\n",
      "[BATCH 141/149] Loss_D: 0.6997 Loss_G: 0.7112 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6874 Loss_G: 0.7125 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.7000 Loss_G: 0.7391 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6767 Loss_G: 0.6958 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6727 Loss_G: 0.6810 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.7029 Loss_G: 0.7071 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.7356 Loss_G: 0.7112 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.7111 Loss_G: 0.7176 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.6669 Loss_G: 0.6991 acc: 87.5%\n",
      "-----THE [38/50] epoch end-----\n",
      "-----THE [39/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.7256 Loss_G: 0.7313 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.6904 Loss_G: 0.7155 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.7033 Loss_G: 0.7185 acc: 87.5%\n",
      "[BATCH 4/149] Loss_D: 0.7175 Loss_G: 0.7262 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6516 Loss_G: 0.7095 acc: 95.3%\n",
      "[BATCH 6/149] Loss_D: 0.7112 Loss_G: 0.7097 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.6854 Loss_G: 0.7153 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6780 Loss_G: 0.6985 acc: 87.5%\n",
      "[BATCH 9/149] Loss_D: 0.6802 Loss_G: 0.6964 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6854 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6716 Loss_G: 0.7099 acc: 95.3%\n",
      "[BATCH 12/149] Loss_D: 0.6720 Loss_G: 0.7062 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.7021 Loss_G: 0.7278 acc: 90.6%\n",
      "[BATCH 14/149] Loss_D: 0.7112 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 15/149] Loss_D: 0.6996 Loss_G: 0.7538 acc: 90.6%\n",
      "[BATCH 16/149] Loss_D: 0.7082 Loss_G: 0.7320 acc: 81.2%\n",
      "[BATCH 17/149] Loss_D: 0.6769 Loss_G: 0.7089 acc: 93.8%\n",
      "[BATCH 18/149] Loss_D: 0.6710 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6985 Loss_G: 0.7074 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.6630 Loss_G: 0.6926 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6701 Loss_G: 0.7089 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6785 Loss_G: 0.7041 acc: 96.9%\n",
      "[BATCH 23/149] Loss_D: 0.6950 Loss_G: 0.7031 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6904 Loss_G: 0.7173 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6586 Loss_G: 0.7106 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6851 Loss_G: 0.7137 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.6772 Loss_G: 0.7151 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.7049 Loss_G: 0.7177 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.7516 Loss_G: 0.7272 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6937 Loss_G: 0.7133 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.7095 Loss_G: 0.7104 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.7028 Loss_G: 0.7109 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6607 Loss_G: 0.7031 acc: 92.2%\n",
      "[BATCH 34/149] Loss_D: 0.7021 Loss_G: 0.6929 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.6846 Loss_G: 0.6918 acc: 89.1%\n",
      "[BATCH 36/149] Loss_D: 0.6550 Loss_G: 0.6856 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6562 Loss_G: 0.6816 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6494 Loss_G: 0.6778 acc: 85.9%\n",
      "[EPOCH 5700] TEST ACC is : 77.0%\n",
      "[BATCH 39/149] Loss_D: 0.6627 Loss_G: 0.7138 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6655 Loss_G: 0.6950 acc: 93.8%\n",
      "[BATCH 41/149] Loss_D: 0.6904 Loss_G: 0.6957 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.6669 Loss_G: 0.6968 acc: 81.2%\n",
      "[BATCH 43/149] Loss_D: 0.7110 Loss_G: 0.7162 acc: 87.5%\n",
      "[BATCH 44/149] Loss_D: 0.6678 Loss_G: 0.6964 acc: 82.8%\n",
      "[BATCH 45/149] Loss_D: 0.6485 Loss_G: 0.6886 acc: 87.5%\n",
      "[BATCH 46/149] Loss_D: 0.6579 Loss_G: 0.6615 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6800 Loss_G: 0.6747 acc: 95.3%\n",
      "[BATCH 48/149] Loss_D: 0.6724 Loss_G: 0.6955 acc: 89.1%\n",
      "[BATCH 49/149] Loss_D: 0.6391 Loss_G: 0.6908 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.6595 Loss_G: 0.6951 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6704 Loss_G: 0.7018 acc: 92.2%\n",
      "[BATCH 52/149] Loss_D: 0.6988 Loss_G: 0.7008 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.6536 Loss_G: 0.6762 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6647 Loss_G: 0.7153 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6861 Loss_G: 0.7009 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.7145 Loss_G: 0.7077 acc: 87.5%\n",
      "[BATCH 57/149] Loss_D: 0.6949 Loss_G: 0.7276 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.6667 Loss_G: 0.6852 acc: 85.9%\n",
      "[BATCH 59/149] Loss_D: 0.6865 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 60/149] Loss_D: 0.6884 Loss_G: 0.7012 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6951 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 62/149] Loss_D: 0.6955 Loss_G: 0.6947 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6761 Loss_G: 0.6882 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6572 Loss_G: 0.6817 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.7009 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6928 Loss_G: 0.7188 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6526 Loss_G: 0.7134 acc: 84.4%\n",
      "[BATCH 68/149] Loss_D: 0.6434 Loss_G: 0.7082 acc: 89.1%\n",
      "[BATCH 69/149] Loss_D: 0.7113 Loss_G: 0.7119 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7151 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6612 Loss_G: 0.7066 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6607 Loss_G: 0.6984 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.6542 Loss_G: 0.6697 acc: 82.8%\n",
      "[BATCH 74/149] Loss_D: 0.6651 Loss_G: 0.6827 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6799 Loss_G: 0.6910 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6631 Loss_G: 0.6761 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6679 Loss_G: 0.6959 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6721 Loss_G: 0.6836 acc: 81.2%\n",
      "[BATCH 79/149] Loss_D: 0.6764 Loss_G: 0.6912 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6886 Loss_G: 0.6941 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6955 Loss_G: 0.6996 acc: 89.1%\n",
      "[BATCH 82/149] Loss_D: 0.6858 Loss_G: 0.7009 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6727 Loss_G: 0.7055 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.6872 Loss_G: 0.7061 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.6569 Loss_G: 0.6773 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6917 Loss_G: 0.7071 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6869 Loss_G: 0.7317 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6615 Loss_G: 0.7207 acc: 84.4%\n",
      "[EPOCH 5750] TEST ACC is : 77.5%\n",
      "[BATCH 89/149] Loss_D: 0.7138 Loss_G: 0.7123 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7170 Loss_G: 0.7172 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.6780 Loss_G: 0.7026 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6754 Loss_G: 0.6967 acc: 93.8%\n",
      "[BATCH 93/149] Loss_D: 0.6978 Loss_G: 0.7227 acc: 93.8%\n",
      "[BATCH 94/149] Loss_D: 0.6636 Loss_G: 0.7035 acc: 90.6%\n",
      "[BATCH 95/149] Loss_D: 0.6672 Loss_G: 0.6908 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6743 Loss_G: 0.7054 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6968 Loss_G: 0.6986 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.6426 Loss_G: 0.6753 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6686 Loss_G: 0.7084 acc: 96.9%\n",
      "[BATCH 100/149] Loss_D: 0.6919 Loss_G: 0.6957 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.6508 Loss_G: 0.6911 acc: 92.2%\n",
      "[BATCH 102/149] Loss_D: 0.6596 Loss_G: 0.6882 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6816 Loss_G: 0.6855 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6747 Loss_G: 0.6867 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6816 Loss_G: 0.6936 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7052 Loss_G: 0.7340 acc: 96.9%\n",
      "[BATCH 107/149] Loss_D: 0.6974 Loss_G: 0.7229 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6730 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6959 Loss_G: 0.7072 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6874 Loss_G: 0.7076 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6537 Loss_G: 0.6940 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7540 Loss_G: 0.7337 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6744 Loss_G: 0.7223 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6972 Loss_G: 0.6997 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.6874 Loss_G: 0.6960 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6693 Loss_G: 0.6891 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.6593 Loss_G: 0.6863 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.6825 Loss_G: 0.7042 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6692 Loss_G: 0.7111 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.7181 Loss_G: 0.7187 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6494 Loss_G: 0.6954 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6375 Loss_G: 0.6723 acc: 84.4%\n",
      "[BATCH 123/149] Loss_D: 0.6731 Loss_G: 0.6930 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6839 Loss_G: 0.6944 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6573 Loss_G: 0.6966 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.6523 Loss_G: 0.7144 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6494 Loss_G: 0.7070 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7427 Loss_G: 0.7227 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6613 Loss_G: 0.7009 acc: 82.8%\n",
      "[BATCH 130/149] Loss_D: 0.6956 Loss_G: 0.7147 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6817 Loss_G: 0.7213 acc: 90.6%\n",
      "[BATCH 132/149] Loss_D: 0.6580 Loss_G: 0.7220 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6946 Loss_G: 0.7263 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6965 Loss_G: 0.7119 acc: 78.1%\n",
      "[BATCH 135/149] Loss_D: 0.6866 Loss_G: 0.7092 acc: 95.3%\n",
      "[BATCH 136/149] Loss_D: 0.7248 Loss_G: 0.7161 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6669 Loss_G: 0.6916 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6950 Loss_G: 0.6919 acc: 82.8%\n",
      "[EPOCH 5800] TEST ACC is : 77.0%\n",
      "[BATCH 139/149] Loss_D: 0.6705 Loss_G: 0.6807 acc: 90.6%\n",
      "[BATCH 140/149] Loss_D: 0.6562 Loss_G: 0.6809 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6730 Loss_G: 0.6757 acc: 84.4%\n",
      "[BATCH 142/149] Loss_D: 0.6845 Loss_G: 0.7001 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6805 Loss_G: 0.7109 acc: 87.5%\n",
      "[BATCH 144/149] Loss_D: 0.6464 Loss_G: 0.6867 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.7392 Loss_G: 0.7279 acc: 84.4%\n",
      "[BATCH 146/149] Loss_D: 0.7381 Loss_G: 0.7496 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6661 Loss_G: 0.7003 acc: 93.8%\n",
      "[BATCH 148/149] Loss_D: 0.6730 Loss_G: 0.6919 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6718 Loss_G: 0.7059 acc: 92.2%\n",
      "-----THE [39/50] epoch end-----\n",
      "-----THE [40/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6604 Loss_G: 0.6820 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.7051 Loss_G: 0.7298 acc: 87.5%\n",
      "[BATCH 3/149] Loss_D: 0.6961 Loss_G: 0.7221 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.6780 Loss_G: 0.7094 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6878 Loss_G: 0.6956 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6765 Loss_G: 0.7243 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6813 Loss_G: 0.7024 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6700 Loss_G: 0.7112 acc: 93.8%\n",
      "[BATCH 9/149] Loss_D: 0.6936 Loss_G: 0.7164 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6710 Loss_G: 0.6808 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.7009 Loss_G: 0.6822 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6938 Loss_G: 0.6842 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.7025 Loss_G: 0.7170 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.6969 Loss_G: 0.7185 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6866 Loss_G: 0.7152 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.7005 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.7218 Loss_G: 0.6940 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.7092 Loss_G: 0.7163 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7322 Loss_G: 0.7222 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.6795 Loss_G: 0.7175 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6706 Loss_G: 0.6920 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.6812 Loss_G: 0.6877 acc: 84.4%\n",
      "[BATCH 23/149] Loss_D: 0.6865 Loss_G: 0.6920 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.7077 Loss_G: 0.7124 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6774 Loss_G: 0.7003 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6544 Loss_G: 0.6733 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.7165 Loss_G: 0.6929 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6804 Loss_G: 0.6886 acc: 87.5%\n",
      "[BATCH 29/149] Loss_D: 0.6887 Loss_G: 0.6898 acc: 84.4%\n",
      "[BATCH 30/149] Loss_D: 0.6924 Loss_G: 0.6874 acc: 79.7%\n",
      "[BATCH 31/149] Loss_D: 0.6947 Loss_G: 0.6913 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.7111 Loss_G: 0.7007 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6880 Loss_G: 0.7000 acc: 79.7%\n",
      "[BATCH 34/149] Loss_D: 0.7138 Loss_G: 0.7112 acc: 84.4%\n",
      "[BATCH 35/149] Loss_D: 0.6513 Loss_G: 0.7099 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6746 Loss_G: 0.7162 acc: 90.6%\n",
      "[BATCH 37/149] Loss_D: 0.6938 Loss_G: 0.7179 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6893 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6847 Loss_G: 0.7158 acc: 89.1%\n",
      "[EPOCH 5850] TEST ACC is : 76.6%\n",
      "[BATCH 40/149] Loss_D: 0.6674 Loss_G: 0.7038 acc: 87.5%\n",
      "[BATCH 41/149] Loss_D: 0.6755 Loss_G: 0.7078 acc: 92.2%\n",
      "[BATCH 42/149] Loss_D: 0.6538 Loss_G: 0.7108 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6689 Loss_G: 0.6976 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7121 Loss_G: 0.7050 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6642 Loss_G: 0.6839 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.7013 Loss_G: 0.6927 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6688 Loss_G: 0.6980 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6800 Loss_G: 0.7202 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6675 Loss_G: 0.6937 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7008 Loss_G: 0.7006 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.7033 Loss_G: 0.7208 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.7000 Loss_G: 0.7278 acc: 87.5%\n",
      "[BATCH 53/149] Loss_D: 0.7141 Loss_G: 0.7129 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6844 Loss_G: 0.7067 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6940 Loss_G: 0.7179 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6604 Loss_G: 0.6912 acc: 95.3%\n",
      "[BATCH 57/149] Loss_D: 0.6598 Loss_G: 0.6886 acc: 89.1%\n",
      "[BATCH 58/149] Loss_D: 0.7006 Loss_G: 0.6885 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6779 Loss_G: 0.6934 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.6812 Loss_G: 0.7028 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6638 Loss_G: 0.7082 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6680 Loss_G: 0.7027 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6780 Loss_G: 0.6840 acc: 84.4%\n",
      "[BATCH 64/149] Loss_D: 0.6786 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.6772 Loss_G: 0.6948 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.6817 Loss_G: 0.7066 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6575 Loss_G: 0.6966 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6681 Loss_G: 0.7113 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.7206 Loss_G: 0.7178 acc: 93.8%\n",
      "[BATCH 70/149] Loss_D: 0.6716 Loss_G: 0.7016 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6983 Loss_G: 0.7003 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6848 Loss_G: 0.7135 acc: 82.8%\n",
      "[BATCH 73/149] Loss_D: 0.6867 Loss_G: 0.7155 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6582 Loss_G: 0.6962 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6774 Loss_G: 0.6804 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.6873 Loss_G: 0.6865 acc: 93.8%\n",
      "[BATCH 77/149] Loss_D: 0.6784 Loss_G: 0.6961 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6638 Loss_G: 0.6855 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6769 Loss_G: 0.7066 acc: 93.8%\n",
      "[BATCH 80/149] Loss_D: 0.6810 Loss_G: 0.7131 acc: 89.1%\n",
      "[BATCH 81/149] Loss_D: 0.6964 Loss_G: 0.7120 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.6634 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 83/149] Loss_D: 0.6647 Loss_G: 0.7127 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.6738 Loss_G: 0.6817 acc: 82.8%\n",
      "[BATCH 85/149] Loss_D: 0.6554 Loss_G: 0.6804 acc: 98.4%\n",
      "[BATCH 86/149] Loss_D: 0.7075 Loss_G: 0.6781 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6771 Loss_G: 0.6944 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6498 Loss_G: 0.6930 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6627 Loss_G: 0.6973 acc: 89.1%\n",
      "[EPOCH 5900] TEST ACC is : 76.4%\n",
      "[BATCH 90/149] Loss_D: 0.6640 Loss_G: 0.6928 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6672 Loss_G: 0.7278 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6809 Loss_G: 0.7104 acc: 81.2%\n",
      "[BATCH 93/149] Loss_D: 0.6681 Loss_G: 0.6808 acc: 89.1%\n",
      "[BATCH 94/149] Loss_D: 0.6575 Loss_G: 0.7211 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.6936 Loss_G: 0.7301 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6816 Loss_G: 0.6912 acc: 84.4%\n",
      "[BATCH 97/149] Loss_D: 0.6985 Loss_G: 0.6923 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6713 Loss_G: 0.6809 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6741 Loss_G: 0.6884 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6548 Loss_G: 0.6977 acc: 96.9%\n",
      "[BATCH 101/149] Loss_D: 0.7104 Loss_G: 0.6917 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6954 Loss_G: 0.7041 acc: 84.4%\n",
      "[BATCH 103/149] Loss_D: 0.6587 Loss_G: 0.6912 acc: 85.9%\n",
      "[BATCH 104/149] Loss_D: 0.6753 Loss_G: 0.6902 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6803 Loss_G: 0.6793 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.6524 Loss_G: 0.6808 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.7407 Loss_G: 0.7089 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7139 Loss_G: 0.7251 acc: 89.1%\n",
      "[BATCH 109/149] Loss_D: 0.6836 Loss_G: 0.7041 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6735 Loss_G: 0.7114 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6674 Loss_G: 0.6752 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.7326 Loss_G: 0.7237 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7121 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6696 Loss_G: 0.6919 acc: 87.5%\n",
      "[BATCH 115/149] Loss_D: 0.6646 Loss_G: 0.6845 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6716 Loss_G: 0.6843 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6683 Loss_G: 0.6846 acc: 87.5%\n",
      "[BATCH 118/149] Loss_D: 0.6397 Loss_G: 0.6737 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6727 Loss_G: 0.7038 acc: 90.6%\n",
      "[BATCH 120/149] Loss_D: 0.6772 Loss_G: 0.7001 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6714 Loss_G: 0.6795 acc: 79.7%\n",
      "[BATCH 122/149] Loss_D: 0.6879 Loss_G: 0.6860 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.6929 Loss_G: 0.7024 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.6702 Loss_G: 0.6754 acc: 76.6%\n",
      "[BATCH 125/149] Loss_D: 0.6494 Loss_G: 0.6840 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6946 Loss_G: 0.6814 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6711 Loss_G: 0.6960 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6619 Loss_G: 0.7403 acc: 95.3%\n",
      "[BATCH 129/149] Loss_D: 0.6804 Loss_G: 0.7368 acc: 93.8%\n",
      "[BATCH 130/149] Loss_D: 0.6998 Loss_G: 0.7066 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6600 Loss_G: 0.6863 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6885 Loss_G: 0.6923 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6969 Loss_G: 0.7120 acc: 87.5%\n",
      "[BATCH 134/149] Loss_D: 0.6576 Loss_G: 0.7394 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.6829 Loss_G: 0.7090 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6817 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6998 Loss_G: 0.6973 acc: 90.6%\n",
      "[BATCH 138/149] Loss_D: 0.6645 Loss_G: 0.6914 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6865 Loss_G: 0.6910 acc: 81.2%\n",
      "[EPOCH 5950] TEST ACC is : 77.3%\n",
      "[BATCH 140/149] Loss_D: 0.6535 Loss_G: 0.6872 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6860 Loss_G: 0.6863 acc: 82.8%\n",
      "[BATCH 142/149] Loss_D: 0.6724 Loss_G: 0.7134 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.6958 Loss_G: 0.7219 acc: 85.9%\n",
      "[BATCH 144/149] Loss_D: 0.6689 Loss_G: 0.7076 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6689 Loss_G: 0.7103 acc: 93.8%\n",
      "[BATCH 146/149] Loss_D: 0.6582 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6560 Loss_G: 0.7019 acc: 90.6%\n",
      "[BATCH 148/149] Loss_D: 0.6671 Loss_G: 0.6827 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.6980 Loss_G: 0.7240 acc: 95.3%\n",
      "-----THE [40/50] epoch end-----\n",
      "-----THE [41/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6758 Loss_G: 0.7243 acc: 92.2%\n",
      "[BATCH 2/149] Loss_D: 0.6777 Loss_G: 0.7074 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.6985 Loss_G: 0.7166 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.6910 Loss_G: 0.7034 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6700 Loss_G: 0.6954 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6922 Loss_G: 0.7063 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.6686 Loss_G: 0.7084 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6423 Loss_G: 0.6846 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.7292 Loss_G: 0.7229 acc: 82.8%\n",
      "[BATCH 10/149] Loss_D: 0.6498 Loss_G: 0.6986 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6870 Loss_G: 0.6864 acc: 90.6%\n",
      "[BATCH 12/149] Loss_D: 0.6448 Loss_G: 0.6897 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7080 Loss_G: 0.7032 acc: 87.5%\n",
      "[BATCH 14/149] Loss_D: 0.7128 Loss_G: 0.7348 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6883 Loss_G: 0.7274 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6873 Loss_G: 0.7238 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6682 Loss_G: 0.7071 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.7644 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 19/149] Loss_D: 0.7026 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.7061 Loss_G: 0.7068 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6744 Loss_G: 0.6942 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6842 Loss_G: 0.6924 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7116 Loss_G: 0.7156 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6355 Loss_G: 0.6789 acc: 93.8%\n",
      "[BATCH 25/149] Loss_D: 0.6534 Loss_G: 0.6770 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6998 Loss_G: 0.6899 acc: 82.8%\n",
      "[BATCH 27/149] Loss_D: 0.6754 Loss_G: 0.7017 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.6449 Loss_G: 0.6852 acc: 93.8%\n",
      "[BATCH 29/149] Loss_D: 0.7057 Loss_G: 0.7049 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6700 Loss_G: 0.7002 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7041 Loss_G: 0.7011 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6718 Loss_G: 0.7178 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6832 Loss_G: 0.7249 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6585 Loss_G: 0.7000 acc: 90.6%\n",
      "[BATCH 35/149] Loss_D: 0.6717 Loss_G: 0.7074 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6835 Loss_G: 0.7044 acc: 93.8%\n",
      "[BATCH 37/149] Loss_D: 0.7078 Loss_G: 0.7170 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.6656 Loss_G: 0.7069 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6935 Loss_G: 0.6885 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6844 Loss_G: 0.6947 acc: 89.1%\n",
      "[EPOCH 6000] TEST ACC is : 77.3%\n",
      "[BATCH 41/149] Loss_D: 0.6933 Loss_G: 0.6788 acc: 82.8%\n",
      "[BATCH 42/149] Loss_D: 0.6772 Loss_G: 0.6981 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6789 Loss_G: 0.6966 acc: 95.3%\n",
      "[BATCH 44/149] Loss_D: 0.6444 Loss_G: 0.6753 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6817 Loss_G: 0.6801 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.7193 Loss_G: 0.7027 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.7180 Loss_G: 0.7373 acc: 90.6%\n",
      "[BATCH 48/149] Loss_D: 0.6589 Loss_G: 0.6953 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6638 Loss_G: 0.7058 acc: 85.9%\n",
      "[BATCH 50/149] Loss_D: 0.6684 Loss_G: 0.7013 acc: 85.9%\n",
      "[BATCH 51/149] Loss_D: 0.6893 Loss_G: 0.7033 acc: 95.3%\n",
      "[BATCH 52/149] Loss_D: 0.7028 Loss_G: 0.7170 acc: 85.9%\n",
      "[BATCH 53/149] Loss_D: 0.6862 Loss_G: 0.7157 acc: 85.9%\n",
      "[BATCH 54/149] Loss_D: 0.7174 Loss_G: 0.7149 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6793 Loss_G: 0.6943 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.6814 Loss_G: 0.6935 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.6782 Loss_G: 0.7195 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.6948 Loss_G: 0.7027 acc: 84.4%\n",
      "[BATCH 59/149] Loss_D: 0.6740 Loss_G: 0.6960 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6529 Loss_G: 0.7097 acc: 93.8%\n",
      "[BATCH 61/149] Loss_D: 0.6866 Loss_G: 0.7121 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.7191 Loss_G: 0.7135 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.6735 Loss_G: 0.6905 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.7096 Loss_G: 0.7227 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.6686 Loss_G: 0.6861 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6676 Loss_G: 0.6863 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7247 Loss_G: 0.7465 acc: 92.2%\n",
      "[BATCH 68/149] Loss_D: 0.7322 Loss_G: 0.7158 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6629 Loss_G: 0.6947 acc: 90.6%\n",
      "[BATCH 70/149] Loss_D: 0.6992 Loss_G: 0.7029 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.7046 Loss_G: 0.7417 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.6471 Loss_G: 0.6884 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.7012 Loss_G: 0.7085 acc: 96.9%\n",
      "[BATCH 74/149] Loss_D: 0.6582 Loss_G: 0.6944 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6856 Loss_G: 0.6953 acc: 90.6%\n",
      "[BATCH 76/149] Loss_D: 0.6592 Loss_G: 0.6941 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6791 Loss_G: 0.7020 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6667 Loss_G: 0.7137 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6915 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6833 Loss_G: 0.7009 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6703 Loss_G: 0.6921 acc: 92.2%\n",
      "[BATCH 82/149] Loss_D: 0.6420 Loss_G: 0.6917 acc: 95.3%\n",
      "[BATCH 83/149] Loss_D: 0.6695 Loss_G: 0.6748 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6993 Loss_G: 0.7044 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6879 Loss_G: 0.7044 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6711 Loss_G: 0.7085 acc: 87.5%\n",
      "[BATCH 87/149] Loss_D: 0.7380 Loss_G: 0.7123 acc: 87.5%\n",
      "[BATCH 88/149] Loss_D: 0.6550 Loss_G: 0.6874 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6382 Loss_G: 0.6798 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.6697 Loss_G: 0.6782 acc: 90.6%\n",
      "[EPOCH 6050] TEST ACC is : 77.1%\n",
      "[BATCH 91/149] Loss_D: 0.6463 Loss_G: 0.6831 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6839 Loss_G: 0.6995 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6562 Loss_G: 0.6870 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.7171 Loss_G: 0.7075 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6638 Loss_G: 0.6837 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.6700 Loss_G: 0.6824 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6855 Loss_G: 0.6991 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6858 Loss_G: 0.7324 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6896 Loss_G: 0.7161 acc: 92.2%\n",
      "[BATCH 100/149] Loss_D: 0.6786 Loss_G: 0.7141 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6469 Loss_G: 0.6949 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.7186 Loss_G: 0.6998 acc: 85.9%\n",
      "[BATCH 103/149] Loss_D: 0.6575 Loss_G: 0.6893 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6723 Loss_G: 0.6879 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6524 Loss_G: 0.6977 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.6567 Loss_G: 0.6827 acc: 87.5%\n",
      "[BATCH 107/149] Loss_D: 0.6557 Loss_G: 0.6681 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6940 Loss_G: 0.7153 acc: 93.8%\n",
      "[BATCH 109/149] Loss_D: 0.6740 Loss_G: 0.7001 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6582 Loss_G: 0.7004 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6685 Loss_G: 0.6979 acc: 93.8%\n",
      "[BATCH 112/149] Loss_D: 0.7006 Loss_G: 0.6977 acc: 92.2%\n",
      "[BATCH 113/149] Loss_D: 0.6645 Loss_G: 0.6943 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6422 Loss_G: 0.6766 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.6732 Loss_G: 0.6806 acc: 84.4%\n",
      "[BATCH 116/149] Loss_D: 0.6568 Loss_G: 0.7255 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6899 Loss_G: 0.7181 acc: 96.9%\n",
      "[BATCH 118/149] Loss_D: 0.6703 Loss_G: 0.7265 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6923 Loss_G: 0.7175 acc: 82.8%\n",
      "[BATCH 120/149] Loss_D: 0.7000 Loss_G: 0.7230 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.6982 Loss_G: 0.7026 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6598 Loss_G: 0.7092 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7078 Loss_G: 0.7041 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.6961 Loss_G: 0.7207 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6700 Loss_G: 0.7115 acc: 92.2%\n",
      "[BATCH 126/149] Loss_D: 0.6656 Loss_G: 0.7112 acc: 93.8%\n",
      "[BATCH 127/149] Loss_D: 0.6530 Loss_G: 0.6822 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6793 Loss_G: 0.6752 acc: 87.5%\n",
      "[BATCH 129/149] Loss_D: 0.6504 Loss_G: 0.6757 acc: 90.6%\n",
      "[BATCH 130/149] Loss_D: 0.7078 Loss_G: 0.6982 acc: 81.2%\n",
      "[BATCH 131/149] Loss_D: 0.6631 Loss_G: 0.6860 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.7306 Loss_G: 0.6905 acc: 82.8%\n",
      "[BATCH 133/149] Loss_D: 0.6966 Loss_G: 0.6935 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6845 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 135/149] Loss_D: 0.6918 Loss_G: 0.7080 acc: 92.2%\n",
      "[BATCH 136/149] Loss_D: 0.6722 Loss_G: 0.7240 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6649 Loss_G: 0.7080 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.6635 Loss_G: 0.6812 acc: 87.5%\n",
      "[BATCH 139/149] Loss_D: 0.7209 Loss_G: 0.7077 acc: 85.9%\n",
      "[BATCH 140/149] Loss_D: 0.7215 Loss_G: 0.7205 acc: 79.7%\n",
      "[EPOCH 6100] TEST ACC is : 77.1%\n",
      "[BATCH 141/149] Loss_D: 0.6783 Loss_G: 0.7018 acc: 93.8%\n",
      "[BATCH 142/149] Loss_D: 0.6670 Loss_G: 0.6751 acc: 81.2%\n",
      "[BATCH 143/149] Loss_D: 0.6641 Loss_G: 0.6990 acc: 92.2%\n",
      "[BATCH 144/149] Loss_D: 0.7128 Loss_G: 0.7091 acc: 92.2%\n",
      "[BATCH 145/149] Loss_D: 0.6638 Loss_G: 0.6934 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6944 Loss_G: 0.7461 acc: 95.3%\n",
      "[BATCH 147/149] Loss_D: 0.6900 Loss_G: 0.7028 acc: 89.1%\n",
      "[BATCH 148/149] Loss_D: 0.6537 Loss_G: 0.6799 acc: 92.2%\n",
      "[BATCH 149/149] Loss_D: 0.6636 Loss_G: 0.6969 acc: 93.8%\n",
      "-----THE [41/50] epoch end-----\n",
      "-----THE [42/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6841 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 2/149] Loss_D: 0.6600 Loss_G: 0.6979 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.7166 Loss_G: 0.7281 acc: 82.8%\n",
      "[BATCH 4/149] Loss_D: 0.6829 Loss_G: 0.7162 acc: 85.9%\n",
      "[BATCH 5/149] Loss_D: 0.6670 Loss_G: 0.6904 acc: 82.8%\n",
      "[BATCH 6/149] Loss_D: 0.6905 Loss_G: 0.7143 acc: 90.6%\n",
      "[BATCH 7/149] Loss_D: 0.6729 Loss_G: 0.7021 acc: 95.3%\n",
      "[BATCH 8/149] Loss_D: 0.7044 Loss_G: 0.6978 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6849 Loss_G: 0.6892 acc: 87.5%\n",
      "[BATCH 10/149] Loss_D: 0.6676 Loss_G: 0.7054 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6537 Loss_G: 0.6933 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7098 Loss_G: 0.7079 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.7391 Loss_G: 0.7614 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.7275 Loss_G: 0.7362 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6493 Loss_G: 0.7043 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6933 Loss_G: 0.7148 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6565 Loss_G: 0.6880 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6914 Loss_G: 0.6797 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6570 Loss_G: 0.6886 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6615 Loss_G: 0.6844 acc: 90.6%\n",
      "[BATCH 21/149] Loss_D: 0.6786 Loss_G: 0.7127 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6570 Loss_G: 0.6873 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6841 Loss_G: 0.6897 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.6975 Loss_G: 0.6999 acc: 87.5%\n",
      "[BATCH 25/149] Loss_D: 0.6395 Loss_G: 0.6827 acc: 92.2%\n",
      "[BATCH 26/149] Loss_D: 0.6819 Loss_G: 0.6835 acc: 85.9%\n",
      "[BATCH 27/149] Loss_D: 0.7088 Loss_G: 0.6930 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6791 Loss_G: 0.6965 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6939 Loss_G: 0.6900 acc: 85.9%\n",
      "[BATCH 30/149] Loss_D: 0.6914 Loss_G: 0.6859 acc: 89.1%\n",
      "[BATCH 31/149] Loss_D: 0.7083 Loss_G: 0.7023 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7151 Loss_G: 0.7288 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6879 Loss_G: 0.7314 acc: 90.6%\n",
      "[BATCH 34/149] Loss_D: 0.6686 Loss_G: 0.6947 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6587 Loss_G: 0.6942 acc: 84.4%\n",
      "[BATCH 36/149] Loss_D: 0.7065 Loss_G: 0.7045 acc: 93.8%\n",
      "[BATCH 37/149] Loss_D: 0.6736 Loss_G: 0.7045 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.6886 Loss_G: 0.6939 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6942 Loss_G: 0.6939 acc: 82.8%\n",
      "[BATCH 40/149] Loss_D: 0.6720 Loss_G: 0.6956 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.6632 Loss_G: 0.6854 acc: 84.4%\n",
      "[EPOCH 6150] TEST ACC is : 77.1%\n",
      "[BATCH 42/149] Loss_D: 0.6598 Loss_G: 0.7032 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6920 Loss_G: 0.6919 acc: 82.8%\n",
      "[BATCH 44/149] Loss_D: 0.6566 Loss_G: 0.6828 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6889 Loss_G: 0.6784 acc: 84.4%\n",
      "[BATCH 46/149] Loss_D: 0.6544 Loss_G: 0.6725 acc: 92.2%\n",
      "[BATCH 47/149] Loss_D: 0.6723 Loss_G: 0.7087 acc: 87.5%\n",
      "[BATCH 48/149] Loss_D: 0.6734 Loss_G: 0.7149 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6546 Loss_G: 0.7047 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6743 Loss_G: 0.7135 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.6534 Loss_G: 0.7209 acc: 85.9%\n",
      "[BATCH 52/149] Loss_D: 0.6742 Loss_G: 0.6971 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6765 Loss_G: 0.6836 acc: 89.1%\n",
      "[BATCH 54/149] Loss_D: 0.6712 Loss_G: 0.7257 acc: 93.8%\n",
      "[BATCH 55/149] Loss_D: 0.6815 Loss_G: 0.6963 acc: 85.9%\n",
      "[BATCH 56/149] Loss_D: 0.6928 Loss_G: 0.6995 acc: 90.6%\n",
      "[BATCH 57/149] Loss_D: 0.6827 Loss_G: 0.7015 acc: 90.6%\n",
      "[BATCH 58/149] Loss_D: 0.6548 Loss_G: 0.6904 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6737 Loss_G: 0.6933 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.6850 Loss_G: 0.7306 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6607 Loss_G: 0.6964 acc: 79.7%\n",
      "[BATCH 62/149] Loss_D: 0.6975 Loss_G: 0.6926 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.6606 Loss_G: 0.6851 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6618 Loss_G: 0.6941 acc: 96.9%\n",
      "[BATCH 65/149] Loss_D: 0.6783 Loss_G: 0.7043 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6532 Loss_G: 0.6856 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6656 Loss_G: 0.6907 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6717 Loss_G: 0.6956 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6751 Loss_G: 0.6980 acc: 95.3%\n",
      "[BATCH 70/149] Loss_D: 0.6903 Loss_G: 0.6828 acc: 84.4%\n",
      "[BATCH 71/149] Loss_D: 0.7130 Loss_G: 0.7057 acc: 87.5%\n",
      "[BATCH 72/149] Loss_D: 0.6726 Loss_G: 0.6964 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6977 Loss_G: 0.7046 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6935 Loss_G: 0.7260 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6840 Loss_G: 0.7050 acc: 84.4%\n",
      "[BATCH 76/149] Loss_D: 0.6917 Loss_G: 0.7361 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6560 Loss_G: 0.6936 acc: 84.4%\n",
      "[BATCH 78/149] Loss_D: 0.6605 Loss_G: 0.6733 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.7142 Loss_G: 0.6779 acc: 81.2%\n",
      "[BATCH 80/149] Loss_D: 0.6811 Loss_G: 0.6901 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6840 Loss_G: 0.7041 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6594 Loss_G: 0.6906 acc: 95.3%\n",
      "[BATCH 83/149] Loss_D: 0.6533 Loss_G: 0.6766 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6691 Loss_G: 0.6779 acc: 90.6%\n",
      "[BATCH 85/149] Loss_D: 0.6942 Loss_G: 0.6947 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6622 Loss_G: 0.7112 acc: 95.3%\n",
      "[BATCH 87/149] Loss_D: 0.7531 Loss_G: 0.7319 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6879 Loss_G: 0.7032 acc: 87.5%\n",
      "[BATCH 89/149] Loss_D: 0.7082 Loss_G: 0.7395 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6602 Loss_G: 0.6947 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6992 Loss_G: 0.6931 acc: 81.2%\n",
      "[EPOCH 6200] TEST ACC is : 77.5%\n",
      "[BATCH 92/149] Loss_D: 0.6738 Loss_G: 0.7189 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6727 Loss_G: 0.7125 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6952 Loss_G: 0.7301 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6737 Loss_G: 0.7088 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6918 Loss_G: 0.6976 acc: 89.1%\n",
      "[BATCH 97/149] Loss_D: 0.6577 Loss_G: 0.6989 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6605 Loss_G: 0.7122 acc: 92.2%\n",
      "[BATCH 99/149] Loss_D: 0.6986 Loss_G: 0.7094 acc: 81.2%\n",
      "[BATCH 100/149] Loss_D: 0.6913 Loss_G: 0.7170 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6609 Loss_G: 0.6966 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6790 Loss_G: 0.7096 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6610 Loss_G: 0.7155 acc: 93.8%\n",
      "[BATCH 104/149] Loss_D: 0.6936 Loss_G: 0.7165 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6491 Loss_G: 0.6842 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.7035 Loss_G: 0.6935 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6703 Loss_G: 0.7123 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6611 Loss_G: 0.7037 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6661 Loss_G: 0.7084 acc: 96.9%\n",
      "[BATCH 110/149] Loss_D: 0.6779 Loss_G: 0.6981 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6774 Loss_G: 0.7405 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6846 Loss_G: 0.7147 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6498 Loss_G: 0.6981 acc: 89.1%\n",
      "[BATCH 114/149] Loss_D: 0.6812 Loss_G: 0.6762 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.7023 Loss_G: 0.7233 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.7047 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6679 Loss_G: 0.7158 acc: 95.3%\n",
      "[BATCH 118/149] Loss_D: 0.6778 Loss_G: 0.7338 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.7184 Loss_G: 0.7401 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.6476 Loss_G: 0.7163 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6787 Loss_G: 0.6792 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6769 Loss_G: 0.6782 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6725 Loss_G: 0.6748 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.7224 Loss_G: 0.7112 acc: 90.6%\n",
      "[BATCH 125/149] Loss_D: 0.6979 Loss_G: 0.7340 acc: 89.1%\n",
      "[BATCH 126/149] Loss_D: 0.7196 Loss_G: 0.7018 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.6690 Loss_G: 0.6892 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.7144 Loss_G: 0.6861 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6577 Loss_G: 0.6854 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6995 Loss_G: 0.7049 acc: 89.1%\n",
      "[BATCH 131/149] Loss_D: 0.6736 Loss_G: 0.7118 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6980 Loss_G: 0.7083 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6698 Loss_G: 0.6958 acc: 82.8%\n",
      "[BATCH 134/149] Loss_D: 0.6707 Loss_G: 0.6933 acc: 93.8%\n",
      "[BATCH 135/149] Loss_D: 0.6762 Loss_G: 0.7049 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6642 Loss_G: 0.6935 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.7108 Loss_G: 0.7525 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6712 Loss_G: 0.7228 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6845 Loss_G: 0.7031 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7075 Loss_G: 0.7141 acc: 85.9%\n",
      "[BATCH 141/149] Loss_D: 0.6937 Loss_G: 0.7146 acc: 93.8%\n",
      "[EPOCH 6250] TEST ACC is : 77.9%\n",
      "[BATCH 142/149] Loss_D: 0.6859 Loss_G: 0.6888 acc: 87.5%\n",
      "[BATCH 143/149] Loss_D: 0.7153 Loss_G: 0.7032 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.6936 Loss_G: 0.7313 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6903 Loss_G: 0.6979 acc: 81.2%\n",
      "[BATCH 146/149] Loss_D: 0.6777 Loss_G: 0.6856 acc: 87.5%\n",
      "[BATCH 147/149] Loss_D: 0.6697 Loss_G: 0.7304 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6504 Loss_G: 0.7005 acc: 93.8%\n",
      "[BATCH 149/149] Loss_D: 0.6706 Loss_G: 0.6836 acc: 82.8%\n",
      "-----THE [42/50] epoch end-----\n",
      "-----THE [43/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6582 Loss_G: 0.7163 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6799 Loss_G: 0.6919 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6863 Loss_G: 0.6902 acc: 92.2%\n",
      "[BATCH 4/149] Loss_D: 0.6676 Loss_G: 0.7087 acc: 95.3%\n",
      "[BATCH 5/149] Loss_D: 0.6486 Loss_G: 0.6965 acc: 93.8%\n",
      "[BATCH 6/149] Loss_D: 0.6373 Loss_G: 0.6682 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.6616 Loss_G: 0.6634 acc: 87.5%\n",
      "[BATCH 8/149] Loss_D: 0.6807 Loss_G: 0.6880 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6849 Loss_G: 0.6889 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.6574 Loss_G: 0.7004 acc: 98.4%\n",
      "[BATCH 11/149] Loss_D: 0.6657 Loss_G: 0.6886 acc: 85.9%\n",
      "[BATCH 12/149] Loss_D: 0.7194 Loss_G: 0.7140 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6811 Loss_G: 0.7015 acc: 81.2%\n",
      "[BATCH 14/149] Loss_D: 0.6907 Loss_G: 0.6942 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.6980 Loss_G: 0.6960 acc: 92.2%\n",
      "[BATCH 16/149] Loss_D: 0.6598 Loss_G: 0.6883 acc: 93.8%\n",
      "[BATCH 17/149] Loss_D: 0.6635 Loss_G: 0.6847 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6725 Loss_G: 0.7003 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.6695 Loss_G: 0.7095 acc: 89.1%\n",
      "[BATCH 20/149] Loss_D: 0.6798 Loss_G: 0.6868 acc: 89.1%\n",
      "[BATCH 21/149] Loss_D: 0.6849 Loss_G: 0.6894 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.7369 Loss_G: 0.6971 acc: 85.9%\n",
      "[BATCH 23/149] Loss_D: 0.6744 Loss_G: 0.7001 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.7087 Loss_G: 0.6968 acc: 81.2%\n",
      "[BATCH 25/149] Loss_D: 0.6956 Loss_G: 0.7042 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6903 Loss_G: 0.6924 acc: 84.4%\n",
      "[BATCH 27/149] Loss_D: 0.7117 Loss_G: 0.7034 acc: 82.8%\n",
      "[BATCH 28/149] Loss_D: 0.6636 Loss_G: 0.6964 acc: 84.4%\n",
      "[BATCH 29/149] Loss_D: 0.6919 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6592 Loss_G: 0.6981 acc: 85.9%\n",
      "[BATCH 31/149] Loss_D: 0.7057 Loss_G: 0.7157 acc: 95.3%\n",
      "[BATCH 32/149] Loss_D: 0.6905 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 33/149] Loss_D: 0.6215 Loss_G: 0.6886 acc: 96.9%\n",
      "[BATCH 34/149] Loss_D: 0.6604 Loss_G: 0.6821 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.6954 Loss_G: 0.6841 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6506 Loss_G: 0.6796 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.7156 Loss_G: 0.6907 acc: 85.9%\n",
      "[BATCH 38/149] Loss_D: 0.6449 Loss_G: 0.7027 acc: 92.2%\n",
      "[BATCH 39/149] Loss_D: 0.7201 Loss_G: 0.7062 acc: 81.2%\n",
      "[BATCH 40/149] Loss_D: 0.6673 Loss_G: 0.7079 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6750 Loss_G: 0.7011 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.6809 Loss_G: 0.7016 acc: 87.5%\n",
      "[EPOCH 6300] TEST ACC is : 77.3%\n",
      "[BATCH 43/149] Loss_D: 0.6861 Loss_G: 0.7193 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.6564 Loss_G: 0.6967 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6795 Loss_G: 0.6863 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6790 Loss_G: 0.6823 acc: 90.6%\n",
      "[BATCH 47/149] Loss_D: 0.6535 Loss_G: 0.6857 acc: 93.8%\n",
      "[BATCH 48/149] Loss_D: 0.6871 Loss_G: 0.6970 acc: 85.9%\n",
      "[BATCH 49/149] Loss_D: 0.6708 Loss_G: 0.6966 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6628 Loss_G: 0.7030 acc: 90.6%\n",
      "[BATCH 51/149] Loss_D: 0.6978 Loss_G: 0.7173 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6834 Loss_G: 0.7212 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6805 Loss_G: 0.7208 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6518 Loss_G: 0.6954 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6798 Loss_G: 0.6964 acc: 82.8%\n",
      "[BATCH 56/149] Loss_D: 0.6568 Loss_G: 0.6857 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.7035 Loss_G: 0.7038 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.7116 Loss_G: 0.7178 acc: 87.5%\n",
      "[BATCH 59/149] Loss_D: 0.6520 Loss_G: 0.7056 acc: 96.9%\n",
      "[BATCH 60/149] Loss_D: 0.6435 Loss_G: 0.6781 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6884 Loss_G: 0.6929 acc: 95.3%\n",
      "[BATCH 62/149] Loss_D: 0.6924 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 63/149] Loss_D: 0.6734 Loss_G: 0.7067 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.6564 Loss_G: 0.7043 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.6669 Loss_G: 0.6991 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.6685 Loss_G: 0.6863 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6894 Loss_G: 0.6803 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.7045 Loss_G: 0.7188 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7084 Loss_G: 0.7311 acc: 92.2%\n",
      "[BATCH 70/149] Loss_D: 0.6649 Loss_G: 0.7043 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.6913 Loss_G: 0.7052 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.7064 Loss_G: 0.7131 acc: 95.3%\n",
      "[BATCH 73/149] Loss_D: 0.6994 Loss_G: 0.7339 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6590 Loss_G: 0.7135 acc: 93.8%\n",
      "[BATCH 75/149] Loss_D: 0.7126 Loss_G: 0.7255 acc: 87.5%\n",
      "[BATCH 76/149] Loss_D: 0.6950 Loss_G: 0.7317 acc: 96.9%\n",
      "[BATCH 77/149] Loss_D: 0.6563 Loss_G: 0.6952 acc: 93.8%\n",
      "[BATCH 78/149] Loss_D: 0.6578 Loss_G: 0.6982 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6443 Loss_G: 0.6772 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6603 Loss_G: 0.6548 acc: 87.5%\n",
      "[BATCH 81/149] Loss_D: 0.6705 Loss_G: 0.6852 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.7047 Loss_G: 0.7090 acc: 82.8%\n",
      "[BATCH 83/149] Loss_D: 0.6778 Loss_G: 0.7234 acc: 90.6%\n",
      "[BATCH 84/149] Loss_D: 0.6803 Loss_G: 0.7247 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.6896 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6878 Loss_G: 0.7267 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6882 Loss_G: 0.7330 acc: 82.8%\n",
      "[BATCH 88/149] Loss_D: 0.7011 Loss_G: 0.7217 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.7219 Loss_G: 0.7341 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6749 Loss_G: 0.7252 acc: 93.8%\n",
      "[BATCH 91/149] Loss_D: 0.7060 Loss_G: 0.7024 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6891 Loss_G: 0.7107 acc: 87.5%\n",
      "[EPOCH 6350] TEST ACC is : 77.0%\n",
      "[BATCH 93/149] Loss_D: 0.6713 Loss_G: 0.6891 acc: 84.4%\n",
      "[BATCH 94/149] Loss_D: 0.6613 Loss_G: 0.6929 acc: 85.9%\n",
      "[BATCH 95/149] Loss_D: 0.6635 Loss_G: 0.6848 acc: 84.4%\n",
      "[BATCH 96/149] Loss_D: 0.7081 Loss_G: 0.6898 acc: 82.8%\n",
      "[BATCH 97/149] Loss_D: 0.6851 Loss_G: 0.6879 acc: 82.8%\n",
      "[BATCH 98/149] Loss_D: 0.7026 Loss_G: 0.6916 acc: 89.1%\n",
      "[BATCH 99/149] Loss_D: 0.6855 Loss_G: 0.7103 acc: 95.3%\n",
      "[BATCH 100/149] Loss_D: 0.6845 Loss_G: 0.7079 acc: 89.1%\n",
      "[BATCH 101/149] Loss_D: 0.6694 Loss_G: 0.7131 acc: 87.5%\n",
      "[BATCH 102/149] Loss_D: 0.7195 Loss_G: 0.7256 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6726 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6941 Loss_G: 0.7031 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6413 Loss_G: 0.6859 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.7035 Loss_G: 0.7145 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6836 Loss_G: 0.7124 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6587 Loss_G: 0.6927 acc: 95.3%\n",
      "[BATCH 109/149] Loss_D: 0.6544 Loss_G: 0.6912 acc: 90.6%\n",
      "[BATCH 110/149] Loss_D: 0.6430 Loss_G: 0.6751 acc: 92.2%\n",
      "[BATCH 111/149] Loss_D: 0.7249 Loss_G: 0.6889 acc: 89.1%\n",
      "[BATCH 112/149] Loss_D: 0.6623 Loss_G: 0.7013 acc: 87.5%\n",
      "[BATCH 113/149] Loss_D: 0.6888 Loss_G: 0.6924 acc: 87.5%\n",
      "[BATCH 114/149] Loss_D: 0.6484 Loss_G: 0.6780 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6797 Loss_G: 0.6844 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6469 Loss_G: 0.6769 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6790 Loss_G: 0.6909 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7224 Loss_G: 0.7203 acc: 85.9%\n",
      "[BATCH 119/149] Loss_D: 0.6693 Loss_G: 0.7145 acc: 95.3%\n",
      "[BATCH 120/149] Loss_D: 0.6723 Loss_G: 0.6943 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7204 Loss_G: 0.7143 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6604 Loss_G: 0.7021 acc: 89.1%\n",
      "[BATCH 123/149] Loss_D: 0.6521 Loss_G: 0.6850 acc: 93.8%\n",
      "[BATCH 124/149] Loss_D: 0.6906 Loss_G: 0.6916 acc: 82.8%\n",
      "[BATCH 125/149] Loss_D: 0.6793 Loss_G: 0.7056 acc: 93.8%\n",
      "[BATCH 126/149] Loss_D: 0.6826 Loss_G: 0.7058 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.6438 Loss_G: 0.6802 acc: 96.9%\n",
      "[BATCH 128/149] Loss_D: 0.7210 Loss_G: 0.6979 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.7075 Loss_G: 0.6838 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6782 Loss_G: 0.6914 acc: 90.6%\n",
      "[BATCH 131/149] Loss_D: 0.6781 Loss_G: 0.7062 acc: 81.2%\n",
      "[BATCH 132/149] Loss_D: 0.6419 Loss_G: 0.6828 acc: 92.2%\n",
      "[BATCH 133/149] Loss_D: 0.6861 Loss_G: 0.6928 acc: 85.9%\n",
      "[BATCH 134/149] Loss_D: 0.6742 Loss_G: 0.7028 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6587 Loss_G: 0.6854 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.7322 Loss_G: 0.7108 acc: 82.8%\n",
      "[BATCH 137/149] Loss_D: 0.6784 Loss_G: 0.7138 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6632 Loss_G: 0.7024 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.6748 Loss_G: 0.7123 acc: 84.4%\n",
      "[BATCH 140/149] Loss_D: 0.7310 Loss_G: 0.7170 acc: 93.8%\n",
      "[BATCH 141/149] Loss_D: 0.6753 Loss_G: 0.7021 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.7259 Loss_G: 0.7227 acc: 84.4%\n",
      "[EPOCH 6400] TEST ACC is : 77.5%\n",
      "[BATCH 143/149] Loss_D: 0.6838 Loss_G: 0.7039 acc: 90.6%\n",
      "[BATCH 144/149] Loss_D: 0.7041 Loss_G: 0.7058 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6821 Loss_G: 0.7260 acc: 89.1%\n",
      "[BATCH 146/149] Loss_D: 0.6753 Loss_G: 0.7142 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.6606 Loss_G: 0.6991 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6972 Loss_G: 0.7090 acc: 84.4%\n",
      "[BATCH 149/149] Loss_D: 0.6508 Loss_G: 0.7070 acc: 85.9%\n",
      "-----THE [43/50] epoch end-----\n",
      "-----THE [44/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6682 Loss_G: 0.6902 acc: 93.8%\n",
      "[BATCH 2/149] Loss_D: 0.6574 Loss_G: 0.6761 acc: 84.4%\n",
      "[BATCH 3/149] Loss_D: 0.6880 Loss_G: 0.6961 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6718 Loss_G: 0.6976 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6505 Loss_G: 0.6811 acc: 87.5%\n",
      "[BATCH 6/149] Loss_D: 0.6823 Loss_G: 0.6781 acc: 82.8%\n",
      "[BATCH 7/149] Loss_D: 0.6509 Loss_G: 0.6682 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6263 Loss_G: 0.6686 acc: 92.2%\n",
      "[BATCH 9/149] Loss_D: 0.7101 Loss_G: 0.6941 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.7063 Loss_G: 0.7427 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6999 Loss_G: 0.7114 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.7113 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6921 Loss_G: 0.7087 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6873 Loss_G: 0.6994 acc: 90.6%\n",
      "[BATCH 15/149] Loss_D: 0.7012 Loss_G: 0.7146 acc: 87.5%\n",
      "[BATCH 16/149] Loss_D: 0.6557 Loss_G: 0.6717 acc: 82.8%\n",
      "[BATCH 17/149] Loss_D: 0.6858 Loss_G: 0.6870 acc: 92.2%\n",
      "[BATCH 18/149] Loss_D: 0.7090 Loss_G: 0.6980 acc: 89.1%\n",
      "[BATCH 19/149] Loss_D: 0.7257 Loss_G: 0.7070 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.6929 Loss_G: 0.6879 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.6960 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6867 Loss_G: 0.7060 acc: 87.5%\n",
      "[BATCH 23/149] Loss_D: 0.6991 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 24/149] Loss_D: 0.7057 Loss_G: 0.6964 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.7032 Loss_G: 0.7027 acc: 85.9%\n",
      "[BATCH 26/149] Loss_D: 0.6862 Loss_G: 0.7280 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.7047 Loss_G: 0.7253 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6507 Loss_G: 0.6873 acc: 81.2%\n",
      "[BATCH 29/149] Loss_D: 0.6934 Loss_G: 0.6945 acc: 87.5%\n",
      "[BATCH 30/149] Loss_D: 0.6384 Loss_G: 0.6872 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6859 Loss_G: 0.6869 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6785 Loss_G: 0.6891 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6705 Loss_G: 0.7202 acc: 85.9%\n",
      "[BATCH 34/149] Loss_D: 0.7403 Loss_G: 0.7478 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.7012 Loss_G: 0.7700 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6584 Loss_G: 0.7012 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.6872 Loss_G: 0.7453 acc: 95.3%\n",
      "[BATCH 38/149] Loss_D: 0.6998 Loss_G: 0.7173 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.7190 Loss_G: 0.7198 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6697 Loss_G: 0.7297 acc: 92.2%\n",
      "[BATCH 41/149] Loss_D: 0.6781 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6633 Loss_G: 0.6998 acc: 92.2%\n",
      "[BATCH 43/149] Loss_D: 0.6847 Loss_G: 0.7150 acc: 89.1%\n",
      "[EPOCH 6450] TEST ACC is : 77.1%\n",
      "[BATCH 44/149] Loss_D: 0.6780 Loss_G: 0.6806 acc: 87.5%\n",
      "[BATCH 45/149] Loss_D: 0.6892 Loss_G: 0.6839 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6695 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 47/149] Loss_D: 0.6976 Loss_G: 0.6923 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.6828 Loss_G: 0.6828 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6723 Loss_G: 0.7060 acc: 92.2%\n",
      "[BATCH 50/149] Loss_D: 0.7242 Loss_G: 0.7003 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6434 Loss_G: 0.6912 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6978 Loss_G: 0.7376 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6755 Loss_G: 0.7031 acc: 92.2%\n",
      "[BATCH 54/149] Loss_D: 0.6528 Loss_G: 0.6967 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.7176 Loss_G: 0.7157 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6441 Loss_G: 0.7122 acc: 93.8%\n",
      "[BATCH 57/149] Loss_D: 0.6786 Loss_G: 0.7213 acc: 95.3%\n",
      "[BATCH 58/149] Loss_D: 0.6978 Loss_G: 0.7252 acc: 95.3%\n",
      "[BATCH 59/149] Loss_D: 0.6959 Loss_G: 0.7542 acc: 85.9%\n",
      "[BATCH 60/149] Loss_D: 0.6802 Loss_G: 0.7279 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6902 Loss_G: 0.7055 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6482 Loss_G: 0.6757 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6825 Loss_G: 0.6940 acc: 89.1%\n",
      "[BATCH 64/149] Loss_D: 0.6771 Loss_G: 0.7014 acc: 90.6%\n",
      "[BATCH 65/149] Loss_D: 0.7308 Loss_G: 0.7155 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6554 Loss_G: 0.6888 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6405 Loss_G: 0.6961 acc: 90.6%\n",
      "[BATCH 68/149] Loss_D: 0.6645 Loss_G: 0.6971 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.7335 Loss_G: 0.7208 acc: 81.2%\n",
      "[BATCH 70/149] Loss_D: 0.6692 Loss_G: 0.7085 acc: 85.9%\n",
      "[BATCH 71/149] Loss_D: 0.6809 Loss_G: 0.6895 acc: 89.1%\n",
      "[BATCH 72/149] Loss_D: 0.6747 Loss_G: 0.7088 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.6958 Loss_G: 0.7010 acc: 87.5%\n",
      "[BATCH 74/149] Loss_D: 0.6966 Loss_G: 0.7029 acc: 90.6%\n",
      "[BATCH 75/149] Loss_D: 0.6492 Loss_G: 0.6962 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6658 Loss_G: 0.6848 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6621 Loss_G: 0.6719 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6792 Loss_G: 0.6920 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6558 Loss_G: 0.6971 acc: 93.8%\n",
      "[BATCH 80/149] Loss_D: 0.6447 Loss_G: 0.6916 acc: 90.6%\n",
      "[BATCH 81/149] Loss_D: 0.6716 Loss_G: 0.6908 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6672 Loss_G: 0.7219 acc: 92.2%\n",
      "[BATCH 83/149] Loss_D: 0.6785 Loss_G: 0.7126 acc: 87.5%\n",
      "[BATCH 84/149] Loss_D: 0.7007 Loss_G: 0.7248 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6623 Loss_G: 0.7136 acc: 96.9%\n",
      "[BATCH 86/149] Loss_D: 0.6771 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6811 Loss_G: 0.6793 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.6562 Loss_G: 0.6964 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6910 Loss_G: 0.6823 acc: 84.4%\n",
      "[BATCH 90/149] Loss_D: 0.6723 Loss_G: 0.6782 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6595 Loss_G: 0.6866 acc: 92.2%\n",
      "[BATCH 92/149] Loss_D: 0.7070 Loss_G: 0.6995 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6695 Loss_G: 0.7323 acc: 90.6%\n",
      "[EPOCH 6500] TEST ACC is : 77.3%\n",
      "[BATCH 94/149] Loss_D: 0.6859 Loss_G: 0.7048 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6653 Loss_G: 0.6972 acc: 92.2%\n",
      "[BATCH 96/149] Loss_D: 0.6902 Loss_G: 0.7028 acc: 87.5%\n",
      "[BATCH 97/149] Loss_D: 0.6781 Loss_G: 0.7156 acc: 87.5%\n",
      "[BATCH 98/149] Loss_D: 0.6986 Loss_G: 0.7361 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6423 Loss_G: 0.7006 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6700 Loss_G: 0.7130 acc: 85.9%\n",
      "[BATCH 101/149] Loss_D: 0.6775 Loss_G: 0.6956 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6748 Loss_G: 0.6804 acc: 89.1%\n",
      "[BATCH 103/149] Loss_D: 0.6698 Loss_G: 0.6932 acc: 87.5%\n",
      "[BATCH 104/149] Loss_D: 0.6767 Loss_G: 0.7243 acc: 92.2%\n",
      "[BATCH 105/149] Loss_D: 0.6934 Loss_G: 0.7581 acc: 82.8%\n",
      "[BATCH 106/149] Loss_D: 0.6810 Loss_G: 0.7423 acc: 89.1%\n",
      "[BATCH 107/149] Loss_D: 0.6728 Loss_G: 0.7194 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6667 Loss_G: 0.6974 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6912 Loss_G: 0.6760 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6630 Loss_G: 0.6716 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6933 Loss_G: 0.6971 acc: 90.6%\n",
      "[BATCH 112/149] Loss_D: 0.6931 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.7225 Loss_G: 0.7190 acc: 85.9%\n",
      "[BATCH 114/149] Loss_D: 0.7362 Loss_G: 0.7262 acc: 79.7%\n",
      "[BATCH 115/149] Loss_D: 0.6587 Loss_G: 0.7132 acc: 85.9%\n",
      "[BATCH 116/149] Loss_D: 0.6802 Loss_G: 0.6818 acc: 82.8%\n",
      "[BATCH 117/149] Loss_D: 0.6747 Loss_G: 0.6999 acc: 90.6%\n",
      "[BATCH 118/149] Loss_D: 0.7095 Loss_G: 0.7002 acc: 84.4%\n",
      "[BATCH 119/149] Loss_D: 0.6530 Loss_G: 0.7031 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6723 Loss_G: 0.6909 acc: 90.6%\n",
      "[BATCH 121/149] Loss_D: 0.6648 Loss_G: 0.6852 acc: 89.1%\n",
      "[BATCH 122/149] Loss_D: 0.7098 Loss_G: 0.7004 acc: 87.5%\n",
      "[BATCH 123/149] Loss_D: 0.6520 Loss_G: 0.6890 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6390 Loss_G: 0.6921 acc: 93.8%\n",
      "[BATCH 125/149] Loss_D: 0.6620 Loss_G: 0.6956 acc: 90.6%\n",
      "[BATCH 126/149] Loss_D: 0.6412 Loss_G: 0.6792 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6693 Loss_G: 0.7036 acc: 89.1%\n",
      "[BATCH 128/149] Loss_D: 0.6883 Loss_G: 0.6868 acc: 84.4%\n",
      "[BATCH 129/149] Loss_D: 0.6903 Loss_G: 0.6976 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6673 Loss_G: 0.6783 acc: 79.7%\n",
      "[BATCH 131/149] Loss_D: 0.6937 Loss_G: 0.7083 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6940 Loss_G: 0.7096 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.7122 Loss_G: 0.7285 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.7240 Loss_G: 0.7452 acc: 87.5%\n",
      "[BATCH 135/149] Loss_D: 0.7049 Loss_G: 0.7450 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.6451 Loss_G: 0.7354 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6633 Loss_G: 0.6903 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6732 Loss_G: 0.6836 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.7107 Loss_G: 0.7057 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.7062 Loss_G: 0.7213 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6912 Loss_G: 0.7026 acc: 85.9%\n",
      "[BATCH 142/149] Loss_D: 0.6655 Loss_G: 0.6850 acc: 79.7%\n",
      "[BATCH 143/149] Loss_D: 0.6906 Loss_G: 0.6755 acc: 90.6%\n",
      "[EPOCH 6550] TEST ACC is : 76.2%\n",
      "[BATCH 144/149] Loss_D: 0.6561 Loss_G: 0.6871 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6918 Loss_G: 0.6818 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.6749 Loss_G: 0.6848 acc: 90.6%\n",
      "[BATCH 147/149] Loss_D: 0.6904 Loss_G: 0.6880 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6538 Loss_G: 0.6877 acc: 89.1%\n",
      "[BATCH 149/149] Loss_D: 0.6577 Loss_G: 0.7072 acc: 87.5%\n",
      "-----THE [44/50] epoch end-----\n",
      "-----THE [45/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6633 Loss_G: 0.6781 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.6972 Loss_G: 0.6970 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6522 Loss_G: 0.7138 acc: 95.3%\n",
      "[BATCH 4/149] Loss_D: 0.6723 Loss_G: 0.6887 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6964 Loss_G: 0.7141 acc: 90.6%\n",
      "[BATCH 6/149] Loss_D: 0.6887 Loss_G: 0.7017 acc: 84.4%\n",
      "[BATCH 7/149] Loss_D: 0.6514 Loss_G: 0.6848 acc: 93.8%\n",
      "[BATCH 8/149] Loss_D: 0.6654 Loss_G: 0.6789 acc: 85.9%\n",
      "[BATCH 9/149] Loss_D: 0.6570 Loss_G: 0.7045 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6693 Loss_G: 0.6901 acc: 89.1%\n",
      "[BATCH 11/149] Loss_D: 0.6526 Loss_G: 0.6705 acc: 81.2%\n",
      "[BATCH 12/149] Loss_D: 0.6714 Loss_G: 0.6889 acc: 87.5%\n",
      "[BATCH 13/149] Loss_D: 0.6499 Loss_G: 0.6732 acc: 82.8%\n",
      "[BATCH 14/149] Loss_D: 0.6830 Loss_G: 0.6973 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6905 Loss_G: 0.7081 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.6733 Loss_G: 0.7163 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6942 Loss_G: 0.7066 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6576 Loss_G: 0.7034 acc: 93.8%\n",
      "[BATCH 19/149] Loss_D: 0.6825 Loss_G: 0.6820 acc: 87.5%\n",
      "[BATCH 20/149] Loss_D: 0.7148 Loss_G: 0.7155 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6583 Loss_G: 0.7074 acc: 85.9%\n",
      "[BATCH 22/149] Loss_D: 0.7241 Loss_G: 0.7675 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6805 Loss_G: 0.7273 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.7086 Loss_G: 0.7106 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.6664 Loss_G: 0.6858 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.7054 Loss_G: 0.6800 acc: 81.2%\n",
      "[BATCH 27/149] Loss_D: 0.6977 Loss_G: 0.6929 acc: 84.4%\n",
      "[BATCH 28/149] Loss_D: 0.6595 Loss_G: 0.6778 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6589 Loss_G: 0.6765 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6771 Loss_G: 0.6845 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6495 Loss_G: 0.6816 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.6835 Loss_G: 0.6869 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.7027 Loss_G: 0.7021 acc: 93.8%\n",
      "[BATCH 34/149] Loss_D: 0.6846 Loss_G: 0.7492 acc: 96.9%\n",
      "[BATCH 35/149] Loss_D: 0.6754 Loss_G: 0.7176 acc: 98.4%\n",
      "[BATCH 36/149] Loss_D: 0.6835 Loss_G: 0.7009 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6598 Loss_G: 0.6771 acc: 79.7%\n",
      "[BATCH 38/149] Loss_D: 0.6702 Loss_G: 0.6794 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6721 Loss_G: 0.6741 acc: 85.9%\n",
      "[BATCH 40/149] Loss_D: 0.7133 Loss_G: 0.7314 acc: 95.3%\n",
      "[BATCH 41/149] Loss_D: 0.6757 Loss_G: 0.7251 acc: 87.5%\n",
      "[BATCH 42/149] Loss_D: 0.6941 Loss_G: 0.7101 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.6628 Loss_G: 0.6948 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.6945 Loss_G: 0.6923 acc: 85.9%\n",
      "[EPOCH 6600] TEST ACC is : 76.8%\n",
      "[BATCH 45/149] Loss_D: 0.6767 Loss_G: 0.6941 acc: 93.8%\n",
      "[BATCH 46/149] Loss_D: 0.6631 Loss_G: 0.7167 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6894 Loss_G: 0.7060 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6779 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 49/149] Loss_D: 0.6903 Loss_G: 0.7247 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6712 Loss_G: 0.7097 acc: 87.5%\n",
      "[BATCH 51/149] Loss_D: 0.6993 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6911 Loss_G: 0.7236 acc: 81.2%\n",
      "[BATCH 53/149] Loss_D: 0.6845 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.7085 Loss_G: 0.7358 acc: 89.1%\n",
      "[BATCH 55/149] Loss_D: 0.6510 Loss_G: 0.7050 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6629 Loss_G: 0.6898 acc: 89.1%\n",
      "[BATCH 57/149] Loss_D: 0.6724 Loss_G: 0.7155 acc: 93.8%\n",
      "[BATCH 58/149] Loss_D: 0.7206 Loss_G: 0.7264 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6712 Loss_G: 0.6998 acc: 84.4%\n",
      "[BATCH 60/149] Loss_D: 0.6657 Loss_G: 0.7052 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6773 Loss_G: 0.6917 acc: 85.9%\n",
      "[BATCH 62/149] Loss_D: 0.6586 Loss_G: 0.6905 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6902 Loss_G: 0.6857 acc: 78.1%\n",
      "[BATCH 64/149] Loss_D: 0.6933 Loss_G: 0.7046 acc: 85.9%\n",
      "[BATCH 65/149] Loss_D: 0.6680 Loss_G: 0.6761 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.6532 Loss_G: 0.6818 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7116 Loss_G: 0.6985 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6351 Loss_G: 0.6892 acc: 92.2%\n",
      "[BATCH 69/149] Loss_D: 0.6733 Loss_G: 0.6849 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6594 Loss_G: 0.6819 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6706 Loss_G: 0.6796 acc: 90.6%\n",
      "[BATCH 72/149] Loss_D: 0.6591 Loss_G: 0.6855 acc: 87.5%\n",
      "[BATCH 73/149] Loss_D: 0.7019 Loss_G: 0.6860 acc: 90.6%\n",
      "[BATCH 74/149] Loss_D: 0.6664 Loss_G: 0.6881 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.6887 Loss_G: 0.6914 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.7324 Loss_G: 0.7137 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6715 Loss_G: 0.6978 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.7156 Loss_G: 0.7173 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6602 Loss_G: 0.6954 acc: 82.8%\n",
      "[BATCH 80/149] Loss_D: 0.6579 Loss_G: 0.6670 acc: 93.8%\n",
      "[BATCH 81/149] Loss_D: 0.6958 Loss_G: 0.6986 acc: 84.4%\n",
      "[BATCH 82/149] Loss_D: 0.6573 Loss_G: 0.6849 acc: 93.8%\n",
      "[BATCH 83/149] Loss_D: 0.6822 Loss_G: 0.7021 acc: 92.2%\n",
      "[BATCH 84/149] Loss_D: 0.6666 Loss_G: 0.6986 acc: 87.5%\n",
      "[BATCH 85/149] Loss_D: 0.6825 Loss_G: 0.7026 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6935 Loss_G: 0.7249 acc: 90.6%\n",
      "[BATCH 87/149] Loss_D: 0.6373 Loss_G: 0.6916 acc: 96.9%\n",
      "[BATCH 88/149] Loss_D: 0.6673 Loss_G: 0.6828 acc: 92.2%\n",
      "[BATCH 89/149] Loss_D: 0.6711 Loss_G: 0.6741 acc: 81.2%\n",
      "[BATCH 90/149] Loss_D: 0.6727 Loss_G: 0.6872 acc: 85.9%\n",
      "[BATCH 91/149] Loss_D: 0.6878 Loss_G: 0.7132 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6578 Loss_G: 0.7410 acc: 90.6%\n",
      "[BATCH 93/149] Loss_D: 0.6704 Loss_G: 0.7063 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6571 Loss_G: 0.6965 acc: 93.8%\n",
      "[EPOCH 6650] TEST ACC is : 77.7%\n",
      "[BATCH 95/149] Loss_D: 0.6930 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 96/149] Loss_D: 0.6499 Loss_G: 0.6904 acc: 90.6%\n",
      "[BATCH 97/149] Loss_D: 0.6413 Loss_G: 0.6708 acc: 89.1%\n",
      "[BATCH 98/149] Loss_D: 0.6454 Loss_G: 0.6604 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6817 Loss_G: 0.6937 acc: 90.6%\n",
      "[BATCH 100/149] Loss_D: 0.6732 Loss_G: 0.6856 acc: 92.2%\n",
      "[BATCH 101/149] Loss_D: 0.6834 Loss_G: 0.7286 acc: 95.3%\n",
      "[BATCH 102/149] Loss_D: 0.6805 Loss_G: 0.7125 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6795 Loss_G: 0.7313 acc: 95.3%\n",
      "[BATCH 104/149] Loss_D: 0.6521 Loss_G: 0.7282 acc: 89.1%\n",
      "[BATCH 105/149] Loss_D: 0.7257 Loss_G: 0.7099 acc: 81.2%\n",
      "[BATCH 106/149] Loss_D: 0.6479 Loss_G: 0.7326 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6643 Loss_G: 0.7118 acc: 87.5%\n",
      "[BATCH 108/149] Loss_D: 0.7046 Loss_G: 0.7196 acc: 81.2%\n",
      "[BATCH 109/149] Loss_D: 0.6969 Loss_G: 0.7396 acc: 89.1%\n",
      "[BATCH 110/149] Loss_D: 0.6724 Loss_G: 0.7302 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6778 Loss_G: 0.7173 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.6925 Loss_G: 0.7097 acc: 90.6%\n",
      "[BATCH 113/149] Loss_D: 0.6761 Loss_G: 0.7107 acc: 95.3%\n",
      "[BATCH 114/149] Loss_D: 0.6343 Loss_G: 0.7036 acc: 93.8%\n",
      "[BATCH 115/149] Loss_D: 0.7303 Loss_G: 0.7344 acc: 92.2%\n",
      "[BATCH 116/149] Loss_D: 0.6956 Loss_G: 0.6998 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6755 Loss_G: 0.6949 acc: 82.8%\n",
      "[BATCH 118/149] Loss_D: 0.6525 Loss_G: 0.6910 acc: 89.1%\n",
      "[BATCH 119/149] Loss_D: 0.6803 Loss_G: 0.6856 acc: 87.5%\n",
      "[BATCH 120/149] Loss_D: 0.6734 Loss_G: 0.6844 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7274 Loss_G: 0.7281 acc: 87.5%\n",
      "[BATCH 122/149] Loss_D: 0.6816 Loss_G: 0.7057 acc: 92.2%\n",
      "[BATCH 123/149] Loss_D: 0.7069 Loss_G: 0.7068 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6695 Loss_G: 0.7141 acc: 87.5%\n",
      "[BATCH 125/149] Loss_D: 0.6968 Loss_G: 0.7082 acc: 85.9%\n",
      "[BATCH 126/149] Loss_D: 0.6563 Loss_G: 0.7086 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6572 Loss_G: 0.7186 acc: 93.8%\n",
      "[BATCH 128/149] Loss_D: 0.6862 Loss_G: 0.6957 acc: 79.7%\n",
      "[BATCH 129/149] Loss_D: 0.6860 Loss_G: 0.6914 acc: 85.9%\n",
      "[BATCH 130/149] Loss_D: 0.6883 Loss_G: 0.6847 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7396 Loss_G: 0.6934 acc: 75.0%\n",
      "[BATCH 132/149] Loss_D: 0.6916 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 133/149] Loss_D: 0.6895 Loss_G: 0.7108 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.7011 Loss_G: 0.7204 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7100 Loss_G: 0.7008 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6824 Loss_G: 0.7085 acc: 90.6%\n",
      "[BATCH 137/149] Loss_D: 0.6899 Loss_G: 0.6874 acc: 93.8%\n",
      "[BATCH 138/149] Loss_D: 0.6935 Loss_G: 0.7080 acc: 95.3%\n",
      "[BATCH 139/149] Loss_D: 0.6789 Loss_G: 0.6878 acc: 82.8%\n",
      "[BATCH 140/149] Loss_D: 0.6757 Loss_G: 0.7063 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.7012 Loss_G: 0.7104 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6890 Loss_G: 0.6843 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6847 Loss_G: 0.6697 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.7174 Loss_G: 0.7011 acc: 89.1%\n",
      "[EPOCH 6700] TEST ACC is : 77.5%\n",
      "[BATCH 145/149] Loss_D: 0.6919 Loss_G: 0.7382 acc: 95.3%\n",
      "[BATCH 146/149] Loss_D: 0.6885 Loss_G: 0.7225 acc: 92.2%\n",
      "[BATCH 147/149] Loss_D: 0.7033 Loss_G: 0.7121 acc: 92.2%\n",
      "[BATCH 148/149] Loss_D: 0.6841 Loss_G: 0.6822 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.7066 Loss_G: 0.6887 acc: 84.4%\n",
      "-----THE [45/50] epoch end-----\n",
      "-----THE [46/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6462 Loss_G: 0.6981 acc: 89.1%\n",
      "[BATCH 2/149] Loss_D: 0.7141 Loss_G: 0.7016 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.6619 Loss_G: 0.7007 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6453 Loss_G: 0.7020 acc: 87.5%\n",
      "[BATCH 5/149] Loss_D: 0.6675 Loss_G: 0.7026 acc: 89.1%\n",
      "[BATCH 6/149] Loss_D: 0.6962 Loss_G: 0.6986 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6712 Loss_G: 0.7186 acc: 92.2%\n",
      "[BATCH 8/149] Loss_D: 0.6553 Loss_G: 0.7082 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6836 Loss_G: 0.6906 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6960 Loss_G: 0.6804 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.6586 Loss_G: 0.6807 acc: 92.2%\n",
      "[BATCH 12/149] Loss_D: 0.6667 Loss_G: 0.6836 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6770 Loss_G: 0.7201 acc: 93.8%\n",
      "[BATCH 14/149] Loss_D: 0.7289 Loss_G: 0.7414 acc: 82.8%\n",
      "[BATCH 15/149] Loss_D: 0.6591 Loss_G: 0.7057 acc: 93.8%\n",
      "[BATCH 16/149] Loss_D: 0.6864 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 17/149] Loss_D: 0.6947 Loss_G: 0.7099 acc: 84.4%\n",
      "[BATCH 18/149] Loss_D: 0.6649 Loss_G: 0.6768 acc: 84.4%\n",
      "[BATCH 19/149] Loss_D: 0.6639 Loss_G: 0.6952 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.6772 Loss_G: 0.6804 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6851 Loss_G: 0.6965 acc: 87.5%\n",
      "[BATCH 22/149] Loss_D: 0.6722 Loss_G: 0.6973 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6839 Loss_G: 0.6930 acc: 90.6%\n",
      "[BATCH 24/149] Loss_D: 0.6832 Loss_G: 0.6957 acc: 95.3%\n",
      "[BATCH 25/149] Loss_D: 0.7374 Loss_G: 0.7222 acc: 93.8%\n",
      "[BATCH 26/149] Loss_D: 0.6930 Loss_G: 0.7193 acc: 92.2%\n",
      "[BATCH 27/149] Loss_D: 0.6682 Loss_G: 0.6989 acc: 84.4%\n",
      "[BATCH 28/149] Loss_D: 0.6968 Loss_G: 0.6966 acc: 89.1%\n",
      "[BATCH 29/149] Loss_D: 0.6715 Loss_G: 0.7004 acc: 93.8%\n",
      "[BATCH 30/149] Loss_D: 0.6920 Loss_G: 0.6928 acc: 84.4%\n",
      "[BATCH 31/149] Loss_D: 0.6869 Loss_G: 0.7114 acc: 82.8%\n",
      "[BATCH 32/149] Loss_D: 0.6706 Loss_G: 0.7084 acc: 90.6%\n",
      "[BATCH 33/149] Loss_D: 0.6535 Loss_G: 0.6885 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.6600 Loss_G: 0.6959 acc: 89.1%\n",
      "[BATCH 35/149] Loss_D: 0.6955 Loss_G: 0.6890 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6598 Loss_G: 0.6805 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6689 Loss_G: 0.6932 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.6797 Loss_G: 0.6908 acc: 90.6%\n",
      "[BATCH 39/149] Loss_D: 0.6612 Loss_G: 0.6888 acc: 92.2%\n",
      "[BATCH 40/149] Loss_D: 0.6520 Loss_G: 0.6832 acc: 85.9%\n",
      "[BATCH 41/149] Loss_D: 0.7111 Loss_G: 0.6837 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.6770 Loss_G: 0.7144 acc: 90.6%\n",
      "[BATCH 43/149] Loss_D: 0.7024 Loss_G: 0.7121 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.6819 Loss_G: 0.7070 acc: 90.6%\n",
      "[BATCH 45/149] Loss_D: 0.6862 Loss_G: 0.6938 acc: 85.9%\n",
      "[EPOCH 6750] TEST ACC is : 77.3%\n",
      "[BATCH 46/149] Loss_D: 0.6949 Loss_G: 0.7094 acc: 78.1%\n",
      "[BATCH 47/149] Loss_D: 0.6779 Loss_G: 0.7017 acc: 92.2%\n",
      "[BATCH 48/149] Loss_D: 0.6499 Loss_G: 0.6950 acc: 95.3%\n",
      "[BATCH 49/149] Loss_D: 0.6333 Loss_G: 0.6907 acc: 93.8%\n",
      "[BATCH 50/149] Loss_D: 0.6964 Loss_G: 0.6809 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6776 Loss_G: 0.6702 acc: 79.7%\n",
      "[BATCH 52/149] Loss_D: 0.6902 Loss_G: 0.6769 acc: 89.1%\n",
      "[BATCH 53/149] Loss_D: 0.6636 Loss_G: 0.7084 acc: 96.9%\n",
      "[BATCH 54/149] Loss_D: 0.7124 Loss_G: 0.7316 acc: 87.5%\n",
      "[BATCH 55/149] Loss_D: 0.6721 Loss_G: 0.7018 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6394 Loss_G: 0.6865 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6598 Loss_G: 0.6705 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7370 Loss_G: 0.7232 acc: 90.6%\n",
      "[BATCH 59/149] Loss_D: 0.6857 Loss_G: 0.7474 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.6822 Loss_G: 0.6893 acc: 87.5%\n",
      "[BATCH 61/149] Loss_D: 0.6749 Loss_G: 0.7043 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6928 Loss_G: 0.7102 acc: 82.8%\n",
      "[BATCH 63/149] Loss_D: 0.6811 Loss_G: 0.6977 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.7025 Loss_G: 0.7142 acc: 92.2%\n",
      "[BATCH 65/149] Loss_D: 0.7009 Loss_G: 0.6893 acc: 89.1%\n",
      "[BATCH 66/149] Loss_D: 0.7161 Loss_G: 0.6887 acc: 82.8%\n",
      "[BATCH 67/149] Loss_D: 0.6807 Loss_G: 0.7021 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6554 Loss_G: 0.6938 acc: 93.8%\n",
      "[BATCH 69/149] Loss_D: 0.6641 Loss_G: 0.6862 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6804 Loss_G: 0.7172 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.7086 Loss_G: 0.7200 acc: 84.4%\n",
      "[BATCH 72/149] Loss_D: 0.6442 Loss_G: 0.7166 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.6857 Loss_G: 0.7061 acc: 89.1%\n",
      "[BATCH 74/149] Loss_D: 0.6649 Loss_G: 0.6731 acc: 89.1%\n",
      "[BATCH 75/149] Loss_D: 0.6600 Loss_G: 0.7071 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6730 Loss_G: 0.6955 acc: 89.1%\n",
      "[BATCH 77/149] Loss_D: 0.6832 Loss_G: 0.6940 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6748 Loss_G: 0.7089 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6536 Loss_G: 0.6919 acc: 87.5%\n",
      "[BATCH 80/149] Loss_D: 0.6701 Loss_G: 0.6766 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.7231 Loss_G: 0.6922 acc: 81.2%\n",
      "[BATCH 82/149] Loss_D: 0.6911 Loss_G: 0.7212 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7055 Loss_G: 0.7050 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6833 Loss_G: 0.6873 acc: 76.6%\n",
      "[BATCH 85/149] Loss_D: 0.6808 Loss_G: 0.6885 acc: 90.6%\n",
      "[BATCH 86/149] Loss_D: 0.7640 Loss_G: 0.7761 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6778 Loss_G: 0.7113 acc: 85.9%\n",
      "[BATCH 88/149] Loss_D: 0.6571 Loss_G: 0.7002 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6829 Loss_G: 0.6827 acc: 92.2%\n",
      "[BATCH 90/149] Loss_D: 0.6932 Loss_G: 0.6934 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6868 Loss_G: 0.6866 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6621 Loss_G: 0.6851 acc: 85.9%\n",
      "[BATCH 93/149] Loss_D: 0.6767 Loss_G: 0.6741 acc: 92.2%\n",
      "[BATCH 94/149] Loss_D: 0.6697 Loss_G: 0.6919 acc: 84.4%\n",
      "[BATCH 95/149] Loss_D: 0.6767 Loss_G: 0.6854 acc: 84.4%\n",
      "[EPOCH 6800] TEST ACC is : 76.4%\n",
      "[BATCH 96/149] Loss_D: 0.6658 Loss_G: 0.6995 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6598 Loss_G: 0.6939 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6948 Loss_G: 0.7005 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6906 Loss_G: 0.7041 acc: 84.4%\n",
      "[BATCH 100/149] Loss_D: 0.6837 Loss_G: 0.7039 acc: 84.4%\n",
      "[BATCH 101/149] Loss_D: 0.7306 Loss_G: 0.7231 acc: 89.1%\n",
      "[BATCH 102/149] Loss_D: 0.6927 Loss_G: 0.7361 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6891 Loss_G: 0.7203 acc: 90.6%\n",
      "[BATCH 104/149] Loss_D: 0.6718 Loss_G: 0.7487 acc: 95.3%\n",
      "[BATCH 105/149] Loss_D: 0.6864 Loss_G: 0.7221 acc: 89.1%\n",
      "[BATCH 106/149] Loss_D: 0.6942 Loss_G: 0.7340 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.7122 Loss_G: 0.7378 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.6887 Loss_G: 0.7013 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.6958 Loss_G: 0.6904 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.7152 Loss_G: 0.7032 acc: 85.9%\n",
      "[BATCH 111/149] Loss_D: 0.7091 Loss_G: 0.7228 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.6886 Loss_G: 0.7178 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7046 Loss_G: 0.7159 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6891 Loss_G: 0.7052 acc: 89.1%\n",
      "[BATCH 115/149] Loss_D: 0.6520 Loss_G: 0.6831 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6766 Loss_G: 0.6756 acc: 85.9%\n",
      "[BATCH 117/149] Loss_D: 0.6584 Loss_G: 0.6959 acc: 95.3%\n",
      "[BATCH 118/149] Loss_D: 0.6456 Loss_G: 0.7187 acc: 93.8%\n",
      "[BATCH 119/149] Loss_D: 0.6824 Loss_G: 0.7031 acc: 89.1%\n",
      "[BATCH 120/149] Loss_D: 0.7197 Loss_G: 0.7406 acc: 89.1%\n",
      "[BATCH 121/149] Loss_D: 0.7063 Loss_G: 0.7142 acc: 81.2%\n",
      "[BATCH 122/149] Loss_D: 0.7214 Loss_G: 0.7045 acc: 85.9%\n",
      "[BATCH 123/149] Loss_D: 0.6600 Loss_G: 0.6867 acc: 92.2%\n",
      "[BATCH 124/149] Loss_D: 0.6699 Loss_G: 0.7092 acc: 92.2%\n",
      "[BATCH 125/149] Loss_D: 0.6697 Loss_G: 0.7098 acc: 93.8%\n",
      "[BATCH 126/149] Loss_D: 0.6985 Loss_G: 0.6860 acc: 84.4%\n",
      "[BATCH 127/149] Loss_D: 0.6602 Loss_G: 0.7154 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6931 Loss_G: 0.7274 acc: 89.1%\n",
      "[BATCH 129/149] Loss_D: 0.6727 Loss_G: 0.7090 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7091 Loss_G: 0.7262 acc: 82.8%\n",
      "[BATCH 131/149] Loss_D: 0.6723 Loss_G: 0.6969 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.6515 Loss_G: 0.6853 acc: 90.6%\n",
      "[BATCH 133/149] Loss_D: 0.6644 Loss_G: 0.6909 acc: 89.1%\n",
      "[BATCH 134/149] Loss_D: 0.6930 Loss_G: 0.6883 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.6872 Loss_G: 0.7055 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6620 Loss_G: 0.6982 acc: 92.2%\n",
      "[BATCH 137/149] Loss_D: 0.6625 Loss_G: 0.6755 acc: 85.9%\n",
      "[BATCH 138/149] Loss_D: 0.6679 Loss_G: 0.6731 acc: 92.2%\n",
      "[BATCH 139/149] Loss_D: 0.6260 Loss_G: 0.6830 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.7141 Loss_G: 0.6953 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6599 Loss_G: 0.7082 acc: 92.2%\n",
      "[BATCH 142/149] Loss_D: 0.6759 Loss_G: 0.6900 acc: 85.9%\n",
      "[BATCH 143/149] Loss_D: 0.6481 Loss_G: 0.6853 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.6729 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 145/149] Loss_D: 0.6773 Loss_G: 0.7034 acc: 90.6%\n",
      "[EPOCH 6850] TEST ACC is : 76.8%\n",
      "[BATCH 146/149] Loss_D: 0.6577 Loss_G: 0.6863 acc: 93.8%\n",
      "[BATCH 147/149] Loss_D: 0.6593 Loss_G: 0.6879 acc: 98.4%\n",
      "[BATCH 148/149] Loss_D: 0.7148 Loss_G: 0.7001 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.6723 Loss_G: 0.7202 acc: 90.6%\n",
      "-----THE [46/50] epoch end-----\n",
      "-----THE [47/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6422 Loss_G: 0.6694 acc: 84.4%\n",
      "[BATCH 2/149] Loss_D: 0.6215 Loss_G: 0.6658 acc: 92.2%\n",
      "[BATCH 3/149] Loss_D: 0.6776 Loss_G: 0.6847 acc: 84.4%\n",
      "[BATCH 4/149] Loss_D: 0.6522 Loss_G: 0.6723 acc: 92.2%\n",
      "[BATCH 5/149] Loss_D: 0.6874 Loss_G: 0.6795 acc: 84.4%\n",
      "[BATCH 6/149] Loss_D: 0.6817 Loss_G: 0.6862 acc: 81.2%\n",
      "[BATCH 7/149] Loss_D: 0.6373 Loss_G: 0.6713 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.7043 Loss_G: 0.6964 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6780 Loss_G: 0.7000 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.7108 Loss_G: 0.7039 acc: 85.9%\n",
      "[BATCH 11/149] Loss_D: 0.6631 Loss_G: 0.6889 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6817 Loss_G: 0.7133 acc: 90.6%\n",
      "[BATCH 13/149] Loss_D: 0.6584 Loss_G: 0.7021 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6843 Loss_G: 0.7234 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.6994 Loss_G: 0.7033 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.7080 Loss_G: 0.6989 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6600 Loss_G: 0.6951 acc: 90.6%\n",
      "[BATCH 18/149] Loss_D: 0.6837 Loss_G: 0.6888 acc: 81.2%\n",
      "[BATCH 19/149] Loss_D: 0.6614 Loss_G: 0.6945 acc: 95.3%\n",
      "[BATCH 20/149] Loss_D: 0.6641 Loss_G: 0.6708 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6659 Loss_G: 0.6717 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6933 Loss_G: 0.7262 acc: 89.1%\n",
      "[BATCH 23/149] Loss_D: 0.6892 Loss_G: 0.7337 acc: 98.4%\n",
      "[BATCH 24/149] Loss_D: 0.7046 Loss_G: 0.7211 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.6631 Loss_G: 0.6981 acc: 81.2%\n",
      "[BATCH 26/149] Loss_D: 0.6952 Loss_G: 0.7194 acc: 89.1%\n",
      "[BATCH 27/149] Loss_D: 0.6694 Loss_G: 0.6900 acc: 85.9%\n",
      "[BATCH 28/149] Loss_D: 0.7342 Loss_G: 0.6926 acc: 85.9%\n",
      "[BATCH 29/149] Loss_D: 0.6565 Loss_G: 0.6981 acc: 89.1%\n",
      "[BATCH 30/149] Loss_D: 0.6603 Loss_G: 0.7091 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6953 Loss_G: 0.7113 acc: 90.6%\n",
      "[BATCH 32/149] Loss_D: 0.6742 Loss_G: 0.6756 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6614 Loss_G: 0.6697 acc: 89.1%\n",
      "[BATCH 34/149] Loss_D: 0.7130 Loss_G: 0.6892 acc: 87.5%\n",
      "[BATCH 35/149] Loss_D: 0.7213 Loss_G: 0.6976 acc: 82.8%\n",
      "[BATCH 36/149] Loss_D: 0.7717 Loss_G: 0.7341 acc: 89.1%\n",
      "[BATCH 37/149] Loss_D: 0.6964 Loss_G: 0.7334 acc: 93.8%\n",
      "[BATCH 38/149] Loss_D: 0.6468 Loss_G: 0.6914 acc: 87.5%\n",
      "[BATCH 39/149] Loss_D: 0.6610 Loss_G: 0.6796 acc: 93.8%\n",
      "[BATCH 40/149] Loss_D: 0.6918 Loss_G: 0.7060 acc: 89.1%\n",
      "[BATCH 41/149] Loss_D: 0.6881 Loss_G: 0.7077 acc: 84.4%\n",
      "[BATCH 42/149] Loss_D: 0.7006 Loss_G: 0.7175 acc: 82.8%\n",
      "[BATCH 43/149] Loss_D: 0.6995 Loss_G: 0.7023 acc: 82.8%\n",
      "[BATCH 44/149] Loss_D: 0.6999 Loss_G: 0.7144 acc: 82.8%\n",
      "[BATCH 45/149] Loss_D: 0.6566 Loss_G: 0.6817 acc: 92.2%\n",
      "[BATCH 46/149] Loss_D: 0.6563 Loss_G: 0.6789 acc: 90.6%\n",
      "[EPOCH 6900] TEST ACC is : 77.3%\n",
      "[BATCH 47/149] Loss_D: 0.6399 Loss_G: 0.6718 acc: 89.1%\n",
      "[BATCH 48/149] Loss_D: 0.6622 Loss_G: 0.6805 acc: 92.2%\n",
      "[BATCH 49/149] Loss_D: 0.6882 Loss_G: 0.7120 acc: 90.6%\n",
      "[BATCH 50/149] Loss_D: 0.6800 Loss_G: 0.7188 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.6535 Loss_G: 0.7034 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6889 Loss_G: 0.6753 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.6718 Loss_G: 0.6939 acc: 82.8%\n",
      "[BATCH 54/149] Loss_D: 0.7028 Loss_G: 0.7132 acc: 95.3%\n",
      "[BATCH 55/149] Loss_D: 0.6469 Loss_G: 0.6987 acc: 89.1%\n",
      "[BATCH 56/149] Loss_D: 0.6833 Loss_G: 0.7105 acc: 92.2%\n",
      "[BATCH 57/149] Loss_D: 0.6903 Loss_G: 0.6947 acc: 92.2%\n",
      "[BATCH 58/149] Loss_D: 0.7157 Loss_G: 0.7125 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.6607 Loss_G: 0.7067 acc: 89.1%\n",
      "[BATCH 60/149] Loss_D: 0.7111 Loss_G: 0.7096 acc: 90.6%\n",
      "[BATCH 61/149] Loss_D: 0.6898 Loss_G: 0.7220 acc: 90.6%\n",
      "[BATCH 62/149] Loss_D: 0.6645 Loss_G: 0.7173 acc: 90.6%\n",
      "[BATCH 63/149] Loss_D: 0.6783 Loss_G: 0.7026 acc: 87.5%\n",
      "[BATCH 64/149] Loss_D: 0.6669 Loss_G: 0.7281 acc: 95.3%\n",
      "[BATCH 65/149] Loss_D: 0.6768 Loss_G: 0.7099 acc: 87.5%\n",
      "[BATCH 66/149] Loss_D: 0.7016 Loss_G: 0.7174 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.6830 Loss_G: 0.6984 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.6796 Loss_G: 0.6878 acc: 87.5%\n",
      "[BATCH 69/149] Loss_D: 0.6932 Loss_G: 0.7217 acc: 93.8%\n",
      "[BATCH 70/149] Loss_D: 0.6805 Loss_G: 0.7597 acc: 92.2%\n",
      "[BATCH 71/149] Loss_D: 0.6806 Loss_G: 0.7056 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6963 Loss_G: 0.6979 acc: 79.7%\n",
      "[BATCH 73/149] Loss_D: 0.6667 Loss_G: 0.7210 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.6989 Loss_G: 0.7013 acc: 84.4%\n",
      "[BATCH 75/149] Loss_D: 0.7027 Loss_G: 0.6994 acc: 89.1%\n",
      "[BATCH 76/149] Loss_D: 0.6481 Loss_G: 0.6889 acc: 90.6%\n",
      "[BATCH 77/149] Loss_D: 0.6835 Loss_G: 0.6914 acc: 87.5%\n",
      "[BATCH 78/149] Loss_D: 0.6981 Loss_G: 0.7077 acc: 87.5%\n",
      "[BATCH 79/149] Loss_D: 0.6930 Loss_G: 0.7114 acc: 84.4%\n",
      "[BATCH 80/149] Loss_D: 0.6765 Loss_G: 0.7144 acc: 95.3%\n",
      "[BATCH 81/149] Loss_D: 0.7501 Loss_G: 0.7269 acc: 85.9%\n",
      "[BATCH 82/149] Loss_D: 0.6645 Loss_G: 0.6945 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6573 Loss_G: 0.6780 acc: 96.9%\n",
      "[BATCH 84/149] Loss_D: 0.6781 Loss_G: 0.6930 acc: 89.1%\n",
      "[BATCH 85/149] Loss_D: 0.7035 Loss_G: 0.7001 acc: 89.1%\n",
      "[BATCH 86/149] Loss_D: 0.6569 Loss_G: 0.6958 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.6506 Loss_G: 0.6978 acc: 92.2%\n",
      "[BATCH 88/149] Loss_D: 0.6853 Loss_G: 0.6906 acc: 85.9%\n",
      "[BATCH 89/149] Loss_D: 0.7012 Loss_G: 0.7071 acc: 93.8%\n",
      "[BATCH 90/149] Loss_D: 0.6847 Loss_G: 0.7282 acc: 93.8%\n",
      "[BATCH 91/149] Loss_D: 0.6927 Loss_G: 0.7213 acc: 85.9%\n",
      "[BATCH 92/149] Loss_D: 0.6896 Loss_G: 0.7187 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6620 Loss_G: 0.6853 acc: 82.8%\n",
      "[BATCH 94/149] Loss_D: 0.6725 Loss_G: 0.6890 acc: 89.1%\n",
      "[BATCH 95/149] Loss_D: 0.6583 Loss_G: 0.7395 acc: 95.3%\n",
      "[BATCH 96/149] Loss_D: 0.6533 Loss_G: 0.6894 acc: 93.8%\n",
      "[EPOCH 6950] TEST ACC is : 75.8%\n",
      "[BATCH 97/149] Loss_D: 0.6631 Loss_G: 0.6832 acc: 85.9%\n",
      "[BATCH 98/149] Loss_D: 0.6551 Loss_G: 0.6939 acc: 85.9%\n",
      "[BATCH 99/149] Loss_D: 0.6915 Loss_G: 0.6975 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6862 Loss_G: 0.6990 acc: 87.5%\n",
      "[BATCH 101/149] Loss_D: 0.6714 Loss_G: 0.7038 acc: 85.9%\n",
      "[BATCH 102/149] Loss_D: 0.6637 Loss_G: 0.6860 acc: 92.2%\n",
      "[BATCH 103/149] Loss_D: 0.6599 Loss_G: 0.6921 acc: 89.1%\n",
      "[BATCH 104/149] Loss_D: 0.6895 Loss_G: 0.7007 acc: 82.8%\n",
      "[BATCH 105/149] Loss_D: 0.7120 Loss_G: 0.7234 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6792 Loss_G: 0.7634 acc: 92.2%\n",
      "[BATCH 107/149] Loss_D: 0.6824 Loss_G: 0.6952 acc: 82.8%\n",
      "[BATCH 108/149] Loss_D: 0.6915 Loss_G: 0.6712 acc: 87.5%\n",
      "[BATCH 109/149] Loss_D: 0.6624 Loss_G: 0.6829 acc: 84.4%\n",
      "[BATCH 110/149] Loss_D: 0.6622 Loss_G: 0.6744 acc: 93.8%\n",
      "[BATCH 111/149] Loss_D: 0.7097 Loss_G: 0.7044 acc: 84.4%\n",
      "[BATCH 112/149] Loss_D: 0.6903 Loss_G: 0.7106 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6843 Loss_G: 0.7144 acc: 96.9%\n",
      "[BATCH 114/149] Loss_D: 0.6828 Loss_G: 0.7245 acc: 90.6%\n",
      "[BATCH 115/149] Loss_D: 0.7089 Loss_G: 0.7103 acc: 79.7%\n",
      "[BATCH 116/149] Loss_D: 0.7000 Loss_G: 0.7017 acc: 87.5%\n",
      "[BATCH 117/149] Loss_D: 0.7118 Loss_G: 0.7292 acc: 85.9%\n",
      "[BATCH 118/149] Loss_D: 0.6622 Loss_G: 0.6974 acc: 92.2%\n",
      "[BATCH 119/149] Loss_D: 0.7118 Loss_G: 0.7034 acc: 92.2%\n",
      "[BATCH 120/149] Loss_D: 0.6617 Loss_G: 0.7005 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.6645 Loss_G: 0.6928 acc: 90.6%\n",
      "[BATCH 122/149] Loss_D: 0.6899 Loss_G: 0.6840 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.7052 Loss_G: 0.6937 acc: 89.1%\n",
      "[BATCH 124/149] Loss_D: 0.6275 Loss_G: 0.7010 acc: 96.9%\n",
      "[BATCH 125/149] Loss_D: 0.6653 Loss_G: 0.6814 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.6852 Loss_G: 0.7064 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6541 Loss_G: 0.7038 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.6676 Loss_G: 0.6873 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6862 Loss_G: 0.6862 acc: 84.4%\n",
      "[BATCH 130/149] Loss_D: 0.6611 Loss_G: 0.7011 acc: 92.2%\n",
      "[BATCH 131/149] Loss_D: 0.6708 Loss_G: 0.6747 acc: 82.8%\n",
      "[BATCH 132/149] Loss_D: 0.6719 Loss_G: 0.6788 acc: 84.4%\n",
      "[BATCH 133/149] Loss_D: 0.6408 Loss_G: 0.6805 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6807 Loss_G: 0.6880 acc: 89.1%\n",
      "[BATCH 135/149] Loss_D: 0.7075 Loss_G: 0.7106 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6533 Loss_G: 0.6890 acc: 87.5%\n",
      "[BATCH 137/149] Loss_D: 0.7173 Loss_G: 0.6992 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6678 Loss_G: 0.7098 acc: 89.1%\n",
      "[BATCH 139/149] Loss_D: 0.6381 Loss_G: 0.6985 acc: 93.8%\n",
      "[BATCH 140/149] Loss_D: 0.6736 Loss_G: 0.6955 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.6466 Loss_G: 0.6970 acc: 93.8%\n",
      "[BATCH 142/149] Loss_D: 0.6926 Loss_G: 0.7100 acc: 89.1%\n",
      "[BATCH 143/149] Loss_D: 0.7341 Loss_G: 0.7024 acc: 81.2%\n",
      "[BATCH 144/149] Loss_D: 0.6651 Loss_G: 0.7161 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7423 Loss_G: 0.7350 acc: 87.5%\n",
      "[BATCH 146/149] Loss_D: 0.6950 Loss_G: 0.7149 acc: 87.5%\n",
      "[EPOCH 7000] TEST ACC is : 76.6%\n",
      "[BATCH 147/149] Loss_D: 0.7173 Loss_G: 0.6990 acc: 84.4%\n",
      "[BATCH 148/149] Loss_D: 0.6979 Loss_G: 0.7061 acc: 79.7%\n",
      "[BATCH 149/149] Loss_D: 0.6815 Loss_G: 0.7009 acc: 85.9%\n",
      "-----THE [47/50] epoch end-----\n",
      "-----THE [48/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6685 Loss_G: 0.6858 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6659 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 3/149] Loss_D: 0.6302 Loss_G: 0.6803 acc: 90.6%\n",
      "[BATCH 4/149] Loss_D: 0.6679 Loss_G: 0.6809 acc: 89.1%\n",
      "[BATCH 5/149] Loss_D: 0.6470 Loss_G: 0.6972 acc: 96.9%\n",
      "[BATCH 6/149] Loss_D: 0.6790 Loss_G: 0.6855 acc: 85.9%\n",
      "[BATCH 7/149] Loss_D: 0.7034 Loss_G: 0.7395 acc: 90.6%\n",
      "[BATCH 8/149] Loss_D: 0.6574 Loss_G: 0.7001 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6658 Loss_G: 0.7005 acc: 90.6%\n",
      "[BATCH 10/149] Loss_D: 0.6728 Loss_G: 0.6878 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6721 Loss_G: 0.6806 acc: 87.5%\n",
      "[BATCH 12/149] Loss_D: 0.6844 Loss_G: 0.6910 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6724 Loss_G: 0.6893 acc: 89.1%\n",
      "[BATCH 14/149] Loss_D: 0.6927 Loss_G: 0.7139 acc: 87.5%\n",
      "[BATCH 15/149] Loss_D: 0.6469 Loss_G: 0.6953 acc: 84.4%\n",
      "[BATCH 16/149] Loss_D: 0.6765 Loss_G: 0.7107 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6676 Loss_G: 0.6920 acc: 87.5%\n",
      "[BATCH 18/149] Loss_D: 0.6752 Loss_G: 0.6912 acc: 79.7%\n",
      "[BATCH 19/149] Loss_D: 0.6855 Loss_G: 0.6904 acc: 90.6%\n",
      "[BATCH 20/149] Loss_D: 0.6733 Loss_G: 0.6896 acc: 85.9%\n",
      "[BATCH 21/149] Loss_D: 0.6746 Loss_G: 0.6989 acc: 90.6%\n",
      "[BATCH 22/149] Loss_D: 0.6666 Loss_G: 0.7036 acc: 92.2%\n",
      "[BATCH 23/149] Loss_D: 0.6808 Loss_G: 0.7183 acc: 89.1%\n",
      "[BATCH 24/149] Loss_D: 0.6824 Loss_G: 0.7021 acc: 84.4%\n",
      "[BATCH 25/149] Loss_D: 0.6693 Loss_G: 0.6926 acc: 87.5%\n",
      "[BATCH 26/149] Loss_D: 0.6764 Loss_G: 0.7067 acc: 87.5%\n",
      "[BATCH 27/149] Loss_D: 0.6509 Loss_G: 0.6863 acc: 89.1%\n",
      "[BATCH 28/149] Loss_D: 0.6682 Loss_G: 0.6774 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6499 Loss_G: 0.6898 acc: 92.2%\n",
      "[BATCH 30/149] Loss_D: 0.6765 Loss_G: 0.6972 acc: 87.5%\n",
      "[BATCH 31/149] Loss_D: 0.6669 Loss_G: 0.6829 acc: 89.1%\n",
      "[BATCH 32/149] Loss_D: 0.7301 Loss_G: 0.7378 acc: 89.1%\n",
      "[BATCH 33/149] Loss_D: 0.6987 Loss_G: 0.6912 acc: 84.4%\n",
      "[BATCH 34/149] Loss_D: 0.6544 Loss_G: 0.6859 acc: 98.4%\n",
      "[BATCH 35/149] Loss_D: 0.7195 Loss_G: 0.6980 acc: 85.9%\n",
      "[BATCH 36/149] Loss_D: 0.6917 Loss_G: 0.6953 acc: 85.9%\n",
      "[BATCH 37/149] Loss_D: 0.6821 Loss_G: 0.7055 acc: 90.6%\n",
      "[BATCH 38/149] Loss_D: 0.6567 Loss_G: 0.6796 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6962 Loss_G: 0.6856 acc: 87.5%\n",
      "[BATCH 40/149] Loss_D: 0.6754 Loss_G: 0.6689 acc: 82.8%\n",
      "[BATCH 41/149] Loss_D: 0.6898 Loss_G: 0.6994 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.7105 Loss_G: 0.7101 acc: 87.5%\n",
      "[BATCH 43/149] Loss_D: 0.6940 Loss_G: 0.7050 acc: 85.9%\n",
      "[BATCH 44/149] Loss_D: 0.6916 Loss_G: 0.7446 acc: 96.9%\n",
      "[BATCH 45/149] Loss_D: 0.7101 Loss_G: 0.7378 acc: 89.1%\n",
      "[BATCH 46/149] Loss_D: 0.6577 Loss_G: 0.7163 acc: 85.9%\n",
      "[BATCH 47/149] Loss_D: 0.7099 Loss_G: 0.7295 acc: 92.2%\n",
      "[EPOCH 7050] TEST ACC is : 77.0%\n",
      "[BATCH 48/149] Loss_D: 0.7109 Loss_G: 0.7209 acc: 82.8%\n",
      "[BATCH 49/149] Loss_D: 0.6812 Loss_G: 0.7372 acc: 87.5%\n",
      "[BATCH 50/149] Loss_D: 0.6768 Loss_G: 0.6946 acc: 84.4%\n",
      "[BATCH 51/149] Loss_D: 0.6775 Loss_G: 0.7042 acc: 93.8%\n",
      "[BATCH 52/149] Loss_D: 0.6564 Loss_G: 0.6952 acc: 90.6%\n",
      "[BATCH 53/149] Loss_D: 0.6764 Loss_G: 0.7067 acc: 90.6%\n",
      "[BATCH 54/149] Loss_D: 0.6694 Loss_G: 0.7057 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.7084 Loss_G: 0.7004 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6640 Loss_G: 0.6927 acc: 85.9%\n",
      "[BATCH 57/149] Loss_D: 0.6996 Loss_G: 0.7195 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6414 Loss_G: 0.7142 acc: 95.3%\n",
      "[BATCH 59/149] Loss_D: 0.6892 Loss_G: 0.7005 acc: 93.8%\n",
      "[BATCH 60/149] Loss_D: 0.7078 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 61/149] Loss_D: 0.6874 Loss_G: 0.7049 acc: 82.8%\n",
      "[BATCH 62/149] Loss_D: 0.6988 Loss_G: 0.7056 acc: 87.5%\n",
      "[BATCH 63/149] Loss_D: 0.6668 Loss_G: 0.6842 acc: 85.9%\n",
      "[BATCH 64/149] Loss_D: 0.7477 Loss_G: 0.7017 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.7012 Loss_G: 0.7040 acc: 82.8%\n",
      "[BATCH 66/149] Loss_D: 0.6575 Loss_G: 0.6920 acc: 85.9%\n",
      "[BATCH 67/149] Loss_D: 0.7156 Loss_G: 0.7098 acc: 87.5%\n",
      "[BATCH 68/149] Loss_D: 0.7107 Loss_G: 0.7160 acc: 85.9%\n",
      "[BATCH 69/149] Loss_D: 0.6700 Loss_G: 0.6903 acc: 87.5%\n",
      "[BATCH 70/149] Loss_D: 0.6670 Loss_G: 0.7315 acc: 96.9%\n",
      "[BATCH 71/149] Loss_D: 0.6713 Loss_G: 0.7013 acc: 92.2%\n",
      "[BATCH 72/149] Loss_D: 0.6828 Loss_G: 0.6988 acc: 93.8%\n",
      "[BATCH 73/149] Loss_D: 0.6790 Loss_G: 0.6937 acc: 93.8%\n",
      "[BATCH 74/149] Loss_D: 0.6528 Loss_G: 0.6848 acc: 85.9%\n",
      "[BATCH 75/149] Loss_D: 0.6763 Loss_G: 0.6981 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6865 Loss_G: 0.7081 acc: 87.5%\n",
      "[BATCH 77/149] Loss_D: 0.6887 Loss_G: 0.7027 acc: 89.1%\n",
      "[BATCH 78/149] Loss_D: 0.6564 Loss_G: 0.6804 acc: 85.9%\n",
      "[BATCH 79/149] Loss_D: 0.6491 Loss_G: 0.7014 acc: 92.2%\n",
      "[BATCH 80/149] Loss_D: 0.6969 Loss_G: 0.7043 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6975 Loss_G: 0.6990 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.6585 Loss_G: 0.6894 acc: 84.4%\n",
      "[BATCH 83/149] Loss_D: 0.6527 Loss_G: 0.6828 acc: 93.8%\n",
      "[BATCH 84/149] Loss_D: 0.6568 Loss_G: 0.6807 acc: 84.4%\n",
      "[BATCH 85/149] Loss_D: 0.6544 Loss_G: 0.6799 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6933 Loss_G: 0.6960 acc: 85.9%\n",
      "[BATCH 87/149] Loss_D: 0.6918 Loss_G: 0.7060 acc: 84.4%\n",
      "[BATCH 88/149] Loss_D: 0.6929 Loss_G: 0.7258 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.7027 Loss_G: 0.7196 acc: 87.5%\n",
      "[BATCH 90/149] Loss_D: 0.7010 Loss_G: 0.7161 acc: 92.2%\n",
      "[BATCH 91/149] Loss_D: 0.7220 Loss_G: 0.7029 acc: 82.8%\n",
      "[BATCH 92/149] Loss_D: 0.6774 Loss_G: 0.7077 acc: 87.5%\n",
      "[BATCH 93/149] Loss_D: 0.6702 Loss_G: 0.7069 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6880 Loss_G: 0.7104 acc: 87.5%\n",
      "[BATCH 95/149] Loss_D: 0.6933 Loss_G: 0.7444 acc: 85.9%\n",
      "[BATCH 96/149] Loss_D: 0.6974 Loss_G: 0.7023 acc: 81.2%\n",
      "[BATCH 97/149] Loss_D: 0.6834 Loss_G: 0.6759 acc: 84.4%\n",
      "[EPOCH 7100] TEST ACC is : 77.1%\n",
      "[BATCH 98/149] Loss_D: 0.6528 Loss_G: 0.6743 acc: 90.6%\n",
      "[BATCH 99/149] Loss_D: 0.6758 Loss_G: 0.6947 acc: 89.1%\n",
      "[BATCH 100/149] Loss_D: 0.6538 Loss_G: 0.7023 acc: 90.6%\n",
      "[BATCH 101/149] Loss_D: 0.6471 Loss_G: 0.7037 acc: 90.6%\n",
      "[BATCH 102/149] Loss_D: 0.6800 Loss_G: 0.6907 acc: 84.4%\n",
      "[BATCH 103/149] Loss_D: 0.6952 Loss_G: 0.7069 acc: 92.2%\n",
      "[BATCH 104/149] Loss_D: 0.6510 Loss_G: 0.6829 acc: 81.2%\n",
      "[BATCH 105/149] Loss_D: 0.6700 Loss_G: 0.6729 acc: 84.4%\n",
      "[BATCH 106/149] Loss_D: 0.6560 Loss_G: 0.6859 acc: 85.9%\n",
      "[BATCH 107/149] Loss_D: 0.6594 Loss_G: 0.6906 acc: 90.6%\n",
      "[BATCH 108/149] Loss_D: 0.6769 Loss_G: 0.7086 acc: 90.6%\n",
      "[BATCH 109/149] Loss_D: 0.6647 Loss_G: 0.7167 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6531 Loss_G: 0.6987 acc: 90.6%\n",
      "[BATCH 111/149] Loss_D: 0.6800 Loss_G: 0.7132 acc: 85.9%\n",
      "[BATCH 112/149] Loss_D: 0.6856 Loss_G: 0.7111 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.7071 Loss_G: 0.7062 acc: 96.9%\n",
      "[BATCH 114/149] Loss_D: 0.6660 Loss_G: 0.6992 acc: 95.3%\n",
      "[BATCH 115/149] Loss_D: 0.6952 Loss_G: 0.7042 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.6721 Loss_G: 0.6911 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6796 Loss_G: 0.7126 acc: 93.8%\n",
      "[BATCH 118/149] Loss_D: 0.6700 Loss_G: 0.6962 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6599 Loss_G: 0.6811 acc: 84.4%\n",
      "[BATCH 120/149] Loss_D: 0.6810 Loss_G: 0.6760 acc: 87.5%\n",
      "[BATCH 121/149] Loss_D: 0.7095 Loss_G: 0.7291 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.6761 Loss_G: 0.7106 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.7121 Loss_G: 0.7419 acc: 96.9%\n",
      "[BATCH 124/149] Loss_D: 0.6884 Loss_G: 0.7374 acc: 85.9%\n",
      "[BATCH 125/149] Loss_D: 0.6895 Loss_G: 0.6969 acc: 87.5%\n",
      "[BATCH 126/149] Loss_D: 0.7218 Loss_G: 0.7000 acc: 85.9%\n",
      "[BATCH 127/149] Loss_D: 0.6631 Loss_G: 0.6966 acc: 90.6%\n",
      "[BATCH 128/149] Loss_D: 0.6877 Loss_G: 0.7130 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.6526 Loss_G: 0.6975 acc: 92.2%\n",
      "[BATCH 130/149] Loss_D: 0.7010 Loss_G: 0.7036 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.7236 Loss_G: 0.7116 acc: 85.9%\n",
      "[BATCH 132/149] Loss_D: 0.6685 Loss_G: 0.6968 acc: 93.8%\n",
      "[BATCH 133/149] Loss_D: 0.6908 Loss_G: 0.7014 acc: 90.6%\n",
      "[BATCH 134/149] Loss_D: 0.6728 Loss_G: 0.7135 acc: 92.2%\n",
      "[BATCH 135/149] Loss_D: 0.6710 Loss_G: 0.7138 acc: 87.5%\n",
      "[BATCH 136/149] Loss_D: 0.6737 Loss_G: 0.7112 acc: 95.3%\n",
      "[BATCH 137/149] Loss_D: 0.6885 Loss_G: 0.7145 acc: 89.1%\n",
      "[BATCH 138/149] Loss_D: 0.6653 Loss_G: 0.6808 acc: 90.6%\n",
      "[BATCH 139/149] Loss_D: 0.7072 Loss_G: 0.6867 acc: 92.2%\n",
      "[BATCH 140/149] Loss_D: 0.7123 Loss_G: 0.7100 acc: 87.5%\n",
      "[BATCH 141/149] Loss_D: 0.7074 Loss_G: 0.7098 acc: 89.1%\n",
      "[BATCH 142/149] Loss_D: 0.6552 Loss_G: 0.6934 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6669 Loss_G: 0.6847 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.6665 Loss_G: 0.6882 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.7034 Loss_G: 0.7134 acc: 93.8%\n",
      "[BATCH 146/149] Loss_D: 0.6680 Loss_G: 0.7081 acc: 85.9%\n",
      "[BATCH 147/149] Loss_D: 0.6813 Loss_G: 0.7048 acc: 87.5%\n",
      "[EPOCH 7150] TEST ACC is : 76.6%\n",
      "[BATCH 148/149] Loss_D: 0.6815 Loss_G: 0.6940 acc: 82.8%\n",
      "[BATCH 149/149] Loss_D: 0.6761 Loss_G: 0.6867 acc: 87.5%\n",
      "-----THE [48/50] epoch end-----\n",
      "-----THE [49/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6675 Loss_G: 0.7157 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6844 Loss_G: 0.6863 acc: 85.9%\n",
      "[BATCH 3/149] Loss_D: 0.6759 Loss_G: 0.7002 acc: 93.8%\n",
      "[BATCH 4/149] Loss_D: 0.6926 Loss_G: 0.6970 acc: 93.8%\n",
      "[BATCH 5/149] Loss_D: 0.7007 Loss_G: 0.6940 acc: 85.9%\n",
      "[BATCH 6/149] Loss_D: 0.6576 Loss_G: 0.6832 acc: 92.2%\n",
      "[BATCH 7/149] Loss_D: 0.6369 Loss_G: 0.6756 acc: 84.4%\n",
      "[BATCH 8/149] Loss_D: 0.6637 Loss_G: 0.6820 acc: 90.6%\n",
      "[BATCH 9/149] Loss_D: 0.6658 Loss_G: 0.6895 acc: 85.9%\n",
      "[BATCH 10/149] Loss_D: 0.7143 Loss_G: 0.7326 acc: 92.2%\n",
      "[BATCH 11/149] Loss_D: 0.6730 Loss_G: 0.7125 acc: 89.1%\n",
      "[BATCH 12/149] Loss_D: 0.6755 Loss_G: 0.6814 acc: 84.4%\n",
      "[BATCH 13/149] Loss_D: 0.6663 Loss_G: 0.7108 acc: 95.3%\n",
      "[BATCH 14/149] Loss_D: 0.7035 Loss_G: 0.6984 acc: 84.4%\n",
      "[BATCH 15/149] Loss_D: 0.6696 Loss_G: 0.7083 acc: 82.8%\n",
      "[BATCH 16/149] Loss_D: 0.6948 Loss_G: 0.7309 acc: 92.2%\n",
      "[BATCH 17/149] Loss_D: 0.6696 Loss_G: 0.6966 acc: 89.1%\n",
      "[BATCH 18/149] Loss_D: 0.6879 Loss_G: 0.6830 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6806 Loss_G: 0.7310 acc: 93.8%\n",
      "[BATCH 20/149] Loss_D: 0.6900 Loss_G: 0.6999 acc: 84.4%\n",
      "[BATCH 21/149] Loss_D: 0.7102 Loss_G: 0.7141 acc: 89.1%\n",
      "[BATCH 22/149] Loss_D: 0.6922 Loss_G: 0.7358 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.6706 Loss_G: 0.7189 acc: 92.2%\n",
      "[BATCH 24/149] Loss_D: 0.6801 Loss_G: 0.7037 acc: 85.9%\n",
      "[BATCH 25/149] Loss_D: 0.6526 Loss_G: 0.6688 acc: 89.1%\n",
      "[BATCH 26/149] Loss_D: 0.6747 Loss_G: 0.6851 acc: 93.8%\n",
      "[BATCH 27/149] Loss_D: 0.6445 Loss_G: 0.6892 acc: 90.6%\n",
      "[BATCH 28/149] Loss_D: 0.6826 Loss_G: 0.6975 acc: 92.2%\n",
      "[BATCH 29/149] Loss_D: 0.6657 Loss_G: 0.7123 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6612 Loss_G: 0.6949 acc: 92.2%\n",
      "[BATCH 31/149] Loss_D: 0.6809 Loss_G: 0.7016 acc: 85.9%\n",
      "[BATCH 32/149] Loss_D: 0.6981 Loss_G: 0.7064 acc: 85.9%\n",
      "[BATCH 33/149] Loss_D: 0.6903 Loss_G: 0.6981 acc: 82.8%\n",
      "[BATCH 34/149] Loss_D: 0.6773 Loss_G: 0.6872 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.6691 Loss_G: 0.6863 acc: 90.6%\n",
      "[BATCH 36/149] Loss_D: 0.6702 Loss_G: 0.6978 acc: 92.2%\n",
      "[BATCH 37/149] Loss_D: 0.6389 Loss_G: 0.6806 acc: 89.1%\n",
      "[BATCH 38/149] Loss_D: 0.7304 Loss_G: 0.6995 acc: 84.4%\n",
      "[BATCH 39/149] Loss_D: 0.6889 Loss_G: 0.6943 acc: 84.4%\n",
      "[BATCH 40/149] Loss_D: 0.6687 Loss_G: 0.6910 acc: 81.2%\n",
      "[BATCH 41/149] Loss_D: 0.6494 Loss_G: 0.7083 acc: 95.3%\n",
      "[BATCH 42/149] Loss_D: 0.6617 Loss_G: 0.6840 acc: 85.9%\n",
      "[BATCH 43/149] Loss_D: 0.6527 Loss_G: 0.7168 acc: 93.8%\n",
      "[BATCH 44/149] Loss_D: 0.7077 Loss_G: 0.6955 acc: 92.2%\n",
      "[BATCH 45/149] Loss_D: 0.6768 Loss_G: 0.6835 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.7194 Loss_G: 0.7226 acc: 89.1%\n",
      "[BATCH 47/149] Loss_D: 0.6647 Loss_G: 0.6954 acc: 84.4%\n",
      "[BATCH 48/149] Loss_D: 0.6847 Loss_G: 0.6923 acc: 89.1%\n",
      "[EPOCH 7200] TEST ACC is : 77.0%\n",
      "[BATCH 49/149] Loss_D: 0.6522 Loss_G: 0.7008 acc: 93.8%\n",
      "[BATCH 50/149] Loss_D: 0.6975 Loss_G: 0.7064 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.7206 Loss_G: 0.7291 acc: 87.5%\n",
      "[BATCH 52/149] Loss_D: 0.6671 Loss_G: 0.7154 acc: 92.2%\n",
      "[BATCH 53/149] Loss_D: 0.7296 Loss_G: 0.7108 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6794 Loss_G: 0.6978 acc: 85.9%\n",
      "[BATCH 55/149] Loss_D: 0.6357 Loss_G: 0.6810 acc: 90.6%\n",
      "[BATCH 56/149] Loss_D: 0.6962 Loss_G: 0.6751 acc: 81.2%\n",
      "[BATCH 57/149] Loss_D: 0.6729 Loss_G: 0.6906 acc: 87.5%\n",
      "[BATCH 58/149] Loss_D: 0.6622 Loss_G: 0.6962 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.6711 Loss_G: 0.6898 acc: 87.5%\n",
      "[BATCH 60/149] Loss_D: 0.7261 Loss_G: 0.7110 acc: 85.9%\n",
      "[BATCH 61/149] Loss_D: 0.6701 Loss_G: 0.7100 acc: 89.1%\n",
      "[BATCH 62/149] Loss_D: 0.6800 Loss_G: 0.7171 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.6600 Loss_G: 0.7138 acc: 92.2%\n",
      "[BATCH 64/149] Loss_D: 0.6931 Loss_G: 0.6950 acc: 89.1%\n",
      "[BATCH 65/149] Loss_D: 0.7022 Loss_G: 0.6998 acc: 85.9%\n",
      "[BATCH 66/149] Loss_D: 0.6995 Loss_G: 0.7290 acc: 89.1%\n",
      "[BATCH 67/149] Loss_D: 0.6721 Loss_G: 0.7150 acc: 89.1%\n",
      "[BATCH 68/149] Loss_D: 0.6577 Loss_G: 0.6836 acc: 90.6%\n",
      "[BATCH 69/149] Loss_D: 0.6820 Loss_G: 0.7021 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.7142 Loss_G: 0.7368 acc: 89.1%\n",
      "[BATCH 71/149] Loss_D: 0.6791 Loss_G: 0.7562 acc: 95.3%\n",
      "[BATCH 72/149] Loss_D: 0.7117 Loss_G: 0.7432 acc: 85.9%\n",
      "[BATCH 73/149] Loss_D: 0.6921 Loss_G: 0.7015 acc: 92.2%\n",
      "[BATCH 74/149] Loss_D: 0.7012 Loss_G: 0.7166 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6700 Loss_G: 0.7035 acc: 93.8%\n",
      "[BATCH 76/149] Loss_D: 0.6861 Loss_G: 0.6925 acc: 85.9%\n",
      "[BATCH 77/149] Loss_D: 0.6531 Loss_G: 0.6783 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6744 Loss_G: 0.6832 acc: 90.6%\n",
      "[BATCH 79/149] Loss_D: 0.6680 Loss_G: 0.6723 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.6745 Loss_G: 0.6867 acc: 84.4%\n",
      "[BATCH 81/149] Loss_D: 0.6760 Loss_G: 0.6926 acc: 93.8%\n",
      "[BATCH 82/149] Loss_D: 0.6634 Loss_G: 0.6988 acc: 89.1%\n",
      "[BATCH 83/149] Loss_D: 0.7015 Loss_G: 0.7064 acc: 81.2%\n",
      "[BATCH 84/149] Loss_D: 0.6917 Loss_G: 0.6988 acc: 85.9%\n",
      "[BATCH 85/149] Loss_D: 0.7170 Loss_G: 0.6993 acc: 85.9%\n",
      "[BATCH 86/149] Loss_D: 0.6732 Loss_G: 0.6946 acc: 89.1%\n",
      "[BATCH 87/149] Loss_D: 0.6654 Loss_G: 0.6974 acc: 89.1%\n",
      "[BATCH 88/149] Loss_D: 0.7036 Loss_G: 0.6962 acc: 84.4%\n",
      "[BATCH 89/149] Loss_D: 0.6763 Loss_G: 0.6737 acc: 85.9%\n",
      "[BATCH 90/149] Loss_D: 0.6951 Loss_G: 0.6858 acc: 89.1%\n",
      "[BATCH 91/149] Loss_D: 0.7023 Loss_G: 0.6906 acc: 87.5%\n",
      "[BATCH 92/149] Loss_D: 0.6748 Loss_G: 0.6934 acc: 84.4%\n",
      "[BATCH 93/149] Loss_D: 0.6508 Loss_G: 0.6929 acc: 90.6%\n",
      "[BATCH 94/149] Loss_D: 0.6756 Loss_G: 0.6926 acc: 92.2%\n",
      "[BATCH 95/149] Loss_D: 0.7087 Loss_G: 0.7071 acc: 90.6%\n",
      "[BATCH 96/149] Loss_D: 0.6955 Loss_G: 0.6976 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6528 Loss_G: 0.6844 acc: 93.8%\n",
      "[BATCH 98/149] Loss_D: 0.6865 Loss_G: 0.6879 acc: 89.1%\n",
      "[EPOCH 7250] TEST ACC is : 78.5%\n",
      "[BATCH 99/149] Loss_D: 0.6618 Loss_G: 0.6985 acc: 87.5%\n",
      "[BATCH 100/149] Loss_D: 0.6499 Loss_G: 0.7403 acc: 93.8%\n",
      "[BATCH 101/149] Loss_D: 0.6753 Loss_G: 0.6933 acc: 81.2%\n",
      "[BATCH 102/149] Loss_D: 0.7098 Loss_G: 0.7310 acc: 87.5%\n",
      "[BATCH 103/149] Loss_D: 0.6688 Loss_G: 0.7244 acc: 95.3%\n",
      "[BATCH 104/149] Loss_D: 0.6677 Loss_G: 0.6820 acc: 87.5%\n",
      "[BATCH 105/149] Loss_D: 0.6756 Loss_G: 0.6904 acc: 85.9%\n",
      "[BATCH 106/149] Loss_D: 0.7216 Loss_G: 0.7127 acc: 84.4%\n",
      "[BATCH 107/149] Loss_D: 0.6564 Loss_G: 0.6993 acc: 98.4%\n",
      "[BATCH 108/149] Loss_D: 0.6758 Loss_G: 0.7146 acc: 84.4%\n",
      "[BATCH 109/149] Loss_D: 0.6655 Loss_G: 0.7054 acc: 87.5%\n",
      "[BATCH 110/149] Loss_D: 0.6648 Loss_G: 0.6947 acc: 89.1%\n",
      "[BATCH 111/149] Loss_D: 0.6621 Loss_G: 0.7000 acc: 87.5%\n",
      "[BATCH 112/149] Loss_D: 0.6895 Loss_G: 0.7066 acc: 84.4%\n",
      "[BATCH 113/149] Loss_D: 0.6826 Loss_G: 0.7261 acc: 92.2%\n",
      "[BATCH 114/149] Loss_D: 0.6762 Loss_G: 0.7094 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.7079 Loss_G: 0.6946 acc: 87.5%\n",
      "[BATCH 116/149] Loss_D: 0.7164 Loss_G: 0.7099 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6799 Loss_G: 0.6981 acc: 89.1%\n",
      "[BATCH 118/149] Loss_D: 0.7211 Loss_G: 0.7138 acc: 90.6%\n",
      "[BATCH 119/149] Loss_D: 0.6744 Loss_G: 0.7007 acc: 85.9%\n",
      "[BATCH 120/149] Loss_D: 0.6676 Loss_G: 0.7153 acc: 92.2%\n",
      "[BATCH 121/149] Loss_D: 0.6639 Loss_G: 0.7069 acc: 84.4%\n",
      "[BATCH 122/149] Loss_D: 0.6633 Loss_G: 0.6821 acc: 90.6%\n",
      "[BATCH 123/149] Loss_D: 0.6887 Loss_G: 0.6886 acc: 84.4%\n",
      "[BATCH 124/149] Loss_D: 0.6855 Loss_G: 0.7190 acc: 95.3%\n",
      "[BATCH 125/149] Loss_D: 0.6903 Loss_G: 0.6802 acc: 81.2%\n",
      "[BATCH 126/149] Loss_D: 0.6678 Loss_G: 0.6794 acc: 82.8%\n",
      "[BATCH 127/149] Loss_D: 0.6762 Loss_G: 0.7030 acc: 87.5%\n",
      "[BATCH 128/149] Loss_D: 0.6736 Loss_G: 0.7168 acc: 90.6%\n",
      "[BATCH 129/149] Loss_D: 0.6724 Loss_G: 0.6991 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6924 Loss_G: 0.7038 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6507 Loss_G: 0.7035 acc: 93.8%\n",
      "[BATCH 132/149] Loss_D: 0.6681 Loss_G: 0.6729 acc: 89.1%\n",
      "[BATCH 133/149] Loss_D: 0.6838 Loss_G: 0.7025 acc: 95.3%\n",
      "[BATCH 134/149] Loss_D: 0.7054 Loss_G: 0.7365 acc: 82.8%\n",
      "[BATCH 135/149] Loss_D: 0.6986 Loss_G: 0.7305 acc: 90.6%\n",
      "[BATCH 136/149] Loss_D: 0.6653 Loss_G: 0.7278 acc: 93.8%\n",
      "[BATCH 137/149] Loss_D: 0.6696 Loss_G: 0.6974 acc: 92.2%\n",
      "[BATCH 138/149] Loss_D: 0.6931 Loss_G: 0.6806 acc: 76.6%\n",
      "[BATCH 139/149] Loss_D: 0.6942 Loss_G: 0.7028 acc: 87.5%\n",
      "[BATCH 140/149] Loss_D: 0.7157 Loss_G: 0.7179 acc: 89.1%\n",
      "[BATCH 141/149] Loss_D: 0.6544 Loss_G: 0.7251 acc: 95.3%\n",
      "[BATCH 142/149] Loss_D: 0.6919 Loss_G: 0.7186 acc: 90.6%\n",
      "[BATCH 143/149] Loss_D: 0.6639 Loss_G: 0.7276 acc: 93.8%\n",
      "[BATCH 144/149] Loss_D: 0.6515 Loss_G: 0.6862 acc: 90.6%\n",
      "[BATCH 145/149] Loss_D: 0.6993 Loss_G: 0.6976 acc: 90.6%\n",
      "[BATCH 146/149] Loss_D: 0.6564 Loss_G: 0.6922 acc: 89.1%\n",
      "[BATCH 147/149] Loss_D: 0.7057 Loss_G: 0.6985 acc: 87.5%\n",
      "[BATCH 148/149] Loss_D: 0.6978 Loss_G: 0.7039 acc: 90.6%\n",
      "[EPOCH 7300] TEST ACC is : 78.3%\n",
      "[BATCH 149/149] Loss_D: 0.6756 Loss_G: 0.6907 acc: 82.8%\n",
      "-----THE [49/50] epoch end-----\n",
      "-----THE [50/50] epoch start-----\n",
      "[BATCH 1/149] Loss_D: 0.6810 Loss_G: 0.6790 acc: 90.6%\n",
      "[BATCH 2/149] Loss_D: 0.6790 Loss_G: 0.6966 acc: 90.6%\n",
      "[BATCH 3/149] Loss_D: 0.6796 Loss_G: 0.7118 acc: 89.1%\n",
      "[BATCH 4/149] Loss_D: 0.6921 Loss_G: 0.7276 acc: 82.8%\n",
      "[BATCH 5/149] Loss_D: 0.6742 Loss_G: 0.7157 acc: 92.2%\n",
      "[BATCH 6/149] Loss_D: 0.7133 Loss_G: 0.7191 acc: 89.1%\n",
      "[BATCH 7/149] Loss_D: 0.6960 Loss_G: 0.7135 acc: 89.1%\n",
      "[BATCH 8/149] Loss_D: 0.7031 Loss_G: 0.7040 acc: 89.1%\n",
      "[BATCH 9/149] Loss_D: 0.6674 Loss_G: 0.6864 acc: 89.1%\n",
      "[BATCH 10/149] Loss_D: 0.6706 Loss_G: 0.6902 acc: 90.6%\n",
      "[BATCH 11/149] Loss_D: 0.6774 Loss_G: 0.7121 acc: 93.8%\n",
      "[BATCH 12/149] Loss_D: 0.6487 Loss_G: 0.6914 acc: 89.1%\n",
      "[BATCH 13/149] Loss_D: 0.6490 Loss_G: 0.6861 acc: 85.9%\n",
      "[BATCH 14/149] Loss_D: 0.6989 Loss_G: 0.6982 acc: 85.9%\n",
      "[BATCH 15/149] Loss_D: 0.6315 Loss_G: 0.7077 acc: 95.3%\n",
      "[BATCH 16/149] Loss_D: 0.6525 Loss_G: 0.6884 acc: 90.6%\n",
      "[BATCH 17/149] Loss_D: 0.6850 Loss_G: 0.6795 acc: 85.9%\n",
      "[BATCH 18/149] Loss_D: 0.6962 Loss_G: 0.7002 acc: 90.6%\n",
      "[BATCH 19/149] Loss_D: 0.6534 Loss_G: 0.7016 acc: 92.2%\n",
      "[BATCH 20/149] Loss_D: 0.6697 Loss_G: 0.6955 acc: 87.5%\n",
      "[BATCH 21/149] Loss_D: 0.6431 Loss_G: 0.6971 acc: 93.8%\n",
      "[BATCH 22/149] Loss_D: 0.6705 Loss_G: 0.6980 acc: 90.6%\n",
      "[BATCH 23/149] Loss_D: 0.7077 Loss_G: 0.6993 acc: 85.9%\n",
      "[BATCH 24/149] Loss_D: 0.6621 Loss_G: 0.7093 acc: 92.2%\n",
      "[BATCH 25/149] Loss_D: 0.6795 Loss_G: 0.7006 acc: 90.6%\n",
      "[BATCH 26/149] Loss_D: 0.6836 Loss_G: 0.7103 acc: 95.3%\n",
      "[BATCH 27/149] Loss_D: 0.6682 Loss_G: 0.6978 acc: 92.2%\n",
      "[BATCH 28/149] Loss_D: 0.6935 Loss_G: 0.6949 acc: 90.6%\n",
      "[BATCH 29/149] Loss_D: 0.6546 Loss_G: 0.6907 acc: 90.6%\n",
      "[BATCH 30/149] Loss_D: 0.6964 Loss_G: 0.7036 acc: 93.8%\n",
      "[BATCH 31/149] Loss_D: 0.6498 Loss_G: 0.6802 acc: 84.4%\n",
      "[BATCH 32/149] Loss_D: 0.6581 Loss_G: 0.6861 acc: 92.2%\n",
      "[BATCH 33/149] Loss_D: 0.6995 Loss_G: 0.7018 acc: 87.5%\n",
      "[BATCH 34/149] Loss_D: 0.6769 Loss_G: 0.6876 acc: 85.9%\n",
      "[BATCH 35/149] Loss_D: 0.7041 Loss_G: 0.7045 acc: 87.5%\n",
      "[BATCH 36/149] Loss_D: 0.6905 Loss_G: 0.6817 acc: 87.5%\n",
      "[BATCH 37/149] Loss_D: 0.6529 Loss_G: 0.6788 acc: 87.5%\n",
      "[BATCH 38/149] Loss_D: 0.6857 Loss_G: 0.7050 acc: 89.1%\n",
      "[BATCH 39/149] Loss_D: 0.6550 Loss_G: 0.6943 acc: 90.6%\n",
      "[BATCH 40/149] Loss_D: 0.6861 Loss_G: 0.7021 acc: 90.6%\n",
      "[BATCH 41/149] Loss_D: 0.6436 Loss_G: 0.6912 acc: 89.1%\n",
      "[BATCH 42/149] Loss_D: 0.6592 Loss_G: 0.6965 acc: 89.1%\n",
      "[BATCH 43/149] Loss_D: 0.6598 Loss_G: 0.6947 acc: 92.2%\n",
      "[BATCH 44/149] Loss_D: 0.7193 Loss_G: 0.6914 acc: 84.4%\n",
      "[BATCH 45/149] Loss_D: 0.6535 Loss_G: 0.6976 acc: 90.6%\n",
      "[BATCH 46/149] Loss_D: 0.6855 Loss_G: 0.7192 acc: 95.3%\n",
      "[BATCH 47/149] Loss_D: 0.6787 Loss_G: 0.7048 acc: 95.3%\n",
      "[BATCH 48/149] Loss_D: 0.6494 Loss_G: 0.6962 acc: 84.4%\n",
      "[BATCH 49/149] Loss_D: 0.6830 Loss_G: 0.6808 acc: 85.9%\n",
      "[EPOCH 7350] TEST ACC is : 77.3%\n",
      "[BATCH 50/149] Loss_D: 0.6831 Loss_G: 0.7004 acc: 92.2%\n",
      "[BATCH 51/149] Loss_D: 0.7093 Loss_G: 0.7312 acc: 89.1%\n",
      "[BATCH 52/149] Loss_D: 0.6670 Loss_G: 0.7093 acc: 93.8%\n",
      "[BATCH 53/149] Loss_D: 0.6675 Loss_G: 0.6942 acc: 87.5%\n",
      "[BATCH 54/149] Loss_D: 0.6661 Loss_G: 0.6962 acc: 90.6%\n",
      "[BATCH 55/149] Loss_D: 0.6778 Loss_G: 0.6874 acc: 87.5%\n",
      "[BATCH 56/149] Loss_D: 0.6878 Loss_G: 0.6976 acc: 82.8%\n",
      "[BATCH 57/149] Loss_D: 0.7407 Loss_G: 0.7238 acc: 84.4%\n",
      "[BATCH 58/149] Loss_D: 0.6715 Loss_G: 0.7089 acc: 93.8%\n",
      "[BATCH 59/149] Loss_D: 0.7247 Loss_G: 0.7040 acc: 78.1%\n",
      "[BATCH 60/149] Loss_D: 0.6432 Loss_G: 0.7214 acc: 92.2%\n",
      "[BATCH 61/149] Loss_D: 0.7020 Loss_G: 0.7140 acc: 92.2%\n",
      "[BATCH 62/149] Loss_D: 0.6880 Loss_G: 0.7104 acc: 84.4%\n",
      "[BATCH 63/149] Loss_D: 0.6853 Loss_G: 0.6829 acc: 82.8%\n",
      "[BATCH 64/149] Loss_D: 0.7080 Loss_G: 0.7020 acc: 87.5%\n",
      "[BATCH 65/149] Loss_D: 0.6419 Loss_G: 0.7124 acc: 98.4%\n",
      "[BATCH 66/149] Loss_D: 0.6665 Loss_G: 0.6978 acc: 90.6%\n",
      "[BATCH 67/149] Loss_D: 0.7199 Loss_G: 0.6962 acc: 82.8%\n",
      "[BATCH 68/149] Loss_D: 0.7040 Loss_G: 0.7000 acc: 84.4%\n",
      "[BATCH 69/149] Loss_D: 0.6751 Loss_G: 0.7041 acc: 89.1%\n",
      "[BATCH 70/149] Loss_D: 0.6735 Loss_G: 0.7194 acc: 90.6%\n",
      "[BATCH 71/149] Loss_D: 0.7028 Loss_G: 0.7076 acc: 85.9%\n",
      "[BATCH 72/149] Loss_D: 0.6748 Loss_G: 0.7124 acc: 90.6%\n",
      "[BATCH 73/149] Loss_D: 0.7045 Loss_G: 0.7279 acc: 95.3%\n",
      "[BATCH 74/149] Loss_D: 0.6774 Loss_G: 0.7162 acc: 92.2%\n",
      "[BATCH 75/149] Loss_D: 0.6978 Loss_G: 0.6944 acc: 85.9%\n",
      "[BATCH 76/149] Loss_D: 0.6772 Loss_G: 0.6941 acc: 84.4%\n",
      "[BATCH 77/149] Loss_D: 0.6850 Loss_G: 0.6991 acc: 90.6%\n",
      "[BATCH 78/149] Loss_D: 0.6684 Loss_G: 0.7073 acc: 89.1%\n",
      "[BATCH 79/149] Loss_D: 0.6537 Loss_G: 0.6978 acc: 90.6%\n",
      "[BATCH 80/149] Loss_D: 0.7497 Loss_G: 0.7115 acc: 85.9%\n",
      "[BATCH 81/149] Loss_D: 0.6697 Loss_G: 0.7050 acc: 87.5%\n",
      "[BATCH 82/149] Loss_D: 0.7024 Loss_G: 0.7020 acc: 87.5%\n",
      "[BATCH 83/149] Loss_D: 0.6867 Loss_G: 0.6995 acc: 85.9%\n",
      "[BATCH 84/149] Loss_D: 0.6556 Loss_G: 0.6829 acc: 93.8%\n",
      "[BATCH 85/149] Loss_D: 0.6606 Loss_G: 0.6962 acc: 87.5%\n",
      "[BATCH 86/149] Loss_D: 0.6927 Loss_G: 0.7099 acc: 93.8%\n",
      "[BATCH 87/149] Loss_D: 0.6956 Loss_G: 0.7169 acc: 90.6%\n",
      "[BATCH 88/149] Loss_D: 0.6994 Loss_G: 0.7020 acc: 90.6%\n",
      "[BATCH 89/149] Loss_D: 0.6818 Loss_G: 0.6968 acc: 90.6%\n",
      "[BATCH 90/149] Loss_D: 0.6621 Loss_G: 0.6763 acc: 87.5%\n",
      "[BATCH 91/149] Loss_D: 0.6876 Loss_G: 0.7082 acc: 89.1%\n",
      "[BATCH 92/149] Loss_D: 0.6713 Loss_G: 0.7010 acc: 89.1%\n",
      "[BATCH 93/149] Loss_D: 0.6581 Loss_G: 0.6849 acc: 87.5%\n",
      "[BATCH 94/149] Loss_D: 0.6842 Loss_G: 0.6951 acc: 81.2%\n",
      "[BATCH 95/149] Loss_D: 0.6773 Loss_G: 0.7336 acc: 96.9%\n",
      "[BATCH 96/149] Loss_D: 0.6995 Loss_G: 0.6959 acc: 85.9%\n",
      "[BATCH 97/149] Loss_D: 0.6562 Loss_G: 0.6954 acc: 92.2%\n",
      "[BATCH 98/149] Loss_D: 0.6708 Loss_G: 0.6885 acc: 95.3%\n",
      "[BATCH 99/149] Loss_D: 0.6827 Loss_G: 0.6768 acc: 89.1%\n",
      "[EPOCH 7400] TEST ACC is : 76.8%\n",
      "[BATCH 100/149] Loss_D: 0.6426 Loss_G: 0.6856 acc: 96.9%\n",
      "[BATCH 101/149] Loss_D: 0.6899 Loss_G: 0.6869 acc: 84.4%\n",
      "[BATCH 102/149] Loss_D: 0.6573 Loss_G: 0.6694 acc: 90.6%\n",
      "[BATCH 103/149] Loss_D: 0.6776 Loss_G: 0.6901 acc: 84.4%\n",
      "[BATCH 104/149] Loss_D: 0.6345 Loss_G: 0.6694 acc: 93.8%\n",
      "[BATCH 105/149] Loss_D: 0.6659 Loss_G: 0.6789 acc: 92.2%\n",
      "[BATCH 106/149] Loss_D: 0.6353 Loss_G: 0.6770 acc: 90.6%\n",
      "[BATCH 107/149] Loss_D: 0.6863 Loss_G: 0.6746 acc: 85.9%\n",
      "[BATCH 108/149] Loss_D: 0.7245 Loss_G: 0.6941 acc: 85.9%\n",
      "[BATCH 109/149] Loss_D: 0.7347 Loss_G: 0.7233 acc: 81.2%\n",
      "[BATCH 110/149] Loss_D: 0.6668 Loss_G: 0.6931 acc: 82.8%\n",
      "[BATCH 111/149] Loss_D: 0.6980 Loss_G: 0.6902 acc: 79.7%\n",
      "[BATCH 112/149] Loss_D: 0.6665 Loss_G: 0.7094 acc: 89.1%\n",
      "[BATCH 113/149] Loss_D: 0.6874 Loss_G: 0.7104 acc: 90.6%\n",
      "[BATCH 114/149] Loss_D: 0.6579 Loss_G: 0.7133 acc: 85.9%\n",
      "[BATCH 115/149] Loss_D: 0.6934 Loss_G: 0.6950 acc: 89.1%\n",
      "[BATCH 116/149] Loss_D: 0.6403 Loss_G: 0.6805 acc: 89.1%\n",
      "[BATCH 117/149] Loss_D: 0.6611 Loss_G: 0.6891 acc: 92.2%\n",
      "[BATCH 118/149] Loss_D: 0.6756 Loss_G: 0.6866 acc: 87.5%\n",
      "[BATCH 119/149] Loss_D: 0.6795 Loss_G: 0.7080 acc: 93.8%\n",
      "[BATCH 120/149] Loss_D: 0.6812 Loss_G: 0.7075 acc: 82.8%\n",
      "[BATCH 121/149] Loss_D: 0.7067 Loss_G: 0.7258 acc: 93.8%\n",
      "[BATCH 122/149] Loss_D: 0.7098 Loss_G: 0.7302 acc: 82.8%\n",
      "[BATCH 123/149] Loss_D: 0.6721 Loss_G: 0.7185 acc: 82.8%\n",
      "[BATCH 124/149] Loss_D: 0.6910 Loss_G: 0.7046 acc: 84.4%\n",
      "[BATCH 125/149] Loss_D: 0.6957 Loss_G: 0.7176 acc: 93.8%\n",
      "[BATCH 126/149] Loss_D: 0.6564 Loss_G: 0.7173 acc: 89.1%\n",
      "[BATCH 127/149] Loss_D: 0.6475 Loss_G: 0.6999 acc: 92.2%\n",
      "[BATCH 128/149] Loss_D: 0.7008 Loss_G: 0.6983 acc: 85.9%\n",
      "[BATCH 129/149] Loss_D: 0.6840 Loss_G: 0.7014 acc: 87.5%\n",
      "[BATCH 130/149] Loss_D: 0.6734 Loss_G: 0.7125 acc: 84.4%\n",
      "[BATCH 131/149] Loss_D: 0.6816 Loss_G: 0.7233 acc: 89.1%\n",
      "[BATCH 132/149] Loss_D: 0.6516 Loss_G: 0.6953 acc: 85.9%\n",
      "[BATCH 133/149] Loss_D: 0.6812 Loss_G: 0.6799 acc: 81.2%\n",
      "[BATCH 134/149] Loss_D: 0.6374 Loss_G: 0.6775 acc: 93.8%\n",
      "[BATCH 135/149] Loss_D: 0.6572 Loss_G: 0.6823 acc: 89.1%\n",
      "[BATCH 136/149] Loss_D: 0.7070 Loss_G: 0.7025 acc: 85.9%\n",
      "[BATCH 137/149] Loss_D: 0.6860 Loss_G: 0.7292 acc: 87.5%\n",
      "[BATCH 138/149] Loss_D: 0.6971 Loss_G: 0.7071 acc: 85.9%\n",
      "[BATCH 139/149] Loss_D: 0.7146 Loss_G: 0.7130 acc: 89.1%\n",
      "[BATCH 140/149] Loss_D: 0.6785 Loss_G: 0.7303 acc: 90.6%\n",
      "[BATCH 141/149] Loss_D: 0.7152 Loss_G: 0.7080 acc: 79.7%\n",
      "[BATCH 142/149] Loss_D: 0.6523 Loss_G: 0.6898 acc: 84.4%\n",
      "[BATCH 143/149] Loss_D: 0.6706 Loss_G: 0.6762 acc: 82.8%\n",
      "[BATCH 144/149] Loss_D: 0.6923 Loss_G: 0.6678 acc: 84.4%\n",
      "[BATCH 145/149] Loss_D: 0.7081 Loss_G: 0.7197 acc: 85.9%\n",
      "[BATCH 146/149] Loss_D: 0.6829 Loss_G: 0.7022 acc: 95.3%\n",
      "[BATCH 147/149] Loss_D: 0.6973 Loss_G: 0.7215 acc: 85.9%\n",
      "[BATCH 148/149] Loss_D: 0.6798 Loss_G: 0.7188 acc: 87.5%\n",
      "[BATCH 149/149] Loss_D: 0.7010 Loss_G: 0.7256 acc: 92.2%\n",
      "[EPOCH 7450] TEST ACC is : 77.7%\n",
      "-----THE [50/50] epoch end-----\n"
     ]
    }
   ],
   "source": [
    "input_data = os.listdir(INPUT_PATH)\n",
    "npy_list = []\n",
    "npy_level_list = []\n",
    "\n",
    "for level_num in range(5): #遍历patient文件夹——study指代每一个study文件夹\n",
    "    if level_num == 2:\n",
    "        continue\n",
    "    npy_file_path = os.path.join(INPUT_PATH, \"malignancy_\" + str(level_num + 1))\n",
    "    npy_files = os.listdir(npy_file_path)\n",
    "    for i in npy_files:\n",
    "        npy_path = os.path.join(npy_file_path, i)\n",
    "        single_npy = np.load(npy_path)\n",
    "        single_fliplr_npy = np.fliplr(single_npy)\n",
    "        single_npy = (single_npy - 127.5) / 127.5\n",
    "        npy_list.append(single_npy)\n",
    "        if level_num < 2:\n",
    "            npy_level_list.append(0)\n",
    "        else:\n",
    "            npy_level_list.append(1)\n",
    "        if level_num > 2:\n",
    "            single_fliplr_npy = (single_fliplr_npy - 127.5) / 127.5\n",
    "            npy_list.append(single_fliplr_npy)\n",
    "            npy_level_list.append(1)\n",
    "\n",
    "random.seed(randnum)\n",
    "random.shuffle(npy_list)\n",
    "random.seed(randnum)\n",
    "random.shuffle(npy_level_list)\n",
    "\n",
    "npy_chunks = chunks(npy_list, 10)\n",
    "npy_level_chunks = chunks(npy_level_list, 10)\n",
    "\n",
    "print(\"NOW the training STARTS:\")\n",
    "training_set, test_set = ten_folder(npy_chunks, 9)\n",
    "training_level, test_level = ten_folder(npy_level_chunks, 9)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(MyDataset(training_set, training_level), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(MyDataset(test_set, test_level), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "np.save(OUTPUT_PATH + \"train_set.npy\", np.array(training_set))\n",
    "np.save(OUTPUT_PATH + \"train_level.npy\", np.array(training_level))\n",
    "np.save(OUTPUT_PATH + \"test_set.npy\", np.array(test_set))\n",
    "np.save(OUTPUT_PATH + \"test_level.npy\", np.array(test_level))\n",
    "\n",
    "for i in range(6):\n",
    "    print(\"The %d * 50 epochs train starts:\" % (i + 1))\n",
    "    train(train_loader, test_loader)\n",
    "    torch.save(generator.state_dict(), OUTPUT_PATH + 'G_params.pkl')\n",
    "    torch.save(discriminator.state_dict(), OUTPUT_PATH + 'D_params.pkl')\n",
    "    np.save(OUTPUT_PATH + \"train_acc.npy\", np.array(train_acc))\n",
    "    np.save(OUTPUT_PATH + \"test_acc.npy\", np.array(test_acc))\n",
    "    np.save(OUTPUT_PATH + \"g_losses.npy\", np.array(g_losses))\n",
    "    np.save(OUTPUT_PATH + \"d_losses.npy\", np.array(d_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(test_acc)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.savefig(OUTPUT_PATH + \"test_acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_acc[::50])\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "plt.savefig(OUTPUT_PATH + \"train_acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(d_losses[::50], label=\"D\")\n",
    "plt.plot(g_losses[::50], label=\"G\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(OUTPUT_PATH + \"loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_img_list = []\n",
    "fake_label_list = []\n",
    "for gen_num in range(1000):\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (BATCH_SIZE, 100))))\n",
    "    gen_labels = Variable(LongTensor(np.random.randint(0, LEVELS_NUM, BATCH_SIZE)))\n",
    "    with torch.no_grad():\n",
    "        fake = generator(z,gen_labels).detach().cpu()\n",
    "        fake = fake.tolist()\n",
    "        fake_label_list.extend(gen_labels)\n",
    "        fake_img_list.extend(fake)\n",
    "np.save(OUTPUT_PATH + \"fake.npy\", np.array(fake_img_list))\n",
    "np.save(OUTPUT_PATH + \"fake_label.npy\", np.array(fake_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
