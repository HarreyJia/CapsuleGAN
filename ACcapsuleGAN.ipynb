{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "INPUT_PATH = \"./output/nodule_npy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "randnum = random.randint(0,100)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 10\n",
    "nz = 100\n",
    "n_epochs = 100\n",
    "ngf = 64\n",
    "ngpu = 1\n",
    "LEVELS_NUM = 2\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def chunks(arr, m):\n",
    "    nchunk = int(math.ceil(len(arr) / float(m)))\n",
    "    return [arr[i:i + nchunk] for i in range(0, len(arr), nchunk)]\n",
    "\n",
    "def ten_folder(arr, number):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    for j in range(len(arr)):\n",
    "        if number == j:\n",
    "            test_set.extend(arr[j])\n",
    "        else:\n",
    "            training_set.extend(arr[j])\n",
    "    return training_set, test_set\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images,labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        img = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.label_emb = nn.Embedding(LEVELS_NUM, 100)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "          nn.Linear(nz, 1 * 5 * 5 * ngf * 4, bias=False)\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "          # input is Z, going into a deconvolution\n",
    "          # state size. BATCH_SIZE x (ngf*4) x 1 x 5 x 5\n",
    "          nn.ConvTranspose3d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.BatchNorm3d(ngf * 2),\n",
    "          nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf*2) x 2 x 10 x 10\n",
    "          nn.ConvTranspose3d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.BatchNorm3d(ngf),\n",
    "          nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf) x 4 x 20 x 20\n",
    "          nn.ConvTranspose3d(ngf, 1, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.Tanh()\n",
    "          # state size. BATCH_SIZE x 1 x 8 x 40 x 40\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        \n",
    "        x = self.project(gen_input)\n",
    "        # Conv3d的规定输入数据格式为(batch, channel, Depth, Height, Width)\n",
    "        x = x.view(-1, ngf * 4, 1, 5, 5)\n",
    "        \n",
    "        if noise.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.deconv, x, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.deconv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "def softmax(input, dim=1):\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=256, kernel_size=(2,9,9)):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels,\n",
    "                                out_channels=out_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                stride=1\n",
    "                              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=(2,9,9), num_routes=32 * 12 * 12 * 6):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "          nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=(1,2,2), padding=0) \n",
    "                      for _ in range(num_capsules)])\n",
    "  \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "        u = torch.cat(u, dim=-1)\n",
    "        return self.squash(u)\n",
    "  \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=1, num_routes=32 * 12 * 12 * 6, in_channels=8, out_channels=16, num_iterations=3):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "        self.num_iterations = num_iterations\n",
    "        self.route_weights = nn.Parameter(torch.randn(num_capsules, num_routes, in_channels, out_channels)).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 矩阵相乘\n",
    "        # x.size(): [1, batch_size, in_capsules, 1, dim_in_capsule]\n",
    "        # weight.size(): [num_capsules, 1, num_route, in_channels, out_channels]\n",
    "        priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :]\n",
    "        logits = Variable(torch.zeros(*priors.size())).to(device)\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            probs = softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "\n",
    "            if i != self.num_routes - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        \n",
    "        outputs = outputs.squeeze()\n",
    "#         print(\"OUTPUT:\", outputs.shape)\n",
    "        return outputs\n",
    "  \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.D_digit_capsules = DigitCaps()\n",
    "        self.C_digit_capsules = DigitCaps(num_capsules=LEVELS_NUM)\n",
    "\n",
    "    def forward(self, data):\n",
    "        if data.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.conv_layer, data, range(self.ngpu))\n",
    "            output = nn.parallel.data_parallel(self.primary_capsules, output, range(self.ngpu))\n",
    "            D_output = nn.parallel.data_parallel(self.D_digit_capsules, output, range(self.ngpu))\n",
    "            C_output = nn.parallel.data_parallel(self.C_digit_capsules, output, range(self.ngpu))\n",
    "            C_output = C_output.transpose(0,1)\n",
    "        else:\n",
    "            output = self.primary_capsules(self.conv_layer(data))\n",
    "            D_output = self.D_digit_capsules(output)\n",
    "            C_output = self.C_digit_capsules(output).transpose(0,1)\n",
    "            \n",
    "\n",
    "        D_classes = (D_output ** 2).sum(dim=-1) ** 0.5\n",
    "        C_classes = (C_output ** 2).sum(dim=-1) ** 0.5\n",
    "#         print(D_classes, C_classes)\n",
    "        return D_classes, C_classes\n",
    "\n",
    "\n",
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "\n",
    "    def forward(self, classes, labels):\n",
    "        left = F.relu(0.9 - classes, inplace=True) ** 2\n",
    "        right = F.relu(classes - 0.1, inplace=True) ** 2\n",
    "\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "\n",
    "        return margin_loss / classes.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "discriminator = CapsNet(ngpu).to(device)\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "generator = Generator(ngpu).to(device)\n",
    "generator.apply(weights_init)\n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "def train(train_loader, test_set, test_level):\n",
    "    test_loader = torch.utils.data.DataLoader(MyDataset(test_set, test_level), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    epoch_num = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"-----THE [{}/{}] epoch start-----\".format(epoch + 1, n_epochs))\n",
    "        for j, (data,labels) in enumerate(train_loader, 0):\n",
    "#                     print(j, data.shape) #torch.Size([32, 1, 8, 40, 40])\n",
    "\n",
    "#                     plt.figure()\n",
    "#                     for lenc in range(data.shape[0]):\n",
    "#                         for len_img in range(8):\n",
    "#                             plt.subplot(2, 4, len_img + 1)\n",
    "#                             pixel_array = data[lenc][0][len_img]\n",
    "#                             print(pixel_array)\n",
    "#                             plt.imshow(pixel_array, cmap=\"gray\")\n",
    "#                         plt.show()\n",
    "\n",
    "\n",
    "#                     pixel_array = data[0][0][0][20]\n",
    "#                     print(pixel_array)\n",
    "\n",
    "#                     data = data / 255.0\n",
    "#                     pixel_array = data[0][0][0][20]\n",
    "#                     print(pixel_array)\n",
    "\n",
    "            \n",
    "            real_imgs = data.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(FloatTensor(BATCH_SIZE, 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(FloatTensor(BATCH_SIZE, 1).fill_(0.0), requires_grad=False)\n",
    "            \n",
    "            \n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (BATCH_SIZE, 100))))\n",
    "            gen_labels = Variable(LongTensor(np.random.randint(0, LEVELS_NUM, BATCH_SIZE)))\n",
    "    \n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            discriminator.train()\n",
    "            discriminator.zero_grad()\n",
    "            \n",
    "            # Loss for real images\n",
    "            real_pred, real_aux = discriminator(real_imgs)\n",
    "            d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "            # Loss for fake images\n",
    "            fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "            d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            # Calculate discriminator accuracy\n",
    "            pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "            gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "            \n",
    "            pred = np.argmax(pred, axis=1)\n",
    "            \n",
    "            d_acc = np.mean(pred == gt)\n",
    "            \n",
    "#             print(pred)\n",
    "#             print(gt)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "            \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            \n",
    "            generator.zero_grad()\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            validity, pred_label = discriminator(gen_imgs)\n",
    "            \n",
    "            g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "\n",
    "            print('[BATCH %d/%d] Loss_D: %.4f Loss_G: %.4f acc: %.1f%%'\n",
    "                  % (j + 1, len(train_loader), d_loss.item(), g_loss.item(), 100 * d_acc))\n",
    "            \n",
    "            d_losses.append(d_loss)\n",
    "            g_losses.append(g_loss)\n",
    "            train_acc.append(d_acc)\n",
    "            \n",
    "            epoch_num += 1\n",
    "            d_acc_total = 0\n",
    "            if epoch_num % 50 == 0:\n",
    "                discriminator.eval()\n",
    "                for j, (data,labels) in enumerate(test_loader, 0):\n",
    "                    real_imgs = data.to(device, dtype=torch.float)\n",
    "                    labels = labels.to(device)\n",
    "                    real_pred, real_aux = discriminator(real_imgs)\n",
    "                    pred = real_aux.data.cpu().numpy()\n",
    "                    pred = np.argmax(pred, axis=1)\n",
    "                    gt = labels.data.cpu().numpy()\n",
    "                    d_acc = np.mean(pred == gt)\n",
    "                    d_acc_total += d_acc\n",
    "                single_test_acc = d_acc_total / len(test_loader)\n",
    "                print(\"[EPOCH %d] TEST ACC is : %.1f%%\" % (epoch_num, 100 * single_test_acc))\n",
    "                test_acc.append(single_test_acc)\n",
    "                \n",
    "\n",
    "        print(\"-----THE [{}/{}] epoch end-----\".format(epoch + 1, n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW the training STARTS:\n",
      "-----THE [1/100] epoch start-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harreyjia/anaconda3/envs/CapsuleGAN/lib/python3.8/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 1/478] Loss_D: 2.9057 Loss_G: 3.3267 acc: 60.0%\n",
      "[BATCH 2/478] Loss_D: 1.4589 Loss_G: 2.8863 acc: 55.0%\n",
      "[BATCH 3/478] Loss_D: 1.4474 Loss_G: 2.7142 acc: 40.0%\n",
      "[BATCH 4/478] Loss_D: 1.4095 Loss_G: 2.6243 acc: 30.0%\n",
      "[BATCH 5/478] Loss_D: 1.3702 Loss_G: 2.5671 acc: 55.0%\n",
      "[BATCH 6/478] Loss_D: 1.4049 Loss_G: 2.5381 acc: 65.0%\n",
      "[BATCH 7/478] Loss_D: 1.3326 Loss_G: 2.5156 acc: 45.0%\n",
      "[BATCH 8/478] Loss_D: 1.3203 Loss_G: 2.5047 acc: 60.0%\n",
      "[BATCH 9/478] Loss_D: 1.3479 Loss_G: 2.4930 acc: 60.0%\n",
      "[BATCH 10/478] Loss_D: 1.2950 Loss_G: 2.4890 acc: 55.0%\n",
      "[BATCH 11/478] Loss_D: 1.2729 Loss_G: 2.4768 acc: 55.0%\n",
      "[BATCH 12/478] Loss_D: 1.3355 Loss_G: 2.4704 acc: 50.0%\n",
      "[BATCH 13/478] Loss_D: 1.3289 Loss_G: 2.4564 acc: 70.0%\n",
      "[BATCH 14/478] Loss_D: 1.3103 Loss_G: 2.4628 acc: 40.0%\n",
      "[BATCH 15/478] Loss_D: 1.2667 Loss_G: 2.4612 acc: 80.0%\n",
      "[BATCH 16/478] Loss_D: 1.3279 Loss_G: 2.4525 acc: 75.0%\n",
      "[BATCH 17/478] Loss_D: 1.2601 Loss_G: 2.4490 acc: 45.0%\n",
      "[BATCH 18/478] Loss_D: 1.2357 Loss_G: 2.4478 acc: 55.0%\n",
      "[BATCH 19/478] Loss_D: 1.2998 Loss_G: 2.4474 acc: 60.0%\n",
      "[BATCH 20/478] Loss_D: 1.2506 Loss_G: 2.4450 acc: 65.0%\n",
      "[BATCH 21/478] Loss_D: 1.2916 Loss_G: 2.4287 acc: 45.0%\n",
      "[BATCH 22/478] Loss_D: 1.2375 Loss_G: 2.4153 acc: 55.0%\n",
      "[BATCH 23/478] Loss_D: 1.1895 Loss_G: 2.4431 acc: 55.0%\n",
      "[BATCH 24/478] Loss_D: 1.2002 Loss_G: 2.4301 acc: 65.0%\n",
      "[BATCH 25/478] Loss_D: 1.1914 Loss_G: 2.4149 acc: 40.0%\n",
      "[BATCH 26/478] Loss_D: 1.2183 Loss_G: 2.4302 acc: 60.0%\n",
      "[BATCH 27/478] Loss_D: 1.1640 Loss_G: 2.4070 acc: 55.0%\n",
      "[BATCH 28/478] Loss_D: 1.2064 Loss_G: 2.3969 acc: 50.0%\n",
      "[BATCH 29/478] Loss_D: 1.0990 Loss_G: 2.3729 acc: 55.0%\n",
      "[BATCH 30/478] Loss_D: 1.1159 Loss_G: 2.3409 acc: 55.0%\n",
      "[BATCH 31/478] Loss_D: 1.2507 Loss_G: 2.2945 acc: 75.0%\n",
      "[BATCH 32/478] Loss_D: 1.2301 Loss_G: 2.1998 acc: 25.0%\n",
      "[BATCH 33/478] Loss_D: 1.1156 Loss_G: 2.2446 acc: 50.0%\n",
      "[BATCH 34/478] Loss_D: 1.0758 Loss_G: 2.1850 acc: 35.0%\n",
      "[BATCH 35/478] Loss_D: 1.1480 Loss_G: 2.1397 acc: 65.0%\n",
      "[BATCH 36/478] Loss_D: 1.1777 Loss_G: 2.1297 acc: 60.0%\n",
      "[BATCH 37/478] Loss_D: 1.1113 Loss_G: 2.1103 acc: 55.0%\n",
      "[BATCH 38/478] Loss_D: 1.1510 Loss_G: 1.9980 acc: 50.0%\n",
      "[BATCH 39/478] Loss_D: 1.0864 Loss_G: 1.9609 acc: 70.0%\n",
      "[BATCH 40/478] Loss_D: 1.0912 Loss_G: 1.9471 acc: 55.0%\n",
      "[BATCH 41/478] Loss_D: 1.1580 Loss_G: 1.8625 acc: 50.0%\n",
      "[BATCH 42/478] Loss_D: 1.0657 Loss_G: 1.9178 acc: 50.0%\n",
      "[BATCH 43/478] Loss_D: 1.0425 Loss_G: 1.7798 acc: 50.0%\n",
      "[BATCH 44/478] Loss_D: 1.0600 Loss_G: 1.7305 acc: 25.0%\n",
      "[BATCH 45/478] Loss_D: 1.1478 Loss_G: 1.6493 acc: 35.0%\n",
      "[BATCH 46/478] Loss_D: 1.0417 Loss_G: 1.6295 acc: 65.0%\n",
      "[BATCH 47/478] Loss_D: 1.0667 Loss_G: 1.6115 acc: 70.0%\n",
      "[BATCH 48/478] Loss_D: 0.9827 Loss_G: 1.6454 acc: 45.0%\n",
      "[BATCH 49/478] Loss_D: 1.0682 Loss_G: 1.4985 acc: 55.0%\n",
      "[BATCH 50/478] Loss_D: 1.0579 Loss_G: 1.5723 acc: 30.0%\n",
      "[EPOCH 50] TEST ACC is : 56.7%\n",
      "[BATCH 51/478] Loss_D: 1.0035 Loss_G: 1.4611 acc: 65.0%\n",
      "[BATCH 52/478] Loss_D: 1.0374 Loss_G: 1.5286 acc: 70.0%\n",
      "[BATCH 53/478] Loss_D: 1.1725 Loss_G: 1.4535 acc: 60.0%\n",
      "[BATCH 54/478] Loss_D: 1.1679 Loss_G: 1.4381 acc: 60.0%\n",
      "[BATCH 55/478] Loss_D: 1.1272 Loss_G: 1.4011 acc: 45.0%\n",
      "[BATCH 56/478] Loss_D: 1.0875 Loss_G: 1.3955 acc: 55.0%\n",
      "[BATCH 57/478] Loss_D: 1.0042 Loss_G: 1.3693 acc: 40.0%\n",
      "[BATCH 58/478] Loss_D: 1.0306 Loss_G: 1.3530 acc: 50.0%\n",
      "[BATCH 59/478] Loss_D: 1.0918 Loss_G: 1.4102 acc: 45.0%\n",
      "[BATCH 60/478] Loss_D: 1.1381 Loss_G: 1.3684 acc: 45.0%\n",
      "[BATCH 61/478] Loss_D: 1.0704 Loss_G: 1.3878 acc: 70.0%\n",
      "[BATCH 62/478] Loss_D: 1.2095 Loss_G: 1.3907 acc: 45.0%\n",
      "[BATCH 63/478] Loss_D: 1.1068 Loss_G: 1.3723 acc: 70.0%\n",
      "[BATCH 64/478] Loss_D: 1.0328 Loss_G: 1.3860 acc: 55.0%\n",
      "[BATCH 65/478] Loss_D: 1.0336 Loss_G: 1.3433 acc: 60.0%\n",
      "[BATCH 66/478] Loss_D: 1.0484 Loss_G: 1.3434 acc: 45.0%\n",
      "[BATCH 67/478] Loss_D: 1.0918 Loss_G: 1.3448 acc: 45.0%\n",
      "[BATCH 68/478] Loss_D: 0.9718 Loss_G: 1.3156 acc: 45.0%\n",
      "[BATCH 69/478] Loss_D: 0.9332 Loss_G: 1.2935 acc: 35.0%\n",
      "[BATCH 70/478] Loss_D: 1.1111 Loss_G: 1.3178 acc: 70.0%\n",
      "[BATCH 71/478] Loss_D: 0.9853 Loss_G: 1.2919 acc: 30.0%\n",
      "[BATCH 72/478] Loss_D: 1.0045 Loss_G: 1.3315 acc: 40.0%\n",
      "[BATCH 73/478] Loss_D: 1.1205 Loss_G: 1.2925 acc: 45.0%\n",
      "[BATCH 74/478] Loss_D: 1.0285 Loss_G: 1.2626 acc: 65.0%\n",
      "[BATCH 75/478] Loss_D: 0.9746 Loss_G: 1.2739 acc: 65.0%\n",
      "[BATCH 76/478] Loss_D: 1.1817 Loss_G: 1.2887 acc: 60.0%\n",
      "[BATCH 77/478] Loss_D: 0.9892 Loss_G: 1.2652 acc: 55.0%\n",
      "[BATCH 78/478] Loss_D: 1.1734 Loss_G: 1.3232 acc: 50.0%\n",
      "[BATCH 79/478] Loss_D: 1.0372 Loss_G: 1.2821 acc: 55.0%\n",
      "[BATCH 80/478] Loss_D: 0.9722 Loss_G: 1.2423 acc: 50.0%\n",
      "[BATCH 81/478] Loss_D: 1.0291 Loss_G: 1.2457 acc: 45.0%\n",
      "[BATCH 82/478] Loss_D: 1.0914 Loss_G: 1.2798 acc: 60.0%\n",
      "[BATCH 83/478] Loss_D: 1.0017 Loss_G: 1.2645 acc: 50.0%\n",
      "[BATCH 84/478] Loss_D: 0.9787 Loss_G: 1.2470 acc: 40.0%\n",
      "[BATCH 85/478] Loss_D: 1.0446 Loss_G: 1.2214 acc: 60.0%\n",
      "[BATCH 86/478] Loss_D: 1.0148 Loss_G: 1.2365 acc: 55.0%\n",
      "[BATCH 87/478] Loss_D: 0.9426 Loss_G: 1.2163 acc: 35.0%\n",
      "[BATCH 88/478] Loss_D: 0.9944 Loss_G: 1.1896 acc: 60.0%\n",
      "[BATCH 89/478] Loss_D: 0.9733 Loss_G: 1.1838 acc: 45.0%\n",
      "[BATCH 90/478] Loss_D: 1.0999 Loss_G: 1.2202 acc: 45.0%\n",
      "[BATCH 91/478] Loss_D: 1.1415 Loss_G: 1.2476 acc: 50.0%\n",
      "[BATCH 92/478] Loss_D: 1.0624 Loss_G: 1.2192 acc: 70.0%\n",
      "[BATCH 93/478] Loss_D: 1.0533 Loss_G: 1.2282 acc: 50.0%\n",
      "[BATCH 94/478] Loss_D: 0.9817 Loss_G: 1.2022 acc: 50.0%\n",
      "[BATCH 95/478] Loss_D: 0.9417 Loss_G: 1.1753 acc: 30.0%\n",
      "[BATCH 96/478] Loss_D: 1.1697 Loss_G: 1.2119 acc: 60.0%\n",
      "[BATCH 97/478] Loss_D: 0.9578 Loss_G: 1.1920 acc: 80.0%\n",
      "[BATCH 98/478] Loss_D: 1.0338 Loss_G: 1.1784 acc: 60.0%\n",
      "[BATCH 99/478] Loss_D: 0.9657 Loss_G: 1.1658 acc: 35.0%\n",
      "[BATCH 100/478] Loss_D: 0.9289 Loss_G: 1.1979 acc: 45.0%\n",
      "[EPOCH 100] TEST ACC is : 57.5%\n",
      "[BATCH 101/478] Loss_D: 1.0119 Loss_G: 1.1658 acc: 60.0%\n",
      "[BATCH 102/478] Loss_D: 0.9866 Loss_G: 1.1571 acc: 40.0%\n",
      "[BATCH 103/478] Loss_D: 0.9645 Loss_G: 1.1493 acc: 40.0%\n",
      "[BATCH 104/478] Loss_D: 1.0375 Loss_G: 1.1600 acc: 45.0%\n",
      "[BATCH 105/478] Loss_D: 0.9558 Loss_G: 1.1570 acc: 60.0%\n",
      "[BATCH 106/478] Loss_D: 1.0182 Loss_G: 1.1476 acc: 65.0%\n",
      "[BATCH 107/478] Loss_D: 0.9922 Loss_G: 1.1489 acc: 60.0%\n",
      "[BATCH 108/478] Loss_D: 1.0656 Loss_G: 1.1481 acc: 50.0%\n",
      "[BATCH 109/478] Loss_D: 0.9116 Loss_G: 1.1550 acc: 70.0%\n",
      "[BATCH 110/478] Loss_D: 0.9857 Loss_G: 1.1391 acc: 35.0%\n",
      "[BATCH 111/478] Loss_D: 0.9877 Loss_G: 1.1396 acc: 70.0%\n",
      "[BATCH 112/478] Loss_D: 0.9814 Loss_G: 1.1342 acc: 80.0%\n",
      "[BATCH 113/478] Loss_D: 0.9641 Loss_G: 1.1400 acc: 50.0%\n",
      "[BATCH 114/478] Loss_D: 0.9514 Loss_G: 1.1238 acc: 50.0%\n",
      "[BATCH 115/478] Loss_D: 1.0469 Loss_G: 1.1338 acc: 40.0%\n",
      "[BATCH 116/478] Loss_D: 0.9505 Loss_G: 1.1305 acc: 45.0%\n",
      "[BATCH 117/478] Loss_D: 0.8874 Loss_G: 1.1285 acc: 45.0%\n",
      "[BATCH 118/478] Loss_D: 0.9881 Loss_G: 1.1293 acc: 45.0%\n",
      "[BATCH 119/478] Loss_D: 0.9903 Loss_G: 1.1184 acc: 65.0%\n",
      "[BATCH 120/478] Loss_D: 0.9731 Loss_G: 1.1062 acc: 55.0%\n",
      "[BATCH 121/478] Loss_D: 1.0392 Loss_G: 1.1239 acc: 75.0%\n",
      "[BATCH 122/478] Loss_D: 1.0415 Loss_G: 1.1326 acc: 45.0%\n",
      "[BATCH 123/478] Loss_D: 0.9616 Loss_G: 1.1213 acc: 60.0%\n",
      "[BATCH 124/478] Loss_D: 0.9055 Loss_G: 1.1234 acc: 60.0%\n",
      "[BATCH 125/478] Loss_D: 1.0556 Loss_G: 1.1170 acc: 65.0%\n",
      "[BATCH 126/478] Loss_D: 1.1867 Loss_G: 1.1571 acc: 55.0%\n",
      "[BATCH 127/478] Loss_D: 1.0138 Loss_G: 1.1375 acc: 70.0%\n",
      "[BATCH 128/478] Loss_D: 0.9969 Loss_G: 1.1193 acc: 60.0%\n",
      "[BATCH 129/478] Loss_D: 1.1183 Loss_G: 1.1186 acc: 45.0%\n",
      "[BATCH 130/478] Loss_D: 1.0134 Loss_G: 1.1280 acc: 35.0%\n",
      "[BATCH 131/478] Loss_D: 1.2294 Loss_G: 1.1986 acc: 35.0%\n",
      "[BATCH 132/478] Loss_D: 0.9352 Loss_G: 1.1901 acc: 60.0%\n",
      "[BATCH 133/478] Loss_D: 0.9859 Loss_G: 1.1614 acc: 70.0%\n",
      "[BATCH 134/478] Loss_D: 1.0952 Loss_G: 1.1353 acc: 70.0%\n",
      "[BATCH 135/478] Loss_D: 0.9761 Loss_G: 1.1251 acc: 65.0%\n",
      "[BATCH 136/478] Loss_D: 0.9381 Loss_G: 1.1161 acc: 60.0%\n",
      "[BATCH 137/478] Loss_D: 1.1164 Loss_G: 1.1415 acc: 40.0%\n",
      "[BATCH 138/478] Loss_D: 1.0137 Loss_G: 1.1335 acc: 65.0%\n",
      "[BATCH 139/478] Loss_D: 0.9031 Loss_G: 1.1099 acc: 40.0%\n",
      "[BATCH 140/478] Loss_D: 0.9834 Loss_G: 1.1036 acc: 60.0%\n",
      "[BATCH 141/478] Loss_D: 0.9634 Loss_G: 1.1047 acc: 75.0%\n",
      "[BATCH 142/478] Loss_D: 0.9313 Loss_G: 1.1070 acc: 45.0%\n",
      "[BATCH 143/478] Loss_D: 0.9835 Loss_G: 1.1045 acc: 45.0%\n",
      "[BATCH 144/478] Loss_D: 0.9742 Loss_G: 1.0989 acc: 55.0%\n",
      "[BATCH 145/478] Loss_D: 0.9892 Loss_G: 1.0973 acc: 40.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 146/478] Loss_D: 1.1644 Loss_G: 1.1179 acc: 40.0%\n",
      "[BATCH 147/478] Loss_D: 1.0387 Loss_G: 1.1323 acc: 50.0%\n",
      "[BATCH 148/478] Loss_D: 0.9704 Loss_G: 1.1203 acc: 60.0%\n",
      "[BATCH 149/478] Loss_D: 1.0130 Loss_G: 1.1126 acc: 55.0%\n",
      "[BATCH 150/478] Loss_D: 1.0028 Loss_G: 1.1157 acc: 50.0%\n",
      "[EPOCH 150] TEST ACC is : 55.6%\n",
      "[BATCH 151/478] Loss_D: 0.9375 Loss_G: 1.0987 acc: 55.0%\n",
      "[BATCH 152/478] Loss_D: 1.0086 Loss_G: 1.1027 acc: 65.0%\n",
      "[BATCH 153/478] Loss_D: 0.9701 Loss_G: 1.0999 acc: 50.0%\n",
      "[BATCH 154/478] Loss_D: 0.9438 Loss_G: 1.0927 acc: 50.0%\n",
      "[BATCH 155/478] Loss_D: 1.0773 Loss_G: 1.1114 acc: 60.0%\n",
      "[BATCH 156/478] Loss_D: 0.9628 Loss_G: 1.1163 acc: 50.0%\n",
      "[BATCH 157/478] Loss_D: 0.9394 Loss_G: 1.1006 acc: 45.0%\n",
      "[BATCH 158/478] Loss_D: 1.0947 Loss_G: 1.1058 acc: 60.0%\n",
      "[BATCH 159/478] Loss_D: 0.9822 Loss_G: 1.1026 acc: 40.0%\n",
      "[BATCH 160/478] Loss_D: 0.9754 Loss_G: 1.0986 acc: 40.0%\n",
      "[BATCH 161/478] Loss_D: 1.0029 Loss_G: 1.0990 acc: 70.0%\n",
      "[BATCH 162/478] Loss_D: 0.9227 Loss_G: 1.0993 acc: 45.0%\n",
      "[BATCH 163/478] Loss_D: 1.1208 Loss_G: 1.1148 acc: 65.0%\n",
      "[BATCH 164/478] Loss_D: 0.9626 Loss_G: 1.1064 acc: 50.0%\n",
      "[BATCH 165/478] Loss_D: 0.8823 Loss_G: 1.0878 acc: 45.0%\n",
      "[BATCH 166/478] Loss_D: 0.9291 Loss_G: 1.0991 acc: 60.0%\n",
      "[BATCH 167/478] Loss_D: 1.0343 Loss_G: 1.1068 acc: 55.0%\n",
      "[BATCH 168/478] Loss_D: 0.8921 Loss_G: 1.0784 acc: 50.0%\n",
      "[BATCH 169/478] Loss_D: 0.9824 Loss_G: 1.0830 acc: 50.0%\n",
      "[BATCH 170/478] Loss_D: 1.0096 Loss_G: 1.0874 acc: 55.0%\n",
      "[BATCH 171/478] Loss_D: 0.9574 Loss_G: 1.0848 acc: 90.0%\n",
      "[BATCH 172/478] Loss_D: 0.9899 Loss_G: 1.0936 acc: 75.0%\n",
      "[BATCH 173/478] Loss_D: 1.0194 Loss_G: 1.0889 acc: 60.0%\n",
      "[BATCH 174/478] Loss_D: 0.9735 Loss_G: 1.0968 acc: 55.0%\n",
      "[BATCH 175/478] Loss_D: 0.8418 Loss_G: 1.0773 acc: 40.0%\n",
      "[BATCH 176/478] Loss_D: 1.1618 Loss_G: 1.1249 acc: 55.0%\n",
      "[BATCH 177/478] Loss_D: 0.9867 Loss_G: 1.1260 acc: 60.0%\n",
      "[BATCH 178/478] Loss_D: 1.0230 Loss_G: 1.1348 acc: 75.0%\n",
      "[BATCH 179/478] Loss_D: 0.9572 Loss_G: 1.1161 acc: 55.0%\n",
      "[BATCH 180/478] Loss_D: 0.9655 Loss_G: 1.1041 acc: 60.0%\n",
      "[BATCH 181/478] Loss_D: 0.9392 Loss_G: 1.0882 acc: 55.0%\n",
      "[BATCH 182/478] Loss_D: 0.9882 Loss_G: 1.0904 acc: 60.0%\n",
      "[BATCH 183/478] Loss_D: 0.9108 Loss_G: 1.0879 acc: 70.0%\n",
      "[BATCH 184/478] Loss_D: 1.0063 Loss_G: 1.0990 acc: 55.0%\n",
      "[BATCH 185/478] Loss_D: 0.9067 Loss_G: 1.0931 acc: 50.0%\n",
      "[BATCH 186/478] Loss_D: 0.8934 Loss_G: 1.0815 acc: 55.0%\n",
      "[BATCH 187/478] Loss_D: 0.9705 Loss_G: 1.0767 acc: 35.0%\n",
      "[BATCH 188/478] Loss_D: 1.0355 Loss_G: 1.0772 acc: 65.0%\n",
      "[BATCH 189/478] Loss_D: 0.8333 Loss_G: 1.0841 acc: 60.0%\n",
      "[BATCH 190/478] Loss_D: 0.8505 Loss_G: 1.0649 acc: 60.0%\n",
      "[BATCH 191/478] Loss_D: 0.8865 Loss_G: 1.0612 acc: 40.0%\n",
      "[BATCH 192/478] Loss_D: 0.9052 Loss_G: 1.0624 acc: 60.0%\n",
      "[BATCH 193/478] Loss_D: 0.8650 Loss_G: 1.0600 acc: 65.0%\n",
      "[BATCH 194/478] Loss_D: 0.8439 Loss_G: 1.0445 acc: 45.0%\n",
      "[BATCH 195/478] Loss_D: 0.9392 Loss_G: 1.0475 acc: 65.0%\n",
      "[BATCH 196/478] Loss_D: 0.9395 Loss_G: 1.0465 acc: 50.0%\n",
      "[BATCH 197/478] Loss_D: 1.0002 Loss_G: 1.0592 acc: 60.0%\n",
      "[BATCH 198/478] Loss_D: 1.0302 Loss_G: 1.0815 acc: 55.0%\n",
      "[BATCH 199/478] Loss_D: 0.9935 Loss_G: 1.0856 acc: 60.0%\n",
      "[BATCH 200/478] Loss_D: 0.9747 Loss_G: 1.0695 acc: 65.0%\n",
      "[EPOCH 200] TEST ACC is : 62.7%\n",
      "[BATCH 201/478] Loss_D: 1.0025 Loss_G: 1.0636 acc: 40.0%\n",
      "[BATCH 202/478] Loss_D: 1.0806 Loss_G: 1.0795 acc: 50.0%\n",
      "[BATCH 203/478] Loss_D: 0.9818 Loss_G: 1.0847 acc: 60.0%\n",
      "[BATCH 204/478] Loss_D: 0.9088 Loss_G: 1.0683 acc: 60.0%\n",
      "[BATCH 205/478] Loss_D: 0.8654 Loss_G: 1.0575 acc: 65.0%\n",
      "[BATCH 206/478] Loss_D: 0.9556 Loss_G: 1.0625 acc: 55.0%\n",
      "[BATCH 207/478] Loss_D: 0.9590 Loss_G: 1.0618 acc: 30.0%\n",
      "[BATCH 208/478] Loss_D: 1.1443 Loss_G: 1.1099 acc: 80.0%\n",
      "[BATCH 209/478] Loss_D: 1.0954 Loss_G: 1.1483 acc: 30.0%\n",
      "[BATCH 210/478] Loss_D: 0.9802 Loss_G: 1.1341 acc: 60.0%\n",
      "[BATCH 211/478] Loss_D: 0.9832 Loss_G: 1.1139 acc: 50.0%\n",
      "[BATCH 212/478] Loss_D: 0.9270 Loss_G: 1.0894 acc: 60.0%\n",
      "[BATCH 213/478] Loss_D: 0.8870 Loss_G: 1.0721 acc: 70.0%\n",
      "[BATCH 214/478] Loss_D: 0.9526 Loss_G: 1.0721 acc: 50.0%\n",
      "[BATCH 215/478] Loss_D: 0.9761 Loss_G: 1.0650 acc: 70.0%\n",
      "[BATCH 216/478] Loss_D: 0.9574 Loss_G: 1.0664 acc: 65.0%\n",
      "[BATCH 217/478] Loss_D: 0.8365 Loss_G: 1.0577 acc: 60.0%\n",
      "[BATCH 218/478] Loss_D: 0.9755 Loss_G: 1.0599 acc: 60.0%\n",
      "[BATCH 219/478] Loss_D: 0.9108 Loss_G: 1.0595 acc: 45.0%\n",
      "[BATCH 220/478] Loss_D: 0.9315 Loss_G: 1.0546 acc: 55.0%\n",
      "[BATCH 221/478] Loss_D: 0.9696 Loss_G: 1.0639 acc: 55.0%\n",
      "[BATCH 222/478] Loss_D: 1.0393 Loss_G: 1.0802 acc: 70.0%\n",
      "[BATCH 223/478] Loss_D: 1.0071 Loss_G: 1.0758 acc: 60.0%\n",
      "[BATCH 224/478] Loss_D: 0.9537 Loss_G: 1.0719 acc: 50.0%\n",
      "[BATCH 225/478] Loss_D: 0.8658 Loss_G: 1.0542 acc: 45.0%\n",
      "[BATCH 226/478] Loss_D: 0.8806 Loss_G: 1.0470 acc: 60.0%\n",
      "[BATCH 227/478] Loss_D: 0.9123 Loss_G: 1.0498 acc: 45.0%\n",
      "[BATCH 228/478] Loss_D: 0.8509 Loss_G: 1.0410 acc: 55.0%\n",
      "[BATCH 229/478] Loss_D: 0.8821 Loss_G: 1.0422 acc: 60.0%\n",
      "[BATCH 230/478] Loss_D: 0.9652 Loss_G: 1.0639 acc: 45.0%\n",
      "[BATCH 231/478] Loss_D: 0.9469 Loss_G: 1.0646 acc: 50.0%\n",
      "[BATCH 232/478] Loss_D: 0.8775 Loss_G: 1.0494 acc: 45.0%\n",
      "[BATCH 233/478] Loss_D: 0.8617 Loss_G: 1.0374 acc: 35.0%\n",
      "[BATCH 234/478] Loss_D: 0.8644 Loss_G: 1.0343 acc: 45.0%\n",
      "[BATCH 235/478] Loss_D: 0.9559 Loss_G: 1.0394 acc: 55.0%\n",
      "[BATCH 236/478] Loss_D: 0.8435 Loss_G: 1.0349 acc: 75.0%\n",
      "[BATCH 237/478] Loss_D: 0.8347 Loss_G: 1.0293 acc: 55.0%\n",
      "[BATCH 238/478] Loss_D: 0.9470 Loss_G: 1.0453 acc: 60.0%\n",
      "[BATCH 239/478] Loss_D: 0.9068 Loss_G: 1.0488 acc: 60.0%\n",
      "[BATCH 240/478] Loss_D: 0.9298 Loss_G: 1.0508 acc: 35.0%\n",
      "[BATCH 241/478] Loss_D: 0.8805 Loss_G: 1.0383 acc: 55.0%\n",
      "[BATCH 242/478] Loss_D: 1.0354 Loss_G: 1.0626 acc: 45.0%\n",
      "[BATCH 243/478] Loss_D: 0.9469 Loss_G: 1.0507 acc: 45.0%\n",
      "[BATCH 244/478] Loss_D: 0.9608 Loss_G: 1.0497 acc: 55.0%\n",
      "[BATCH 245/478] Loss_D: 0.9624 Loss_G: 1.0562 acc: 65.0%\n",
      "[BATCH 246/478] Loss_D: 1.0196 Loss_G: 1.0694 acc: 55.0%\n",
      "[BATCH 247/478] Loss_D: 0.9164 Loss_G: 1.0574 acc: 60.0%\n",
      "[BATCH 248/478] Loss_D: 0.9366 Loss_G: 1.0472 acc: 55.0%\n",
      "[BATCH 249/478] Loss_D: 1.0165 Loss_G: 1.0514 acc: 50.0%\n",
      "[BATCH 250/478] Loss_D: 0.8821 Loss_G: 1.0494 acc: 60.0%\n",
      "[EPOCH 250] TEST ACC is : 56.2%\n",
      "[BATCH 251/478] Loss_D: 0.8920 Loss_G: 1.0459 acc: 50.0%\n",
      "[BATCH 252/478] Loss_D: 0.8756 Loss_G: 1.0434 acc: 50.0%\n",
      "[BATCH 253/478] Loss_D: 0.9173 Loss_G: 1.0475 acc: 45.0%\n",
      "[BATCH 254/478] Loss_D: 0.8690 Loss_G: 1.0448 acc: 30.0%\n",
      "[BATCH 255/478] Loss_D: 0.9213 Loss_G: 1.0555 acc: 50.0%\n",
      "[BATCH 256/478] Loss_D: 0.9690 Loss_G: 1.0633 acc: 50.0%\n",
      "[BATCH 257/478] Loss_D: 0.9728 Loss_G: 1.0653 acc: 35.0%\n",
      "[BATCH 258/478] Loss_D: 0.9819 Loss_G: 1.0589 acc: 40.0%\n",
      "[BATCH 259/478] Loss_D: 0.9969 Loss_G: 1.0706 acc: 55.0%\n",
      "[BATCH 260/478] Loss_D: 1.0693 Loss_G: 1.0951 acc: 55.0%\n",
      "[BATCH 261/478] Loss_D: 0.8771 Loss_G: 1.0853 acc: 65.0%\n",
      "[BATCH 262/478] Loss_D: 1.0951 Loss_G: 1.0922 acc: 45.0%\n",
      "[BATCH 263/478] Loss_D: 0.9364 Loss_G: 1.0847 acc: 55.0%\n",
      "[BATCH 264/478] Loss_D: 1.0126 Loss_G: 1.0819 acc: 50.0%\n",
      "[BATCH 265/478] Loss_D: 0.9391 Loss_G: 1.0733 acc: 60.0%\n",
      "[BATCH 266/478] Loss_D: 0.8935 Loss_G: 1.0567 acc: 50.0%\n",
      "[BATCH 267/478] Loss_D: 1.0831 Loss_G: 1.0770 acc: 35.0%\n",
      "[BATCH 268/478] Loss_D: 0.9250 Loss_G: 1.0792 acc: 50.0%\n",
      "[BATCH 269/478] Loss_D: 0.8373 Loss_G: 1.0572 acc: 55.0%\n",
      "[BATCH 270/478] Loss_D: 0.8743 Loss_G: 1.0527 acc: 70.0%\n",
      "[BATCH 271/478] Loss_D: 0.8859 Loss_G: 1.0627 acc: 35.0%\n",
      "[BATCH 272/478] Loss_D: 0.9198 Loss_G: 1.0496 acc: 40.0%\n",
      "[BATCH 273/478] Loss_D: 0.8169 Loss_G: 1.0297 acc: 35.0%\n",
      "[BATCH 274/478] Loss_D: 0.9057 Loss_G: 1.0321 acc: 65.0%\n",
      "[BATCH 275/478] Loss_D: 0.8481 Loss_G: 1.0251 acc: 55.0%\n",
      "[BATCH 276/478] Loss_D: 0.8683 Loss_G: 1.0245 acc: 60.0%\n",
      "[BATCH 277/478] Loss_D: 0.9105 Loss_G: 1.0382 acc: 45.0%\n",
      "[BATCH 278/478] Loss_D: 0.8976 Loss_G: 1.0336 acc: 50.0%\n",
      "[BATCH 279/478] Loss_D: 0.9424 Loss_G: 1.0432 acc: 70.0%\n",
      "[BATCH 280/478] Loss_D: 1.0806 Loss_G: 1.0775 acc: 65.0%\n",
      "[BATCH 281/478] Loss_D: 0.9727 Loss_G: 1.0892 acc: 45.0%\n",
      "[BATCH 282/478] Loss_D: 0.8309 Loss_G: 1.0759 acc: 60.0%\n",
      "[BATCH 283/478] Loss_D: 0.8532 Loss_G: 1.0512 acc: 55.0%\n",
      "[BATCH 284/478] Loss_D: 0.9357 Loss_G: 1.0464 acc: 55.0%\n",
      "[BATCH 285/478] Loss_D: 0.9833 Loss_G: 1.0593 acc: 50.0%\n",
      "[BATCH 286/478] Loss_D: 0.8948 Loss_G: 1.0582 acc: 45.0%\n",
      "[BATCH 287/478] Loss_D: 1.0212 Loss_G: 1.0626 acc: 65.0%\n",
      "[BATCH 288/478] Loss_D: 0.9697 Loss_G: 1.0645 acc: 65.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 289/478] Loss_D: 0.8947 Loss_G: 1.0500 acc: 25.0%\n",
      "[BATCH 290/478] Loss_D: 0.9398 Loss_G: 1.0424 acc: 60.0%\n",
      "[BATCH 291/478] Loss_D: 0.8334 Loss_G: 1.0408 acc: 50.0%\n",
      "[BATCH 292/478] Loss_D: 0.9273 Loss_G: 1.0373 acc: 55.0%\n",
      "[BATCH 293/478] Loss_D: 0.8495 Loss_G: 1.0341 acc: 45.0%\n",
      "[BATCH 294/478] Loss_D: 0.9596 Loss_G: 1.0383 acc: 35.0%\n",
      "[BATCH 295/478] Loss_D: 0.9674 Loss_G: 1.0382 acc: 40.0%\n",
      "[BATCH 296/478] Loss_D: 0.9763 Loss_G: 1.0442 acc: 55.0%\n",
      "[BATCH 297/478] Loss_D: 0.8682 Loss_G: 1.0502 acc: 55.0%\n",
      "[BATCH 298/478] Loss_D: 0.8437 Loss_G: 1.0425 acc: 50.0%\n",
      "[BATCH 299/478] Loss_D: 0.9604 Loss_G: 1.0461 acc: 50.0%\n",
      "[BATCH 300/478] Loss_D: 0.8653 Loss_G: 1.0466 acc: 65.0%\n",
      "[EPOCH 300] TEST ACC is : 56.5%\n",
      "[BATCH 301/478] Loss_D: 0.9112 Loss_G: 1.0529 acc: 65.0%\n",
      "[BATCH 302/478] Loss_D: 0.9130 Loss_G: 1.0601 acc: 55.0%\n",
      "[BATCH 303/478] Loss_D: 0.9334 Loss_G: 1.0623 acc: 50.0%\n",
      "[BATCH 304/478] Loss_D: 0.8941 Loss_G: 1.0597 acc: 50.0%\n",
      "[BATCH 305/478] Loss_D: 0.9818 Loss_G: 1.0682 acc: 55.0%\n",
      "[BATCH 306/478] Loss_D: 0.8990 Loss_G: 1.0697 acc: 40.0%\n",
      "[BATCH 307/478] Loss_D: 1.1154 Loss_G: 1.0854 acc: 45.0%\n",
      "[BATCH 308/478] Loss_D: 0.8889 Loss_G: 1.0995 acc: 55.0%\n",
      "[BATCH 309/478] Loss_D: 0.8659 Loss_G: 1.0935 acc: 85.0%\n",
      "[BATCH 310/478] Loss_D: 1.0596 Loss_G: 1.0974 acc: 55.0%\n",
      "[BATCH 311/478] Loss_D: 0.8713 Loss_G: 1.0925 acc: 55.0%\n",
      "[BATCH 312/478] Loss_D: 0.8120 Loss_G: 1.0929 acc: 40.0%\n",
      "[BATCH 313/478] Loss_D: 0.7901 Loss_G: 1.0878 acc: 65.0%\n",
      "[BATCH 314/478] Loss_D: 0.9728 Loss_G: 1.0855 acc: 65.0%\n",
      "[BATCH 315/478] Loss_D: 0.8927 Loss_G: 1.0770 acc: 60.0%\n",
      "[BATCH 316/478] Loss_D: 0.8743 Loss_G: 1.0638 acc: 45.0%\n",
      "[BATCH 317/478] Loss_D: 0.8671 Loss_G: 1.0520 acc: 60.0%\n",
      "[BATCH 318/478] Loss_D: 1.0276 Loss_G: 1.0677 acc: 40.0%\n",
      "[BATCH 319/478] Loss_D: 0.9096 Loss_G: 1.0399 acc: 50.0%\n",
      "[BATCH 320/478] Loss_D: 0.9762 Loss_G: 1.0227 acc: 55.0%\n",
      "[BATCH 321/478] Loss_D: 0.9069 Loss_G: 0.9935 acc: 50.0%\n",
      "[BATCH 322/478] Loss_D: 0.9041 Loss_G: 0.9870 acc: 45.0%\n",
      "[BATCH 323/478] Loss_D: 0.9931 Loss_G: 0.9951 acc: 40.0%\n",
      "[BATCH 324/478] Loss_D: 0.8413 Loss_G: 0.9949 acc: 50.0%\n",
      "[BATCH 325/478] Loss_D: 0.9018 Loss_G: 0.9722 acc: 60.0%\n",
      "[BATCH 326/478] Loss_D: 0.8949 Loss_G: 0.9709 acc: 50.0%\n",
      "[BATCH 327/478] Loss_D: 0.9083 Loss_G: 0.9788 acc: 60.0%\n",
      "[BATCH 328/478] Loss_D: 0.8246 Loss_G: 0.9901 acc: 75.0%\n",
      "[BATCH 329/478] Loss_D: 0.9478 Loss_G: 0.9863 acc: 45.0%\n",
      "[BATCH 330/478] Loss_D: 1.1211 Loss_G: 1.0523 acc: 60.0%\n",
      "[BATCH 331/478] Loss_D: 0.8600 Loss_G: 1.0356 acc: 60.0%\n",
      "[BATCH 332/478] Loss_D: 0.9704 Loss_G: 1.0331 acc: 70.0%\n",
      "[BATCH 333/478] Loss_D: 0.9371 Loss_G: 1.0253 acc: 55.0%\n",
      "[BATCH 334/478] Loss_D: 0.8504 Loss_G: 1.0066 acc: 65.0%\n",
      "[BATCH 335/478] Loss_D: 0.9531 Loss_G: 1.0000 acc: 70.0%\n",
      "[BATCH 336/478] Loss_D: 0.8889 Loss_G: 1.0051 acc: 55.0%\n",
      "[BATCH 337/478] Loss_D: 0.9263 Loss_G: 1.0230 acc: 40.0%\n",
      "[BATCH 338/478] Loss_D: 1.0063 Loss_G: 1.0149 acc: 30.0%\n",
      "[BATCH 339/478] Loss_D: 1.0513 Loss_G: 1.0535 acc: 30.0%\n",
      "[BATCH 340/478] Loss_D: 0.9521 Loss_G: 1.0589 acc: 45.0%\n",
      "[BATCH 341/478] Loss_D: 0.9291 Loss_G: 1.0577 acc: 45.0%\n",
      "[BATCH 342/478] Loss_D: 1.0144 Loss_G: 1.0664 acc: 65.0%\n",
      "[BATCH 343/478] Loss_D: 0.9142 Loss_G: 1.0471 acc: 60.0%\n",
      "[BATCH 344/478] Loss_D: 0.8439 Loss_G: 1.0304 acc: 45.0%\n",
      "[BATCH 345/478] Loss_D: 0.8332 Loss_G: 1.0175 acc: 70.0%\n",
      "[BATCH 346/478] Loss_D: 0.9540 Loss_G: 1.0205 acc: 35.0%\n",
      "[BATCH 347/478] Loss_D: 0.8564 Loss_G: 1.0116 acc: 80.0%\n",
      "[BATCH 348/478] Loss_D: 1.0653 Loss_G: 1.0293 acc: 55.0%\n",
      "[BATCH 349/478] Loss_D: 0.9988 Loss_G: 1.0492 acc: 45.0%\n",
      "[BATCH 350/478] Loss_D: 0.8939 Loss_G: 1.0414 acc: 65.0%\n",
      "[EPOCH 350] TEST ACC is : 63.1%\n",
      "[BATCH 351/478] Loss_D: 1.0493 Loss_G: 1.0548 acc: 55.0%\n",
      "[BATCH 352/478] Loss_D: 0.8681 Loss_G: 1.0413 acc: 80.0%\n",
      "[BATCH 353/478] Loss_D: 0.9796 Loss_G: 1.0486 acc: 50.0%\n",
      "[BATCH 354/478] Loss_D: 0.9480 Loss_G: 1.0388 acc: 40.0%\n",
      "[BATCH 355/478] Loss_D: 0.8805 Loss_G: 1.0297 acc: 60.0%\n",
      "[BATCH 356/478] Loss_D: 0.9034 Loss_G: 1.0205 acc: 45.0%\n",
      "[BATCH 357/478] Loss_D: 0.9617 Loss_G: 1.0312 acc: 45.0%\n",
      "[BATCH 358/478] Loss_D: 1.0365 Loss_G: 1.0531 acc: 50.0%\n",
      "[BATCH 359/478] Loss_D: 0.9044 Loss_G: 1.0543 acc: 75.0%\n",
      "[BATCH 360/478] Loss_D: 0.9368 Loss_G: 1.0535 acc: 25.0%\n",
      "[BATCH 361/478] Loss_D: 0.9382 Loss_G: 1.0479 acc: 45.0%\n",
      "[BATCH 362/478] Loss_D: 0.8566 Loss_G: 1.0414 acc: 45.0%\n",
      "[BATCH 363/478] Loss_D: 0.8636 Loss_G: 1.0250 acc: 45.0%\n",
      "[BATCH 364/478] Loss_D: 0.9751 Loss_G: 1.0332 acc: 65.0%\n",
      "[BATCH 365/478] Loss_D: 1.0200 Loss_G: 1.0580 acc: 60.0%\n",
      "[BATCH 366/478] Loss_D: 0.9102 Loss_G: 1.0663 acc: 50.0%\n",
      "[BATCH 367/478] Loss_D: 0.8495 Loss_G: 1.0708 acc: 70.0%\n",
      "[BATCH 368/478] Loss_D: 0.8440 Loss_G: 1.0459 acc: 60.0%\n",
      "[BATCH 369/478] Loss_D: 0.8906 Loss_G: 1.0433 acc: 70.0%\n",
      "[BATCH 370/478] Loss_D: 0.8951 Loss_G: 1.0407 acc: 35.0%\n",
      "[BATCH 371/478] Loss_D: 0.9035 Loss_G: 1.0364 acc: 45.0%\n",
      "[BATCH 372/478] Loss_D: 0.8898 Loss_G: 1.0242 acc: 40.0%\n",
      "[BATCH 373/478] Loss_D: 0.9655 Loss_G: 1.0375 acc: 50.0%\n",
      "[BATCH 374/478] Loss_D: 0.9257 Loss_G: 1.0393 acc: 80.0%\n",
      "[BATCH 375/478] Loss_D: 0.9429 Loss_G: 1.0472 acc: 50.0%\n",
      "[BATCH 376/478] Loss_D: 1.0314 Loss_G: 1.0695 acc: 55.0%\n",
      "[BATCH 377/478] Loss_D: 0.9548 Loss_G: 1.0651 acc: 70.0%\n",
      "[BATCH 378/478] Loss_D: 0.8864 Loss_G: 1.0480 acc: 55.0%\n",
      "[BATCH 379/478] Loss_D: 0.9998 Loss_G: 1.0529 acc: 45.0%\n",
      "[BATCH 380/478] Loss_D: 1.0682 Loss_G: 1.0640 acc: 55.0%\n",
      "[BATCH 381/478] Loss_D: 0.8901 Loss_G: 1.0623 acc: 55.0%\n",
      "[BATCH 382/478] Loss_D: 0.9189 Loss_G: 1.0541 acc: 40.0%\n",
      "[BATCH 383/478] Loss_D: 0.9640 Loss_G: 1.0731 acc: 55.0%\n",
      "[BATCH 384/478] Loss_D: 0.9800 Loss_G: 1.0715 acc: 70.0%\n",
      "[BATCH 385/478] Loss_D: 1.0090 Loss_G: 1.0716 acc: 65.0%\n",
      "[BATCH 386/478] Loss_D: 0.8526 Loss_G: 1.0531 acc: 55.0%\n",
      "[BATCH 387/478] Loss_D: 0.8885 Loss_G: 1.0347 acc: 60.0%\n",
      "[BATCH 388/478] Loss_D: 0.9326 Loss_G: 1.0295 acc: 55.0%\n",
      "[BATCH 389/478] Loss_D: 0.8814 Loss_G: 1.0280 acc: 60.0%\n"
     ]
    }
   ],
   "source": [
    "input_data = os.listdir(INPUT_PATH)\n",
    "npy_list = []\n",
    "npy_level_list = []\n",
    "\n",
    "for level_num in range(5): #遍历patient文件夹——study指代每一个study文件夹\n",
    "    if level_num == 2:\n",
    "        continue\n",
    "    npy_file_path = os.path.join(INPUT_PATH, \"malignancy_\" + str(level_num + 1))\n",
    "    npy_files = os.listdir(npy_file_path)\n",
    "    for i in npy_files:\n",
    "        npy_path = os.path.join(npy_file_path, i)\n",
    "        single_npy = np.load(npy_path)\n",
    "        single_fliplr_npy = np.fliplr(single_npy)\n",
    "        single_npy = (single_npy - 127.5) / 127.5\n",
    "        npy_list.append(single_npy)\n",
    "        if level_num < 2:\n",
    "            npy_level_list.append(0)\n",
    "        else:\n",
    "            npy_level_list.append(1)\n",
    "        if level_num > 2:\n",
    "            single_fliplr_npy = (single_fliplr_npy - 127.5) / 127.5\n",
    "            npy_list.append(single_fliplr_npy)\n",
    "            npy_level_list.append(1)\n",
    "\n",
    "random.seed(randnum)\n",
    "random.shuffle(npy_list)\n",
    "random.seed(randnum)\n",
    "random.shuffle(npy_level_list)\n",
    "\n",
    "npy_chunks = chunks(npy_list, 10)\n",
    "npy_level_chunks = chunks(npy_level_list, 10)\n",
    "\n",
    "print(\"NOW the training STARTS:\")\n",
    "training_set, test_set = ten_folder(npy_chunks, 9)\n",
    "training_level, test_level = ten_folder(npy_level_chunks, 9)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(MyDataset(training_set, training_level), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "train(train_loader, test_set, test_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CapsuleGAN",
   "language": "python",
   "name": "capsulegan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
