{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "INPUT_PATH = \"./output/nodule_npy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "randnum = random.randint(0,100)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 25\n",
    "nz = 100\n",
    "n_epochs = 100\n",
    "ngf = 64\n",
    "ngpu = 1\n",
    "LEVELS_NUM = 2\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def chunks(arr, m):\n",
    "    nchunk = int(math.ceil(len(arr) / float(m)))\n",
    "    return [arr[i:i + nchunk] for i in range(0, len(arr), nchunk)]\n",
    "\n",
    "def five_folder(arr, number):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    for j in range(len(arr)):\n",
    "        if number == j:\n",
    "            test_set.extend(arr[j])\n",
    "        else:\n",
    "            training_set.extend(arr[j])\n",
    "    return training_set, test_set\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images,labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        img = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.label_emb = nn.Embedding(LEVELS_NUM, 100)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "          nn.Linear(nz, 1 * 5 * 5 * ngf * 4, bias=False)\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "          # input is Z, going into a deconvolution\n",
    "          # state size. BATCH_SIZE x (ngf*4) x 1 x 5 x 5\n",
    "          nn.ConvTranspose3d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.BatchNorm3d(ngf * 2),\n",
    "          nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf*2) x 2 x 10 x 10\n",
    "          nn.ConvTranspose3d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.BatchNorm3d(ngf),\n",
    "          nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf) x 4 x 20 x 20\n",
    "          nn.ConvTranspose3d(ngf, 1, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.Tanh()\n",
    "          # state size. BATCH_SIZE x 1 x 8 x 40 x 40\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        \n",
    "        x = self.project(gen_input)\n",
    "        # Conv3d的规定输入数据格式为(batch, channel, Depth, Height, Width)\n",
    "        x = x.view(-1, ngf * 4, 1, 5, 5)\n",
    "        \n",
    "        if noise.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.deconv, x, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.deconv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "def softmax(input, dim=1):\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=256, kernel_size=(2,9,9)):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels,\n",
    "                                out_channels=out_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                stride=1\n",
    "                              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=(2,9,9), num_routes=32 * 12 * 12 * 6):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "          nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=(1,2,2), padding=0) \n",
    "                      for _ in range(num_capsules)])\n",
    "  \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "        u = torch.cat(u, dim=-1)\n",
    "        return self.squash(u)\n",
    "  \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=1, num_routes=32 * 12 * 12 * 6, in_channels=8, out_channels=16, num_iterations=3):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "        self.num_iterations = num_iterations\n",
    "        self.route_weights = nn.Parameter(torch.randn(num_capsules, num_routes, in_channels, out_channels)).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 矩阵相乘\n",
    "        # x.size(): [1, batch_size, in_capsules, 1, dim_in_capsule]\n",
    "        # weight.size(): [num_capsules, 1, num_route, in_channels, out_channels]\n",
    "        priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :]\n",
    "        logits = Variable(torch.zeros(*priors.size())).to(device)\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            probs = softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "\n",
    "            if i != self.num_routes - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        \n",
    "        outputs = outputs.squeeze()\n",
    "#         print(\"OUTPUT:\", outputs.shape)\n",
    "        return outputs\n",
    "  \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.D_digit_capsules = DigitCaps()\n",
    "        self.C_digit_capsules = DigitCaps(num_capsules=LEVELS_NUM)\n",
    "\n",
    "    def forward(self, data):\n",
    "        if data.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.conv_layer, data, range(self.ngpu))\n",
    "            output = nn.parallel.data_parallel(self.primary_capsules, output, range(self.ngpu))\n",
    "            output = nn.parallel.data_parallel(self.digit_capsules, output, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.primary_capsules(self.conv_layer(data))\n",
    "            D_output = self.D_digit_capsules(output)\n",
    "            C_output = self.C_digit_capsules(output).transpose(0,1)\n",
    "            \n",
    "\n",
    "        D_classes = (D_output ** 2).sum(dim=-1) ** 0.5\n",
    "        C_classes = (C_output ** 2).sum(dim=-1) ** 0.5\n",
    "#         print(D_classes, C_classes)\n",
    "        return D_classes, C_classes\n",
    "\n",
    "\n",
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "\n",
    "    def forward(self, classes, labels):\n",
    "        left = F.relu(0.9 - classes, inplace=True) ** 2\n",
    "        right = F.relu(classes - 0.1, inplace=True) ** 2\n",
    "\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "\n",
    "        return margin_loss / classes.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "discriminator = CapsNet(ngpu).to(device)\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "generator = Generator(ngpu).to(device)\n",
    "generator.apply(weights_init)\n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "def train(train_loader, test_set, test_level):\n",
    "    test_loader = torch.utils.data.DataLoader(MyDataset(test_set, test_level), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    epoch_num = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"-----THE [{}/{}] epoch start-----\".format(epoch + 1, n_epochs))\n",
    "        for j, (data,labels) in enumerate(train_loader, 0):\n",
    "#                     print(j, data.shape) #torch.Size([32, 1, 8, 40, 40])\n",
    "\n",
    "#                     plt.figure()\n",
    "#                     for lenc in range(data.shape[0]):\n",
    "#                         for len_img in range(8):\n",
    "#                             plt.subplot(2, 4, len_img + 1)\n",
    "#                             pixel_array = data[lenc][0][len_img]\n",
    "#                             print(pixel_array)\n",
    "#                             plt.imshow(pixel_array, cmap=\"gray\")\n",
    "#                         plt.show()\n",
    "\n",
    "\n",
    "#                     pixel_array = data[0][0][0][20]\n",
    "#                     print(pixel_array)\n",
    "\n",
    "#                     data = data / 255.0\n",
    "#                     pixel_array = data[0][0][0][20]\n",
    "#                     print(pixel_array)\n",
    "\n",
    "            \n",
    "            real_imgs = data.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(FloatTensor(BATCH_SIZE, 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(FloatTensor(BATCH_SIZE, 1).fill_(0.0), requires_grad=False)\n",
    "            \n",
    "            \n",
    "            z = Variable(FloatTensor(np.random.normal(0, 1, (BATCH_SIZE, 100))))\n",
    "            gen_labels = Variable(LongTensor(np.random.randint(0, LEVELS_NUM, BATCH_SIZE)))\n",
    "    \n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            discriminator.zero_grad()\n",
    "            \n",
    "            # Loss for real images\n",
    "            real_pred, real_aux = discriminator(real_imgs)\n",
    "            d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "            # Loss for fake images\n",
    "            fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "            d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "            # Calculate discriminator accuracy\n",
    "            pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "            gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "            \n",
    "            pred = np.argmax(pred, axis=1)\n",
    "            \n",
    "            d_acc = np.mean(pred == gt)\n",
    "            \n",
    "#             print(pred)\n",
    "#             print(gt)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "            \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            \n",
    "            generator.zero_grad()\n",
    "            \n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            validity, pred_label = discriminator(gen_imgs)\n",
    "            \n",
    "            g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "\n",
    "            print('[BATCH %d/%d] Loss_D: %.4f Loss_G: %.4f acc: %d%%'\n",
    "                  % (j + 1, len(train_loader), d_loss.item(), g_loss.item(), 100 * d_acc))\n",
    "            \n",
    "            epoch_num += 1\n",
    "            d_acc_total = 0\n",
    "            if epoch_num % 50 == 0:\n",
    "                for j, (data,labels) in enumerate(test_loader, 0):\n",
    "                    real_imgs = data.to(device, dtype=torch.float)\n",
    "                    labels = labels.to(device)\n",
    "                    pred = real_aux.data.cpu().numpy()\n",
    "                    gt = labels.data.cpu().numpy()\n",
    "                    pred = np.argmax(pred, axis=1)\n",
    "                    d_acc = np.mean(pred == gt)\n",
    "                    d_acc_total += d_acc\n",
    "                print(\"[EPOCH %d] TEST ACC is : %.4f%%\" % (epoch_num, d_acc_total * 100 / len(test_loader)))\n",
    "\n",
    "        print(\"-----THE [{}/{}] epoch end-----\".format(epoch + 1, n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW the training STARTS:\n",
      "-----THE [1/100] epoch start-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harreyjia/anaconda3/envs/CapsuleGAN/lib/python3.8/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 1 1 3 3 1 0 3 3 0 1 4 1 3 2 1 1 3 1]\n",
      "[2 2 2 0 3 1 1 4 2 2 4 1 2 0 4 3 3 3 0 4]\n",
      "[BATCH 1/980] Loss_D: 3.1254 Loss_G: 6.9276 acc: 20%\n",
      "[1 0 0 4 4 0 4 0 0 0 1 1 1 4 1 1 1 1 1 1]\n",
      "[4 2 2 1 2 2 4 1 0 1 4 3 4 3 2 4 0 1 2 1]\n",
      "[BATCH 2/980] Loss_D: 1.9911 Loss_G: 4.1289 acc: 20%\n",
      "[4 4 4 2 4 4 4 4 4 4 4 1 4 1 1 4 1 1 1 1]\n",
      "[1 2 1 4 2 2 1 1 0 2 2 4 4 1 1 4 4 2 3 1]\n",
      "[BATCH 3/980] Loss_D: 1.9247 Loss_G: 3.5481 acc: 25%\n",
      "[4 4 4 1 4 4 4 4 4 4 1 1 4 4 1 1 1 4 1 1]\n",
      "[2 2 2 2 3 1 0 1 3 3 1 2 2 3 1 1 3 1 1 3]\n",
      "[BATCH 4/980] Loss_D: 1.8817 Loss_G: 3.3577 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 4 4 4 1 4]\n",
      "[3 0 2 0 3 2 4 1 2 2 0 4 2 2 0 1 0 2 2 0]\n",
      "[BATCH 5/980] Loss_D: 1.8455 Loss_G: 3.2162 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[1 1 2 2 2 2 1 1 2 0 0 4 1 4 2 4 3 2 3 3]\n",
      "[BATCH 6/980] Loss_D: 1.8607 Loss_G: 3.1473 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[1 1 2 3 1 1 0 0 2 3 4 3 1 4 3 1 4 0 2 3]\n",
      "[BATCH 7/980] Loss_D: 1.8067 Loss_G: 3.1213 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 4 4 4]\n",
      "[0 1 1 1 2 2 1 2 2 0 1 0 1 4 2 0 1 4 3 4]\n",
      "[BATCH 8/980] Loss_D: 1.8199 Loss_G: 3.1159 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 4 1 1 1 4 4]\n",
      "[0 1 1 1 2 2 1 2 2 3 3 3 4 3 2 3 3 3 4 3]\n",
      "[BATCH 9/980] Loss_D: 1.8077 Loss_G: 3.1061 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 4 1 1 1 1]\n",
      "[2 0 4 3 1 2 1 2 2 2 3 4 0 1 1 0 4 0 1 1]\n",
      "[BATCH 10/980] Loss_D: 1.8039 Loss_G: 3.1043 acc: 25%\n",
      "[4 4 4 1 4 4 4 4 4 4 1 1 1 1 1 4 1 1 1 1]\n",
      "[2 1 3 4 2 2 1 3 3 2 3 2 3 4 2 1 3 3 4 1]\n",
      "[BATCH 11/980] Loss_D: 1.7908 Loss_G: 3.1106 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[3 1 3 2 3 0 2 2 0 2 4 0 0 2 2 4 2 4 4 0]\n",
      "[BATCH 12/980] Loss_D: 1.7456 Loss_G: 3.0882 acc: 0%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 0 2 0 0 2 2 3 4 1 4 3 0 3 3 4 0 3 4 2]\n",
      "[BATCH 13/980] Loss_D: 1.7450 Loss_G: 3.0723 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 3 4 3 1 0 0 0 1 2 2 4 4 4 1 3 2 1 4 1]\n",
      "[BATCH 14/980] Loss_D: 1.7395 Loss_G: 3.0753 acc: 20%\n",
      "[4 1 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 4 1 0 1 4 2 2 3 2 3 3 1 3 2 1 4 2 4 3]\n",
      "[BATCH 15/980] Loss_D: 1.7235 Loss_G: 3.0877 acc: 15%\n",
      "[1 4 4 4 1 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[4 1 0 0 4 2 0 4 4 3 2 0 4 2 2 3 2 0 0 3]\n",
      "[BATCH 16/980] Loss_D: 1.7184 Loss_G: 3.0959 acc: 10%\n",
      "[4 0 1 4 4 4 1 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 3 2 4 4 2 1 3 4 0 0 1 2 1 1 3 1 2]\n",
      "[BATCH 17/980] Loss_D: 1.7065 Loss_G: 3.0758 acc: 25%\n",
      "[4 4 4 4 4 1 4 4 4 4 4 1 1 1 1 1 1 1 1 1]\n",
      "[1 2 0 2 3 3 2 1 2 1 0 1 1 0 2 3 0 2 1 3]\n",
      "[BATCH 18/980] Loss_D: 1.7215 Loss_G: 3.0640 acc: 15%\n",
      "[4 4 4 4 4 4 4 1 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[3 1 2 2 1 4 2 3 3 2 4 3 0 2 2 1 2 3 3 0]\n",
      "[BATCH 19/980] Loss_D: 1.6601 Loss_G: 3.0453 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 2 0 0 0 0 0 0 4 0 2 0 4 3 2 2 1 4 1 4]\n",
      "[BATCH 20/980] Loss_D: 1.6529 Loss_G: 3.0577 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 1 1 2 2 1 0 3 2 0 4 3 1 4 4 0 3 1 4]\n",
      "[BATCH 21/980] Loss_D: 1.6929 Loss_G: 3.0754 acc: 10%\n",
      "[4 4 1 4 4 1 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 2 1 1 2 4 1 4 2 1 0 1 0 1 4 4 2 4 3 0]\n",
      "[BATCH 22/980] Loss_D: 1.7305 Loss_G: 3.0935 acc: 20%\n",
      "[4 0 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[3 4 3 2 2 2 2 4 1 4 2 3 4 3 3 0 4 2 2 4]\n",
      "[BATCH 23/980] Loss_D: 1.6502 Loss_G: 3.0998 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 3 4 4 1 2 2 0 3 2 4 4 3 2 0 3 3 3 4 4]\n",
      "[BATCH 24/980] Loss_D: 1.7015 Loss_G: 3.0975 acc: 10%\n",
      "[4 4 4 4 4 1 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 3 1 3 4 4 3 2 2 1 4 1 2 2 4 2 4 0 2 3]\n",
      "[BATCH 25/980] Loss_D: 1.6203 Loss_G: 3.1165 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 1 4]\n",
      "[2 1 2 0 2 1 2 0 2 1 3 1 3 2 1 0 0 4 1 4]\n",
      "[BATCH 26/980] Loss_D: 1.6411 Loss_G: 3.1461 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 1 1 1 1]\n",
      "[1 4 1 0 2 2 2 2 0 4 4 0 3 3 4 0 2 3 4 1]\n",
      "[BATCH 27/980] Loss_D: 1.6214 Loss_G: 3.1735 acc: 25%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 4 4 1 1 4]\n",
      "[3 4 3 1 0 1 1 2 4 2 4 2 4 1 0 2 0 2 0 2]\n",
      "[BATCH 28/980] Loss_D: 1.5754 Loss_G: 3.2143 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[2 2 1 2 0 2 3 0 0 1 0 3 4 0 2 3 2 3 0 3]\n",
      "[BATCH 29/980] Loss_D: 1.5182 Loss_G: 3.2416 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 4 1 4 4]\n",
      "[0 2 2 4 2 2 0 2 2 3 1 4 0 4 2 2 3 2 1 1]\n",
      "[BATCH 30/980] Loss_D: 1.6655 Loss_G: 3.2561 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 4 4 1 1 1 4 1 1 1]\n",
      "[2 1 2 3 2 0 2 0 1 2 4 4 3 3 0 2 4 1 2 0]\n",
      "[BATCH 31/980] Loss_D: 1.6472 Loss_G: 3.2814 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[0 3 1 2 2 3 0 2 2 2 4 4 1 0 4 1 0 3 4 1]\n",
      "[BATCH 32/980] Loss_D: 1.5767 Loss_G: 3.2391 acc: 20%\n",
      "[4 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[2 4 0 1 0 2 1 3 1 1 1 3 0 1 4 1 4 2 4 1]\n",
      "[BATCH 33/980] Loss_D: 1.6597 Loss_G: 3.2460 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[3 1 2 3 2 2 2 2 1 1 3 1 2 3 2 2 3 4 1 3]\n",
      "[BATCH 34/980] Loss_D: 1.5652 Loss_G: 3.2478 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 4 1 4 4]\n",
      "[0 2 2 3 1 0 2 1 2 1 2 3 1 2 3 2 2 4 3 2]\n",
      "[BATCH 35/980] Loss_D: 1.5328 Loss_G: 3.2494 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 4 4 1 1 1 1 1]\n",
      "[2 1 1 2 0 3 0 3 3 2 0 2 2 3 4 4 1 3 0 3]\n",
      "[BATCH 36/980] Loss_D: 1.4825 Loss_G: 3.2609 acc: 10%\n",
      "[4 4 4 4 4 3 4 4 4 3 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 4 2 2 2 3 3 4 3 2 4 4 2 1 1 3 3 0]\n",
      "[BATCH 37/980] Loss_D: 1.4651 Loss_G: 3.2964 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 4 4 4 0 4 3]\n",
      "[2 2 0 1 1 4 2 3 1 2 4 4 4 1 2 1 3 2 0 3]\n",
      "[BATCH 38/980] Loss_D: 1.5346 Loss_G: 3.2635 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 0 1 4 0 1 1 4 0 0]\n",
      "[3 4 1 2 4 2 2 0 2 2 1 3 0 1 4 1 2 4 0 1]\n",
      "[BATCH 39/980] Loss_D: 1.5567 Loss_G: 3.1898 acc: 30%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 0 1 1 1 1 1 4 1]\n",
      "[3 2 2 4 1 2 2 2 0 0 0 4 0 4 2 3 2 4 3 0]\n",
      "[BATCH 40/980] Loss_D: 1.5505 Loss_G: 3.1269 acc: 10%\n",
      "[4 4 1 4 4 4 4 4 4 4 1 1 1 1 1 1 0 1 0 1]\n",
      "[2 1 3 2 3 2 0 4 0 2 3 4 3 2 1 1 0 2 2 4]\n",
      "[BATCH 41/980] Loss_D: 1.5369 Loss_G: 3.1432 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 4 1 1 4 1 4 1 4]\n",
      "[1 2 2 1 2 3 2 2 0 1 3 4 1 1 2 2 4 2 4 0]\n",
      "[BATCH 42/980] Loss_D: 1.4968 Loss_G: 2.9965 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 1 1 4 4 1 1 1 4 0]\n",
      "[0 0 2 1 0 0 0 1 2 1 2 1 1 0 0 0 4 2 4 1]\n",
      "[BATCH 43/980] Loss_D: 1.5118 Loss_G: 3.0101 acc: 15%\n",
      "[4 4 4 3 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[3 2 1 2 3 0 1 3 4 2 0 3 4 2 4 4 3 0 3 4]\n",
      "[BATCH 44/980] Loss_D: 1.4552 Loss_G: 3.0025 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 3 1 1 1 4]\n",
      "[0 3 0 2 2 3 0 2 0 2 1 2 4 4 4 1 4 0 4 4]\n",
      "[BATCH 45/980] Loss_D: 1.4974 Loss_G: 2.9995 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[3 2 2 0 0 2 3 2 1 1 2 3 3 3 4 2 2 0 0 4]\n",
      "[BATCH 46/980] Loss_D: 1.4567 Loss_G: 2.9424 acc: 0%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 2 1 1 2 1 0 3 2 1 1 2 2 1 3 0 3 4 4]\n",
      "[BATCH 47/980] Loss_D: 1.4941 Loss_G: 2.9949 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 1 0 0 2 2 3 0 2 1 3 3 4 1 3 1 2 1 2 0]\n",
      "[BATCH 48/980] Loss_D: 1.4638 Loss_G: 2.8311 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[3 2 3 0 2 2 1 3 0 2 4 4 4 1 1 1 1 0 3 1]\n",
      "[BATCH 49/980] Loss_D: 1.3740 Loss_G: 3.0251 acc: 25%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1]\n",
      "[4 1 2 2 2 1 2 2 4 1 1 1 1 3 3 2 4 4 1 4]\n",
      "[BATCH 50/980] Loss_D: 1.4917 Loss_G: 2.8064 acc: 30%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 0 1]\n",
      "[0 1 3 2 2 1 0 1 2 2 1 3 3 3 3 0 2 1 1 2]\n",
      "[BATCH 51/980] Loss_D: 1.4762 Loss_G: 2.7374 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 1 1 0 1 4 1 1 1 1]\n",
      "[0 1 1 2 2 2 3 1 3 3 2 4 0 1 1 2 4 0 0 1]\n",
      "[BATCH 52/980] Loss_D: 1.4222 Loss_G: 2.9968 acc: 10%\n",
      "[4 4 4 4 4 0 4 4 4 4 1 1 1 0 1 1 4 1 1 4]\n",
      "[3 4 1 3 3 3 3 2 0 2 4 2 4 0 0 0 4 3 3 2]\n",
      "[BATCH 53/980] Loss_D: 1.5235 Loss_G: 2.6751 acc: 15%\n",
      "[4 4 4 4 1 4 4 4 4 4 4 1 1 1 3 4 3 4 4 3]\n",
      "[0 3 2 0 4 2 2 3 0 2 2 4 1 3 3 2 3 2 4 4]\n",
      "[BATCH 54/980] Loss_D: 1.4988 Loss_G: 2.5057 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 3 1 1 1 4 0 4 1 1]\n",
      "[1 2 1 0 2 2 1 2 1 0 4 0 0 4 1 2 1 2 1 4]\n",
      "[BATCH 55/980] Loss_D: 1.4477 Loss_G: 2.5293 acc: 10%\n",
      "[4 3 4 4 4 4 4 4 4 4 1 4 4 4 4 1 1 4 1 4]\n",
      "[0 1 2 1 3 2 3 3 1 4 0 0 3 3 1 3 4 2 0 1]\n",
      "[BATCH 56/980] Loss_D: 1.5554 Loss_G: 2.6019 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4]\n",
      "[2 2 2 2 2 3 4 1 0 2 2 4 4 0 2 1 3 0 4 2]\n",
      "[BATCH 57/980] Loss_D: 1.4816 Loss_G: 2.3772 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4]\n",
      "[4 0 2 3 4 4 2 1 0 4 0 2 1 1 0 2 0 2 4 2]\n",
      "[BATCH 58/980] Loss_D: 1.5078 Loss_G: 2.2899 acc: 20%\n",
      "[4 4 4 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[3 0 1 1 1 2 3 2 4 2 0 1 0 3 4 0 1 3 1 1]\n",
      "[BATCH 59/980] Loss_D: 1.4878 Loss_G: 2.0969 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[4 3 2 0 2 2 1 0 3 2 0 0 1 2 1 4 2 4 2 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH 60/980] Loss_D: 1.4490 Loss_G: 2.1568 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 4 4 4 4 3 3 4 1 4]\n",
      "[3 3 3 2 3 2 2 1 1 1 0 0 3 0 1 1 3 2 1 0]\n",
      "[BATCH 61/980] Loss_D: 1.3575 Loss_G: 2.2123 acc: 10%\n",
      "[4 4 4 4 4 4 3 4 4 4 4 4 4 4 1 3 4 4 3 4]\n",
      "[4 2 2 4 0 2 3 1 4 2 4 3 4 0 1 1 0 1 1 4]\n",
      "[BATCH 62/980] Loss_D: 1.4545 Loss_G: 2.1239 acc: 40%\n",
      "[4 4 4 4 4 4 1 4 4 4 3 4 3 4 4 1 4 3 3 4]\n",
      "[1 2 0 2 2 2 4 2 1 2 4 2 3 4 1 3 1 3 4 2]\n",
      "[BATCH 63/980] Loss_D: 1.4776 Loss_G: 2.2406 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 1 0 0 4 4 4 3 4 3 4]\n",
      "[2 0 0 2 4 1 1 2 3 1 0 3 3 4 0 2 4 4 3 2]\n",
      "[BATCH 64/980] Loss_D: 1.3452 Loss_G: 2.2061 acc: 20%\n",
      "[4 4 3 4 4 4 4 4 4 4 1 4 4 4 4 4 0 1 4 4]\n",
      "[2 1 2 1 0 0 3 1 2 1 2 3 2 2 0 1 0 1 1 3]\n",
      "[BATCH 65/980] Loss_D: 1.4414 Loss_G: 2.1326 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 3 3 0 4 4 4 4 4 4]\n",
      "[1 2 1 0 3 4 2 2 3 0 1 3 4 4 1 2 2 0 1 3]\n",
      "[BATCH 66/980] Loss_D: 1.4397 Loss_G: 1.9826 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 1 4 1 4 0 4 3 4 4]\n",
      "[2 2 3 2 1 0 2 1 3 1 4 3 1 3 3 0 3 3 4 2]\n",
      "[BATCH 67/980] Loss_D: 1.4274 Loss_G: 2.1823 acc: 20%\n",
      "[4 4 1 4 4 4 4 4 4 4 4 1 4 3 1 3 4 4 4 4]\n",
      "[2 1 0 1 2 2 1 2 2 3 0 1 3 0 2 0 2 2 2 3]\n",
      "[BATCH 68/980] Loss_D: 1.4727 Loss_G: 2.0103 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 0 4 0 0 4 0 4 0 4 1]\n",
      "[0 0 1 0 1 2 0 4 2 0 2 3 3 1 4 4 0 3 2 2]\n",
      "[BATCH 69/980] Loss_D: 1.4200 Loss_G: 2.0098 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 3 4 3 4 4 4 4 0 4]\n",
      "[2 1 0 0 0 2 1 2 1 4 1 0 2 4 3 3 0 1 3 2]\n",
      "[BATCH 70/980] Loss_D: 1.4931 Loss_G: 1.9433 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 2 4 4 4 1 4 4 3 4 0 4]\n",
      "[2 4 2 0 4 0 2 2 0 4 4 4 2 4 0 2 3 2 0 2]\n",
      "[BATCH 71/980] Loss_D: 1.5192 Loss_G: 2.0575 acc: 30%\n",
      "[4 4 4 1 4 4 4 4 4 4 4 0 4 4 1 4 0 0 4 4]\n",
      "[2 4 2 4 1 1 2 1 2 0 3 2 3 3 3 1 4 0 2 2]\n",
      "[BATCH 72/980] Loss_D: 1.4426 Loss_G: 1.9110 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 0 4 4 4]\n",
      "[3 4 0 2 2 0 1 1 2 2 2 1 4 2 3 0 2 2 2 1]\n",
      "[BATCH 73/980] Loss_D: 1.4789 Loss_G: 1.8914 acc: 10%\n",
      "[4 4 4 4 4 4 3 4 4 4 0 3 4 4 3 4 4 4 4 1]\n",
      "[0 2 2 1 1 1 3 1 1 4 0 1 0 3 3 3 4 0 0 4]\n",
      "[BATCH 74/980] Loss_D: 1.4309 Loss_G: 1.8685 acc: 25%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 1 3 4 4 3 4 4 0]\n",
      "[0 3 2 0 1 2 1 2 1 2 0 4 1 1 2 0 4 0 3 2]\n",
      "[BATCH 75/980] Loss_D: 1.4251 Loss_G: 1.8449 acc: 10%\n",
      "[4 1 4 4 4 3 4 4 4 4 0 3 4 4 4 4 0 4 4 4]\n",
      "[0 4 2 1 2 4 2 0 2 2 4 4 3 0 3 4 0 1 4 2]\n",
      "[BATCH 76/980] Loss_D: 1.5316 Loss_G: 1.8338 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 3 1 4 4 0 4 0 1 4 3]\n",
      "[2 0 0 2 2 4 0 2 2 2 2 4 2 2 1 0 0 2 2 0]\n",
      "[BATCH 77/980] Loss_D: 1.3909 Loss_G: 1.8885 acc: 10%\n",
      "[4 4 4 4 4 4 4 2 4 4 4 0 4 4 3 4 4 3 4 3]\n",
      "[4 0 1 2 2 0 2 2 2 0 0 1 4 0 1 4 2 1 3 0]\n",
      "[BATCH 78/980] Loss_D: 1.4329 Loss_G: 1.8107 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 3 3 4 4 4 4 3 4 3 4]\n",
      "[1 2 1 1 3 0 3 2 4 2 0 0 3 1 3 0 3 2 3 3]\n",
      "[BATCH 79/980] Loss_D: 1.4341 Loss_G: 1.7966 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 0 3 1 3 4 4 4 4 4 3]\n",
      "[1 2 2 4 1 2 1 3 0 0 3 0 1 0 2 0 3 2 1 4]\n",
      "[BATCH 80/980] Loss_D: 1.3587 Loss_G: 1.7896 acc: 10%\n",
      "[2 4 4 4 4 4 4 4 4 0 0 0 3 3 4 3 3 3 2 4]\n",
      "[4 2 1 2 2 1 0 2 2 1 0 0 1 2 1 1 3 4 4 2]\n",
      "[BATCH 81/980] Loss_D: 1.4248 Loss_G: 1.7836 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 3 3 0 4 4 4 4 3 0 4]\n",
      "[2 1 1 1 0 2 2 0 1 2 2 2 1 0 2 3 3 0 3 1]\n",
      "[BATCH 82/980] Loss_D: 1.4477 Loss_G: 1.8157 acc: 0%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 0 3 4 4 4 4 0 4]\n",
      "[3 1 2 3 2 1 2 3 1 4 0 2 2 1 4 0 2 2 1 2]\n",
      "[BATCH 83/980] Loss_D: 1.3899 Loss_G: 1.7797 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 0 3 4]\n",
      "[3 2 1 1 0 2 0 3 2 2 0 1 3 2 0 0 4 3 0 0]\n",
      "[BATCH 84/980] Loss_D: 1.4592 Loss_G: 1.7765 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 3 4 0 4 0 4 3 4 3 4]\n",
      "[0 4 0 2 3 4 1 1 2 4 1 0 1 1 3 2 2 4 1 3]\n",
      "[BATCH 85/980] Loss_D: 1.4336 Loss_G: 1.7117 acc: 20%\n",
      "[4 4 1 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 3 4]\n",
      "[4 2 4 2 2 0 1 2 2 3 2 3 4 3 4 2 1 2 4 3]\n",
      "[BATCH 86/980] Loss_D: 1.4339 Loss_G: 1.7064 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4]\n",
      "[1 3 2 2 1 2 0 1 4 3 4 0 2 3 4 4 3 2 0 1]\n",
      "[BATCH 87/980] Loss_D: 1.3249 Loss_G: 1.7360 acc: 15%\n",
      "[4 4 2 4 4 4 4 4 4 4 3 4 3 4 4 4 3 4 4 4]\n",
      "[3 0 4 0 3 0 1 2 0 1 3 4 3 0 3 4 4 4 2 2]\n",
      "[BATCH 88/980] Loss_D: 1.4174 Loss_G: 1.7055 acc: 25%\n",
      "[4 4 4 4 4 4 4 4 4 4 3 4 4 0 0 3 3 3 3 3]\n",
      "[3 0 0 2 0 4 2 2 4 1 3 3 0 3 2 1 2 3 4 2]\n",
      "[BATCH 89/980] Loss_D: 1.4169 Loss_G: 1.7350 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 2 4 3 4 4 4 0 3 4 4 3]\n",
      "[2 0 1 0 1 3 2 2 2 2 2 0 2 2 3 0 0 0 2 1]\n",
      "[BATCH 90/980] Loss_D: 1.4667 Loss_G: 1.6936 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 0 4 4 0 4 4 3 3 4]\n",
      "[2 2 2 2 2 1 4 1 2 0 4 3 1 2 4 0 2 3 1 3]\n",
      "[BATCH 91/980] Loss_D: 1.4684 Loss_G: 1.7060 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4]\n",
      "[3 2 1 1 1 0 0 3 3 3 4 0 0 1 2 2 1 1 4 1]\n",
      "[BATCH 92/980] Loss_D: 1.4456 Loss_G: 1.6899 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 3 4 4]\n",
      "[1 2 1 2 2 2 1 2 1 1 1 4 0 1 1 2 1 0 0 0]\n",
      "[BATCH 93/980] Loss_D: 1.3461 Loss_G: 1.6896 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4]\n",
      "[0 3 2 3 0 2 3 2 2 2 3 0 1 1 0 4 3 4 4 2]\n",
      "[BATCH 94/980] Loss_D: 1.3413 Loss_G: 1.6867 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 0 4 3 3 4 4 3 3 0 4]\n",
      "[0 1 1 2 0 0 2 2 3 3 1 0 0 2 2 2 3 2 1 4]\n",
      "[BATCH 95/980] Loss_D: 1.3635 Loss_G: 1.6862 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 3 3 4 4 4 4 4 3]\n",
      "[1 1 0 4 1 2 1 2 3 1 0 2 1 1 1 0 4 1 0 0]\n",
      "[BATCH 96/980] Loss_D: 1.3179 Loss_G: 1.6526 acc: 10%\n",
      "[0 4 4 4 4 4 4 4 4 4 4 4 0 4 4 3 3 4 3 4]\n",
      "[0 1 2 4 1 2 2 2 1 4 2 2 2 2 4 3 4 4 0 1]\n",
      "[BATCH 97/980] Loss_D: 1.3895 Loss_G: 1.6666 acc: 30%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 3 4 4]\n",
      "[1 2 2 2 2 0 3 3 2 2 3 0 4 4 2 3 0 1 1 3]\n",
      "[BATCH 98/980] Loss_D: 1.4892 Loss_G: 1.6454 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4]\n",
      "[2 4 1 4 2 4 2 2 2 1 2 1 2 1 1 1 1 0 2 1]\n",
      "[BATCH 99/980] Loss_D: 1.5973 Loss_G: 1.6625 acc: 15%\n",
      "[0 0 0 4 4 0 0 0 4 0 4 0 0 4 4 4 4 0 4 4]\n",
      "[1 2 1 4 1 2 2 2 1 2 1 1 4 4 4 1 4 3 3 3]\n",
      "[BATCH 100/980] Loss_D: 1.5438 Loss_G: 1.6697 acc: 20%\n",
      "[4 4 4 4 4 4 4 0 4 4 0 4 4 4 0 4 4 0 4 4]\n",
      "[0 3 0 2 2 1 0 2 1 2 4 3 3 1 0 0 2 3 0 3]\n",
      "[BATCH 101/980] Loss_D: 1.3855 Loss_G: 1.6559 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 0 4 4 4 4]\n",
      "[1 2 2 2 0 2 3 2 2 3 3 0 4 2 1 3 0 0 3 2]\n",
      "[BATCH 102/980] Loss_D: 1.4687 Loss_G: 1.6757 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 0 0 4 0 4 4 4 4]\n",
      "[2 1 3 4 3 2 1 4 2 0 3 1 4 1 2 1 4 1 1 1]\n",
      "[BATCH 103/980] Loss_D: 1.3971 Loss_G: 1.6554 acc: 15%\n",
      "[4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4]\n",
      "[0 1 2 0 2 2 2 1 0 2 4 2 0 0 2 2 2 0 0 2]\n",
      "[BATCH 104/980] Loss_D: 1.4278 Loss_G: 1.6418 acc: 5%\n",
      "[4 0 4 0 4 4 4 0 0 4 0 4 0 0 4 4 4 0 0 4]\n",
      "[1 3 1 2 0 4 2 2 2 2 1 1 1 1 0 4 0 0 2 2]\n",
      "[BATCH 105/980] Loss_D: 1.4389 Loss_G: 1.6393 acc: 15%\n",
      "[4 4 4 4 4 0 3 0 4 4 4 0 0 4 4 0 4 4 4 4]\n",
      "[2 0 2 0 1 3 2 2 2 2 2 1 3 3 4 2 3 3 2 0]\n",
      "[BATCH 106/980] Loss_D: 1.3529 Loss_G: 1.6330 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[2 0 2 3 0 1 3 4 1 0 0 2 3 2 2 4 3 2 3 3]\n",
      "[BATCH 107/980] Loss_D: 1.4228 Loss_G: 1.6152 acc: 10%\n",
      "[4 4 4 4 4 3 4 4 4 4 4 4 0 4 4 4 4 4 4 4]\n",
      "[0 1 3 2 0 4 0 2 2 0 0 2 2 1 4 1 1 0 0 3]\n",
      "[BATCH 108/980] Loss_D: 1.4342 Loss_G: 1.6417 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4]\n",
      "[1 2 3 4 2 2 3 0 2 2 1 0 1 1 0 3 1 2 4 4]\n",
      "[BATCH 109/980] Loss_D: 1.3616 Loss_G: 1.6233 acc: 15%\n",
      "[0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[3 2 4 1 0 2 3 1 1 4 0 0 4 2 4 2 3 0 0 1]\n",
      "[BATCH 110/980] Loss_D: 1.4585 Loss_G: 1.6299 acc: 20%\n",
      "[4 4 4 0 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4 4]\n",
      "[2 2 1 2 1 2 2 2 3 2 2 2 2 2 2 3 3 2 0 3]\n",
      "[BATCH 111/980] Loss_D: 1.4067 Loss_G: 1.6070 acc: 0%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 0]\n",
      "[2 2 4 0 2 1 1 1 2 0 2 3 1 4 4 1 2 4 4 1]\n",
      "[BATCH 112/980] Loss_D: 1.3918 Loss_G: 1.6053 acc: 25%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4 4 4]\n",
      "[1 2 3 1 3 2 2 2 1 2 3 3 3 4 4 2 2 1 3 0]\n",
      "[BATCH 113/980] Loss_D: 1.4174 Loss_G: 1.6251 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0]\n",
      "[2 1 0 2 3 2 4 2 1 1 2 1 4 2 3 0 1 3 4 0]\n",
      "[BATCH 114/980] Loss_D: 1.4173 Loss_G: 1.6055 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4]\n",
      "[1 4 2 0 3 2 2 2 2 1 1 0 0 3 0 1 3 3 4 0]\n",
      "[BATCH 115/980] Loss_D: 1.3475 Loss_G: 1.6094 acc: 5%\n",
      "[4 1 4 4 4 4 4 4 4 4 0 4 4 0 0 4 4 4 4 4]\n",
      "[2 3 1 2 0 0 1 2 4 2 0 2 1 1 3 3 4 2 4 2]\n",
      "[BATCH 116/980] Loss_D: 1.4770 Loss_G: 1.6243 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[2 2 1 3 1 3 2 0 4 2 0 1 2 3 1 1 0 0 0 3]\n",
      "[BATCH 117/980] Loss_D: 1.4198 Loss_G: 1.5789 acc: 5%\n",
      "[4 4 4 0 4 4 4 4 2 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[1 1 2 4 4 1 1 2 3 2 2 2 3 3 3 2 0 3 4 0]\n",
      "[BATCH 118/980] Loss_D: 1.4153 Loss_G: 1.5570 acc: 10%\n",
      "[4 2 4 4 4 4 4 4 4 4 4 4 0 4 0 1 4 0 4 4]\n",
      "[2 4 4 3 1 4 2 1 1 2 0 1 2 2 4 1 4 1 1 3]\n",
      "[BATCH 119/980] Loss_D: 1.4063 Loss_G: 1.5951 acc: 20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 0]\n",
      "[3 2 2 2 1 1 2 2 2 2 3 2 4 0 3 2 0 4 4 3]\n",
      "[BATCH 120/980] Loss_D: 1.4446 Loss_G: 1.5629 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 3 4 4 4 4 4 0 4 4 4 4 4]\n",
      "[3 3 0 4 2 2 4 2 2 4 2 4 3 0 3 2 4 2 2 3]\n",
      "[BATCH 121/980] Loss_D: 1.6778 Loss_G: 1.5858 acc: 25%\n",
      "[0 4 4 0 4 4 4 4 4 4 4 4 4 4 0 4 4 4 4 4]\n",
      "[1 2 3 0 3 1 2 2 2 0 3 1 4 3 4 4 0 4 4 2]\n",
      "[BATCH 122/980] Loss_D: 1.4254 Loss_G: 1.5801 acc: 25%\n",
      "[4 4 4 4 0 4 4 4 4 4 4 4 4 4 4 0 0 4 0 4]\n",
      "[1 1 2 0 2 2 2 0 3 1 2 0 2 2 1 3 0 2 0 3]\n",
      "[BATCH 123/980] Loss_D: 1.3381 Loss_G: 1.5706 acc: 10%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[3 1 2 0 2 1 3 2 3 2 3 3 1 2 2 3 1 2 1 2]\n",
      "[BATCH 124/980] Loss_D: 1.4874 Loss_G: 1.5658 acc: 0%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "[1 0 3 2 1 2 1 0 4 1 0 4 4 3 0 3 4 0 2 2]\n",
      "[BATCH 125/980] Loss_D: 1.4020 Loss_G: 1.5539 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4]\n",
      "[2 1 1 1 2 2 1 1 2 1 3 3 0 3 1 1 0 2 1 4]\n",
      "[BATCH 126/980] Loss_D: 1.3510 Loss_G: 1.5614 acc: 5%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4 4]\n",
      "[3 4 2 1 2 2 0 2 2 0 3 3 1 1 3 2 0 2 2 2]\n",
      "[BATCH 127/980] Loss_D: 1.4636 Loss_G: 1.5661 acc: 10%\n",
      "[4 4 4 4 4 4 1 2 4 4 4 0 4 4 4 4 4 4 4 4]\n",
      "[0 2 4 0 2 0 4 1 2 4 0 3 1 0 2 4 3 2 1 0]\n",
      "[BATCH 128/980] Loss_D: 1.5952 Loss_G: 1.5526 acc: 15%\n",
      "[4 4 4 4 4 4 4 4 4 0 4 4 0 4 4 4 4 4 4 4]\n",
      "[4 2 2 2 2 2 3 1 4 1 0 1 1 2 3 2 3 0 4 4]\n",
      "[BATCH 129/980] Loss_D: 1.4743 Loss_G: 1.5528 acc: 20%\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 0 4 4]\n",
      "[2 0 4 2 3 2 2 4 0 1 4 3 4 3 3 0 1 3 3 3]\n",
      "[BATCH 130/980] Loss_D: 1.4092 Loss_G: 1.5545 acc: 20%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-10a9e5180eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-ea2ad32058c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, single_test_set)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mauxiliary_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CapsuleGAN/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CapsuleGAN/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_data = os.listdir(INPUT_PATH)\n",
    "npy_list = []\n",
    "npy_level_list = []\n",
    "\n",
    "for level_num in range(5): #遍历patient文件夹——study指代每一个study文件夹\n",
    "    if level_num == 2:\n",
    "        continue\n",
    "    npy_file_path = os.path.join(INPUT_PATH, \"malignancy_\" + str(level_num + 1))\n",
    "    npy_files = os.listdir(npy_file_path)\n",
    "    for i in npy_files:\n",
    "        npy_path = os.path.join(npy_file_path, i)\n",
    "        single_npy = np.load(npy_path)\n",
    "        single_fliplr_npy = np.fliplr(single_npy)\n",
    "        single_npy = (single_npy - 127.5) / 127.5\n",
    "        npy_list.append(single_npy)\n",
    "        if level_num < 2:\n",
    "            npy_level_list.append(0)\n",
    "        else:\n",
    "            npy_level_list.append(1)\n",
    "        if level_num > 2:\n",
    "            single_fliplr_npy = (single_fliplr_npy - 127.5) / 127.5\n",
    "            npy_list.append(single_fliplr_npy)\n",
    "            npy_level_list.append(1)\n",
    "        \n",
    "\n",
    "#         print(npy_file_path, np.array(npy_list).shape)\n",
    "random.seed(randnum)\n",
    "random.shuffle(npy_list)\n",
    "random.seed(randnum)\n",
    "random.shuffle(npy_level_list)\n",
    "\n",
    "npy_chunks = chunks(npy_list, 5)\n",
    "npy_level_chunks = chunks(npy_level_list, 5)\n",
    "\n",
    "print(\"NOW the training STARTS:\")\n",
    "training_set, test_set = five_folder(npy_chunks, 4)\n",
    "training_level, test_level = five_folder(npy_level_chunks, 4)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(MyDataset(training_set, training_level), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "train(train_loader, test_set, test_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CapsuleGAN",
   "language": "python",
   "name": "capsulegan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
