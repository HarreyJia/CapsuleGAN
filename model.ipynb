{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "INPUT_PATH = \"./output/nodule_npy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "nz = 100\n",
    "n_epochs = 50\n",
    "ngf = 64\n",
    "ngpu = 1\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def chunks(arr, m):\n",
    "    nchunk = int(math.ceil(len(arr) / float(m)))\n",
    "    return [arr[i:i + nchunk] for i in range(0, len(arr), nchunk)]\n",
    "\n",
    "def five_folder(arr, number):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    for j in range(len(arr)):\n",
    "        if number == j:\n",
    "            test_set.extend(arr[j])\n",
    "        else:\n",
    "            training_set.extend(arr[j])\n",
    "    return training_set, test_set\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        img = self.images[index]\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "          nn.Linear(nz, 1 * 5 * 5 * ngf * 4, bias=False)\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "          # input is Z, going into a deconvolution\n",
    "          # state size. BATCH_SIZE x (ngf*8) x 4 x 4 x 4\n",
    "#           nn.ConvTranspose3d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "#           nn.BatchNorm3d(ngf * 4),\n",
    "#           nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf*4) x 1 x 5 x 5\n",
    "          nn.ConvTranspose3d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.BatchNorm3d(ngf * 2),\n",
    "          nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf*2) x 2 x 10 x 10\n",
    "          nn.ConvTranspose3d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.BatchNorm3d(ngf),\n",
    "          nn.ReLU(True),\n",
    "          # state size. BATCH_SIZE x (ngf) x 4 x 20 x 20\n",
    "          nn.ConvTranspose3d(ngf, 1, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "          nn.Tanh()\n",
    "          # state size. BATCH_SIZE x 1 x 8 x 40 x 40\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.project(input)\n",
    "        # Conv3d的规定输入数据格式为(batch, channel, Depth, Height, Width)\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, ngf * 4, 1, 5, 5)\n",
    "#         x = self.deconv(x)\n",
    "        \n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.deconv, x, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.deconv(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "def softmax(input, dim=1):\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=256, kernel_size=(2,9,9)):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels,\n",
    "                                out_channels=out_channels,\n",
    "                                kernel_size=kernel_size,\n",
    "                                stride=1\n",
    "                              )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=(2,9,9), num_routes=32 * 12 * 12 * 6):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "          nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=(1,2,2), padding=0) \n",
    "                      for _ in range(num_capsules)])\n",
    "  \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "        u = torch.cat(u, dim=-1)\n",
    "        return self.squash(u)\n",
    "  \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=1, num_routes=32 * 12 * 12 * 6, in_channels=8, out_channels=16, num_iterations=3):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "        self.num_iterations = num_iterations\n",
    "        self.route_weights = nn.Parameter(torch.randn(num_capsules, num_routes, in_channels, out_channels)).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 矩阵相乘\n",
    "        # x.size(): [1, batch_size, in_capsules, 1, dim_in_capsule]\n",
    "        # weight.size(): [num_capsules, 1, num_route, in_channels, out_channels]\n",
    "        priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :]\n",
    "#         priors = priors.to(device)\n",
    "#         print()\n",
    "#         print(x[None, :, :, None, :].size())\n",
    "#         print(self.route_weights[:, None, :, :, :].size())\n",
    "#         print(priors.size())\n",
    "#         print()\n",
    "\n",
    "        logits = Variable(torch.zeros(*priors.size())).to(device)\n",
    "#         logits = Variable(torch.zeros(*priors.size()))\n",
    "        for i in range(self.num_iterations):\n",
    "            probs = softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "\n",
    "            if i != self.num_routes - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        \n",
    "#         return outputs.squeeze().transpose(0, 1)\n",
    "        return outputs.squeeze()\n",
    "  \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "    \n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "          nn.Linear(16 * 10, 512),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Linear(512, 1024),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Linear(1024, 784),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "      \n",
    "    def forward(self, x, data):\n",
    "        classes = (x ** 2).sum(dim=-1) ** 0.5\n",
    "        classes = F.softmax(classes, dim=-1)\n",
    "\n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        masked = Variable(torch.sparse.torch.eye(10))\n",
    "        if USE_CUDA:\n",
    "              masked = masked.cuda()\n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n",
    "    \n",
    "        reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n",
    "        reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
    "\n",
    "        return reconstructions, masked\n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        # self.decoder = Decoder()\n",
    "\n",
    "        # self.mse_loss = nn.MSELoss()\n",
    "      \n",
    "    def forward(self, data):\n",
    "#         output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n",
    "        \n",
    "        if data.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.conv_layer, data, range(self.ngpu))\n",
    "            output = nn.parallel.data_parallel(self.primary_capsules, output, range(self.ngpu))\n",
    "            output = nn.parallel.data_parallel(self.digit_capsules, output, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n",
    "\n",
    "#         print(\"OUTPUTS: \",output.shape)\n",
    "        classes = (output ** 2).sum(dim=-1) ** 0.5\n",
    "#         classes = F.softmax(classes, dim=-1)\n",
    "        \n",
    "        return classes, output\n",
    "\n",
    "        #reconstructions, masked = self.decoder(output, data)\n",
    "        #return output, reconstructions, masked\n",
    "\n",
    "    # def loss(self, data, x, target, reconstructions):\n",
    "      #   return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n",
    "\n",
    "    # def margin_loss(self, x, labels, size_average=True):\n",
    "      #   batch_size = x.size(0)\n",
    "\n",
    "      #   v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "      #   left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "      #   right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "\n",
    "      #   loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "      #   loss = loss.sum(dim=1).mean()\n",
    "\n",
    "      #   return loss\n",
    "\n",
    "    # def reconstruction_loss(self, data, reconstructions):\n",
    "      #   loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "      #   return loss * 0.0005\n",
    "\n",
    "\n",
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        # self.reconstruction_loss = nn.MSELoss(size_average=False)\n",
    "\n",
    "    # def forward(self, images, labels, classes, reconstructions):\n",
    "    def forward(self, classes, labels):\n",
    "        left = F.relu(0.9 - classes, inplace=True) ** 2\n",
    "        right = F.relu(classes - 0.1, inplace=True) ** 2\n",
    "\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "\n",
    "        return margin_loss\n",
    "\n",
    "        # assert torch.numel(images) == torch.numel(reconstructions)\n",
    "        # images = images.view(reconstructions.size()[0], -1)\n",
    "        # reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
    "\n",
    "        # return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cross_part, train_loader, single_test_set):\n",
    "    netD = CapsNet(ngpu).to(device)\n",
    "    criterion = CapsuleLoss()\n",
    "\n",
    "    netG = Generator(ngpu).to(device)\n",
    "    netG.apply(weights_init)\n",
    "\n",
    "    fixed_noise = torch.randn(BATCH_SIZE, nz, device=device)\n",
    "    real_label = 1\n",
    "    fake_label = 0\n",
    "\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"-----THE [{}/{}] epoch start-----\".format(epoch + 1, n_epochs))\n",
    "        for j, data in enumerate(train_loader, 0):\n",
    "#                     print(j, data.shape) #torch.Size([32, 1, 8, 40, 40])\n",
    "\n",
    "#                     plt.figure()\n",
    "#                     for lenc in range(data.shape[0]):\n",
    "#                         for len_img in range(8):\n",
    "#                             plt.subplot(2, 4, len_img + 1)\n",
    "#                             pixel_array = data[lenc][0][len_img]\n",
    "#                             print(pixel_array)\n",
    "#                             plt.imshow(pixel_array, cmap=\"gray\")\n",
    "#                         plt.show()\n",
    "\n",
    "\n",
    "#                     pixel_array = data[0][0][0][20]\n",
    "#                     print(pixel_array)\n",
    "\n",
    "#                     data = data / 255.0\n",
    "#                     pixel_array = data[0][0][0][20]\n",
    "#                     print(pixel_array)\n",
    "            ############################\n",
    "            # (1) Update D network: maximize Lm(D(x), T = 0) + Lm(D(G(z)), T = 1)\n",
    "            ###########################\n",
    "            # train with real\n",
    "            netD.zero_grad()\n",
    "            real_cpu = data.to(device, dtype=torch.float)\n",
    "            batch_size = real_cpu.size(0)\n",
    "            label = torch.full((batch_size,), real_label, device=device)   \n",
    "\n",
    "            classes_d_real, output_d_real = netD(real_cpu)\n",
    "#                     print(\"ERR_D_REAL:\",classes_d_real)\n",
    "            errD_real = criterion(classes_d_real, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output_d_real.mean().item()\n",
    "\n",
    "            # train with fake\n",
    "            noise = torch.randn(batch_size, nz, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            classes_d_fake, output_d_fake = netD(fake.detach())\n",
    "\n",
    "#                     print(\"ERR_D_FAKE:\",classes_d_fake)\n",
    "            errD_fake = criterion(classes_d_fake, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output_d_fake.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize Lm(D(G(z)), T=0)\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            netD.eval()\n",
    "\n",
    "            classes_g, output_g = netD(fake)\n",
    "#                     print(\"ERRG:\",classes_g)\n",
    "            errG = criterion(classes_g, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output_g.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "#                     print(\"The batch data shape is {}\".format(data.shape))\n",
    "\n",
    "            print('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                  % (j + 1, len(train_loader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "\n",
    "        print(\"-----THE [{}/{}] epoch end-----\".format(epoch + 1, n_epochs))\n",
    "    \n",
    "    ###TEST###\n",
    "    test_loader = torch.utils.data.DataLoader(MyDataset(single_test_set), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_sum = 0\n",
    "    for i, test_data in enumerate(test_loader, 0):\n",
    "        test_data = test_data.to(device, dtype=torch.float)\n",
    "        test_classes, test_outputs = netD(test_data)\n",
    "        \n",
    "        test_classes = test_classes.cpu()\n",
    "        test_classes = test_classes.detach().numpy()\n",
    "        thresh = 0.5\n",
    "\n",
    "        test_classes[test_classes > thresh] = 1\n",
    "        test_classes[test_classes < thresh] = 0\n",
    "        \n",
    "        single_test_sum = test_classes.sum()\n",
    "        test_sum += single_test_sum\n",
    "        \n",
    "    print(\"{}/{} is discriminated for right. The ACCURACY is {}%.\".format(test_sum, len(single_test_set), 100 * test_sum / len(test_loader)))\n",
    "    \n",
    "    return test_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS the data from ./output/nodule_npy/Radiologist_1. AND NOW the 5-folder cross-valiation start:\n",
      "-------------THE 1 part(as the test set) cross-valiation start----------------------\n",
      "-----THE [1/50] epoch start-----\n",
      "[1/10] Loss_D: 25.8905 Loss_G: 25.8261 D(x): 0.0000 D(G(z)): -0.0000 / 0.0001\n",
      "[2/10] Loss_D: 23.3455 Loss_G: 25.8196 D(x): 0.0031 D(G(z)): 0.0001 / 0.0001\n",
      "[3/10] Loss_D: 21.2638 Loss_G: 25.8824 D(x): 0.0058 D(G(z)): 0.0001 / 0.0000\n",
      "[4/10] Loss_D: 19.2242 Loss_G: 25.8095 D(x): 0.0093 D(G(z)): 0.0001 / 0.0001\n",
      "[5/10] Loss_D: 17.7059 Loss_G: 25.6931 D(x): 0.0119 D(G(z)): 0.0002 / 0.0003\n",
      "[6/10] Loss_D: 18.0797 Loss_G: 25.6661 D(x): 0.0117 D(G(z)): 0.0004 / 0.0003\n",
      "[7/10] Loss_D: 18.2215 Loss_G: 25.6250 D(x): 0.0117 D(G(z)): 0.0004 / 0.0004\n",
      "[8/10] Loss_D: 14.8814 Loss_G: 25.6188 D(x): 0.0178 D(G(z)): 0.0005 / 0.0004\n",
      "[9/10] Loss_D: 16.6658 Loss_G: 25.4610 D(x): 0.0149 D(G(z)): 0.0005 / 0.0006\n",
      "[10/10] Loss_D: 11.5287 Loss_G: 19.1062 D(x): 0.0171 D(G(z)): 0.0007 / 0.0006\n",
      "-----THE [1/50] epoch end-----\n",
      "-----THE [2/50] epoch start-----\n",
      "[1/10] Loss_D: 13.6588 Loss_G: 25.5425 D(x): 0.0204 D(G(z)): 0.0006 / 0.0006\n",
      "[2/10] Loss_D: 15.2144 Loss_G: 25.4084 D(x): 0.0192 D(G(z)): 0.0007 / 0.0006\n",
      "[3/10] Loss_D: 16.9142 Loss_G: 25.3241 D(x): 0.0146 D(G(z)): 0.0007 / 0.0008\n",
      "[4/10] Loss_D: 14.7940 Loss_G: 25.5287 D(x): 0.0181 D(G(z)): 0.0009 / 0.0006\n",
      "[5/10] Loss_D: 13.6531 Loss_G: 25.4007 D(x): 0.0216 D(G(z)): 0.0007 / 0.0007\n",
      "[6/10] Loss_D: 13.8532 Loss_G: 25.3935 D(x): 0.0206 D(G(z)): 0.0007 / 0.0008\n",
      "[7/10] Loss_D: 11.2704 Loss_G: 25.4523 D(x): 0.0269 D(G(z)): 0.0009 / 0.0007\n",
      "[8/10] Loss_D: 17.1047 Loss_G: 25.2923 D(x): 0.0149 D(G(z)): 0.0007 / 0.0008\n",
      "[9/10] Loss_D: 13.4519 Loss_G: 25.3883 D(x): 0.0219 D(G(z)): 0.0008 / 0.0008\n",
      "[10/10] Loss_D: 9.5413 Loss_G: 19.0442 D(x): 0.0239 D(G(z)): 0.0008 / 0.0008\n",
      "-----THE [2/50] epoch end-----\n",
      "-----THE [3/50] epoch start-----\n",
      "[1/10] Loss_D: 11.1792 Loss_G: 25.3918 D(x): 0.0280 D(G(z)): 0.0008 / 0.0007\n",
      "[2/10] Loss_D: 11.2753 Loss_G: 25.4225 D(x): 0.0281 D(G(z)): 0.0007 / 0.0007\n",
      "[3/10] Loss_D: 15.6905 Loss_G: 25.4400 D(x): 0.0177 D(G(z)): 0.0007 / 0.0007\n",
      "[4/10] Loss_D: 12.3280 Loss_G: 25.4142 D(x): 0.0252 D(G(z)): 0.0007 / 0.0007\n",
      "[5/10] Loss_D: 13.8009 Loss_G: 25.3930 D(x): 0.0221 D(G(z)): 0.0007 / 0.0007\n",
      "[6/10] Loss_D: 12.9822 Loss_G: 25.3561 D(x): 0.0230 D(G(z)): 0.0007 / 0.0008\n",
      "[7/10] Loss_D: 11.2676 Loss_G: 25.3550 D(x): 0.0279 D(G(z)): 0.0009 / 0.0008\n",
      "[8/10] Loss_D: 11.3739 Loss_G: 25.2942 D(x): 0.0275 D(G(z)): 0.0009 / 0.0010\n",
      "[9/10] Loss_D: 15.9947 Loss_G: 25.3320 D(x): 0.0175 D(G(z)): 0.0009 / 0.0008\n",
      "[10/10] Loss_D: 9.3180 Loss_G: 18.9536 D(x): 0.0234 D(G(z)): 0.0009 / 0.0010\n",
      "-----THE [3/50] epoch end-----\n",
      "-----THE [4/50] epoch start-----\n",
      "[1/10] Loss_D: 10.9850 Loss_G: 25.3664 D(x): 0.0285 D(G(z)): 0.0010 / 0.0008\n",
      "[2/10] Loss_D: 12.9700 Loss_G: 25.2123 D(x): 0.0234 D(G(z)): 0.0008 / 0.0010\n",
      "[3/10] Loss_D: 14.4632 Loss_G: 25.1738 D(x): 0.0210 D(G(z)): 0.0011 / 0.0011\n",
      "[4/10] Loss_D: 13.8444 Loss_G: 25.1970 D(x): 0.0219 D(G(z)): 0.0012 / 0.0011\n",
      "[5/10] Loss_D: 10.2140 Loss_G: 25.2177 D(x): 0.0305 D(G(z)): 0.0012 / 0.0010\n",
      "[6/10] Loss_D: 11.1627 Loss_G: 25.0565 D(x): 0.0308 D(G(z)): 0.0011 / 0.0013\n",
      "[7/10] Loss_D: 14.2627 Loss_G: 25.0162 D(x): 0.0209 D(G(z)): 0.0014 / 0.0013\n",
      "[8/10] Loss_D: 11.7936 Loss_G: 25.1107 D(x): 0.0262 D(G(z)): 0.0013 / 0.0012\n",
      "[9/10] Loss_D: 9.5388 Loss_G: 24.6622 D(x): 0.0314 D(G(z)): 0.0015 / 0.0018\n",
      "[10/10] Loss_D: 8.1396 Loss_G: 18.5516 D(x): 0.0293 D(G(z)): 0.0016 / 0.0017\n",
      "-----THE [4/50] epoch end-----\n",
      "-----THE [5/50] epoch start-----\n",
      "[1/10] Loss_D: 11.8413 Loss_G: 24.5610 D(x): 0.0278 D(G(z)): 0.0023 / 0.0019\n",
      "[2/10] Loss_D: 13.3537 Loss_G: 24.1096 D(x): 0.0245 D(G(z)): 0.0025 / 0.0026\n",
      "[3/10] Loss_D: 10.6706 Loss_G: 24.3160 D(x): 0.0284 D(G(z)): 0.0025 / 0.0023\n",
      "[4/10] Loss_D: 10.7576 Loss_G: 23.8606 D(x): 0.0299 D(G(z)): 0.0030 / 0.0031\n",
      "[5/10] Loss_D: 10.6554 Loss_G: 23.6359 D(x): 0.0299 D(G(z)): 0.0031 / 0.0033\n",
      "[6/10] Loss_D: 11.5386 Loss_G: 23.6136 D(x): 0.0278 D(G(z)): 0.0036 / 0.0034\n",
      "[7/10] Loss_D: 11.1976 Loss_G: 23.3989 D(x): 0.0281 D(G(z)): 0.0037 / 0.0037\n",
      "[8/10] Loss_D: 9.2430 Loss_G: 23.3343 D(x): 0.0330 D(G(z)): 0.0044 / 0.0038\n",
      "[9/10] Loss_D: 15.1147 Loss_G: 22.8150 D(x): 0.0203 D(G(z)): 0.0039 / 0.0046\n",
      "[10/10] Loss_D: 7.0208 Loss_G: 17.2226 D(x): 0.0338 D(G(z)): 0.0046 / 0.0044\n",
      "-----THE [5/50] epoch end-----\n",
      "-----THE [6/50] epoch start-----\n",
      "[1/10] Loss_D: 10.7861 Loss_G: 23.1160 D(x): 0.0298 D(G(z)): 0.0047 / 0.0042\n",
      "[2/10] Loss_D: 12.3495 Loss_G: 22.5442 D(x): 0.0242 D(G(z)): 0.0044 / 0.0050\n",
      "[3/10] Loss_D: 10.9467 Loss_G: 22.7509 D(x): 0.0293 D(G(z)): 0.0051 / 0.0047\n",
      "[4/10] Loss_D: 7.6448 Loss_G: 22.2639 D(x): 0.0368 D(G(z)): 0.0051 / 0.0055\n",
      "[5/10] Loss_D: 13.0855 Loss_G: 21.9805 D(x): 0.0248 D(G(z)): 0.0055 / 0.0058\n",
      "[6/10] Loss_D: 12.3220 Loss_G: 22.0214 D(x): 0.0271 D(G(z)): 0.0061 / 0.0056\n",
      "[7/10] Loss_D: 9.9602 Loss_G: 22.2015 D(x): 0.0309 D(G(z)): 0.0058 / 0.0057\n",
      "[8/10] Loss_D: 11.8360 Loss_G: 22.0108 D(x): 0.0274 D(G(z)): 0.0056 / 0.0056\n",
      "[9/10] Loss_D: 10.5913 Loss_G: 22.2238 D(x): 0.0311 D(G(z)): 0.0059 / 0.0055\n",
      "[10/10] Loss_D: 7.0850 Loss_G: 16.4414 D(x): 0.0342 D(G(z)): 0.0058 / 0.0060\n",
      "-----THE [6/50] epoch end-----\n",
      "-----THE [7/50] epoch start-----\n",
      "[1/10] Loss_D: 10.3531 Loss_G: 22.0385 D(x): 0.0313 D(G(z)): 0.0060 / 0.0057\n",
      "[2/10] Loss_D: 10.0639 Loss_G: 22.1841 D(x): 0.0310 D(G(z)): 0.0059 / 0.0056\n",
      "[3/10] Loss_D: 8.4804 Loss_G: 21.9847 D(x): 0.0336 D(G(z)): 0.0056 / 0.0057\n",
      "[4/10] Loss_D: 11.5591 Loss_G: 21.2316 D(x): 0.0268 D(G(z)): 0.0059 / 0.0070\n",
      "[5/10] Loss_D: 12.5556 Loss_G: 20.7004 D(x): 0.0266 D(G(z)): 0.0072 / 0.0078\n",
      "[6/10] Loss_D: 14.5097 Loss_G: 20.8852 D(x): 0.0212 D(G(z)): 0.0079 / 0.0075\n",
      "[7/10] Loss_D: 9.8808 Loss_G: 21.0969 D(x): 0.0325 D(G(z)): 0.0077 / 0.0073\n",
      "[8/10] Loss_D: 7.5086 Loss_G: 21.4626 D(x): 0.0382 D(G(z)): 0.0070 / 0.0067\n",
      "[9/10] Loss_D: 10.9997 Loss_G: 21.3651 D(x): 0.0311 D(G(z)): 0.0069 / 0.0068\n",
      "[10/10] Loss_D: 8.7787 Loss_G: 16.2147 D(x): 0.0284 D(G(z)): 0.0068 / 0.0064\n",
      "-----THE [7/50] epoch end-----\n",
      "-----THE [8/50] epoch start-----\n",
      "[1/10] Loss_D: 9.7756 Loss_G: 21.5500 D(x): 0.0332 D(G(z)): 0.0066 / 0.0065\n",
      "[2/10] Loss_D: 10.6402 Loss_G: 21.7206 D(x): 0.0304 D(G(z)): 0.0065 / 0.0061\n",
      "[3/10] Loss_D: 9.0392 Loss_G: 21.7263 D(x): 0.0331 D(G(z)): 0.0062 / 0.0062\n",
      "[4/10] Loss_D: 10.4387 Loss_G: 21.4104 D(x): 0.0309 D(G(z)): 0.0062 / 0.0066\n",
      "[5/10] Loss_D: 10.3557 Loss_G: 20.9203 D(x): 0.0298 D(G(z)): 0.0068 / 0.0075\n",
      "[6/10] Loss_D: 12.2588 Loss_G: 20.7138 D(x): 0.0259 D(G(z)): 0.0075 / 0.0078\n",
      "[7/10] Loss_D: 12.6350 Loss_G: 20.8089 D(x): 0.0264 D(G(z)): 0.0079 / 0.0076\n",
      "[8/10] Loss_D: 9.8719 Loss_G: 20.8468 D(x): 0.0335 D(G(z)): 0.0076 / 0.0076\n",
      "[9/10] Loss_D: 8.1266 Loss_G: 21.1606 D(x): 0.0365 D(G(z)): 0.0077 / 0.0072\n",
      "[10/10] Loss_D: 8.0983 Loss_G: 15.5102 D(x): 0.0307 D(G(z)): 0.0073 / 0.0078\n",
      "-----THE [8/50] epoch end-----\n",
      "-----THE [9/50] epoch start-----\n",
      "[1/10] Loss_D: 12.7236 Loss_G: 21.3542 D(x): 0.0244 D(G(z)): 0.0077 / 0.0067\n",
      "[2/10] Loss_D: 9.4205 Loss_G: 20.8101 D(x): 0.0331 D(G(z)): 0.0069 / 0.0078\n",
      "[3/10] Loss_D: 9.4785 Loss_G: 21.0676 D(x): 0.0348 D(G(z)): 0.0078 / 0.0072\n",
      "[4/10] Loss_D: 8.4899 Loss_G: 21.1800 D(x): 0.0340 D(G(z)): 0.0071 / 0.0070\n",
      "[5/10] Loss_D: 9.0108 Loss_G: 21.0929 D(x): 0.0350 D(G(z)): 0.0072 / 0.0073\n",
      "[6/10] Loss_D: 12.0462 Loss_G: 20.5623 D(x): 0.0262 D(G(z)): 0.0073 / 0.0079\n",
      "[7/10] Loss_D: 12.1163 Loss_G: 20.4677 D(x): 0.0280 D(G(z)): 0.0081 / 0.0082\n",
      "[8/10] Loss_D: 9.0074 Loss_G: 20.7169 D(x): 0.0352 D(G(z)): 0.0082 / 0.0079\n",
      "[9/10] Loss_D: 9.5184 Loss_G: 20.7833 D(x): 0.0333 D(G(z)): 0.0079 / 0.0076\n",
      "[10/10] Loss_D: 5.8848 Loss_G: 15.5242 D(x): 0.0385 D(G(z)): 0.0077 / 0.0077\n",
      "-----THE [9/50] epoch end-----\n",
      "-----THE [10/50] epoch start-----\n",
      "[1/10] Loss_D: 11.5181 Loss_G: 21.4411 D(x): 0.0276 D(G(z)): 0.0075 / 0.0065\n",
      "[2/10] Loss_D: 9.6461 Loss_G: 20.8987 D(x): 0.0308 D(G(z)): 0.0067 / 0.0076\n",
      "[3/10] Loss_D: 10.9934 Loss_G: 21.0534 D(x): 0.0295 D(G(z)): 0.0075 / 0.0069\n",
      "[4/10] Loss_D: 11.1556 Loss_G: 20.1433 D(x): 0.0288 D(G(z)): 0.0070 / 0.0083\n",
      "[5/10] Loss_D: 9.4020 Loss_G: 20.7705 D(x): 0.0344 D(G(z)): 0.0085 / 0.0079\n",
      "[6/10] Loss_D: 10.6448 Loss_G: 20.2928 D(x): 0.0303 D(G(z)): 0.0078 / 0.0082\n",
      "[7/10] Loss_D: 9.0667 Loss_G: 20.5123 D(x): 0.0374 D(G(z)): 0.0084 / 0.0080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10] Loss_D: 8.2138 Loss_G: 21.1244 D(x): 0.0359 D(G(z)): 0.0079 / 0.0073\n",
      "[9/10] Loss_D: 9.1252 Loss_G: 20.1058 D(x): 0.0350 D(G(z)): 0.0075 / 0.0084\n",
      "[10/10] Loss_D: 6.5256 Loss_G: 15.2907 D(x): 0.0347 D(G(z)): 0.0085 / 0.0081\n",
      "-----THE [10/50] epoch end-----\n",
      "-----THE [11/50] epoch start-----\n",
      "[1/10] Loss_D: 9.0028 Loss_G: 20.9683 D(x): 0.0341 D(G(z)): 0.0083 / 0.0073\n",
      "[2/10] Loss_D: 7.8301 Loss_G: 20.6153 D(x): 0.0379 D(G(z)): 0.0073 / 0.0076\n",
      "[3/10] Loss_D: 8.9047 Loss_G: 20.8001 D(x): 0.0341 D(G(z)): 0.0078 / 0.0075\n",
      "[4/10] Loss_D: 9.1514 Loss_G: 20.8980 D(x): 0.0357 D(G(z)): 0.0076 / 0.0073\n",
      "[5/10] Loss_D: 7.3957 Loss_G: 20.1204 D(x): 0.0388 D(G(z)): 0.0074 / 0.0085\n",
      "[6/10] Loss_D: 10.0494 Loss_G: 20.7337 D(x): 0.0325 D(G(z)): 0.0086 / 0.0076\n",
      "[7/10] Loss_D: 8.3849 Loss_G: 20.0096 D(x): 0.0344 D(G(z)): 0.0077 / 0.0088\n",
      "[8/10] Loss_D: 13.6858 Loss_G: 20.8117 D(x): 0.0249 D(G(z)): 0.0088 / 0.0075\n",
      "[9/10] Loss_D: 10.7940 Loss_G: 19.5862 D(x): 0.0293 D(G(z)): 0.0076 / 0.0093\n",
      "[10/10] Loss_D: 9.0886 Loss_G: 15.0544 D(x): 0.0268 D(G(z)): 0.0094 / 0.0087\n",
      "-----THE [11/50] epoch end-----\n",
      "-----THE [12/50] epoch start-----\n",
      "[1/10] Loss_D: 6.9695 Loss_G: 20.7622 D(x): 0.0410 D(G(z)): 0.0091 / 0.0076\n",
      "[2/10] Loss_D: 12.0251 Loss_G: 20.1656 D(x): 0.0261 D(G(z)): 0.0076 / 0.0085\n",
      "[3/10] Loss_D: 7.6499 Loss_G: 21.4083 D(x): 0.0380 D(G(z)): 0.0084 / 0.0065\n",
      "[4/10] Loss_D: 10.0257 Loss_G: 20.0533 D(x): 0.0322 D(G(z)): 0.0068 / 0.0085\n",
      "[5/10] Loss_D: 14.1958 Loss_G: 20.7462 D(x): 0.0225 D(G(z)): 0.0084 / 0.0076\n",
      "[6/10] Loss_D: 9.3598 Loss_G: 20.0122 D(x): 0.0320 D(G(z)): 0.0077 / 0.0087\n",
      "[7/10] Loss_D: 8.4415 Loss_G: 20.3533 D(x): 0.0380 D(G(z)): 0.0086 / 0.0080\n",
      "[8/10] Loss_D: 7.6565 Loss_G: 20.2368 D(x): 0.0384 D(G(z)): 0.0082 / 0.0084\n",
      "[9/10] Loss_D: 9.1795 Loss_G: 20.7564 D(x): 0.0343 D(G(z)): 0.0085 / 0.0074\n",
      "[10/10] Loss_D: 5.7657 Loss_G: 15.5084 D(x): 0.0382 D(G(z)): 0.0075 / 0.0078\n",
      "-----THE [12/50] epoch end-----\n",
      "-----THE [13/50] epoch start-----\n",
      "[1/10] Loss_D: 8.6338 Loss_G: 20.4870 D(x): 0.0356 D(G(z)): 0.0078 / 0.0078\n",
      "[2/10] Loss_D: 11.2724 Loss_G: 21.0551 D(x): 0.0292 D(G(z)): 0.0078 / 0.0071\n",
      "[3/10] Loss_D: 7.7123 Loss_G: 20.6686 D(x): 0.0373 D(G(z)): 0.0071 / 0.0077\n",
      "[4/10] Loss_D: 9.5113 Loss_G: 21.3737 D(x): 0.0336 D(G(z)): 0.0078 / 0.0065\n",
      "[5/10] Loss_D: 10.8347 Loss_G: 19.3734 D(x): 0.0296 D(G(z)): 0.0066 / 0.0096\n",
      "[6/10] Loss_D: 9.9265 Loss_G: 20.1994 D(x): 0.0337 D(G(z)): 0.0097 / 0.0084\n",
      "[7/10] Loss_D: 8.1264 Loss_G: 20.1626 D(x): 0.0379 D(G(z)): 0.0086 / 0.0085\n",
      "[8/10] Loss_D: 9.4607 Loss_G: 20.1981 D(x): 0.0318 D(G(z)): 0.0084 / 0.0084\n",
      "[9/10] Loss_D: 10.5103 Loss_G: 20.6314 D(x): 0.0304 D(G(z)): 0.0085 / 0.0077\n",
      "[10/10] Loss_D: 5.0443 Loss_G: 15.2818 D(x): 0.0410 D(G(z)): 0.0078 / 0.0081\n",
      "-----THE [13/50] epoch end-----\n",
      "-----THE [14/50] epoch start-----\n",
      "[1/10] Loss_D: 8.3584 Loss_G: 20.7224 D(x): 0.0368 D(G(z)): 0.0081 / 0.0076\n",
      "[2/10] Loss_D: 7.7670 Loss_G: 20.1339 D(x): 0.0380 D(G(z)): 0.0075 / 0.0082\n",
      "[3/10] Loss_D: 8.2829 Loss_G: 21.2126 D(x): 0.0373 D(G(z)): 0.0083 / 0.0069\n",
      "[4/10] Loss_D: 10.6032 Loss_G: 19.7398 D(x): 0.0305 D(G(z)): 0.0071 / 0.0089\n",
      "[5/10] Loss_D: 10.6588 Loss_G: 20.2517 D(x): 0.0315 D(G(z)): 0.0089 / 0.0082\n",
      "[6/10] Loss_D: 9.2112 Loss_G: 20.5662 D(x): 0.0332 D(G(z)): 0.0082 / 0.0078\n",
      "[7/10] Loss_D: 9.7646 Loss_G: 20.3167 D(x): 0.0329 D(G(z)): 0.0080 / 0.0079\n",
      "[8/10] Loss_D: 7.9844 Loss_G: 20.9328 D(x): 0.0362 D(G(z)): 0.0079 / 0.0074\n",
      "[9/10] Loss_D: 9.8023 Loss_G: 20.2324 D(x): 0.0348 D(G(z)): 0.0075 / 0.0082\n",
      "[10/10] Loss_D: 5.9002 Loss_G: 15.0983 D(x): 0.0364 D(G(z)): 0.0082 / 0.0084\n",
      "-----THE [14/50] epoch end-----\n",
      "-----THE [15/50] epoch start-----\n",
      "[1/10] Loss_D: 6.5667 Loss_G: 20.5879 D(x): 0.0410 D(G(z)): 0.0085 / 0.0076\n",
      "[2/10] Loss_D: 8.5676 Loss_G: 20.2361 D(x): 0.0368 D(G(z)): 0.0076 / 0.0082\n",
      "[3/10] Loss_D: 11.5089 Loss_G: 20.7377 D(x): 0.0285 D(G(z)): 0.0083 / 0.0074\n",
      "[4/10] Loss_D: 9.6664 Loss_G: 20.3479 D(x): 0.0346 D(G(z)): 0.0073 / 0.0080\n",
      "[5/10] Loss_D: 9.1820 Loss_G: 20.4446 D(x): 0.0333 D(G(z)): 0.0081 / 0.0078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f747f294256d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m#print(len(train_loader), type(train_loader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0msingle_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msingle_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0macc_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msingle_test_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-20a9d336ccf2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cross_part, train_loader, single_test_set)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#                     print(\"ERRG:\",classes_g)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0moptimizerG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CapsuleGAN/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CapsuleGAN/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_data = os.listdir(INPUT_PATH)\n",
    "\n",
    "for radiologist in input_data: #遍历patient文件夹——study指代每一个study文件夹\n",
    "    if not radiologist.startswith('.'): #忽略.DS文件        \n",
    "        npy_file_path = os.path.join(INPUT_PATH, radiologist)\n",
    "        npy_files = os.listdir(npy_file_path)\n",
    "        npy_list = []\n",
    "        for i in npy_files:\n",
    "            npy_path = os.path.join(npy_file_path, i)\n",
    "            single_npy = np.load(npy_path)\n",
    "            #print(single_npy.shape)\n",
    "            npy_list.append(single_npy)\n",
    "\n",
    "#         print(npy_file_path, np.array(npy_list).shape)\n",
    "        random.shuffle(npy_list)\n",
    "        npy_chunks = chunks(npy_list, 5)\n",
    "\n",
    "        print(\"THIS IS the data from {}. AND NOW the 5-folder cross-valiation start:\".format(npy_file_path))\n",
    "        \n",
    "        acc_total = 0\n",
    "        for i in range(5):\n",
    "            #training\n",
    "            print(\"-------------THE {} part(as the test set) cross-valiation start----------------------\".format(i + 1))\n",
    "            single_training_set, single_test_set = five_folder(npy_chunks, i)\n",
    "#             print(i, np.array(single_training_set).shape, np.array(single_test_set).shape)\n",
    "            train_loader = torch.utils.data.DataLoader(MyDataset(single_training_set), batch_size=BATCH_SIZE, shuffle=True)\n",
    "            \n",
    "            #print(len(train_loader), type(train_loader))\n",
    "            single_test_acc = train(i, train_loader,single_test_set)\n",
    "            acc_total += single_test_acc\n",
    "        \n",
    "        print(\"THIS IS the data from {}. {}/{} is discriminated for right. The TOTAL ACCURACY is {}%.\".format(npy_file_path, acc_total, len(npy_list), 100 * acc_total / len(npy_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CapsuleGAN",
   "language": "python",
   "name": "capsulegan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
